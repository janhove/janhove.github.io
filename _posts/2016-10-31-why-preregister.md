---
layout: post
title: "The Centre for Open Science's Preregistration Challenge: Why it's relevant and some recommended background reading"
category: [Design]
tags: [significance, multiple comparisons, organisation, open science]
---

_This blog post is an edited version of a mail I sent round to my colleagues at the various language and linguistics departments in Fribourg. Nothing in this post is new per se, but I haven't seen much discussion of these issues among linguists, applied linguists and bilingualism researchers._

I'd like to point you to an initiative of the [Center for Open Science](https://centerforopenscience.org/): 
the [$1,000,000 Preregistration Challenge](https://cos.io/prereg/).
The basic idea is to foster research transparency by offering a monetary reward to researchers who've outlined their study design and planned analyses 
in advance and report the results of these analyses in the report.

I'm not affiliated with this organisation, but I do think both it and its initiative are important developments. 
For those interested in knowing why I think so, I've written a brief text below that includes links to more detailed articles or examples;
if you prefer reference lists, there's one of those down below. 
Most of articles were written by and for psychologists, 
but I reckon pretty much all of it applies equally to research in linguistics and language learning.

<!--more-->

<hr />

Ever wondered why the literature is [filled](http://dx.doi.org/10.1080/01621459.1959.10501497) to 
the [brim](http://www.tandfonline.com/doi/abs/10.1080/00031305.1995.10476125) with statistically significant results -- 
often mutually contradictory -- 
even though we [know](https://www.researchgate.net/profile/Gerd_Gigerenzer/publication/232481541_Do_Studies_of_Statistical_Power_Have_an_Effect_on_the_Power_of_Studies/links/55c3598c08aeb975673ea348.pdf) 
that you shouldn't be able to find so many of them _even if everyone's theories were correct_?
A big part of the answer is that researchers enjoy a great degree of flexibility in terms of [running their studies](http://dx.doi.org/10.1177/0956797611430953),
[analysing](http://dx.doi.org/10.1177/0956797611417632)
[their](http://www.flexiblemeasures.com/)
[data](http://www.stat.columbia.edu/~gelman/research/unpublished/p_hacking.pdf), 
and [interpreting](http://dx.doi.org/10.1207/s15327957pspr0203_4)
the findings, and -- wittingly or unwittingly -- use this flexibility to [increase](http://dx.doi.org/10.1177/0956797611417632) 
their chances of finding a significant result.

All of this, incidentally, leaves aside false findings due to 
errors in research design and data analysis,
including common ones such as
[reading]({% post_url 2014-10-28-assessing-differences-of-significance %}) too much into the difference between significant and non-significant results,
[taking]({% post_url 2015-08-24-caveats-confounds-correlational-designs %}) the results of an analysis that 'statistically controls for' confounding variables at face value,
or [carving up]({% post_url 2015-10-16-nonlinear-relationships %}) continuous variables into groups,
as well as mistakes in [reporting](http://spp.sagepub.com/content/early/2016/10/06/1948550616673876.abstract), 
and flat-out [fraud](https://en.wikipedia.org/wiki/Diederik_Stapel#Scientific_misconduct).

Scientific thoroughness is a virtue, but such flexibility [messes up](http://www.ejwagenmakers.com/inpress/DeGroot1956_TA.pdf) 
statistical inferences in a big way, 
and the odds that a statistically significant finding represents a [fluke](http://journals.plos.org/plosmedicine/article?id=10.1371/journal.pmed.0020124) [sky-rockets](http://dx.doi.org/10.1177/0956797611417632). 
As this flexibility almost always remains undisclosed, 
perusers of the scholarly literature have no way of calibrating their expectations of what _p < 0.05_ means in terms of providing support for a theory, 
and budding researchers may find themselves scratching their heads wondering why everyone seems to 'achieve' significance 
but they [can't](https://dx.doi.org/10.1126%2Fscience.aac4716).

[Preregistering](https://www.theguardian.com/science/head-quarters/2014/may/20/psychology-registration-revolution) your study -- i.e., writing down, to the extent possible, 
what you'll do in your study and how you'll analyse the data -- and then following through on these decisions won't solve all of these problems.
But it'll help researchers and readers to distinguish more clearly between planned and post-hoc decisions, 
which will in turn allow them to calibrate their interpretation of the results more accurately.

### References

Brown, Nicholas J. L. & James A. J. Heathers. 2016. [The GRIM test: A simple technique detects numerous anomalies in the reporting of results in psychology.](http://spp.sagepub.com/content/early/2016/10/06/1948550616673876.abstract) _Social Psychological & Personality Science_.

De Groot, Adriaan D. 2014. [The meaning of "significance"" for different types of research.](http://dx.doi.org/10.1016/j.actpsy.2014.02.001) Translated and annotated by Eric-Jan Wagenmakers, Denny Borsboom, Josine Verhagen, Rogier Kievit, Marjan Bakker, Angelique Cramer, Dora Matzke, Don Mellenbergh, and Han L. J. van der Maas. _Acta Psychologica_ 148. 188-194.

Elson, Malte. [FlexibleMeasures.com](http://www.flexiblemeasures.com/).

Gelman, Andrew & Eric Loken. 2013. [The garden of forking paths: Why multiple comparisons can be a problem, even when there is no "fishing expedition" or "$p$-hacking" and the research hypothesis was posited ahead of time.](http://www.stat.columbia.edu/~gelman/research/unpublished/p_hacking.pdf).

Ioannidis, John P. A. 2005. [Why most published research findings are false.](http://journals.plos.org/plosmedicine/article?id=10.1371/journal.pmed.0020124) _PLOS Medicine_ 2. e124.

John, Leslie K., George Loewenstein & Drazen Prelec. 2012. [Measuring the prevalence of questionable research practices with incentives for truth telling.](http://dx.doi.org/10.1177/0956797611430953) _Psychological Science_ 23. 524-532.

Kerr, Norbert L. 1998. [HARKing: Hypothesizing After the Results are Known.](http://dx.doi.org/10.1207/s15327957pspr0203_4) _Personality and Social Psychology Review_ 2. 196-217.

Open Science Collaboration. 2015. [Estimating the reproducibility of psychological science.](https://dx.doi.org/10.1126%2Fscience.aac4716) _Science_ 349.

Sedlmeier, Peter & Gerd Gigerenzer. 1989. [Do studies of statistical power have an effect on the power of studies?](https://www.researchgate.net/profile/Gerd_Gigerenzer/publication/232481541_Do_Studies_of_Statistical_Power_Have_an_Effect_on_the_Power_of_Studies/links/55c3598c08aeb975673ea348.pdf) _Psychological Bulletin_ 105. 309-316.

Simmons, Joseph P., Leif D. Nelson & Uri Simonsohn. 2011. [False-positive psychology: Undisclosed flexibility in data collection and analysis allows presenting anything as significant.](http://dx.doi.org/10.1177/0956797611417632) _Psychological Science_ 22. 1359-1366.

Sterling, Theodore D. 1959. [Publication decisions and their possible effects on inferences drawn from tests of significance--or vice versa.](http://dx.doi.org/10.1080/01621459.1959.10501497) _Journal of the American Statistical Association_ 54. 30-34. 

Sterling, Theodore D., W. L. Rosenbaum & J. J. Weinkam. 1995. [Publication decisions revisited: The effect of the outcome of statistical tests on the decision to publish and vice versa.](http://www.jstor.org/stable/2684823) _The American Statistician_ 49. 108-112.
