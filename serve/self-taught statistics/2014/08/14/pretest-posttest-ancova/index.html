
<!DOCTYPE html>
<html lang="en">
  <head>
    <meta charset="utf-8">
    <title>Analysing pretest/posttest data</title>
    
    <meta name="author" content="Jan Vanhove">
    <meta name="viewport" content="width=device-width,initial-scale=1.0">
    <!-- Le HTML5 shim, for IE6-8 support of HTML elements -->
    <!--[if lt IE 9]>
      <script src="http://html5shim.googlecode.com/svn/trunk/html5.js"></script>
    <![endif]-->

    <!-- Le styles -->
    <link href="/assets/themes/twitter-2.0/css/bootstrap.min.css" rel="stylesheet">
    <style type="text/css" media="screen">
      body {
        padding-top: 60px; /* 60px to make the container go all the way to the bottom of the topbar */
      }
    </style>
    <link href="/assets/themes/twitter-2.0/css/bootstrap-responsive.min.css" rel="stylesheet">
    <link href="/assets/themes/twitter-2.0/css/syntax.css" rel="stylesheet" type="text/css">
    <link href="/assets/themes/twitter-2.0/css/style.css?body=1" rel="stylesheet" type="text/css" media="all">

    <!-- Le fav and touch icons -->
  <!-- Update these with your own images
    <link rel="shortcut icon" href="images/favicon.ico">
    <link rel="apple-touch-icon" href="images/apple-touch-icon.png">
    <link rel="apple-touch-icon" sizes="72x72" href="images/apple-touch-icon-72x72.png">
    <link rel="apple-touch-icon" sizes="114x114" href="images/apple-touch-icon-114x114.png">
  -->
  </head>

  <body>

    <div class="navbar navbar-fixed-top">
      <div class="navbar-inner">
        <div class="container">
          <a class="btn btn-navbar" data-toggle="collapse" data-target=".nav-collapse">
            <span class="icon-bar"></span>
            <span class="icon-bar"></span>
            <span class="icon-bar"></span>
          </a>
          <a class="brand" href="/">Jan Vanhove</a>
          <div class="nav-collapse">
            <ul class="nav">
              <li><a href="/about.html">About</a></li>        
        <li><a href="/publications.html">Publications</a></li>
        <li><a href="/archive.html">Blog archive</a></li>
        <li><a href="/categories.html">Blog categories</a></li>
        <li><a href="/tags.html">Blog tags</a></li>
            </ul>
          </div><!--/.nav-collapse -->
        </div>
      </div>
    </div>

    <div class="container">

      <div class="content">
        
<div class="page-header">
  <h1>Analysing pretest/posttest data</h1>
</div>

<div class="row">
  <div class="span8">
    <p>Assigning participants randomly to the control and experimental programmes and testing them before and after the programme is the gold standard for determining the efficacy of pedagogical interventions. But the analyses reported in research articles are often needlessly complicated and may be suboptimal in terms of statistical power.</p>
<!--more-->
<h3 id='a_randomised_pretestposttest_control_group_study'>A randomised pretest/posttest control group study</h3>

<p>Say you&#8217;ve developed a new method for autonomously learning to read a related foreign language and you want to establish if your method is more efficient than the one currently used. To address this question, you design an experiment along the following lines:</p>

<ul>
<li>You recruite 40 motivated students and randomly assign half of them to the control group (current method) and half to the experimental group (new method).</li>

<li>To take pre-existing differences in foreign-language reading skills into account, you administer a pretest to all participants.</li>

<li>Six weeks into the programme, the participants are tested again.</li>
</ul>

<p>There you have it &#8211; a classic randomised pretest/posttest control group experiment! But how do you go about analysing the data?</p>

<h3 id='four_analytical_options'>Four analytical options</h3>

<p>By and large, analyses of pretest/posttest experiments in the literature fall into four categories: ANOVAs on the posttest scores only, repeated-measures ANOVAs, ANOVAs on the pretest/posttest differences, and ANCOVAs. The first two are underpowered and overcomplicated, respectively, whereas the third is subject to an assumption that is likely to be violated in real data. The points I want to make aren&#8217;t new (see <a href='http://www.jstor.org/stable/20151'>Hendrix et al. 1978</a>; <a href='http://dx.doi.org/10.1037/h0076767'>Huck &amp; McLean, 1975</a>), but it can&#8217;t hurt to reiterate them &#8211; especially since I wasn&#8217;t aware of them myself until a couple of days ago.</p>

<h4 id='anova_on_the_posttest_scores'>ANOVA on the posttest scores</h4>

<p>One (thankfully infrequent) option is to compare the control and experimental groups by running an ANOVA or, equivalently, a t-test on the posttest scores whilst disregarding the pretest scores. This amounts to pretending you&#8217;ve ran a posttest-only experiment and forgoes the benefits afforded by the pretest/posttest design: Since the participants have been randomly assigned to the conditions, your estimate of the new method&#8217;s effect will be correct <em>on average</em> (as they would&#8217;ve been in a posttest-only experiment). But by not taking into account pre-existing individual differences, the uncertainty about this estimate (i.e. its standard error) is larger than it needs to be, resulting in a loss of statistical power, as the simulations below show.</p>

<p>Sometimes, the pretest scores are used in a complementary ANOVA or t-test that is intended to verify whether the two groups were comparable at the start of the programme. A discussion of such &#8216;randomisation checks&#8217; or &#8216;balance tests&#8217; could be the topic of another blog post; suffice it to say for now that such additional analyses are completely superfluous and uninformative in randomised experiments and that acting on them can <a href='http://www.math.upenn.edu/~pemantle/papers/Preprints/perils.pdf'>invalidate</a> the p-values of the main analysis.</p>

<h4 id='repeatedmeasures_anova'>Repeated-measures ANOVA</h4>

<p>A far superior alternative is to take both the pretest and the posttest into account in the main analysis. This is often accomplished by fitting a 2 (control vs experimental group) Ã— 2 (pretest vs posttest) repeated-measures ANOVA. This method is superior to merely using the posttest scores as every participant now serves as their own control, which reduced the error variance and hence the statistical power.</p>

<p>As <a href='http://dx.doi.org/10.1037/h0076767'>Huck &amp; McLean (1975)</a> point out, however, it is also needlessly complicated: the RM-ANOVA table features 3 effects (main effect of condition, main effect of test as well as the interaction between condition and test), only one of which (the interaction) is relevant to the research question. The other two terms provide either irrelevant (main effect of condition) or trivial (main effect of test) information and are bound to lead to faulty interpretations. In short, RM-ANOVAs is likely to cause information overload for both researchers and readers.</p>

<h4 id='anova_on_the_gain_scores'>ANOVA on the gain scores</h4>

<p>An altogether more straightforward and more reader-friendly tack is to compute gain scores by subtracting the pretest scores from the posttest scores and running a one-way ANOVA (or t-test) on them. The p value associated with the effect of condition will be <em>identical</em> to the one associated with the interaction term in the RM-ANOVA. In a nutshell, RM-ANOVAs don&#8217;t offer anything relevant over and beyond an ordinary ANOVA or a simple t-test when analysing simple pretest/posttest data.</p>

<h4 id='were_not_there_yet_pretest_scores_as_a_covariate_ancova'>We&#8217;re not there yet: Pretest scores as a covariate (ANCOVA)</h4>

<p>RM-ANOVAs or, equivalently, one-way ANOVAs on gain scores come with an assumption that I don&#8217;t think is widely appreciated &#8211; viz. that the pretest and posttest scores are linearly related with a slope equal to 1 (see <a href='http://www.jstor.org/stable/20151'>Hendrix et al. 1978</a>; <a href='http://dx.doi.org/10.1037/h0076767'>Huck &amp; McLean, 1975</a>). At least, I wasn&#8217;t aware of this assumption until a while ago! The &#8216;slope = 1&#8217; assumption is clearly violated when the pretest and posttest scores are on different scales, e.g. a 7-point scale pretest and a 100-point scale posttest. Less obviously, the assumption can be violated by mere everyday <strong>measurement error</strong> that results in <a href='http://en.wikipedia.org/wiki/Regression_toward_the_mean'><strong>regression to the mean</strong></a>.</p>

<p>When the construct of, say, foreign-language reading skills is operationalised by means of a necessarily imperfect test, the test result will overestimate some participants&#8217; true skills and underestimate others&#8217; due to extraneous factors such as form on the day, topic of the reading test etc. &#8211; in a word: luck. When the same participants are tested again at posttest, participants who over- or underperformed by a wide margin at pretest aren&#8217;t likely to be as lucky or unlucky at posttest. The result is that the slope of the linear relationship between pretest and posttest scores will tend to be less than 1, even if both tests are scored on the same scale.</p>

<p>With ANCOVA (analysis of covariance), we can bring the pretest scores into the model as a covariate. Unlike when using RM-ANOVAs or gain score ANOVAs, we wouldn&#8217;t have to <em>assume</em> that the slope linking the pretest and the posttest scores was 1: we can estimate the slope from the data. This, in principle, would make for more accurate inferences with regard to the effect of condition, but at the cost of one degree of freedom. So how do the two methods (ANOVA and ANCOVA) compare in terms of statistical power and Type-I error rate?</p>

<h3 id='a_simulation'>A simulation</h3>

<p>To get an idea of the Type-I error rate and statistical power associated with posttest score ANOVAs, gain score ANOVAs and ANCOVAs, I programmed a simulation of the hypothetical study described above (R code below).</p>

<p>The participants pretest ability (the underlying construct) is programmed to be normally distributed with a to-be-specified standard deviation (<code>sdPretestAbility</code>). The average expected improvement due to the control method and the experimental method are specified as <code>ControlEffect</code> and <code>ExperimentEffect</code>, respectively. Additionally, participants are allowed to differ in their learning progress; their learning aptitude, if you will, is normally distributed with a standard deviation set in <code>sdSensitivity</code>. Lastly, the pre- and posttests have independent but identically distributed measurement errors, whose standard deviation is set in <code>sdMeasurement</code>. This means that the tests are equally accurate but that &#8216;being lucky&#8217; on the pretest shouldn&#8217;t be associated with being lucky on the posttest. (If pretest ability is distributed with a standard deviation of 2 and the standard deviation of the measurement errors is 1, the pretest scores account for 80% of the variance in pretest ability (RÂ² = 2Â² / (2Â² + 1Â²) = 80%). For <code>sdMeasurement</code> values of 0, 2 and 4, the RÂ² values are 100%, 50% and 20%, respectively.)</p>
<div class='highlight'><pre><code class='r'><span class='c1'># Parameters</span>
n <span class='o'>=</span> <span class='m'>20</span> <span class='c1'># number of participants in each condition</span>
sdPretestAbility <span class='o'>=</span> <span class='m'>2</span> <span class='c1'># standard deviation of ABILITY at pretest</span>
ControlEffect <span class='o'>=</span> <span class='m'>1</span> <span class='c1'># average improvement in ABILITY for control group</span>
ExperimentEffect <span class='o'>=</span> <span class='m'>1</span> <span class='c1'># average improvement in ABILITY for experimental group</span>
sdSensitivity <span class='o'>=</span> <span class='m'>0.5</span> <span class='c1'># standard deviation of the participants&#39; sensitivity to the treatment</span>
sdMeasurement <span class='o'>=</span> <span class='m'>0</span> <span class='c1'># standard deviation of measurement error at pre- and posttest</span>
</code></pre>
</div>
<p>The function <code>simulatePrePost.fnc()</code> simulates a single experiment and conducts three analyses on it: a one-way ANOVA on the posttest scores, a one-way ANOVA on the gain scores (again, this is equivalent to running a RM-ANOVA) and an ANCOVA on the posttest scores with the pretest scores as a covariate. The p values associated with the effect of condition in the three analyses are then returned. <code>replicatePrePost.fnc()</code> runs <code>simulatePrePost.fnc()</code> a number of times (e.g. 1000 times) and returns the proportion of significant p values for each analysis type as well as some additional bits and pieces (e.g. the average slope linking pretest and posttest scores in the simulations).</p>

<p>The parameters for the simulation were set as specified above with the exception of <code>ExperimentEffect</code> and <code>sdMeasurement</code>, which varied between 1 and 2.6 (as effective to more than twice as effective as the control) and 0 and 4 (no measurement error to only a very rough approximation of reading skills), respectively. For every combination of <code>ExperimentEffect</code> and <code>sdMeasurement</code> I simulated 1000 datasets, which were analysed by means of posttest score ANOVA, gain score ANOVA and ANCOVA. The results of this simulation are available <a href='/datasets/simulation_PrePost.csv'>here</a>.</p>

<h4 id='typei_error_rate'>Type-I error rate</h4>

<p>&#8216;Type-I error rate&#8217; is just stats speak for &#8216;How often do we find a significant effect when there isn&#8217;t any?&#8217; By tradition, we typically accept a nominal Type-I error rate of 5%, meaning that <em>even if</em> the control and experimental treatments are equally effective, we expect to find a significant difference in our sample in about 50 out of 1000 runs.</p>

<p>To investigate the Type-I error rate, I just consider the simulation runs for which I set <code>ExperimentEffect</code> to the same value as <code>ControlEffect</code> (i.e. 1). The following graph plots the observed Type-I error rate by analysis method and measurement error. The solid horizontal line represents the nominal 5% Type-I error rate; the dashed lines give you an idea by how much the error rate can vary due to random sampling: if the true Type-I error rate is 0.05, the points will lie between the dashed lines in 95% of cases.</p>

<p><img alt='center' src='/figs/2014-08-14-pretest-posttest-ancova/unnamed-chunk-2.png' /></p>

<p>All methods perform on par in terms of Type-I error rate &#8211; any differences between them don&#8217;t seem to be systematic and can likely be accounted for by sampling error.</p>

<h4 id='statistical_power'>Statistical power</h4>

<p>&#8216;Statistical power&#8217; refers to your chances of finding a significant effect when the treatments do differ in efficacy. Power increases with increasing effects and more precise measurement &#8211; a truism that is reflected in the graphs below. As is also obvious, posttest-only ANOVAs compare poorly to analyses that take the pretest scores into consideration. For datasets characterised by substantial measurement error, ANCOVAs outperform gain score ANOVAs fairly systematically, but for datasets with negligible measurement error, both methods are roughly equally as good.</p>

<p><img alt='center' src='/figs/2014-08-14-pretest-posttest-ancova/unnamed-chunk-3.png' /></p>

<h3 id='conclusions'>Conclusions</h3>

<p>Here&#8217;s the tl;dr summary:</p>

<blockquote>
<p>Use pretest scores if available.<br />Repeated-measures ANOVA is too fancy-shmancy for a pretest/posttest design.<br />ANCOVA is (a bit) more powerful.</p>
</blockquote>

<p>My intuition is that gain score ANOVAs will outperform ANCOVAs in <em>very small samples</em> when the measurement errors are negligible (due to the loss of one degree of freedom that goes into estimating the slope parameter). That said, one advantage of ANCOVAs that we haven&#8217;t looked at is that they don&#8217;t require that the pre- and posttests be measured on the same scale. Additionally, they can account for non-linear relationships between pretest and posttest scores by adding higher-order terms. But that&#8217;ll be for another time.</p>

<h3 id='simulation_code'>Simulation code</h3>

<p>To run these simulations yourself or extend them, you can use the following <a href='http://r-project.org/'>R</a> code:</p>
<div class='highlight'><pre><code class='r'>simulatePrePost.fnc <span class='o'>&lt;-</span> <span class='kr'>function</span><span class='p'>(</span>n <span class='o'>=</span> <span class='m'>20</span><span class='p'>,</span>
                                sdPretestAbility <span class='o'>=</span> <span class='m'>3</span><span class='p'>,</span>
                                ExperimentEffect <span class='o'>=</span> <span class='m'>2</span><span class='p'>,</span>
                                ControlEffect <span class='o'>=</span> <span class='m'>2</span><span class='p'>,</span>
                                sdSensitivity <span class='o'>=</span> <span class='m'>1</span><span class='p'>,</span>
                                sdMeasurement <span class='o'>=</span> <span class='m'>1</span><span class='p'>)</span> <span class='p'>{</span>
  <span class='c1'># Simulate pretest ability</span>
  PretestAbility <span class='o'>&lt;-</span> rnorm<span class='p'>(</span>n<span class='o'>*</span><span class='m'>2</span><span class='p'>,</span> <span class='m'>10</span><span class='p'>,</span> sdPretestAbility<span class='p'>)</span>
  <span class='c1'># Control and experiment effects</span>
  InterventionEffect <span class='o'>&lt;-</span> c<span class='p'>(</span>rep<span class='p'>(</span>ControlEffect<span class='p'>,</span> n<span class='p'>),</span> <span class='c1'># control group</span>
                          rep<span class='p'>(</span>ExperimentEffect<span class='p'>,</span> n<span class='p'>))</span> <span class='c1'># intervention group </span>
  <span class='c1'># Individual sensitivity to the effects</span>
  InterventionSensitivity <span class='o'>&lt;-</span> rnorm<span class='p'>(</span>n<span class='o'>*</span><span class='m'>2</span><span class='p'>,</span> <span class='m'>1</span><span class='p'>,</span> sd <span class='o'>=</span> sdSensitivity<span class='p'>)</span>
  <span class='c1'># Add group labels</span>
  Group <span class='o'>&lt;-</span> c<span class='p'>(</span>rep<span class='p'>(</span><span class='s'>&quot;Control&quot;</span><span class='p'>,</span> n<span class='p'>),</span>
             rep<span class='p'>(</span><span class='s'>&quot;Intervention&quot;</span><span class='p'>,</span> n<span class='p'>))</span>  
  <span class='c1'># Pretest scores (with measurement error)</span>
  Pretest <span class='o'>&lt;-</span> PretestAbility <span class='o'>+</span> rnorm<span class='p'>(</span>n<span class='o'>*</span><span class='m'>2</span><span class='p'>,</span> <span class='m'>0</span><span class='p'>,</span> sdMeasurement<span class='p'>)</span>
  <span class='c1'># Posttest scores: pretest ability + effect * sensitivity + measurement error</span>
  Posttest <span class='o'>&lt;-</span> PretestAbility <span class='o'>+</span> InterventionEffect <span class='o'>*</span> InterventionSensitivity <span class='o'>+</span> rnorm<span class='p'>(</span>n<span class='o'>*</span><span class='m'>2</span><span class='p'>,</span> <span class='m'>0</span><span class='p'>,</span> sdMeasurement<span class='p'>)</span>
  
  <span class='c1'># p-value ANOVA on posttests</span>
  pANOVAPost <span class='o'>=</span> anova<span class='p'>(</span>lm<span class='p'>(</span>Posttest <span class='o'>~</span> Group<span class='p'>))</span><span class='o'>$</span><span class='s'>&#39;Pr(&gt;F)&#39;</span><span class='p'>[[</span><span class='m'>1</span><span class='p'>]]</span>
  <span class='c1'># p-value ANOVA on gain scores</span>
  pANOVAGain <span class='o'>=</span> anova<span class='p'>(</span>lm<span class='p'>(</span>I<span class='p'>(</span>Posttest<span class='o'>-</span>Pretest<span class='p'>)</span> <span class='o'>~</span> Group<span class='p'>))</span><span class='o'>$</span><span class='s'>&#39;Pr(&gt;F)&#39;</span><span class='p'>[[</span><span class='m'>1</span><span class='p'>]]</span>
  <span class='c1'># p-value ANCOVA</span>
  pANCOVA <span class='o'>=</span>  anova<span class='p'>(</span>lm<span class='p'>(</span>Posttest <span class='o'>~</span> Pretest <span class='o'>+</span> Group<span class='p'>))</span><span class='o'>$</span><span class='s'>&#39;Pr(&gt;F)&#39;</span><span class='p'>[[</span><span class='m'>2</span><span class='p'>]]</span>
  <span class='c1'># slope between pretest and posttest</span>
  slope <span class='o'>=</span> coef<span class='p'>(</span>lm<span class='p'>(</span>Posttest <span class='o'>~</span> Pretest <span class='o'>+</span> Group<span class='p'>))[</span><span class='s'>&#39;Pretest&#39;</span><span class='p'>]</span>
  
  <span class='c1'># spit it all out</span>
  <span class='kr'>return</span><span class='p'>(</span>list<span class='p'>(</span>pANOVAPost <span class='o'>=</span> pANOVAPost<span class='p'>,</span>
              pANOVAGain <span class='o'>=</span> pANOVAGain<span class='p'>,</span>
              pANCOVA <span class='o'>=</span> pANCOVA<span class='p'>,</span>
              slope<span class='p'>))</span>
<span class='p'>}</span>

replicatePrePost.fnc <span class='o'>&lt;-</span> <span class='kr'>function</span><span class='p'>(</span>runs <span class='o'>=</span> <span class='m'>1000</span><span class='p'>,</span>
                                 n <span class='o'>=</span> <span class='m'>200</span><span class='p'>,</span>
                                 sdPretestAbility <span class='o'>=</span> <span class='m'>3</span><span class='p'>,</span>
                                 ExperimentEffect <span class='o'>=</span> <span class='m'>3</span><span class='p'>,</span>
                                 ControlEffect <span class='o'>=</span> <span class='m'>2</span><span class='p'>,</span>
                                 sdSensitivity <span class='o'>=</span> <span class='m'>1</span><span class='p'>,</span>
                                 sdMeasurement <span class='o'>=</span> <span class='m'>1</span><span class='p'>)</span> <span class='p'>{</span>
  
  <span class='c1'># run simulatePrePost.fnc() n times</span>
  sims <span class='o'>&lt;-</span> replicate<span class='p'>(</span>runs<span class='p'>,</span> simulatePrePost.fnc<span class='p'>(</span>n<span class='p'>,</span>
                                              sdPretestAbility<span class='p'>,</span>
                                              ExperimentEffect<span class='p'>,</span>
                                              ControlEffect<span class='p'>,</span>
                                              sdSensitivity<span class='p'>,</span>
                                              sdMeasurement<span class='p'>))</span>
  <span class='c1'># Compute proportion of significant results and average slope</span>
  sigANOVAPost <span class='o'>=</span> mean<span class='p'>(</span>unlist<span class='p'>(</span>sims<span class='p'>[</span><span class='m'>1</span><span class='p'>,])</span><span class='o'>&lt;=</span><span class='m'>0.05</span><span class='p'>)</span>
  sigANOVAGain <span class='o'>=</span> mean<span class='p'>(</span>unlist<span class='p'>(</span>sims<span class='p'>[</span><span class='m'>2</span><span class='p'>,])</span><span class='o'>&lt;=</span><span class='m'>0.05</span><span class='p'>)</span>
  sigANCOVA <span class='o'>=</span> mean<span class='p'>(</span>unlist<span class='p'>(</span>sims<span class='p'>[</span><span class='m'>3</span><span class='p'>,])</span><span class='o'>&lt;=</span><span class='m'>0.05</span><span class='p'>)</span>
  meanSlope <span class='o'>=</span> mean<span class='p'>(</span>unlist<span class='p'>(</span>sims<span class='p'>[</span><span class='m'>4</span><span class='p'>,]))</span>
  
  <span class='c1'># Spit it all out</span>
  <span class='kr'>return</span><span class='p'>(</span>list<span class='p'>(</span>sigANOVAPost <span class='o'>=</span> sigANOVAPost<span class='p'>,</span>
              sigANOVAGain <span class='o'>=</span> sigANOVAGain<span class='p'>,</span>
              sigANCOVA <span class='o'>=</span> sigANCOVA<span class='p'>,</span>
              sdMeasurement <span class='o'>=</span> sdMeasurement<span class='p'>,</span>
              Effect <span class='o'>=</span> ExperimentEffect <span class='o'>-</span> ControlEffect<span class='p'>,</span>
              meanSlope <span class='o'>=</span> meanSlope<span class='p'>))</span>
<span class='p'>}</span>

<span class='c1'># This tabulates all relevant combinations of sdMeasurement and ExperimentEffect</span>
grid <span class='o'>&lt;-</span> expand.grid<span class='p'>(</span>sdMeasurement <span class='o'>=</span> seq<span class='p'>(</span><span class='m'>0</span><span class='p'>,</span> <span class='m'>4</span><span class='p'>,</span> <span class='m'>0.5</span><span class='p'>),</span>
                    ExperimentEffect <span class='o'>=</span> seq<span class='p'>(</span><span class='m'>1</span><span class='p'>,</span> <span class='m'>2.6</span><span class='p'>,</span> <span class='m'>0.2</span><span class='p'>))</span>

<span class='c1'># Load parallel package to speed up computations</span>
library<span class='p'>(</span>parallel<span class='p'>)</span>
<span class='c1'># Run replicatePrePost.fnc for every combination of sdMeasurement and ExperimentEffect contained in &#39;grid&#39;</span>
<span class='c1'># I&#39;m not sure whether this works on Mac or Windows; perhaps use mapply instead of mcmapply.</span>
simulatedResults <span class='o'>&lt;-</span> mcmapply<span class='p'>(</span>replicatePrePost.fnc<span class='p'>,</span>
       sdMeasurement <span class='o'>=</span> grid<span class='o'>$</span>sdMeasurement<span class='p'>,</span>
       ExperimentEffect <span class='o'>=</span> grid<span class='o'>$</span>ExperimentEffect<span class='p'>,</span>
       <span class='c1'># set fixed parameters</span>
       MoreArgs <span class='o'>=</span> list<span class='p'>(</span>runs <span class='o'>=</span> <span class='m'>1000</span><span class='p'>,</span>
                       ControlEffect <span class='o'>=</span> <span class='m'>1</span><span class='p'>,</span>
                       sdPretestAbility <span class='o'>=</span> <span class='m'>2</span><span class='p'>,</span>
                       sdSensitivity <span class='o'>=</span> <span class='m'>0.5</span><span class='p'>,</span>
                       n <span class='o'>=</span> <span class='m'>20</span><span class='p'>),</span>
       <span class='c1'># distribute work over CPU cores</span>
       mc.cores <span class='o'>=</span> detectCores<span class='p'>())</span>
<span class='c1'># Output results (transposed for clarity)</span>
simulatedResults <span class='o'>&lt;-</span> data.frame<span class='p'>(</span>t<span class='p'>(</span>simulatedResults<span class='p'>))</span>
</code></pre>
</div>
    <hr>
    <div class="pagination">
      <ul>
      
        <li class="prev disabled"><a>&larr; Previous</a></li>
      
        <li><a href="/archive.html">Archive</a></li>
      
        <li class="next disabled"><a>Next &rarr;</a>
      
      </ul>
    </div>
  </div>

  <div class="span4">
    <h4>Published</h4>
    <div class="date"><span>14 August 2014</span></div>

  
    <h4>Tags</h4>
    <ul class="tag_box">
    
    


  
     
    	<li><a href="/tags.html#statistics-ref">statistics <span>1</span></a></li>
     
    	<li><a href="/tags.html#pretest/posttest design-ref">pretest/posttest design <span>1</span></a></li>
     
    	<li><a href="/tags.html#experiments-ref">experiments <span>1</span></a></li>
    
  



    </ul>
  
  </div>
</div>

<div class="row">
  <div class="span8">
    <hr>
    


  <div id="disqus_thread"></div>
<script type="text/javascript">
    var disqus_developer = 1;
    var disqus_shortname = 'jekyllbootstrap'; // required: replace example with your forum shortname
    
    /* * * DON'T EDIT BELOW THIS LINE * * */
    (function() {
        var dsq = document.createElement('script'); dsq.type = 'text/javascript'; dsq.async = true;
        dsq.src = 'http://' + disqus_shortname + '.disqus.com/embed.js';
        (document.getElementsByTagName('head')[0] || document.getElementsByTagName('body')[0]).appendChild(dsq);
    })();
</script>
<noscript>Please enable JavaScript to view the <a href="http://disqus.com/?ref_noscript">comments powered by Disqus.</a></noscript>
<a href="http://disqus.com" class="dsq-brlink">blog comments powered by <span class="logo-disqus">Disqus</span></a>




  </div>
</div>


      </div>

      <footer>
        <p>&copy; Jan Vanhove 2014
          with help from <a href="http://jekyllbootstrap.com" target="_blank" title="The Definitive Jekyll Blogging Framework">Jekyll Bootstrap</a>
          and <a href="http://twitter.github.com/bootstrap/" target="_blank">Twitter Bootstrap</a>
        </p>
      </footer>

    </div> <!-- /container -->

    <script src="//ajax.googleapis.com/ajax/libs/jquery/1.7.1/jquery.min.js"></script>
    <script>window.jQuery || document.write('<script src="/assets/themes/twitter-2.0/js/jquery.min.js"><\/script>')</script>
    <script src="/assets/themes/twitter-2.0/js/bootstrap.min.js"></script>
    
  </body>
</html>

