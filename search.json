[
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "I’m a maître d’enseignment et de recherche / Lehr- und Forschungsrat (which I think corresponds to ‘senior lecturer’ elsewhere. I get a pay check at the end of the month is all I know) at the Department of Multilingualism and Foreign Language Education at the University of Fribourg in Switzerland. I also started a Bachelor’s in computer science and mathematics at the University of Bern in 2020.\nI’m also a saxophonist with Beat Moustache, a ska-pop band that mostly plays original songs in Bernese German, and Pädu & Mätthus belgischer Fanclub, a three-man band (Pädu on guitar/voice, Mätthu on drums/voice, and yours truly as eye-candy and for some saxophonistic and Belgian input) that plays mostly covers."
  },
  {
    "objectID": "about.html#short-cv",
    "href": "about.html#short-cv",
    "title": "About",
    "section": "Short CV",
    "text": "Short CV\n\n2020–present: B.Sc. Computer science with a minor in mathematics, University of Bern (CH)\n2017–present: Senior lecturer, Department of Multilingualism and Foreign Language Teaching, University of Fribourg (CH)\n2014–2017: Lecturer, Department of Multilingualism and Foreign Language Teaching, University of Fribourg (CH)\n2014: Ph.D., University of Fribourg (CH)\n2010: Research assistant, Scandinavian Department, University of Groningen (NL)\n2010: M.A. European linguistics, University of Freiburg (D)\n2009: Internship, Scandinavian Department, University of Groningen (NL)\n2007: B.A. English and Swedish linguistics and literature, Ghent University (B)"
  },
  {
    "objectID": "about.html#contact",
    "href": "about.html#contact",
    "title": "About",
    "section": "Contact",
    "text": "Contact\nUniversity of Fribourg\nMultilingualism and Foreign Language Education\nRue de Rome 1\nCH-1700 Fribourg\nSwitzerland\njan.vanhove@unifr.ch"
  },
  {
    "objectID": "statintro.html",
    "href": "statintro.html",
    "title": "Statistische Grundlagen: eine Einführung mit Beispielen aus der Sprachforschung",
    "section": "",
    "text": "Das Skript Statistische Grundlagen ist eine Einführung in die Analyse quantitativer Daten, die sich an erster Stelle an Forscherinnen und Forscher im Bereich der angewandten Linguistik richtet. Das Skript und die verwendeten Datensätze können Sie kostenlos auf GitHub herunterladen."
  },
  {
    "objectID": "archive.html",
    "href": "archive.html",
    "title": "Blog archive",
    "section": "",
    "text": "Blog archive\n\n\n\n\n\n\n\n   \n     \n     \n       Order By\n       Default\n         \n          Date - Oldest\n        \n         \n          Date - Newest\n        \n         \n          Title\n        \n     \n  \n    \n      \n      \n    \n\n\n\n\n\n\nDate\n\n\nTitle\n\n\n\n\n\n\nMar 7, 2023\n\n\nAdjusting to Julia: Piecewise regression\n\n\n\n\nFeb 23, 2023\n\n\nAdjusting to Julia: Tea tasting\n\n\n\n\nFeb 9, 2023\n\n\nAdjusting to Julia: The Levenshtein algorithm\n\n\n\n\nDec 20, 2022\n\n\nAdjusting to Julia: Generating the Fibonacci sequence\n\n\n\n\nFeb 18, 2022\n\n\nIn research, don’t do things you don’t see the point of\n\n\n\n\nFeb 17, 2022\n\n\nAn R function for computing Levenshtein distances between texts using the word as the unit of comparison\n\n\n\n\nJun 29, 2021\n\n\nThe consequences of controlling for a post-treatment variable\n\n\n\n\nDec 16, 2020\n\n\nQuantitative methodology: An introduction\n\n\n\n\nSep 2, 2020\n\n\nCapitalising on covariates in cluster-randomised experiments\n\n\n\n\nJun 29, 2020\n\n\nTutorial: Visualising statistical uncertainty using model-based graphs\n\n\n\n\nJun 12, 2020\n\n\nInterpreting regression models: a reading list\n\n\n\n\n\nMay 23, 2020\n\n\nTutorial: Obtaining directly interpretable regression coefficients by recoding categorical predictors\n\n\n\n\nMay 23, 2020\n\n\nNonparametric tests aren’t a silver bullet when parametric assumptions are violated\n\n\n\n\nFeb 18, 2020\n\n\nBaby steps in Bayes: Incorporating reliability estimates in regression models\n\n\n\n\nJan 21, 2020\n\n\nBaby steps in Bayes: Accounting for measurement error on a control variable\n\n\n\n\nDec 5, 2019\n\n\nFive suggestions for simplifying research reports\n\n\n\n\nNov 28, 2019\n\n\nDrawing scatterplot matrices\n\n\n\n\nNov 28, 2019\n\n\nAdjusting for a covariate in cluster-randomised experiments\n\n\n\n\nSep 11, 2019\n\n\nCollinearity isn’t a disease that needs curing\n\n\n\n\nAug 7, 2019\n\n\nInteractions in logistic regression models\n\n\n\n\nApr 16, 2019\n\n\nWalkthrough: A significance test for a two-group comparison\n\n\n\n\nApr 11, 2019\n\n\nBefore worrying about model assumptions, think about model relevance\n\n\n\n\nJan 14, 2019\n\n\nGuarantees in the long run vs. interpreting the data at hand: Two analyses of clustered data\n\n\n\n\nDec 20, 2018\n\n\nBaby steps in Bayes: Recoding predictors and homing in on specific comparisons\n\n\n\n\nOct 29, 2018\n\n\nBaby steps in Bayes: Piecewise regression with two breakpoints\n\n\n\n\nOct 29, 2018\n\n\nA closer look at a classic study (Bailey et al. 1974)\n\n\n\n\nOct 29, 2018\n\n\nLooking for comments on a paper on model assumptions\n\n\n\n\nSep 26, 2018\n\n\nIntroducing cannonball - Tools for teaching statistics\n\n\n\n\nJul 6, 2018\n\n\nA data entry form with failsafes\n\n\n\n\nJul 4, 2018\n\n\nBaby steps in Bayes: Piecewise regression\n\n\n\n\nJun 27, 2018\n\n\nA brief comment on research questions\n\n\n\n\nApr 25, 2018\n\n\nChecking model assumptions without getting paranoid\n\n\n\n\nFeb 12, 2018\n\n\nConsider generalisability\n\n\n\n\nNov 20, 2017\n\n\nSuggestions for more informative replication studies\n\n\n\n\nNov 20, 2017\n\n\nIncreasing power and precision using covariates\n\n\n\n\nSep 19, 2017\n\n\nConfidence interval-based optional stopping\n\n\n\n\nSep 14, 2017\n\n\nCreating comparable sets of stimuli\n\n\n\n\nJul 14, 2017\n\n\nAbandoning standardised effect sizes and opening up other roads to power\n\n\n\n\nJun 26, 2017\n\n\nFitting interactions between continuous variables\n\n\n\n\nMay 12, 2017\n\n\nTutorial: Adding confidence bands to effect displays\n\n\n\n\nApr 23, 2017\n\n\nTutorial: Plotting regression models\n\n\n\n\nFeb 22, 2017\n\n\nConfidence intervals for standardised mean differences\n\n\n\n\nFeb 15, 2017\n\n\nWhich predictor is most important? Predictive utility vs. construct importance\n\n\n\n\nJan 31, 2017\n\n\nAutomatise repetitive tasks\n\n\n\n\nDec 20, 2016\n\n\nA few examples of bootstrapping\n\n\n\n\nNov 21, 2016\n\n\nWhat data patterns can lie behind a correlation coefficient?\n\n\n\n\nNov 16, 2016\n\n\nCommon-language effect sizes\n\n\n\n\nOct 31, 2016\n\n\nThe Centre for Open Science’s Preregistration Challenge: Why it’s relevant and some recommended background reading\n\n\n\n\nAug 30, 2016\n\n\nTutorial: Drawing a dot plot\n\n\n\n\nAug 18, 2016\n\n\nR tip: Ordering factor levels more easily\n\n\n\n\nJul 5, 2016\n\n\nClassifying second-language learners as native- or non-nativelike: Don’t neglect classification error rates\n\n\n\n\nJun 21, 2016\n\n\nTutorial: Drawing a boxplot\n\n\n\n\nJun 13, 2016\n\n\nTutorial: Drawing a line chart\n\n\n\n\nJun 2, 2016\n\n\nTutorial: Drawing a scatterplot\n\n\n\n\nMay 18, 2016\n\n\nSurviving the ANOVA onslaught\n\n\n\n\nApr 22, 2016\n\n\nWhy reported R² values are often too high\n\n\n\n\nApr 1, 2016\n\n\nOn correcting for multiple comparisons: Five scenarios\n\n\n\n\nFeb 23, 2016\n\n\nSilly significance tests: The main effects no one is interested in\n\n\n\n\nFeb 16, 2016\n\n\nExperiments with intact groups: spurious significance with improperly weighted t-tests\n\n\n\n\nDec 14, 2015\n\n\nSome advantages of sharing your data and code\n\n\n\n\nNov 17, 2015\n\n\nDrawing a scatterplot with a non-linear trend line\n\n\n\n\nNov 2, 2015\n\n\nCauses and consequences of unequal sample sizes\n\n\n\n\nOct 16, 2015\n\n\nThe problem with cutting up continuous variables and what to do when things aren’t linear\n\n\n\n\nSep 17, 2015\n\n\nAnalysing experiments with intact groups: the problem and an easy solution\n\n\n\n\nSep 2, 2015\n\n\nCovariate adjustment in logistic mixed models: Is it worth the effort?\n\n\n\n\nAug 24, 2015\n\n\nControlling for confounding variables in correlational research: Four caveats\n\n\n\n\nJul 17, 2015\n\n\nCovariate adjustment in logistic regression — and some counterintuitive findings\n\n\n\n\nJun 18, 2015\n\n\nSome tips on preparing your data for analysis\n\n\n\n\nJun 8, 2015\n\n\nSilly significance tests: Tests unrelated to the genuine research questions\n\n\n\n\nApr 14, 2015\n\n\nPower simulations for comparing independent correlations\n\n\n\n\nMar 16, 2015\n\n\nMore on why I don’t like standardised effect sizes\n\n\n\n\nMar 3, 2015\n\n\nA more selective approach to reporting statistics\n\n\n\n\nFeb 26, 2015\n\n\nExplaining key concepts using permutation tests\n\n\n\n\nFeb 21, 2015\n\n\nThinking about graphs\n\n\n\n\nFeb 5, 2015\n\n\nWhy I don’t like standardised effect sizes\n\n\n\n\nJan 14, 2015\n\n\nOveraccuracy and false precision\n\n\n\n\nJan 7, 2015\n\n\nSome alternatives to bar plots\n\n\n\n\nNov 18, 2014\n\n\nThe curious neglect of covariates in discussions of statistical power\n\n\n\n\nOct 28, 2014\n\n\nAssessing differences of significance\n\n\n\n\nOct 15, 2014\n\n\nSilly significance tests: Tautological tests\n\n\n\n\nSep 26, 2014\n\n\nSilly significance tests: Balance tests\n\n\n\n\nSep 12, 2014\n\n\nA purely graphical explanation of p-values\n\n\n\n\nAug 20, 2014\n\n\nCalibrating p-values in ‘flexible’ piecewise regression models\n\n\n\n\nAug 14, 2014\n\n\nAnalysing pretest/posttest data\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "publications.html",
    "href": "publications.html",
    "title": "Publications",
    "section": "",
    "text": "2021\n\n\nRaphael Berthele, Jan Vanhove, Carina Steiner, Isabelle Udry and Hansjakob Schneider. 2021. Predicting L2 achievement: Results from a test battery measuring language aptitude, general learning ability, and affective factors. In Raphael Berthele & Isabelle Udry (eds.), Individual differences in early instructed language learning: The role of language aptitude, cognition, and motivation (pp. 91-103). Berlin, Germany: Language Science Press. Data and R code available from the Open Science Framework.\n\n\nIsabelle Udry and Jan Vanhove. 2021. The stability of language aptitude: Insights from a longitudinal study on young learners’ language analytic abilities. In Raphael Berthele & Isabelle Udry (eds.), Individual differences in early instructed language learning: The role of language aptitude, cognition, and motivation (pp. 197-209). Berlin, Germany: Language Science Press. Data and R code available from the Open Science Framework.\n\n\nJan Vanhove. 2021. Language aptitude in primary school. Technical report.\n\n\nJan Vanhove. 2021. Towards simpler and more transparent quantitative research reports. ITL - International Journal of Applied Linguistics 172(1). 3–25. Preprint. Data and R code available from the Open Science Framework.\n\n\nJan Vanhove. 2021. Collinearity isn’t a disease that needs curing. Meta-Psychology 5.\n\n\n2020\n\n\nJan Vanhove. 2020. Quantitative methodology. An introduction.\n\n\nJan Vanhove. 2020. When labeling L2 users as nativelike or not, consider classification errors. Second Language Research 36(4). 709–724. Preprint. Data and R code available from the Open Science Framework.\n\n\nJan Vanhove. 2020. Capitalising on covariates in cluster-randomised experiments. PsyArXiv Preprints. Data and R code available from the Open Science Framework.\n\n\nAmelia Lambelet, Raphael Berthele, Jan Vanhove, Magalie Desgrippes, Carlos Pestana and Fabricio Decandio. 2020. Heritage language and school language: are literacy skills transferable?. Fribourg, Switzerland: Institute of Multilingualism.\n\n\nRaphael Berthele and Jan Vanhove. 2020. What would disprove interdependence? Lessons learned from a study on biliteracy in Portuguese heritage language speakers in Switzerland. International Journal of Bilingual Education and Bilingualism 23(5). 550-566. Data and R code available from figshare.\n\n\n2019\n\n\nJan Vanhove. 2019. [Review of the book De vele gezichten van het Nederlands in Vlaanderen. Een inleiding tot de variatietaalkunde by Gert De Sutter (ed.)]. Taal en Tongval 71. 89-93.\n\nJan Vanhove. 2019. Metalinguistic knowledge about the native language and language transfer in gender assignment. Studies in Second Language Learning and Teaching 9(2). 397-419. Data, elicitation materials, and R code available from the Open Science Framework.\n\n\nJan Vanhove, Audrey Bonvin, Amelia Lambelet and Raphael Berthele. 2019. Predicting perceptions of the lexical richness of short French, German, and Portuguese texts. Journal of Writing Research 10(3). 499-525. Technical report, data (including texts), elicitation materials, and R code available from the Open Science Framework.\n\n\nGregory J. Poarch, Jan Vanhove and Raphael Berthele. 2019. The effect of bidialectalism on executive function. International Journal of Bilingualism 23(2). 612-628. Data and R code available from figshare.\n\n\n2018\n\n\nJan Vanhove. Checking the assumptions of your statistical model without getting paranoid. PsyArXiv Preprints.\n\n\nAudrey Bonvin, Jan Vanhove, Raphael Berthele and Amelia Lambelet. 2018. Die Entwicklung von produktiven lexikalischen Kompetenzen bei Schüler(innen) mit portugiesischem Migrationshintergrund in der Schweiz. Zeitschrift für Interkulturellen Fremdsprachenunterricht 23(1). 135-148. Data and R code available from figshare.\n\n\n2017\n\n\nAmelia Lambelet, Raphael Berthele, Magalie Desgrippes, Carlos Pestana and Jan Vanhove. 2017. Chapter 2: Testing interdependence in Portuguese Heritage speakers in Switzerland: the HELASCOT project. In Raphael Berthele and Amelia Lambelet (eds.), Heritage and school language literacy development in migrant children: Interdependence or independence?, pp. 26-33. Multilingual Matters.\n\n\nCarlos Pestana, Amelia Lambelet and Jan Vanhove. 2017. Chapter 4: Reading comprehension development in Portuguese heritage speakers in Switzerland (HELASCOT project). In Raphael Berthele and Amelia Lambelet (eds.), Heritage and school language literacy development in migrant children: Interdependence or independence?, pp. 58-82. Multilingual Matters.\n\n\nMagalie Desgrippes, Amelia Lambelet and Jan Vanhove. 2017. Chapter 5: The development of argumentative and narrative writing skills in Portuguese heritage speakers in Switzerland (HELASCOT project). In Raphael Berthele and Amelia Lambelet (eds.), Heritage and school language literacy development in migrant children: Interdependence or independence?, pp. 83-96. Multilingual Matters.\n\n\nJan Vanhove and Raphael Berthele. 2017. Chapter 6: Testing the interdependence of languages (HELASCOT project). In Raphael Berthele and Amelia Lambelet (eds.), Heritage and school language literacy development in migrant children: Interdependence or independence?, pp. 97-118. Multilingual Matters.\n\n\nJan Vanhove. 2017. The influence of Standard and substandard Dutch on gender assignment in second language German. Language Learning. 67(2). 431–460. Preprint. Preregistration, elicitation materials, data and R code available from the Open Science Framework.\n\n\nJan Vanhove and Raphael Berthele. 2017. Interactions between formal distance and participant-related variables in receptive multilingualism. International Review of Applied Linguistics in Language Teaching. 55(1). 23–40. Data and R code available from figshare.\n\n\n2016\n\n\nDavid Birdsong and Jan Vanhove. 2016. Age of second-language acquisition: Critical periods and social concerns. In E. Nicoladis and S. Montanari (eds), Bilingualism across the lifespan: Factors moderating language proficiency, 163–182. Berlin/Washington, DC: De Gruyter Mouton/American Psychological Association. Postprint.\n\n\nJan Vanhove. 2016. The early learning of interlingual correspondence rules in receptive multilingualism. International Journal of Bilingualism 20(5). 580–593. Postprint. Data, R code, scoring protocol and stimulus lists available from figshare.\n\n\n2015\n\n\nJan Vanhove and Raphael Berthele. 2015. Item-related determinants of cognate guessing in multilinguals. In G. De Angelis, U. Jessner and M. Kresić (eds), Crosslinguistic influence and crosslinguistic interaction in multilingual language learning, 95–118. London: Bloomsbury. Postprint. Data and R code available from figshare.\n\n\nJan Vanhove. 2015. Statistische Grundlagen: eine Einführung mit Beispielen aus der Sprachforschung.\n\n\nJan Vanhove. 2015. Analyzing randomized controlled interventions: Three notes for applied linguists. Studies in Second Language Learning and Teaching 5(1). 135–152. Correction: See the correction note for this paper.\n\n\nJan Vanhove and Raphael Berthele. 2015. The lifespan development of cognate guessing skills in an unknown related language. International Review of Applied Linguistics in Language Teaching 53(1). 1–38. Data and R code available from figshare.\n\n\n2014\n\n\nRaphael Berthele and Jan Vanhove. 2014. Entre jeunes barbes et vieux de la vieille. Usages du répertoire plurilingue dans une tâche d’intercompréhension à travers les âges. Bulletin suisse de linguistique appliquée 99. 31–50.\n\n\nJan Vanhove. 2014. Receptive multilingualism across the lifespan: Cognitive and linguistic factors in cognate guessing. Ph.D. thesis. University of Fribourg. Data and R code available from figshare.\n\n\n2013\n\n\nJan Vanhove and Raphael Berthele. 2013. Factoren bij het herkennen van cognaten in onbekende talen: algemeen of taalspecifiek? Taal en Tongval 65. 171–210.. Data and R code available from figshare.\n\n\nJan Vanhove. 2013. The critical period hypothesis in second language acquisition: A statistical critique and a reanalysis. PLOS ONE 8(7). e69172. The data used in this paper were culled from scatterplots in a paper by Robert DeKeyser and colleagues. The reanalysed data and the R script can be downloaded from the article’s webpage."
  },
  {
    "objectID": "posts/2018-09-12-preprint-assumptions/index.html",
    "href": "posts/2018-09-12-preprint-assumptions/index.html",
    "title": "Looking for comments on a paper on model assumptions",
    "section": "",
    "text": "I’ve written a paper titled Checking the assumptions of your statistical model without getting paranoid and I’d like to solicit your feedback. The paper is geared towards beginning analysts, so I’m particularly interested in hearing from readers who don’t consider themselves expert statisticians if there is anything that isn’t entirely clear to them. If you’re a more experienced analyst and you spot an error in the paper or accompanying tutorial, I’d be grateful if you could let me know, too, of course.\n\nPreprint on PsyArxiv\nOnline tutorial with R code"
  },
  {
    "objectID": "posts/2016-06-21-drawing-a-boxplot/index.html",
    "href": "posts/2016-06-21-drawing-a-boxplot/index.html",
    "title": "Tutorial: Drawing a boxplot",
    "section": "",
    "text": "In the two previous blog posts, you’ve learnt to draw simple but informative scatterplots and line charts. This time, you’ll learn how to draw boxplots."
  },
  {
    "objectID": "posts/2016-06-21-drawing-a-boxplot/index.html#whats-a-boxplot",
    "href": "posts/2016-06-21-drawing-a-boxplot/index.html#whats-a-boxplot",
    "title": "Tutorial: Drawing a boxplot",
    "section": "What’s a boxplot?",
    "text": "What’s a boxplot?\nA boxplot is a graphical display of a sample’s five-number summary: the minimum, the maximum, the median (i.e., the middle value if you sort the data from high to low), and the 25th and 75th percentiles. The 25th percentile is the value below which 25% of the data lie; the 75th percentile is the value below which 75% of the data lie. The range between the 25th and 75th percentiles contains 50% of the sample and is known as the inter-quartile range.\nA boxplot might look like the one below–the median is highlighted by a thick line, the 25th and 75th are displayed by a box, and the minimum and maximum are plotted as ‘whiskers’:\n\n\n\n\n\nOften, though, you’ll also see some points that lie beyond the whiskers. These are values that lie too far from the bulk of data, and they’re commonly referred to as outliers. Here’s an example:\n\n\n\n\n\nIt’s important to note, however, that these outliers may be perfectly valid observations, so you shouldn’t remove them from the data set just because they show up in a boxplot."
  },
  {
    "objectID": "posts/2016-06-21-drawing-a-boxplot/index.html#when-to-use-boxplots-and-when-there-are-better-choices",
    "href": "posts/2016-06-21-drawing-a-boxplot/index.html#when-to-use-boxplots-and-when-there-are-better-choices",
    "title": "Tutorial: Drawing a boxplot",
    "section": "When to use boxplots, and when there are better choices",
    "text": "When to use boxplots, and when there are better choices\nBoxplots are often a good choice when you want to compare two or more groups. In particular, I’ll take a boxplot over the ubiquitous ‘mean-only’ barplot any day. The plot belows illustrates the problem with barplots: the ‘mean-only’ barplots on the left make a crisp impression, but we have no way of knowing how large the 0.6-point difference between the means is in practical terms. (Error bars only slightly alleviate this for me.) Additionally, we don’t know how the data are really distributed—knowing this is usually important when interpreting the results. The boxplots on the right, by contrast, show that—relative to the spread of the data—a 0.6-point difference is tiny. Moreover, they show that both samples have a skewed distribution: the lower 50% of the data is restricted to a small range, whereas the upper 50% is spread out much more.\n\n\n\n\n\nThat said, there are situations where boxplots aren’t optimal. One such situation is when the median is actually fairly atypical of the data. From the left panel below, we would correctly gather that the sample median is slightly higher than 0.4 and that 50% of the data lies between 0.1 and 0.9. A moment’s thought shows that this means that a quarter of the data lies squished between 0 and 0.1 and another quarter between 0.9 and 1, but this may not be something you consciously think of when leafing through a paper—what you focus on is the big white box and the thick line. In this case, the histogram on the right does a much better job of highlighting the interesting patterns in the data—viz., that the distribution is strongly bimodal.\n\n\n\n\n\nMy point is this: Boxplots are much better than your run-of-the-mill barplots showing the group means, but be willing to look for alternatives when you need to—or ought to—highlight particular aspects of the data."
  },
  {
    "objectID": "posts/2016-06-21-drawing-a-boxplot/index.html#tutorial-drawing-boxplots-in-ggplot2",
    "href": "posts/2016-06-21-drawing-a-boxplot/index.html#tutorial-drawing-boxplots-in-ggplot2",
    "title": "Tutorial: Drawing a boxplot",
    "section": "Tutorial: Drawing boxplots in ggplot2",
    "text": "Tutorial: Drawing boxplots in ggplot2\n\nWhat you’ll need\n\nThe free program R, the graphical user interface RStudio, and the add-on package ggplot2. See my previous post when you need help with this.\n\nA dataset. For this tutorial we’re going to work with another dataset on receptive multilingualism that you can download to your hard disk. The dataset contains three variables; what interests us is whether the proportion of responses to Dutch words with the digraph ij (PropCorrect) differs according the LearningCondition to which the participants were assigned.\n\n\n\nPreliminaries\nIn RStudio, read in the data.\n\ndat &lt;- read.csv(\"https://homeweb.unifr.ch/VanhoveJ/Pub/Data/VowelChoices_ij.csv\",\n                stringsAsFactors = TRUE)\n\nIf the summary looks like this, you’re good to go.\n\nsummary(dat)\n\n    Subject           LearningCondition  PropCorrect    \n S10    : 1   &lt;ij&gt; participants:43      Min.   :0.0000  \n S100   : 1   &lt;oe&gt; participants:37      1st Qu.:0.2262  \n S11    : 1                             Median :0.3333  \n S12    : 1                             Mean   :0.3685  \n S14    : 1                             3rd Qu.:0.4762  \n S15    : 1                             Max.   :0.9048  \n (Other):74                                             \n\n\nNow load the ggplot2 package we’ll be using.\nUpdate (2023-08-08): You could also load the entire tidyverse suite instead.\n\nlibrary(ggplot2)\n\n\n\nA first attempt\nThe first three lines of code specify the data frame that the data are to be read from and the variables that go on the x- and y-axis. By setting LearningCondition as the x-variable, we make it clear that we want to compare the accuracy data between these groups of participants. The fourth line specifies that these data should be rendered as boxplots.\n\nggplot(data = dat,\n       aes(x = LearningCondition,\n           y = PropCorrect)) +\n  geom_boxplot()\n\n\n\n\nThis first attempt already produces a very respectable result. We would eventually like to label the axes more appropriately and get rid of the grey background, but all in all, this is pretty decent.\n\n\nAdding the individual data points\nOne more substantial thing we can add to the graph is the individual data points that the boxplots are based on. You don’t have to do this, but particularly when the number of observations is fairly small and the data aren’t too coarse, it may be interesting to see how the data are distributed within the boxes and whiskers.\nTo add the individual data points to the boxplots, simply add a geom_point() layer to the previous code. I’ve specified that the points should be grey circles, but you can simply use geom_point() instead of the fifth line.\n\nggplot(data = dat,\n       aes(x = LearningCondition,\n           y = PropCorrect)) +\n  geom_boxplot() +\n  geom_point(pch = 21, fill = \"grey\")\n\n\n\n\nNow, some participants had the same number of correct responses, but from the graph you can’t tell which: the points are just plotted on top of each other. To remedy this, we can ‘jitter’ the position of the data points using the position_jitter command. Note that I set the height parameter to 0 as I don’t want to render proportions of, say, 0.24 as 0.28; I just want to spread apart the data points horizontally:\n\nggplot(data = dat,\n       aes(x = LearningCondition,\n           y = PropCorrect)) +\n  geom_boxplot() +\n  geom_point(pch = 21, fill = \"grey\",\n             position = position_jitter(width = 0.25, height = 0))\n\n\n\n\n\n\nThe final product\nYou’ll notice that in the plot above, it appears as though three ‘’ participants have score of around 62%, whereas in actual fact only two do: we plotted a symbol to represent the outliers when drawing the boxplots and we drew the data points individually. When you’re plotting the individual data points anyway, there’s no need to also draw the outliers for the boxplots; you can turn this off by specifying outlier.shape = NA in the geom_boxplot() call.\nSecond, the y-axis should be properly labelled, whereas the label for the x-axis seems to be superfluous (the information is already contained in the tick labels). Change this using the ylab() and xlab() calls.\nThird, personally I prefer white backgrounds. Simply adding theme_bw() to the call overrides the grey default. theme_bw(8) means that the font size will be slightly smaller, which is okay.\nLastly, we can flip the x- and y-axes using coord_flip(). The main advantage here is that doing so saves some vertical space on the page, which means there’s more room for other graphs!\n\nggplot(data = dat,\n       aes(x = LearningCondition,\n           y = PropCorrect)) +\n  geom_boxplot(outlier.shape = NA) +\n  geom_point(pch = 21, fill = \"grey\",\n             position = position_jitter(width = 0.25, height = 0)) +\n  xlab(\"\") +\n  ylab(\"Proportion of correct vowel choices\") +\n  theme_bw(8) +\n  coord_flip()"
  },
  {
    "objectID": "posts/2016-06-21-drawing-a-boxplot/index.html#software-versions",
    "href": "posts/2016-06-21-drawing-a-boxplot/index.html#software-versions",
    "title": "Tutorial: Drawing a boxplot",
    "section": "Software versions",
    "text": "Software versions\n\ndevtools::session_info()\n\n─ Session info ───────────────────────────────────────────────────────────────\n setting  value\n version  R version 4.3.1 (2023-06-16 ucrt)\n os       Windows 10 x64 (build 18363)\n system   x86_64, mingw32\n ui       RTerm\n language (EN)\n collate  English_United Kingdom.utf8\n ctype    English_United Kingdom.utf8\n tz       Europe/Zurich\n date     2023-08-09\n pandoc   3.1.1 @ C:/Program Files/RStudio/resources/app/bin/quarto/bin/tools/ (via rmarkdown)\n\n─ Packages ───────────────────────────────────────────────────────────────────\n package     * version date (UTC) lib source\n cachem        1.0.8   2023-05-01 [1] CRAN (R 4.3.1)\n callr         3.7.3   2022-11-02 [1] CRAN (R 4.3.1)\n cli           3.6.1   2023-03-23 [1] CRAN (R 4.3.1)\n colorspace    2.1-0   2023-01-23 [1] CRAN (R 4.3.1)\n crayon        1.5.2   2022-09-29 [1] CRAN (R 4.3.1)\n devtools      2.4.5   2022-10-11 [1] CRAN (R 4.3.1)\n digest        0.6.33  2023-07-07 [1] CRAN (R 4.3.1)\n dplyr         1.1.2   2023-04-20 [1] CRAN (R 4.3.1)\n ellipsis      0.3.2   2021-04-29 [1] CRAN (R 4.3.1)\n evaluate      0.21    2023-05-05 [1] CRAN (R 4.3.1)\n fansi         1.0.4   2023-01-22 [1] CRAN (R 4.3.1)\n farver        2.1.1   2022-07-06 [1] CRAN (R 4.3.1)\n fastmap       1.1.1   2023-02-24 [1] CRAN (R 4.3.1)\n fs            1.6.3   2023-07-20 [1] CRAN (R 4.3.1)\n generics      0.1.3   2022-07-05 [1] CRAN (R 4.3.1)\n ggplot2     * 3.4.2   2023-04-03 [1] CRAN (R 4.3.1)\n glue          1.6.2   2022-02-24 [1] CRAN (R 4.3.1)\n gtable        0.3.3   2023-03-21 [1] CRAN (R 4.3.1)\n htmltools     0.5.5   2023-03-23 [1] CRAN (R 4.3.1)\n htmlwidgets   1.6.2   2023-03-17 [1] CRAN (R 4.3.1)\n httpuv        1.6.11  2023-05-11 [1] CRAN (R 4.3.1)\n jsonlite      1.8.7   2023-06-29 [1] CRAN (R 4.3.1)\n knitr         1.43    2023-05-25 [1] CRAN (R 4.3.1)\n labeling      0.4.2   2020-10-20 [1] CRAN (R 4.3.0)\n later         1.3.1   2023-05-02 [1] CRAN (R 4.3.1)\n lifecycle     1.0.3   2022-10-07 [1] CRAN (R 4.3.1)\n magrittr      2.0.3   2022-03-30 [1] CRAN (R 4.3.1)\n memoise       2.0.1   2021-11-26 [1] CRAN (R 4.3.1)\n mime          0.12    2021-09-28 [1] CRAN (R 4.3.0)\n miniUI        0.1.1.1 2018-05-18 [1] CRAN (R 4.3.1)\n munsell       0.5.0   2018-06-12 [1] CRAN (R 4.3.1)\n pillar        1.9.0   2023-03-22 [1] CRAN (R 4.3.1)\n pkgbuild      1.4.2   2023-06-26 [1] CRAN (R 4.3.1)\n pkgconfig     2.0.3   2019-09-22 [1] CRAN (R 4.3.1)\n pkgload       1.3.2.1 2023-07-08 [1] CRAN (R 4.3.1)\n prettyunits   1.1.1   2020-01-24 [1] CRAN (R 4.3.1)\n processx      3.8.2   2023-06-30 [1] CRAN (R 4.3.1)\n profvis       0.3.8   2023-05-02 [1] CRAN (R 4.3.1)\n promises      1.2.0.1 2021-02-11 [1] CRAN (R 4.3.1)\n ps            1.7.5   2023-04-18 [1] CRAN (R 4.3.1)\n purrr         1.0.1   2023-01-10 [1] CRAN (R 4.3.1)\n R6            2.5.1   2021-08-19 [1] CRAN (R 4.3.1)\n Rcpp          1.0.11  2023-07-06 [1] CRAN (R 4.3.1)\n remotes       2.4.2.1 2023-07-18 [1] CRAN (R 4.3.1)\n rlang         1.1.1   2023-04-28 [1] CRAN (R 4.3.1)\n rmarkdown     2.23    2023-07-01 [1] CRAN (R 4.3.1)\n rstudioapi    0.15.0  2023-07-07 [1] CRAN (R 4.3.1)\n scales        1.2.1   2022-08-20 [1] CRAN (R 4.3.1)\n sessioninfo   1.2.2   2021-12-06 [1] CRAN (R 4.3.1)\n shiny         1.7.4.1 2023-07-06 [1] CRAN (R 4.3.1)\n stringi       1.7.12  2023-01-11 [1] CRAN (R 4.3.0)\n stringr       1.5.0   2022-12-02 [1] CRAN (R 4.3.1)\n tibble        3.2.1   2023-03-20 [1] CRAN (R 4.3.1)\n tidyselect    1.2.0   2022-10-10 [1] CRAN (R 4.3.1)\n urlchecker    1.0.1   2021-11-30 [1] CRAN (R 4.3.1)\n usethis       2.2.2   2023-07-06 [1] CRAN (R 4.3.1)\n utf8          1.2.3   2023-01-31 [1] CRAN (R 4.3.1)\n vctrs         0.6.3   2023-06-14 [1] CRAN (R 4.3.1)\n withr         2.5.0   2022-03-03 [1] CRAN (R 4.3.1)\n xfun          0.39    2023-04-20 [1] CRAN (R 4.3.1)\n xtable        1.8-4   2019-04-21 [1] CRAN (R 4.3.1)\n yaml          2.3.7   2023-01-23 [1] CRAN (R 4.3.0)\n\n [1] C:/Users/VanhoveJ/AppData/Local/R/win-library/4.3\n [2] C:/Program Files/R/R-4.3.1/library\n\n──────────────────────────────────────────────────────────────────────────────"
  },
  {
    "objectID": "posts/2016-08-30-drawing-a-dotplot/index.html",
    "href": "posts/2016-08-30-drawing-a-dotplot/index.html",
    "title": "Tutorial: Drawing a dot plot",
    "section": "",
    "text": "In the fourth tutorial on drawing useful plots with ggplot2, we’re taking a closer look at dot plots – a useful and more flexible alternative to bar and pie charts."
  },
  {
    "objectID": "posts/2016-08-30-drawing-a-dotplot/index.html#whats-a-dot-plot",
    "href": "posts/2016-08-30-drawing-a-dotplot/index.html#whats-a-dot-plot",
    "title": "Tutorial: Drawing a dot plot",
    "section": "What’s a dot plot?",
    "text": "What’s a dot plot?\nThe three panels below show a same data in a pie chart, a bar chart and a dot plot. For data like these, the bar chart and the dot plot allow us to compare the sales of different kinds of pie about equally well. The dot plot has a higher data-ink ratio (Update (2023-08-08): This external link appears to be broken.), but I don’t think that’s too decisive a factor.\n\n\n\n\n\nWhere dot plots excel is when you want to display data with more than two dimensions. In the plots above, the data had two dimensions: the kind of pie and the proportion of sales. In the dot plot below, you find an additional dimension: year (2015 vs. 2016). You couldn’t display this additional dimension in a single pie chart, and you’d need side-by-side bars to do it in a bar chart, which usually looks cluttered."
  },
  {
    "objectID": "posts/2016-08-30-drawing-a-dotplot/index.html#tutorial-drawing-a-dotchart-in-ggplot2",
    "href": "posts/2016-08-30-drawing-a-dotplot/index.html#tutorial-drawing-a-dotchart-in-ggplot2",
    "title": "Tutorial: Drawing a dot plot",
    "section": "Tutorial: Drawing a dotchart in ggplot2",
    "text": "Tutorial: Drawing a dotchart in ggplot2\n\nWhat you’ll need\n\nThe free program R, the graphical user interface RStudio, and the add-on package ggplot2.\nA dataset. The data we’ll use were collected in a project on language transfer (download). About 200 native speakers of Dutch from The Netherlands and Belgium (Country) were asked to pick a German gender-marked definite article (der, die or das) for 44 German nouns (Stimulus). These nouns all had cognates in Dutch (DutchCognate), which had either common or neuter gender (DutchGender). The expectation is that Dutch speakers from either country will tend to assign the neuter German article (das) to German words with neuter Dutch cognates compared to words with common-gender Dutch cognates. The dataset also lists the German words’ actual gender (GermanGender).\n\n\n\nPreliminaries\n\n# Read in data\ngermart &lt;- read.csv(\"http://homeweb.unifr.ch/VanhoveJ/Pub/Data/GermanArticleChoices.csv\", \n                    encoding = \"UTF-8\", stringsAsFactors = TRUE)\n\n\n# Load the ggplot2 package\nlibrary(ggplot2)\n\nI don’t like ggplot2’s default grey background, so let’s change the default theme to black and white:\n\n# Set black and white theme, font size 16\ntheme_set(theme_bw(16))\n\n\n\nA first attempt\nLet’s plot the proportion of neuter article (das) choices by both the Belgian and the Dutch participants for each German noun. Dot plots show the numeric information along the x-axis and the categorical information (labels) along the y-axis, so we specify those mappings in second and third lines. In the fourth line, we specify that the data points need to be plotted as points or dots, and lastly we customise the axis labels.\n\nggplot(germart,                 # name of the dataset\n       aes(x = NeuterResponses, # x-variable\n           y = Stimulus,        # y-variable \n           shape = Country)) +  # different symbols by Country\n  geom_point() +                # plot as dots/points\n  xlab(\"Proportion of neuter (das) choices\") +\n  ylab(\"German noun\")\n\n\n\n\n\n\nFacetting\nThe main comparison is between German words that have neuter Dutch cognates and those that have common-gender Dutch cognates. To highlight this comparison, we can plot the data for both word categories in different panel. Using the facet_grid layer, we can specify that the words with common and with neuter Dutch gender are to be plotted on different rows of a grid (x ~ .). (. ~ x would’ve plotted them in different columns, but having them in different rows but the same column makes for an easier comparison.)\nSetting the scales and space arguments to \"free_y\" ensures that items for which data is available in only one panel aren’t shown in the other panels as well (scales) and that the size of the panels is proportionate to the number of items in them (space). If you set these arguments to \"fixed\", you’ll see what I mean.\n\nggplot(germart,\n       aes(x = NeuterResponses,\n           y = Stimulus,\n           shape = Country)) +\n  geom_point() +\n  xlab(\"Proportion of neuter (das) choices\") +\n  ylab(\"German noun\") +\n  facet_grid(DutchGender ~ .,   # different vertical panels\n             scales = \"free_y\", # flexible y-axis\n             space = \"free_y\")\n\n\n\n\nThis plot strongly suggests that the gender of the German words’ Dutch cognates has a major effect on how often Dutch speakers pick das as their article: with the exception of one word, Boot, the ranges in the two panels don’t even overlap.\nHowever, the German words are ordered alphabetically. While we’re at it, we might as well sort them more meaningfully – for instance, according to the average proportion of das responses per word. Additionally, I don’t find the default filled circle and triangle symbols that represent the Belgian and Dutch responses very distinctive, so we’ll change these, too.\n\n\nSorting the items by their average value\nIn my previous post, I introduced a custom function for sorting the levels of a factor according to the average value of another variable per level. Here we use this function to sort the levels of Stimulus according to their average value of NeuterResponses. We also use another custom function to put the words with neuter cognates in the top instead of in the bottom panel.\n\n# Download sorting function\nsource(\"https://janhove.github.io/RCode/sortLvls.R\")\n\n# Sort Stimulus by NeuterResponses\n# *Update (2023-08-08):* Using reorder() is probably easier.\ngermart$Stimulus &lt;- sortLvlsByVar.fnc(germart$Stimulus, germart$NeuterResponses)\n\n# Put DutchGender == neuter above DutchGender == common\ngermart$DutchGender &lt;- sortLvls.fnc(germart$DutchGender, 2)\n\nTo change the default symbols, we use scale_shape_manual(). For black and white plots, I prefer empty circles and crosses, which are known internally as symbols 1 and 3, respectively:\n\nggplot(germart,\n       aes(x = NeuterResponses,\n           y = Stimulus,\n           shape = Country)) +\n  geom_point() +\n  xlab(\"Proportion of neuter (das) choices\") +\n  ylab(\"German noun\") +\n  scale_shape_manual(values = c(1, 3)) + # custom symbols\n  facet_grid(DutchGender ~ .,\n             scales = \"free_y\",\n             space = \"free_y\")\n\n\n\n\nThe difference between responses to words with neuter cognates and to those with common-gender cognates is now particularly clear. Nevertheless, there is a substantial degree of variation between the items, particularly in to words with neuter cognates. Aficionados of the German language may’ve noticed, however, that the top words within each panel all have neuter gender in German, i.e., the article das is the correct choice for these words. The bottom words, by contrast, all have masculine or feminine gender in German. As this factor – whether the word actually is neuter in German or not – can straightforwardly account for some variation within each panel due to people having learnt the correct gender, it makes sense to include this information in the plot, too.\n\n\nAdding another facetting variable\nFirst we create a new variable that specifies whether the German word actually has neuter gender or not.\n\ngermart$GermanNeuter &lt;- factor(germart$GermanGender == \"neuter\",\n                               levels = c(\"TRUE\", \"FALSE\"))\n\nThen we add this new variable to the facet_grid layer.\n\nggplot(germart,\n       aes(x = NeuterResponses,\n           y = Stimulus,\n           shape = Country)) +\n  geom_point() +\n  xlab(\"Proportion of neuter (das) choices\") +\n  ylab(\"German noun\") +\n  scale_shape_manual(values = c(1, 3)) +\n  facet_grid(DutchGender + GermanNeuter~ ., # another facetting variable\n             scales = \"free_y\",\n             space = \"free_y\")\n\n\n\n\nAdding this additional facetting variable may be useful for making it immediately clear to the casual reader that the study featured a mixture of both congruent (neuter–neuter and non-neuter–common) and incongruent (neuter–common and non-neuter–neuter) cognates\nAdditionally, it shows that while the Dutch participants consistently and correctly choose more neuter responses than the Belgians for neuter–neuter cognates, they don’t pick the correct neuter article more often for neuter–common cognates, nor do they choose the neuter article less often than the Belgians for non-neuter words. To me, this suggests that the actual knowledge of German gender didn’t greatly differ between the Belgian and the Dutch participants.\nLastly, the word standing out in all of this is Boot, for which most participants correctly picked neuter das even though its highly transparent cognate in Dutch, boot, is common-gender.\n\n\n\nFinishing touches: facet labels\nFinally, as a courtesy to the reader, we’ll give the facet labels more transparent titles. For this, we need to map the current default labels to more descriptive labels using as_labeller():\n\nlabels &lt;- as_labeller(\n  c(`common` = \"Du.: common\",\n    `neuter` = \"Du.: neuter\",\n    `TRUE`   = \"Gm.: neuter\",\n    `FALSE`  = \"Gm.: non-neuter\")\n)\n\nThen, we add these labels to the facet_grid() call.\n\nggplot(germart,\n       aes(x = NeuterResponses,\n           y = Stimulus,\n           shape = Country)) +\n  geom_point() +\n  xlab(\"Proportion of neuter (das) choices\") +\n  ylab(\"German noun\") +\n  scale_shape_manual(values = c(1, 3)) +\n  facet_grid(DutchGender + GermanNeuter ~ .,\n             scales = \"free_y\",\n             space = \"free_y\",\n             labeller = labels) # add labels"
  },
  {
    "objectID": "posts/2016-08-30-drawing-a-dotplot/index.html#software-versions",
    "href": "posts/2016-08-30-drawing-a-dotplot/index.html#software-versions",
    "title": "Tutorial: Drawing a dot plot",
    "section": "Software versions",
    "text": "Software versions\n\ndevtools::session_info()\n\n─ Session info ───────────────────────────────────────────────────────────────\n setting  value\n version  R version 4.3.1 (2023-06-16 ucrt)\n os       Windows 10 x64 (build 18363)\n system   x86_64, mingw32\n ui       RTerm\n language (EN)\n collate  English_United Kingdom.utf8\n ctype    English_United Kingdom.utf8\n tz       Europe/Zurich\n date     2023-08-08\n pandoc   3.1.1 @ C:/Program Files/RStudio/resources/app/bin/quarto/bin/tools/ (via rmarkdown)\n\n─ Packages ───────────────────────────────────────────────────────────────────\n package     * version date (UTC) lib source\n cachem        1.0.8   2023-05-01 [1] CRAN (R 4.3.1)\n callr         3.7.3   2022-11-02 [1] CRAN (R 4.3.1)\n cli           3.6.1   2023-03-23 [1] CRAN (R 4.3.1)\n colorspace    2.1-0   2023-01-23 [1] CRAN (R 4.3.1)\n crayon        1.5.2   2022-09-29 [1] CRAN (R 4.3.1)\n devtools      2.4.5   2022-10-11 [1] CRAN (R 4.3.1)\n digest        0.6.33  2023-07-07 [1] CRAN (R 4.3.1)\n dplyr       * 1.1.2   2023-04-20 [1] CRAN (R 4.3.1)\n ellipsis      0.3.2   2021-04-29 [1] CRAN (R 4.3.1)\n evaluate      0.21    2023-05-05 [1] CRAN (R 4.3.1)\n fansi         1.0.4   2023-01-22 [1] CRAN (R 4.3.1)\n farver        2.1.1   2022-07-06 [1] CRAN (R 4.3.1)\n fastmap       1.1.1   2023-02-24 [1] CRAN (R 4.3.1)\n fs            1.6.3   2023-07-20 [1] CRAN (R 4.3.1)\n generics      0.1.3   2022-07-05 [1] CRAN (R 4.3.1)\n ggplot2     * 3.4.2   2023-04-03 [1] CRAN (R 4.3.1)\n glue          1.6.2   2022-02-24 [1] CRAN (R 4.3.1)\n gtable        0.3.3   2023-03-21 [1] CRAN (R 4.3.1)\n htmltools     0.5.5   2023-03-23 [1] CRAN (R 4.3.1)\n htmlwidgets   1.6.2   2023-03-17 [1] CRAN (R 4.3.1)\n httpuv        1.6.11  2023-05-11 [1] CRAN (R 4.3.1)\n jsonlite      1.8.7   2023-06-29 [1] CRAN (R 4.3.1)\n knitr         1.43    2023-05-25 [1] CRAN (R 4.3.1)\n labeling      0.4.2   2020-10-20 [1] CRAN (R 4.3.0)\n later         1.3.1   2023-05-02 [1] CRAN (R 4.3.1)\n lifecycle     1.0.3   2022-10-07 [1] CRAN (R 4.3.1)\n magrittr    * 2.0.3   2022-03-30 [1] CRAN (R 4.3.1)\n memoise       2.0.1   2021-11-26 [1] CRAN (R 4.3.1)\n mime          0.12    2021-09-28 [1] CRAN (R 4.3.0)\n miniUI        0.1.1.1 2018-05-18 [1] CRAN (R 4.3.1)\n munsell       0.5.0   2018-06-12 [1] CRAN (R 4.3.1)\n pillar        1.9.0   2023-03-22 [1] CRAN (R 4.3.1)\n pkgbuild      1.4.2   2023-06-26 [1] CRAN (R 4.3.1)\n pkgconfig     2.0.3   2019-09-22 [1] CRAN (R 4.3.1)\n pkgload       1.3.2.1 2023-07-08 [1] CRAN (R 4.3.1)\n prettyunits   1.1.1   2020-01-24 [1] CRAN (R 4.3.1)\n processx      3.8.2   2023-06-30 [1] CRAN (R 4.3.1)\n profvis       0.3.8   2023-05-02 [1] CRAN (R 4.3.1)\n promises      1.2.0.1 2021-02-11 [1] CRAN (R 4.3.1)\n ps            1.7.5   2023-04-18 [1] CRAN (R 4.3.1)\n purrr         1.0.1   2023-01-10 [1] CRAN (R 4.3.1)\n R6            2.5.1   2021-08-19 [1] CRAN (R 4.3.1)\n Rcpp          1.0.11  2023-07-06 [1] CRAN (R 4.3.1)\n remotes       2.4.2.1 2023-07-18 [1] CRAN (R 4.3.1)\n rlang         1.1.1   2023-04-28 [1] CRAN (R 4.3.1)\n rmarkdown     2.23    2023-07-01 [1] CRAN (R 4.3.1)\n rstudioapi    0.15.0  2023-07-07 [1] CRAN (R 4.3.1)\n scales        1.2.1   2022-08-20 [1] CRAN (R 4.3.1)\n sessioninfo   1.2.2   2021-12-06 [1] CRAN (R 4.3.1)\n shiny         1.7.4.1 2023-07-06 [1] CRAN (R 4.3.1)\n stringi       1.7.12  2023-01-11 [1] CRAN (R 4.3.0)\n stringr       1.5.0   2022-12-02 [1] CRAN (R 4.3.1)\n tibble        3.2.1   2023-03-20 [1] CRAN (R 4.3.1)\n tidyselect    1.2.0   2022-10-10 [1] CRAN (R 4.3.1)\n urlchecker    1.0.1   2021-11-30 [1] CRAN (R 4.3.1)\n usethis       2.2.2   2023-07-06 [1] CRAN (R 4.3.1)\n utf8          1.2.3   2023-01-31 [1] CRAN (R 4.3.1)\n vctrs         0.6.3   2023-06-14 [1] CRAN (R 4.3.1)\n withr         2.5.0   2022-03-03 [1] CRAN (R 4.3.1)\n xfun          0.39    2023-04-20 [1] CRAN (R 4.3.1)\n xtable        1.8-4   2019-04-21 [1] CRAN (R 4.3.1)\n yaml          2.3.7   2023-01-23 [1] CRAN (R 4.3.0)\n\n [1] C:/Users/VanhoveJ/AppData/Local/R/win-library/4.3\n [2] C:/Program Files/R/R-4.3.1/library\n\n──────────────────────────────────────────────────────────────────────────────"
  },
  {
    "objectID": "posts/2019-10-28-cluster-covariates/index.html",
    "href": "posts/2019-10-28-cluster-covariates/index.html",
    "title": "Adjusting for a covariate in cluster-randomised experiments",
    "section": "",
    "text": "Cluster-randomised experiments are experiments in which groups of participants (e.g., classes) are assigned randomly but in their entirety to the experiments’ conditions. Crucially, the fact that entire groups of participants were randomly assigned to conditions - rather than each participant individually - should be taken into account in the analysis, as outlined in a previous blog post. In this blog post, I use simulations to explore the strengths and weaknesses of different ways of analysing cluster-randomised experiments when a covariate (e.g., a pretest score) is available."
  },
  {
    "objectID": "posts/2019-10-28-cluster-covariates/index.html#tldr",
    "href": "posts/2019-10-28-cluster-covariates/index.html#tldr",
    "title": "Adjusting for a covariate in cluster-randomised experiments",
    "section": "tl;dr",
    "text": "tl;dr\nCluster-randomised experiments in applied linguistics typically involve a fairly small number of clusters that are randomly assigned to conditions (e.g., perhaps 10 to 20 classes at best). The sizes of these clusters tend to be fairly similar (e.g., perhaps 15 to 25 pupils per class). The simulations indicate that cluster-randomised experiments with these characteristics are best analysed in a surprisingly simple way: Compute the mean outcome per cluster and run the analysis on the cluster means. If a covariate (e.g., pretest scores) is available, also compute the mean covariate value per cluster and add it to the analysis on the cluster means as a control variable."
  },
  {
    "objectID": "posts/2019-10-28-cluster-covariates/index.html#five-different-analytical-approaches",
    "href": "posts/2019-10-28-cluster-covariates/index.html#five-different-analytical-approaches",
    "title": "Adjusting for a covariate in cluster-randomised experiments",
    "section": "Five different analytical approaches",
    "text": "Five different analytical approaches\nI’ll compare the strengths and weaknesses of five methods for analysing cluster-randomised experiments in which a covariate (e.g., pretest scores) are available. I’ll first create some data whose properties reflect those found in cluster-randomised experiments and then run the five analyses. If you have R, you can follow along with the commands below.\nThe tidyverse, lme4 and lmerTest packages can be installed using install.packages(c(\"tidyverse\", \"lme4\", \"lmerTest\")). For the cannonball package, see the installation instructions.\n\n# Load packages\nlibrary(tidyverse)\nlibrary(cannonball) # for generating clustered data\nlibrary(lme4)       # fitting multilevel models\nlibrary(lmerTest)   # for p-values in multilevel models\n\n# Generate clustered data; set seed for reproducibility\nRNGversion(\"3.5.3\")\nset.seed(2019-10-28, kind = \"Mersenne-Twister\")\nd_example &lt;- clustered_data(\n  n_per_class = sample(15:25, size = 14, replace = TRUE),\n  ICC = 0.15, effect = 0.2, \n  reliability_post = 1, reliability_pre = 0.7^2)\n\nYou can consult the help page for the clustered_data() command (type ?clustered_data at the R prompt) for more information about these settings; here’s the summary:\n\nThe dataset d_example now contains simulated data from 14 classes with between 15 and 25 pupils each (n_per_class).\nBefore factoring in an effect of the intervention, the variance between the classes is 15% of the total variance of the outcome (ICC; intra-class correlation). The clustered_data() function generates outcome data that is normally distributed within classes with variance = 1, so the variance between the classes is 0.18. (\\(\\textrm{ICC} = 0.15 = \\frac{\\textrm{variance between}}{\\textrm{variance between} + \\textrm{variance within (= 1)}} \\leftrightarrow \\textrm{variance between} = \\frac{0.15}{1-0.15} \\approx 0.18\\)) ICCs in the 0.15–0.20 bracket are fairly typical in educational settings (Hedges & Hedberg 2007; Schochet 2008).\nThe simulated intervention effect was 0.2, meaning that 0.2 points were added to the outcome values for the pupils in the intervention classes.\nThe reliability_post and reliability_pre parameters are useful for generating pretest data that are correlated with each other. By setting reliability_post = 1 and reliability_pre = 0.7^2, pretest scores are generated that are correlated at \\(\\rho = 0.7\\) with the outcome.\n\nFigures 1 and 2 show what the simulated data look like.\n\nggplot(data = d_example,\n       aes(x = reorder(class, outcome, median), \n           y = outcome)) +\n  geom_boxplot(outlier.shape = NA) +\n  geom_point(shape = 1, \n             position = position_jitter(width = 0.2)) +\n  xlab(\"class\") +\n  facet_wrap(~ condition, scales = \"free_x\")\n\n\n\n\nFigure 1. Simulated data for a cluster-randomised experiment in which seven classes were assigned to the control condition and seven to the intervention condition.\n\n\n\n\n\nggplot(data = d_example,\n       aes(x = pretest, \n           y = outcome,\n           linetype = condition,\n           colour = condition)) +\n  geom_point(shape = 1) +\n  geom_smooth(method = \"lm\", se = FALSE, formula = 'y~x')\n\n\n\n\nFigure 2. Pretest vs. outcome scores in the simulated cluster-randomised experiment. The pretest could actually be any covariate that is predictive of the outcome.\n\n\n\n\nLet’s now analyse these data using five different approaches.\n\nApproach 1: Analyse the cluster means, ignore the covariate\nIn the first approach, the covariate is ignored entirely. The dependencies in the data (pupils in classes) are taken into account by computing the mean outcome per class; the class means are entirely independent of each other.\n\nd_per_class &lt;- d_example |&gt; \n  group_by(class, condition) |&gt; \n  summarise(mean_class = mean(outcome),\n            .groups = \"drop\")\n\nFigure 3 shows what this looks like.\n\n\n\n\n\nFigure 3. In Approach 1, the outcome is averaged per class, and it is these averages that are analysed in the statistical model.\n\n\n\n\nThen, these class means are compared, for instance by means of a two-sample t-test. This is identical to fitting a linear model with the class means as the outcome and the condition to which the class was assigned as the predictor:\n\nm1 &lt;- lm(mean_class ~ condition, data = d_per_class)\nsummary(m1)\n\n\nCall:\nlm(formula = mean_class ~ condition, data = d_per_class)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-0.8311 -0.3386  0.0264  0.2692  0.8889 \n\nCoefficients:\n                      Estimate Std. Error t value Pr(&gt;|t|)\n(Intercept)            -0.0478     0.1831   -0.26    0.798\nconditionintervention   0.4973     0.2590    1.92    0.079\n\nResidual standard error: 0.485 on 12 degrees of freedom\nMultiple R-squared:  0.235, Adjusted R-squared:  0.171 \nF-statistic: 3.69 on 1 and 12 DF,  p-value: 0.0789\n\n\nThe estimated intervention effect in this example is 0.50 ± 0.26, and the result of the significance test is t(12) = 1.92, p = 0.08. (You find the degrees of freedom for the t-test on the third line from the bottom.)\nSome researchers may object that this approach reduces the original data set of 280 observations to just the 14 class means and hence throws away vital information. Having reached a certain age, I’ll quote myself on this topic:\n\n“[R]esearchers may find it psychologically difficult to reduce a dataset to a fraction of its original size—if the analysis is carried out on ten cluster means, by bother recruiting several participants per cluster? However, larger clusters reduce the variance of the cluster means within each treatment group, which in turn makes the intervention effect stand out more clearly (Barcikowski, 1981). Put differently, cluster-level analyses are more powerful when the clusters are large compared to when they are small. That said, when given the choice between running an experiment on ten clusters with 50 observations each or on 50 clusters with ten observations each, the latter is vastly preferred due to its higher power (…).” (Vanhove 2015:145).\n\nThe assumptions of this analysis are the same as for any general linear model, it’s just that they now concern the class means rather than the raw observations. Independence can be assumed if the experimental protocol was followed. Normality isn’t too important but seems reasonable given that we’re working with aggregated data. But homoskedasticity (i.e., equal variances) may occasionally be a problem: The variance of any given class mean will be inversely proportional to the size of the class, meaning that small classes will tend to have more extreme means. In this case, the classes are all of similar sizes, as is typical in experiments with school classes, so this shouldn’t pose too great a threat to the results. In the simulations below, I’ll also consider experiments with wildly differing class sizes.\n\n\nApproach 2: Fit a multilevel model, ignore the covariate\nIn the second approach, too, the covariate is ignored completely. Instead of analysing the cluster means, the individual observations are analyses. The clustering is taken into account by fitting the class effects by means of random effects. Different methods for computing p-values for multilevel (or mixed-effects) models exist. In the output below as well as in the simulations, Satterthwaite’s method was used as it isn’t prohibitively expensive computationally and as it performs well on balanced data; see Luke (2017). If you load the lmerTest package, p-values computed using Satterthwaite degrees of freedom are added to the lmer() output.\n\nm2 &lt;- lmer(outcome ~ condition + (1|class), data = d_example)\nsummary(m2)\n\nLinear mixed model fit by REML. t-tests use Satterthwaite's method [\nlmerModLmerTest]\nFormula: outcome ~ condition + (1 | class)\n   Data: d_example\n\nREML criterion at convergence: 833\n\nScaled residuals: \n    Min      1Q  Median      3Q     Max \n-2.6984 -0.5908  0.0535  0.6678  2.2562 \n\nRandom effects:\n Groups   Name        Variance Std.Dev.\n class    (Intercept) 0.169    0.411   \n Residual             1.064    1.031   \nNumber of obs: 280, groups:  class, 14\n\nFixed effects:\n                      Estimate Std. Error      df t value Pr(&gt;|t|)\n(Intercept)            -0.0614     0.1781 11.2342   -0.34    0.737\nconditionintervention   0.5100     0.2527 11.4011    2.02    0.068\n\nCorrelation of Fixed Effects:\n            (Intr)\ncndtnntrvnt -0.705\n\n\nThe estimated intervention effect in this example is 0.51 ± 0.25, and the result of the significance test is t(11.4) = 2.02, p = 0.07. This is slightly different from but highly similar to the results for Approach 1. Both approaches will yield identical results if all clusters have the same size, so keep things simple if this is the case for your data (also see Murtaugh 2007).\nA possible advantage of this approach compared to Approach 1 is that it may be better able to cope with differences in class sizes. Disadvantages of Approach 2 are that the multilevel models may occasionally throw warnings and that it requires a certain number of clusters to be useful. Gelman and Hill (2007:247) point out that with fewer than five clusters, multilevel models will typically not be able to estimate the between-cluster variance. In fact, Hayes and Moulton (2009:223) suggest that multilevel modelling be used only from about 15 clusters per condition onwards.\n\n\nApproach 3: Residualise the outcome against the covariate and analyse the cluster mean residuals\nThe approach recommended by Hayes and Moulton (2009) for taking covariates into account in analyses of cluster-randomised designs is to first fit a model in which the outcome is regressed against the covariate. This model does not take the condition nor the clustering into account. The model residuals are then extracted:\n\ncovariate_model &lt;- lm(outcome ~ pretest, data = d_example)\nd_example$residual &lt;- resid(covariate_model)\n\nIn the next step, the residuals from this model are averaged per class:\n\nd_per_class &lt;- d_example |&gt; \n  group_by(class, condition) |&gt; \n  summarise(mean_residual = mean(residual),\n            .groups = \"drop\")\n\nFigure 4 shows what this looks like.\n\n\n\n\n\nFigure 4. In Approach 3, the outcome is first regressed against the covariate. The residuals of this regression are then averaged per class, and it is these averaged residuals that are analysed in the statistical model.\n\n\n\n\nThen, similarly to Approach 1, these class means are analysed, e.g., in a general linear model:\n\nm3 &lt;- lm(mean_residual ~ condition, data = d_per_class)\nsummary(m3)\n\n\nCall:\nlm(formula = mean_residual ~ condition, data = d_per_class)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-0.4615 -0.2164  0.0701  0.1637  0.4497 \n\nCoefficients:\n                      Estimate Std. Error t value Pr(&gt;|t|)\n(Intercept)             -0.220      0.113   -1.96   0.0740\nconditionintervention    0.492      0.159    3.09   0.0094\n\nResidual standard error: 0.298 on 12 degrees of freedom\nMultiple R-squared:  0.443, Adjusted R-squared:  0.397 \nF-statistic: 9.55 on 1 and 12 DF,  p-value: 0.00936\n\n\nThe estimated intervention effect in this example is 0.49 ± 0.16, and the result of the significance test is t(12) = 3.09, p = 0.009. Note how in this example, the standard error for the intervention effect estimate is considerably reduced compared to the two approaches that ignore the covariate.\n\n\nApproach 4: Analyse the cluster means, adjust for the cluster mean covariate values\nIn the fourth approach, both the outcome and the covariate are averaged per class, and the class mean covariates are entered into the general linear model on the class mean outcomes as a covariate:\n\nd_per_class &lt;- d_example |&gt; \n  group_by(class, condition) |&gt; \n  summarise(mean_class = mean(outcome),\n            mean_pretest = mean(pretest),\n            .groups = \"drop\")\n\nFigure 5 shows the data that are submitted to the statistical analysis.\n\n\n\n\n\nFigure 5. In Approach 4, both the outcome and the covariate are averaged per class. The averaged covariate is then used as a covariate in a statistical model with the averaged outcome as the dependent variable.\n\n\n\n\n\nm4 &lt;- lm(mean_class ~ condition + mean_pretest, data = d_per_class)\nsummary(m4)\n\n\nCall:\nlm(formula = mean_class ~ condition + mean_pretest, data = d_per_class)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-0.2885 -0.1814  0.0371  0.1027  0.5279 \n\nCoefficients:\n                      Estimate Std. Error t value Pr(&gt;|t|)\n(Intercept)             0.0455     0.0889    0.51   0.6188\nconditionintervention   0.4870     0.1241    3.93   0.0024\nmean_pretest            0.8994     0.1399    6.43  4.9e-05\n\nResidual standard error: 0.232 on 11 degrees of freedom\nMultiple R-squared:  0.839, Adjusted R-squared:  0.81 \nF-statistic: 28.7 on 2 and 11 DF,  p-value: 4.32e-05\n\n\nThe estimated intervention effect in this example is 0.49 ± 0.12, and the result of the significance test is t(11) = 3.93, p = 0.0024. Note how in this example, too, the standard error is considerably lower than in the two approaches that ignore the covariate.\nIncidentally, the pretest effect that is reported in the output is entirely uninteresting: we included it in the analysis to reduce the residual variance, not because we have a research question concerning the pretest covariate.\nThis approach may be particularly useful compared to the other approaches if some standardised measure of the pupils’ pre-intervention performance or general skill level is available but if teachers, parents or administrators are unwilling to share the individual results: You could try asking for just the average score per class instead, as this is all you need!\n\n\nApproach 5: Fit a multilevel model, include the covariate\nFinally, you could fit a multilevel model as in Approach 2, but with the covariate included.\n\nm5 &lt;- lmer(outcome ~ condition + pretest + (1|class), data = d_example)\nsummary(m5)\n\nLinear mixed model fit by REML. t-tests use Satterthwaite's method [\nlmerModLmerTest]\nFormula: outcome ~ condition + pretest + (1 | class)\n   Data: d_example\n\nREML criterion at convergence: 680\n\nScaled residuals: \n    Min      1Q  Median      3Q     Max \n-2.5381 -0.6974  0.0239  0.6771  2.7744 \n\nRandom effects:\n Groups   Name        Variance Std.Dev.\n class    (Intercept) 0.0577   0.240   \n Residual             0.6143   0.784   \nNumber of obs: 280, groups:  class, 14\n\nFixed effects:\n                      Estimate Std. Error       df t value Pr(&gt;|t|)\n(Intercept)            -0.0131     0.1122  10.8640   -0.12   0.9095\nconditionintervention   0.5054     0.1594  11.0897    3.17   0.0088\npretest                 0.4628     0.0317 275.3617   14.62   &lt;2e-16\n\nCorrelation of Fixed Effects:\n            (Intr) cndtnn\ncndtnntrvnt -0.703       \npretest      0.033 -0.004\n\n\nThe estimated intervention effect in this example is 0.51 ± 0.16, and the result of the significance test is t(11.1) = 3.17, p = 0.009. Again the pretest effect is entirely uninteresting; just include it in the analysis but otherwise ignore it.\nSo we have at least five ways of analysing data from cluster-randomised experiments when a covariate is available. Trying out several of them and then reporting the one that fits the narrative best is an excellent way of invalidating the inferential results, however, so I ran a simulation to find out which approach I should recommend to students and colleagues."
  },
  {
    "objectID": "posts/2019-10-28-cluster-covariates/index.html#set-up-for-the-simulations",
    "href": "posts/2019-10-28-cluster-covariates/index.html#set-up-for-the-simulations",
    "title": "Adjusting for a covariate in cluster-randomised experiments",
    "section": "Set-up for the simulations",
    "text": "Set-up for the simulations\nWhile it stands to reason that an optimal analysis will take into account the participants’ pretest (or other covariate) scores, I have found no guidance on which approach works best. Quite possibly, different approaches work best in different circumstances, so I wrote a couple of simulations to get a handle on this.\nFor the simulation I made use of the clustered_data() function in the cannonball package. See the help page for details about this function (?clustered_data). The following parameters were varied:\n\nThe number of participants either varied fairly little per class or varied a lot. For the simulations in which the class sizes were similar, the number of participants varied between 15 and 25 per class, which reflects typical school class sizes, and they were 14 classes in total. The homoskedasticity assumption is approximately met in these cases. For the simulations in which the class sizes differed more wildly, they were 10 classes, and the number of ‘pupils’ in these classes was 2, 4, 8, …, 1024. These clearly are untypical sizes for school classes, and the different class sizes induce substantial heteroskedasticity.\nThe (unconditional) intra-class correlation was either 0.17 or 0.03. An intra-class correlation of 0.17 is typical of cluster-randomised experiments in educational settings (Hedges & Hedberg 2007; Schochet 2008); an intra-class correlation of 0.03 is considerably lower than that but still enough to inflate Type-I errors considerably when it isn’t taken into account.\nA covariate was available that was either pretty strongly correlated to the pupils’ pre-intervention outcome (\\(\\rho = 0.7\\)) or fairly weakly correlated to it (\\(\\rho = 0.3\\)). The strong covariate may be thought of as a pretest score; the weak covariate could be a weak proxy of pre-intervention performance (perhaps some self-assessment).\nTo check the Type-I error rate, the effect of the intervention was set to 0. To check the different methods’ power, the effect of the intervention was set to 0.4.\n\nFor each combination of parameters, 10,000 datasets were generated and analysed using each of the five approaches outlined above. For the simulation code and the raw results, see the bottom of this page."
  },
  {
    "objectID": "posts/2019-10-28-cluster-covariates/index.html#scenario-1-typical-cluster-sizes-typical-intra-class-correlation",
    "href": "posts/2019-10-28-cluster-covariates/index.html#scenario-1-typical-cluster-sizes-typical-intra-class-correlation",
    "title": "Adjusting for a covariate in cluster-randomised experiments",
    "section": "Scenario 1: Typical cluster sizes, typical intra-class correlation",
    "text": "Scenario 1: Typical cluster sizes, typical intra-class correlation\nLet’s first take a look at how the analytical approaches compare in a typical cluster-randomised experiment in applied linguistics: The class sizes aren’t identical but fairly similar, and the intra-class correlation is 0.17.\nWhen there is no effect of the intervention, we should observe a significant difference in only 5% of cases. In other words, the Type-I error rate should be 0.05. As Figure 6 shows, all five analytical approaches seem to have the advertised Type-I error rate of 0.05.\n\n\n\n\n\nFigure 6. Observed Type-I error rates for scenario 1 (typical cluster sizes, intra-class correlation of 0.17). If the true Type-I error rate is 0.05, there is a 95% probability that the observed Type-I error rate lies between the dashed horizontal lines. All five approaches seem to perform adequately in terms of their Type-I error rate.\n\n\n\n\nWhen there is an effect of the intervention, we should observe significant differences more often. As Figure 7 shows, Approach 4 performs either on par with or considerably better than the alternative approaches.\n\n\n\n\n\nFigure 7. Observed power for scenario 1 (typical cluster sizes, intra-class correlation of 0.17). When the covariate is only weakly correlated with the (pre-intervention) outcome (\\(\\rho = 0.3\\)), the three approaches that consider it slightly outperform the two approaches that don’t, with little difference between these three approaches. If the correlation is stronger (\\(\\rho = 0.7\\)), the approach in which both the outcome and the covariate are averaged per class (Approach 4) performs considerably better than the alternatives.\n\n\n\n\nIn summary, for what I consider to be a typical cluster-randomised experiment in applied linguistics, Approach 4 seems to be the best way to analyse the data."
  },
  {
    "objectID": "posts/2019-10-28-cluster-covariates/index.html#scenario-2-typical-cluster-sizes-low-intra-class-correlation",
    "href": "posts/2019-10-28-cluster-covariates/index.html#scenario-2-typical-cluster-sizes-low-intra-class-correlation",
    "title": "Adjusting for a covariate in cluster-randomised experiments",
    "section": "Scenario 2: Typical cluster sizes, low intra-class correlation",
    "text": "Scenario 2: Typical cluster sizes, low intra-class correlation\nIn scenario 2, the class sizes are still typical of what is found in applied linguistics, but the intra-class correlation is lower (0.03). As Figure 8 shows, Approach 5 (multilevel model with covariate) may be somewhat too conservative if the intra-correlation is low and the covariate is fairly strongly related to the outcome. In spite of this conservatism, it performs well power-wise, as shown in Figure 9.\n\n\n\n\n\nFigure 8. Observed Type-I error rates for scenario 2 (typical cluster sizes, intra-class correlation of 0.03). If the true Type-I error rate is 0.05, there is a 95% probability that the observed Type-I error rate lies between the dashed horizontal lines. The multilevel model that takes the covariate into account (Approach 5) may be slightly too conservative if the covariate is fairly strongly related to the outcome, but otherwise all five approaches seem to perform adequately in terms of their Type-I error rate.\n\n\n\n\n\n\n\n\n\nFigure 9. Observed power for scenario 2 (typical cluster sizes, intra-class correlation of 0.03). When the covariate is fairly strong (\\(\\rho = 0.7\\)), the three approaches that take the covariate into account perform roughly equally well; when the covariate is weaker (\\(\\rho = 0.3\\)), Approaches 3 and 5 perform slightly better than Approach 4."
  },
  {
    "objectID": "posts/2019-10-28-cluster-covariates/index.html#scenario-3-wildly-different-cluster-sizes-typical-intra-class-correlation",
    "href": "posts/2019-10-28-cluster-covariates/index.html#scenario-3-wildly-different-cluster-sizes-typical-intra-class-correlation",
    "title": "Adjusting for a covariate in cluster-randomised experiments",
    "section": "Scenario 3: Wildly different cluster sizes, typical intra-class correlation",
    "text": "Scenario 3: Wildly different cluster sizes, typical intra-class correlation\nNow let’s consider a more unrealistic scenario. The ICC is 0.17, as in scenario 1, but the classes aren’t all of approximately equal size, but instead we have one class of size 2, one class of size 4, up till one class of size 1024 (\\(2^1, 2^2, 2^3, \\dots, 2^{10}\\)). As Figures 10 and 11 show, Approach 4 may be slightly too conservative in terms of its Type-I error rate in this setting, yet performs best power-wise.\n\n\n\n\n\nFigure 10. Observed Type-I error rates for scenario 3 (wildly different cluster sizes, intra-class correlation of 0.17). Approach 4 seems to be too conservative, especially when the covariate is fairly strongly related to the outcome. The other four approaches seem to be perform adequately in terms of their Type-I error rate.\n\n\n\n\n\n\n\n\n\nFigure 11. Observed power for scenario 3 (wildly different cluster sizes, intra-class correlation of 0.17). When the covariate is fairly strong (\\(\\rho = 0.7\\)), the three approaches that take the covariate into account perform roughly equally well; when the covariate is weaker (\\(\\rho = 0.3\\)), Approaches 3 and 5 perform slightly better than Approach 4."
  },
  {
    "objectID": "posts/2019-10-28-cluster-covariates/index.html#scenario-4-wildly-different-cluster-sizes-low-intra-class-correlation",
    "href": "posts/2019-10-28-cluster-covariates/index.html#scenario-4-wildly-different-cluster-sizes-low-intra-class-correlation",
    "title": "Adjusting for a covariate in cluster-randomised experiments",
    "section": "Scenario 4: Wildly different cluster sizes, low intra-class correlation",
    "text": "Scenario 4: Wildly different cluster sizes, low intra-class correlation\nIn the fourth scenario, the cluster sizes again differ wildly, but the ICC is only 0.03. As Figure 12 shows, the cluster-level analyses are all too conservative, whereas the multilevel approaches aren’t conservative enough. The observed power associated with each approach was correspondingly adjusted, see Figure 13. Even with its power properly adjusted, the multilevel model with a covariate (Approach 5) outperforms the other approaches when the correlation between outcome and covariate is fairly strong.\n\n\n\n\n\nFigure 12. Observed Type-I error rates for scenario 4 (wildly different cluster sizes, intra-class correlation of 0.03). The multilevel approaches (Approaches 2 and 5) seem to be both anticonservative; the cluster-level analyses (Approaches 1, 3 and 4) are too conservative.\n\n\n\n\n\n\n\n\n\nFigure 13. Since the observed Type-I error rate varied considerably between the five approaches in scenario 4 (see Figure 12), the observed power was adjusted for the approaches’ observed Type-I error rate. This figure shows, for datasets with an intervention effect, how often an approach returned a p-value that was lower than the 5th percentile p-value that the same approach returned when there was no intervention effect."
  },
  {
    "objectID": "posts/2019-10-28-cluster-covariates/index.html#summary",
    "href": "posts/2019-10-28-cluster-covariates/index.html#summary",
    "title": "Adjusting for a covariate in cluster-randomised experiments",
    "section": "Summary",
    "text": "Summary\nIn sum, for typical cluster-randomised experiments in applied linguistics (as simulated in scenarios 1 and perhaps 2), Approach 4 either considerably outperforms the other approaches or performs about equally well. Multilevel models only seem to have some added value when the cluster sizes are wildly different, the intra-class correlation is pretty low and the covariate is strongly related to the outcome. On balance, therefore, I think that it is reasonably for me to recommend students and colleagues to analyse cluster-randomised experiments using Approach 4.\n(Incidentally, if someone could explain to me why Approach 4 outperforms Approaches 3 and 5 in Scenario 1, that would be highly appreciated.)\nOne strategy I definitely do not recommend is to try out several different approaches and then report the one that returns the lowest p-value. As Figure 14 shows, different analyses ran on the same data can produce very different p-values. If you try out two or more approaches and always report the lowest p-value, your Type-I error rate will blow up (see Simmons et al. 2011).\n\n\n\n\n\nFigure 14. Left: p-values for Approaches 4 (x-axis) and 5 (y-axis) ran on the same data when there is no intervention effect. Right: Same, but ran on data with an intervention effect."
  },
  {
    "objectID": "posts/2019-10-28-cluster-covariates/index.html#how-about-fewer-clusters",
    "href": "posts/2019-10-28-cluster-covariates/index.html#how-about-fewer-clusters",
    "title": "Adjusting for a covariate in cluster-randomised experiments",
    "section": "How about fewer clusters?",
    "text": "How about fewer clusters?\nIn scenarios 1 and 2, fourteen classes participated in the experiment. I’ve rarely seen cluster-randomised experiments in applied linguistics with more than 20 classes, but cluster-randomised experiments with just a handful of classes (say 4 or 6) do occur. (Unfortunately, cluster-randomised experiments with just two classes also occur. But these can’t be analysed properly.) I therefore ran some additional simulations for experiments in which only 6 classes with between 15 and 25 pupils participated, with an ICC of 0.17, and in which the covariate was fairly strong, as would be typical in experiments with pretests.\nAs Figure 15 shows, all approaches perform well in terms of their Type-I error rate. Power-wise, Figure 16 shows that Approach 4 pips Approaches 3 and 5, which in turn perform much better than Approaches 1 and 2. The default recommendation to use Approach 4 therefore also seems reasonable for cluster-randomised experiments with few clusters.\n\n\n\n\n\nFigure 15. Observed Type-I error rates for a cluster-randomised experiment with just 6 classes of similar size, an ICC of 0.17 and a fairly strong covariate (\\(\\rho = 0.7\\)). All approaches perform adequately in terms of their Type-I error.\n\n\n\n\n\n\n\n\n\nFigure 16. Observed power for a cluster-randomised experiment with just 6 classes of similar size, an ICC of 0.17 and a fairly strong covariate ($ ho = 0.7$). Approach 4 slighly outperforms Approaches 3 and 5, and all three perform markedly better than Approaches 1 and 2. Power is fairly dreadful overall, though this of course depends on the size of the intervention effect and the variability between and within classes."
  },
  {
    "objectID": "posts/2019-10-28-cluster-covariates/index.html#r-code-and-simulation-results",
    "href": "posts/2019-10-28-cluster-covariates/index.html#r-code-and-simulation-results",
    "title": "Adjusting for a covariate in cluster-randomised experiments",
    "section": "R code and simulation results",
    "text": "R code and simulation results\nThe R code for the simulations is available here. The simulation output for scenarios 1 through 4 is available here (84.5 MB). The simulation output for experiments with only 6 classes is available here (11 MB). In the output, each row corresponds to one simulated dataset that was analysed in five different ways. Columns 1 through 5 contain the p-values associated with each analysis, columns 6 through 10 contain the estimates for the intervention effect that each analysis yields, columns 11 through 15 contain the corresponding standard errors, columns 16 through 25 contain the lower and upper bounds of the 95% confidence intervals, and the last four columns specify the simulation parameters."
  },
  {
    "objectID": "posts/2019-10-28-cluster-covariates/index.html#references",
    "href": "posts/2019-10-28-cluster-covariates/index.html#references",
    "title": "Adjusting for a covariate in cluster-randomised experiments",
    "section": "References",
    "text": "References\nGelman, Andrew and Jennifer Hill. 2007. Data analysis using regression and multilevel/hierarchical models. Cambridge, UK: Cambridge University Press.\nHayes, Richard J. and Lawrence H. Moulton. 2009. Cluster randomised trials. Boca Raton, FL: Chapman & Hall/CRC.\nHedges, Larry V. and E. C. Hedberg. 2007. Intraclass correlation values for planning- group-randomized trials in education. Educational Evaluation and Policy Analysis 29(1). 60-87.\nLuke, Steven G. 2017. Evaluating significance in linear mixed-effects models in R. Behavioral Research Methods 49. 1494–1502.\nMurtaugh, Paul A. 2007. Simplicity and complexity in ecological data analysis. Ecology 88(1). 56–62.\nSchochet, Peter Z. 2008. Statistical power for random assignment evaluations of education programs. Journal of Educational and Behavioral Statistics 33(1). 62-87.\nSimmons, Joseph P., Leif D. Nelson and Uri Simonsohn. 2011. False-positive psychology: Undisclosed flexibility in data collection and analysis allows presenting anything as significant. Psychological Science 22(11). 1359-1366.\nVanhove, Jan. 2015. Analyzing randomized controlled interventions: Three notes for applied linguists. Studies in Second Language Learning and Teaching 5. 135–152. Also see the correction note for this paper."
  },
  {
    "objectID": "posts/2016-08-18-ordering-factor-levels/index.html",
    "href": "posts/2016-08-18-ordering-factor-levels/index.html",
    "title": "R tip: Ordering factor levels more easily",
    "section": "",
    "text": "By default, R sorts the levels of a factor alphabetically. When drawing graphs, this results in ‘Alabama First’ graphs, and it’s usually better to sort the elements of a graph by more meaningful principles than alphabetical order. This post illustrates three convenience functions you can use to sort factor levels in R according to another covariate, their frequency of occurrence, or manually.\nUpdate (2023-08-08): The reorder() function has pretty much the same functionality as the functions introduced in this blog post. In the original blog post, I loaded some packages that are now part of the tidyverse suite separately; now I just use the tidyverse.\nFirst you’ll need the tidyverse:\ninstall.packages(c(\"tidyverse\"))\nYou can download the convenience functions from my Github page or read them in directly into R:\nsource(\"https://janhove.github.io/RCode/sortLvls.R\")"
  },
  {
    "objectID": "posts/2016-08-18-ordering-factor-levels/index.html#sorting-factor-levels-by-another-variable",
    "href": "posts/2016-08-18-ordering-factor-levels/index.html#sorting-factor-levels-by-another-variable",
    "title": "R tip: Ordering factor levels more easily",
    "section": "Sorting factor levels by another variable",
    "text": "Sorting factor levels by another variable\nThe code below creates an example dataset with a factor and a covariate:\n\n# Load packages\nlibrary(tidyverse)\n\n# Generate same data\nset.seed(18-08-2016)\n\n# Create data frame\ndf &lt;- data.frame(factorBefore = factor(rep(letters[1:5], 3)),\n                 covariate = rnorm(15, 50, 30))\n\n# Current order of factor levels (alphabetically)\nlevels(df$factorBefore)\n\n[1] \"a\" \"b\" \"c\" \"d\" \"e\"\n\n# Covariate mean per factor level\ndf |&gt; group_by(factorBefore) |&gt; summarise(mean(covariate))\n\n# A tibble: 5 × 2\n  factorBefore `mean(covariate)`\n  &lt;fct&gt;                    &lt;dbl&gt;\n1 a                         8.04\n2 b                        36.9 \n3 c                        45.0 \n4 d                        71.6 \n5 e                        41.9 \n\n\nWhat we want is to sort the levels of the factor by the covariate mean per factor level (i.e., a-b-e-c-d). The function sortLvlsByVar.fnc accomplishes this:\n\n# Reorder\ndf$factorAfter1 &lt;- sortLvlsByVar.fnc(df$factorBefore, df$covariate)\n\nLoading required package: magrittr\n\n\n\nAttaching package: 'magrittr'\n\n\nThe following object is masked from 'package:purrr':\n\n    set_names\n\n\nThe following object is masked from 'package:tidyr':\n\n    extract\n\n# New order of factor levels\nlevels(df$factorAfter1)\n\n[1] \"a\" \"b\" \"e\" \"c\" \"d\"\n\n\nBy setting the ascending parameter to FALSE, the factor levels are sorting descendingly according to the covariate mean:\n\n# Reorder descendingly\ndf$factorAfter2 &lt;- sortLvlsByVar.fnc(df$factorBefore, df$covariate, ascending = FALSE)\nlevels(df$factorAfter2)\n\n[1] \"d\" \"c\" \"e\" \"b\" \"a\"\n\n\nHow this looks like when graphed:\n\n# Alphabetical order\np1 &lt;- ggplot(df, aes(x = factorBefore, y = covariate)) +\n  geom_boxplot()\n# Sorted ascendingly\np2 &lt;- ggplot(df, aes(x = factorAfter1, y = covariate)) +\n  geom_boxplot()\n# Sorted descendingly\np3 &lt;- ggplot(df, aes(x = factorAfter2, y = covariate)) +\n  geom_boxplot()\ngridExtra::grid.arrange(p1, p2, p3, ncol = 3)\n\n\n\n\nYou can change the R code from the Github page so that the levels are sorted by another summary statistics, e.g., the covariate median per factor level."
  },
  {
    "objectID": "posts/2016-08-18-ordering-factor-levels/index.html#sorting-factor-levels-by-their-frequency-of-occurrence",
    "href": "posts/2016-08-18-ordering-factor-levels/index.html#sorting-factor-levels-by-their-frequency-of-occurrence",
    "title": "R tip: Ordering factor levels more easily",
    "section": "Sorting factor levels by their frequency of occurrence",
    "text": "Sorting factor levels by their frequency of occurrence\nAgain we’ll first create some data:\n\ndf2 &lt;- data.frame(factorBefore = factor(rep(letters[1:5], c(7, 3, 80, 15, 107))),\n                  covariate = rnorm(sum(c(7, 3, 80, 15, 107)), 50, 30))\ntable(df2$factorBefore)\n\n\n  a   b   c   d   e \n  7   3  80  15 107 \n\n\nWe want to order these factor levels by their frequency of occurrence in the dataset (i.e., b-a-d-c-e). sortLvlsByN.fnc() accomplishes this:\n\ndf2$factorAfter1 &lt;- sortLvlsByN.fnc(df2$factorBefore)\ntable(df2$factorAfter1)\n\n\n  b   a   d   c   e \n  3   7  15  80 107 \n\n\nOr descendingly:\n\ndf2$factorAfter2 &lt;- sortLvlsByN.fnc(df2$factorBefore, ascending = FALSE)\ntable(df2$factorAfter2)\n\n\n  e   c   d   a   b \n107  80  15   7   3 \n\n\nWhen plotted:\n\np4 &lt;- ggplot(df2, aes(x = factorBefore, y = covariate)) +\n  geom_boxplot(varwidth = TRUE)\np5 &lt;- ggplot(df2, aes(x = factorAfter1, y = covariate)) +\n  geom_boxplot(varwidth = TRUE)\np6 &lt;- ggplot(df2, aes(x = factorAfter2, y = covariate)) +\n  geom_boxplot(varwidth = TRUE)\ngridExtra::grid.arrange(p4, p5, p6, ncol = 3)"
  },
  {
    "objectID": "posts/2016-08-18-ordering-factor-levels/index.html#customising-the-order-of-factor-levels",
    "href": "posts/2016-08-18-ordering-factor-levels/index.html#customising-the-order-of-factor-levels",
    "title": "R tip: Ordering factor levels more easily",
    "section": "Customising the order of factor levels",
    "text": "Customising the order of factor levels\nIf you want to put the factor levels in a custom order, you can use the sortLvls.fnc() function.\n\n# Create data\ndf3 &lt;- data.frame(factorBefore = factor(rep(letters[1:5], 3)),\n                  covariate = rnorm(15, 50, 30))\nlevels(df3$factorBefore)\n\n[1] \"a\" \"b\" \"c\" \"d\" \"e\"\n\n\nLet’s say we, for some reason, want to put the current 5th level (e) first, the current 3rd level (c) second, the 4th 3rd, the 4th 2nd and the 1st last:\n\ndf3$factorAfter1 &lt;- sortLvls.fnc(df3$factorBefore, c(5, 3, 4, 2, 1))\nlevels(df3$factorAfter1)\n\n[1] \"e\" \"c\" \"d\" \"b\" \"a\"\n\n\nYou can also just specify which factor levels need to go up front; the order of the other ones stays the same:\n\n# Put the current 3rd and 2nd in front; leave the rest as they were:\ndf3$factorAfter2 &lt;- sortLvls.fnc(df3$factorBefore, c(3, 2))\nlevels(df3$factorAfter2)\n\n[1] \"c\" \"b\" \"a\" \"d\" \"e\""
  },
  {
    "objectID": "posts/2016-08-18-ordering-factor-levels/index.html#software-versions",
    "href": "posts/2016-08-18-ordering-factor-levels/index.html#software-versions",
    "title": "R tip: Ordering factor levels more easily",
    "section": "Software versions",
    "text": "Software versions\n\ndevtools::session_info()\n\n─ Session info ───────────────────────────────────────────────────────────────\n setting  value\n version  R version 4.3.1 (2023-06-16)\n os       Ubuntu 22.04.2 LTS\n system   x86_64, linux-gnu\n ui       X11\n language en_US\n collate  en_US.UTF-8\n ctype    en_US.UTF-8\n tz       Europe/Zurich\n date     2023-08-08\n pandoc   3.1.1 @ /usr/lib/rstudio/resources/app/bin/quarto/bin/tools/ (via rmarkdown)\n\n─ Packages ───────────────────────────────────────────────────────────────────\n package     * version date (UTC) lib source\n cachem        1.0.6   2021-08-19 [2] CRAN (R 4.2.0)\n callr         3.7.3   2022-11-02 [1] CRAN (R 4.3.1)\n cli           3.6.1   2023-03-23 [1] CRAN (R 4.3.0)\n colorspace    2.1-0   2023-01-23 [1] CRAN (R 4.3.0)\n crayon        1.5.2   2022-09-29 [1] CRAN (R 4.3.1)\n devtools      2.4.5   2022-10-11 [1] CRAN (R 4.3.1)\n digest        0.6.29  2021-12-01 [2] CRAN (R 4.2.0)\n dplyr       * 1.1.2   2023-04-20 [1] CRAN (R 4.3.0)\n ellipsis      0.3.2   2021-04-29 [2] CRAN (R 4.2.0)\n evaluate      0.15    2022-02-18 [2] CRAN (R 4.2.0)\n fansi         1.0.4   2023-01-22 [1] CRAN (R 4.3.1)\n farver        2.1.1   2022-07-06 [1] CRAN (R 4.3.0)\n fastmap       1.1.0   2021-01-25 [2] CRAN (R 4.2.0)\n forcats     * 1.0.0   2023-01-29 [1] CRAN (R 4.3.0)\n fs            1.5.2   2021-12-08 [2] CRAN (R 4.2.0)\n generics      0.1.3   2022-07-05 [1] CRAN (R 4.3.0)\n ggplot2     * 3.4.2   2023-04-03 [1] CRAN (R 4.3.0)\n glue          1.6.2   2022-02-24 [2] CRAN (R 4.2.0)\n gridExtra     2.3     2017-09-09 [1] CRAN (R 4.3.0)\n gtable        0.3.3   2023-03-21 [1] CRAN (R 4.3.0)\n hms           1.1.3   2023-03-21 [1] CRAN (R 4.3.0)\n htmltools     0.5.5   2023-03-23 [1] CRAN (R 4.3.0)\n htmlwidgets   1.6.2   2023-03-17 [1] CRAN (R 4.3.1)\n httpuv        1.6.11  2023-05-11 [1] CRAN (R 4.3.1)\n jsonlite      1.8.7   2023-06-29 [1] CRAN (R 4.3.1)\n knitr         1.39    2022-04-26 [2] CRAN (R 4.2.0)\n labeling      0.4.2   2020-10-20 [1] CRAN (R 4.3.0)\n later         1.3.1   2023-05-02 [1] CRAN (R 4.3.1)\n lifecycle     1.0.3   2022-10-07 [1] CRAN (R 4.3.0)\n lubridate   * 1.9.2   2023-02-10 [1] CRAN (R 4.3.0)\n magrittr    * 2.0.3   2022-03-30 [1] CRAN (R 4.3.0)\n memoise       2.0.1   2021-11-26 [2] CRAN (R 4.2.0)\n mime          0.10    2021-02-13 [2] CRAN (R 4.0.2)\n miniUI        0.1.1.1 2018-05-18 [1] CRAN (R 4.3.1)\n munsell       0.5.0   2018-06-12 [1] CRAN (R 4.3.0)\n pillar        1.9.0   2023-03-22 [1] CRAN (R 4.3.0)\n pkgbuild      1.4.2   2023-06-26 [1] CRAN (R 4.3.1)\n pkgconfig     2.0.3   2019-09-22 [2] CRAN (R 4.2.0)\n pkgload       1.3.2.1 2023-07-08 [1] CRAN (R 4.3.1)\n prettyunits   1.1.1   2020-01-24 [2] CRAN (R 4.2.0)\n processx      3.8.2   2023-06-30 [1] CRAN (R 4.3.1)\n profvis       0.3.8   2023-05-02 [1] CRAN (R 4.3.1)\n promises      1.2.0.1 2021-02-11 [1] CRAN (R 4.3.1)\n ps            1.7.5   2023-04-18 [1] CRAN (R 4.3.1)\n purrr       * 1.0.1   2023-01-10 [1] CRAN (R 4.3.0)\n R6            2.5.1   2021-08-19 [2] CRAN (R 4.2.0)\n Rcpp          1.0.11  2023-07-06 [1] CRAN (R 4.3.1)\n readr       * 2.1.4   2023-02-10 [1] CRAN (R 4.3.0)\n remotes       2.4.2   2021-11-30 [2] CRAN (R 4.2.0)\n rlang         1.1.1   2023-04-28 [1] CRAN (R 4.3.0)\n rmarkdown     2.21    2023-03-26 [1] CRAN (R 4.3.0)\n rstudioapi    0.14    2022-08-22 [1] CRAN (R 4.3.0)\n scales        1.2.1   2022-08-20 [1] CRAN (R 4.3.0)\n sessioninfo   1.2.2   2021-12-06 [2] CRAN (R 4.2.0)\n shiny         1.7.4.1 2023-07-06 [1] CRAN (R 4.3.1)\n stringi       1.7.12  2023-01-11 [1] CRAN (R 4.3.1)\n stringr     * 1.5.0   2022-12-02 [1] CRAN (R 4.3.0)\n tibble      * 3.2.1   2023-03-20 [1] CRAN (R 4.3.0)\n tidyr       * 1.3.0   2023-01-24 [1] CRAN (R 4.3.0)\n tidyselect    1.2.0   2022-10-10 [1] CRAN (R 4.3.0)\n tidyverse   * 2.0.0   2023-02-22 [1] CRAN (R 4.3.1)\n timechange    0.2.0   2023-01-11 [1] CRAN (R 4.3.0)\n tzdb          0.4.0   2023-05-12 [1] CRAN (R 4.3.0)\n urlchecker    1.0.1   2021-11-30 [1] CRAN (R 4.3.1)\n usethis       2.2.2   2023-07-06 [1] CRAN (R 4.3.1)\n utf8          1.2.3   2023-01-31 [1] CRAN (R 4.3.1)\n vctrs         0.6.3   2023-06-14 [1] CRAN (R 4.3.0)\n withr         2.5.0   2022-03-03 [2] CRAN (R 4.2.0)\n xfun          0.39    2023-04-20 [1] CRAN (R 4.3.0)\n xtable        1.8-4   2019-04-21 [1] CRAN (R 4.3.1)\n yaml          2.3.5   2022-02-21 [2] CRAN (R 4.2.0)\n\n [1] /home/jan/R/x86_64-pc-linux-gnu-library/4.3\n [2] /usr/local/lib/R/site-library\n [3] /usr/lib/R/site-library\n [4] /usr/lib/R/library\n\n──────────────────────────────────────────────────────────────────────────────"
  },
  {
    "objectID": "posts/2018-07-06-data-entry-failsafes/index.html",
    "href": "posts/2018-07-06-data-entry-failsafes/index.html",
    "title": "A data entry form with failsafes",
    "section": "",
    "text": "I’m currently working on a large longitudinal project as a programmer/analyst. Most of the data are collected using paper/pencil tasks and questionnaires and need to be entered into the database by student assistants. In previous projects, this led to some minor irritations since some assistants occasionally entered some words with capitalisation and others without, or they inadvertently added a trailing space to the entry, or used participant IDs that didn’t exist – all small things that cause difficulties during the analysis.\nTo reduce the chances of such mishaps in the current project, I created an on-line platform that uses HTML, JavaScript and PHP to homogenise how research assistants can enter data and that throws errors and warnings when they enter impossible data. Nothing that will my name pop up at Google board meetings, but useful enough.\nAnyway, you can download a slimmed-down version of this platform here. The comments in the PHP files should tell you what I try to accomplish; if something’s not clear, there’s a comment section at the bottom of this page. You’ll need a webserver that supports PHP, and you’ll need to change the permissions of the Data directory to 777.\nUpdate (2023-08-06): The links below no longer work.\nYou can also check out the demo. To log in, use one of the following e-mail addresses: first.assistant@university.ch, second.assistant@university.ch, third.assistant@university.ch. (You can change the accepted e-mail address in index.php). The password is projectpassword.\nThen enter some data. You can only enter data for participants you’ve already created an ID for, though. For this project, the participant IDs consist of the number 4 or 5 (= the participant’s grade), followed by a dot, followed by a two digit number between 0 and 39 (= the participant’s class), followed by a dot and another two digit number between 0 and 99. The entry for Grade needs to match the first number in ID.\nIf you enter task data for a participant for whom someone has already task data at that data collection wave, you’ll receive an error. You can override this error by ticking the Correct existing entry? box at the bottom. This doesn’t overwrite the existing entry, but adds the new entry, which is flagged as the accurate one. During the analysis, you can then filter out data that was later updated.\nHopefully this is of some use to some of you!"
  },
  {
    "objectID": "posts/2016-11-21-what-correlations-look-like/index.html",
    "href": "posts/2016-11-21-what-correlations-look-like/index.html",
    "title": "What data patterns can lie behind a correlation coefficient?",
    "section": "",
    "text": "In this post, I want to, first, help you to improve your intuition of what data patterns correlation coefficients can represent and, second, hammer home the point that to sensibly interpret a correlation coefficient, you need the corresponding scatterplot."
  },
  {
    "objectID": "posts/2016-11-21-what-correlations-look-like/index.html#the-briefest-of-introductions-to-correlation-coefficients",
    "href": "posts/2016-11-21-what-correlations-look-like/index.html#the-briefest-of-introductions-to-correlation-coefficients",
    "title": "What data patterns can lie behind a correlation coefficient?",
    "section": "The briefest of introductions to correlation coefficients",
    "text": "The briefest of introductions to correlation coefficients\nIf you haven’t dealt with correlation coefficients much, a correlation coefficient, abbreviated as r, is a number between -1 and 1 that captures the strength of the linear relationship between two numeric variables. For instance, say you’ve asked 30 people about their weight and body height and plot these 30 (weight, height)-pairs in a scatterplot.\n\nIf all 30 data points fall perfectly on an increasing line, then the correlation between these two variables will be r = 1.\nIf, however, the general shape of the (weight, height) relationship is an increasing one but the 30 data points don’t fall perfectly on a single line, then r will be somewhere between 0 and 1; the closer the data points are to a straight line, the closer to 1 r will be.\nIf the relationship is a decreasing one, then r will lie between 0 and -1,\nand if there’s no linear relationship between weight and height at all, r will be 0.\n\nYou’ll find plenty of examples below."
  },
  {
    "objectID": "posts/2016-11-21-what-correlations-look-like/index.html#one-correlation-coefficient-can-represent-any-number-of-patterns",
    "href": "posts/2016-11-21-what-correlations-look-like/index.html#one-correlation-coefficient-can-represent-any-number-of-patterns",
    "title": "What data patterns can lie behind a correlation coefficient?",
    "section": "One correlation coefficient can represent any number of patterns",
    "text": "One correlation coefficient can represent any number of patterns\nCorrelation coefficients are popular among researchers because they allow them to summarise the relationship between two variables in a single number. However, a given correlation coefficient can represent any number of patterns between two variables, and without more information (ideally in the form of a scatterplot), the researchers themselves and their readers have no way of knowing which one.\nTo illustrate this, I’ve written an R function, plot_r(), that takes as its input a correlation coefficient and a sample size and outputs 16 quite different scatterplots that are all characterised by the same correlation coefficient. Below I first show and comment on how scatterplots for correlation coefficients of 0.5 and 0 based on 50 pairs might look like. Then, for those of you who don’t use R, I provide scatterplots for a couple of other correlation coefficients so you can develop a sense of what the patterns underlying these correlation coefficients could look like."
  },
  {
    "objectID": "posts/2016-11-21-what-correlations-look-like/index.html#what-r-0.5-might-look-like",
    "href": "posts/2016-11-21-what-correlations-look-like/index.html#what-r-0.5-might-look-like",
    "title": "What data patterns can lie behind a correlation coefficient?",
    "section": "What r = 0.5 might look like",
    "text": "What r = 0.5 might look like\nFirst, source() the plot_r() function or download. Then, in R, you can draw scatterplots corresponding to a correlation coefficient of r = 0.5 and 50 observations like this. (You’ll end up with a different set of scatterplots as the scatterplots are created randomly.)\nUpdate (2023-08-08): The plot_r() function is now also part of the cannonball package for R, which you can download from GitHub.\n\nsource(\"http://janhove.github.io/RCode/plot_r.R\")\nplot_r(r = 0.5, n = 50)\n\n\n\n\n\nSome comments\n\nTop row\nI suspect that when people think of a relationship with a correlation coefficient of 0.5, they have something like plots (1) and (2) in their mind’s eye. For both plots, the underlying relationship between X and Y is linear, and the Y values are normally distributed about the best-fitting straight line. The minor difference between (1) and (2) is that for (1), X is normally distributed and for (2), X is uniformly distributed. These two plots represent the kind of relationship that r was meant to capture.\nPlot (3) differs from (1) and (2) in that the X variable is now sampled from a skewed distribution. In this case, most X values are comparatively low but one X value is fairly large; if you run the code yourself, you may find that there are no outlying values or that there are more than one. Such a distribution may occur occur when X represents, for instance, participants’ performance on a task that was too difficult (floor effect). In such a case, one or a couple of outlying but genuine X values may (not will) have ‘high leverage’, that is, they may unduly affect the correlation coefficient by pulling it up or down.\nThe problem in (4) is similar to the one in (3), but now most X values are comparatively large and a handful are fairly low, perhaps because X represents participants’ performance on a task that was too easy (ceiling effects). Here, too, outlying points may have ‘high leverage’, i.e., they may unduly affect the correlation coefficient such that it doesn’t accurately characterise the bulk of the data.\n\n\nSecond row\nPlots (5) and (6) are variations on the same theme as in plots (3) and (4): The Y values aren’t normally distributed about the regression line but are skewed. In such cases, too, some outlying but genuine Y values may (not will) have ‘high leverage’, i.e., they may pull the correlation coefficient up or down much more than ordinary data points.\nPlots (7) and (8) are two examples where the variability of the Y values about the straight line increases and decreases, respectively, as X becomes larger, though admittedly, it isn’t very clear in this example. This is known as heteroskedasticity. The main problems with blindly relying on correlation coefficients in the presence of heteroskedasticity, in my view, are that (a) ‘r = 0.5’ both _under_sells how well Y can be estimated from X for low (high) X values and _over_sells how well Y can be estimated from X for high (low) X values, and (b) by just reporting the correlation coefficient you gloss over an important aspect of the data. Additionally, heteroskedasticity may affect your inferential statistics.\n\n\nThird row\nPlot (9) illustrates that correlation coefficients express the strength of the linear relationship between two variables. If the relationship isn’t linear, they’re hardly informative. In this case, r = 0.5 seriously understates the strength of the XY relationship, which happens to be non-linear (quadratic in this case). The same goes for (10), where r = 0.5 understates the strength of the XY relationship and misses out on the cyclical nature of the relationship.\nPlots (11) and (12) illustrate how a single outlying point, e.g., due to a technical error, can produce misleading correlation coefficients. In (11), a single outlying data point produces the significant positive correlation; had only the 49 data points on the left been considered, a negative relationship would’ve been observed (the dashed red line). Blindly going by r = 0.5 mischaracterises the bulk of the data. In (12), the relationship is considerably stronger than r = 0.5 suggests for the bulk of the data (the dashed red line); the outlier pulls the correlation coefficient down. Plots (11) and (12) differ from plots (3) and (4) in that in plots (3) and (4), the X values were all sampled from the same–but skewed–distribution and are, as such, genuine data points; in plots (11) and (12), the outliers were caused by a different mechanism from the other data points (e.g., a coding error or a technical glitch).\n\n\nFourth row\nIn (13), the Y values are bimodally distributed about the regression line. This suggests that we have overlooked an important aspect of the data, such as grouping factor: perhaps the datapoints above the regression line were sampled from a different population than those below the regression line.\nThe situation in plot (14) is similar to but considerably worse than the one in (13): The dataset contains two groups, but unlike in (13), the overall trend captured by r = 0.5 betrays the fact that within each of these groups, the XY relationship is actually negative. Plot (14) will often, but not always, produce such a pattern, which is known as Simpson’s paradox.\nPlot (15) depicts a situation where the researchers, rather than investigating the XY relationship along the entire X range, only investigated the cases with the most extreme X values. Sampling at the extremes inflates correlation coefficients (see reason no. 2 why I don’t particularly like correlation coefficients to begin with). In other words, if you took a sample of 150 XY cases and only looked at the 50 most extreme X observations, you’d end up with a correlation coefficient that is very likely to be larger than the one you’d observe if you looked at all 150 cases.\nPlot (16), finally, is what I suspect many correlation coefficients actually represent. The X and Y data are lumpy, for instance, because they represent count data or responses to questionnaire data. I don’t think correlation coefficients for such patterns are deceptive per se, but we’re clearly talking about a different pattern than in plots (1) and (2)."
  },
  {
    "objectID": "posts/2016-11-21-what-correlations-look-like/index.html#what-r-0-might-look-like",
    "href": "posts/2016-11-21-what-correlations-look-like/index.html#what-r-0-might-look-like",
    "title": "What data patterns can lie behind a correlation coefficient?",
    "section": "What r = 0 might look like",
    "text": "What r = 0 might look like\n\nplot_r(r = 0, n = 50)\n\n\n\n\n\nSome comments\nThe main point I want to make here is that r = 0 doesn’t necessarily mean that there’s no XY relationship. This is clear from plots (9) and (10), which evince strong non-linear relationships. Plots (11) and (12) similarly underscore this point: There exists a strong relationship for the bulk of the data, but this trend is cancelled out by a single outlying data point. Occasionally, in plot (14), a trend present in two subgroups may not be visible in an aggregated analysis; this doesn’t seem to be the case in this example, though."
  },
  {
    "objectID": "posts/2016-11-21-what-correlations-look-like/index.html#two-other-examples",
    "href": "posts/2016-11-21-what-correlations-look-like/index.html#two-other-examples",
    "title": "What data patterns can lie behind a correlation coefficient?",
    "section": "Two other examples",
    "text": "Two other examples\n\nA weak positive correlation: r = 0.1\n\nplot_r(r = 0.1, n = 50)\n\n\n\n\n\n\nA strong negative correlation: r = -0.9\n\nplot_r(r = -0.9, n = 50)"
  },
  {
    "objectID": "posts/2016-11-21-what-correlations-look-like/index.html#storing-the-data",
    "href": "posts/2016-11-21-what-correlations-look-like/index.html#storing-the-data",
    "title": "What data patterns can lie behind a correlation coefficient?",
    "section": "Storing the data",
    "text": "Storing the data\nIf you want to store the data underlying one or all of the plots, you can set the optional showdata parameter to either all or a number between 1 and 16 corresponding to one of the plots:\n\n# Show data for plot 11 (not run)\nplot_r(r = 0.4, n = 10, showdata = 11)\n# Show data for all plots (not run)\nplot_r(r = 0.8, n = 15, showdata = \"all\")\n\n\nI hope the plot_r() function helps you to develop a feel for what correlation coefficients may actually represent and that this post may convince more researchers to draw scatterplots before running any correlation analyses—or regression analyses, for that matter—and to actually show them when reporting correlations."
  },
  {
    "objectID": "posts/2016-11-21-what-correlations-look-like/index.html#software-versions",
    "href": "posts/2016-11-21-what-correlations-look-like/index.html#software-versions",
    "title": "What data patterns can lie behind a correlation coefficient?",
    "section": "Software versions",
    "text": "Software versions\n\ndevtools::session_info()\n\n─ Session info ───────────────────────────────────────────────────────────────\n setting  value\n version  R version 4.3.1 (2023-06-16 ucrt)\n os       Windows 10 x64 (build 18363)\n system   x86_64, mingw32\n ui       RTerm\n language (EN)\n collate  English_United Kingdom.utf8\n ctype    English_United Kingdom.utf8\n tz       Europe/Zurich\n date     2023-08-08\n pandoc   3.1.1 @ C:/Program Files/RStudio/resources/app/bin/quarto/bin/tools/ (via rmarkdown)\n\n─ Packages ───────────────────────────────────────────────────────────────────\n package     * version date (UTC) lib source\n cachem        1.0.8   2023-05-01 [1] CRAN (R 4.3.1)\n callr         3.7.3   2022-11-02 [1] CRAN (R 4.3.1)\n cli           3.6.1   2023-03-23 [1] CRAN (R 4.3.1)\n crayon        1.5.2   2022-09-29 [1] CRAN (R 4.3.1)\n devtools      2.4.5   2022-10-11 [1] CRAN (R 4.3.1)\n digest        0.6.33  2023-07-07 [1] CRAN (R 4.3.1)\n ellipsis      0.3.2   2021-04-29 [1] CRAN (R 4.3.1)\n evaluate      0.21    2023-05-05 [1] CRAN (R 4.3.1)\n fastmap       1.1.1   2023-02-24 [1] CRAN (R 4.3.1)\n fs            1.6.3   2023-07-20 [1] CRAN (R 4.3.1)\n glue          1.6.2   2022-02-24 [1] CRAN (R 4.3.1)\n htmltools     0.5.5   2023-03-23 [1] CRAN (R 4.3.1)\n htmlwidgets   1.6.2   2023-03-17 [1] CRAN (R 4.3.1)\n httpuv        1.6.11  2023-05-11 [1] CRAN (R 4.3.1)\n jsonlite      1.8.7   2023-06-29 [1] CRAN (R 4.3.1)\n knitr         1.43    2023-05-25 [1] CRAN (R 4.3.1)\n later         1.3.1   2023-05-02 [1] CRAN (R 4.3.1)\n lifecycle     1.0.3   2022-10-07 [1] CRAN (R 4.3.1)\n magrittr      2.0.3   2022-03-30 [1] CRAN (R 4.3.1)\n memoise       2.0.1   2021-11-26 [1] CRAN (R 4.3.1)\n mime          0.12    2021-09-28 [1] CRAN (R 4.3.0)\n miniUI        0.1.1.1 2018-05-18 [1] CRAN (R 4.3.1)\n pkgbuild      1.4.2   2023-06-26 [1] CRAN (R 4.3.1)\n pkgload       1.3.2.1 2023-07-08 [1] CRAN (R 4.3.1)\n prettyunits   1.1.1   2020-01-24 [1] CRAN (R 4.3.1)\n processx      3.8.2   2023-06-30 [1] CRAN (R 4.3.1)\n profvis       0.3.8   2023-05-02 [1] CRAN (R 4.3.1)\n promises      1.2.0.1 2021-02-11 [1] CRAN (R 4.3.1)\n ps            1.7.5   2023-04-18 [1] CRAN (R 4.3.1)\n purrr         1.0.1   2023-01-10 [1] CRAN (R 4.3.1)\n R6            2.5.1   2021-08-19 [1] CRAN (R 4.3.1)\n Rcpp          1.0.11  2023-07-06 [1] CRAN (R 4.3.1)\n remotes       2.4.2.1 2023-07-18 [1] CRAN (R 4.3.1)\n rlang         1.1.1   2023-04-28 [1] CRAN (R 4.3.1)\n rmarkdown     2.23    2023-07-01 [1] CRAN (R 4.3.1)\n rstudioapi    0.15.0  2023-07-07 [1] CRAN (R 4.3.1)\n sessioninfo   1.2.2   2021-12-06 [1] CRAN (R 4.3.1)\n shiny         1.7.4.1 2023-07-06 [1] CRAN (R 4.3.1)\n stringi       1.7.12  2023-01-11 [1] CRAN (R 4.3.0)\n stringr       1.5.0   2022-12-02 [1] CRAN (R 4.3.1)\n urlchecker    1.0.1   2021-11-30 [1] CRAN (R 4.3.1)\n usethis       2.2.2   2023-07-06 [1] CRAN (R 4.3.1)\n vctrs         0.6.3   2023-06-14 [1] CRAN (R 4.3.1)\n xfun          0.39    2023-04-20 [1] CRAN (R 4.3.1)\n xtable        1.8-4   2019-04-21 [1] CRAN (R 4.3.1)\n yaml          2.3.7   2023-01-23 [1] CRAN (R 4.3.0)\n\n [1] C:/Users/VanhoveJ/AppData/Local/R/win-library/4.3\n [2] C:/Program Files/R/R-4.3.1/library\n\n──────────────────────────────────────────────────────────────────────────────"
  },
  {
    "objectID": "posts/2020-02-18-measurement-error/index.html",
    "href": "posts/2020-02-18-measurement-error/index.html",
    "title": "Baby steps in Bayes: Incorporating reliability estimates in regression models",
    "section": "",
    "text": "Researchers sometimes calculate reliability indices such as Cronbach’s \\(\\alpha\\) or Revelle’s \\(\\omega_T\\), but their statistical models rarely take these reliability indices into account. Here I want to show you how you can incorporate information about the reliability about your measurements in a statistical model so as to obtain more honest and more readily interpretable parameter estimates."
  },
  {
    "objectID": "posts/2020-02-18-measurement-error/index.html#reliability-and-measurement-error",
    "href": "posts/2020-02-18-measurement-error/index.html#reliability-and-measurement-error",
    "title": "Baby steps in Bayes: Incorporating reliability estimates in regression models",
    "section": "Reliability and measurement error",
    "text": "Reliability and measurement error\nThis blog post is the sequel to the previous one, where I demonstrated how imperfectly measured control variables undercorrect for the actual confounding in observational studies (also see Berthele & Vanhove 2017; Brunner & Austin 2009; Westfall & Yarkoni 2016). A model that doesn’t account for measurement error on the confounding variable—and hence implicitly assumes that the confound was measured perfectly—may confidently conclude that the variable of actual interest is related to the outcome even when taking into account the confound. From such a finding, researchers typically infer that the variable of actual interest is causally related to the outcome even in absence of the confound. But once this measurement error is duly accounted for, you may find that the evidence for a causal link between the variable of interest and the outcome is more tenuous than originally believed.\nSo especially in observational studies, where confounds abound, it behooves researchers to account for the measurement error in their variables so that they don’t draw unwarranted conclusions from their data too often. The amount of measurement error on your variables is usually unknown. But if you’ve calculated some reliability estimate such as Cronbach’s \\(\\alpha\\) for your variables, you can use this to obtain an estimate of the amount of measurement error.\nTo elaborate, in classical test theory, the reliability \\(\\rho_{XX'}\\) of a measure is equal to the ratio of the variance of the (error-free) true scores to the variance of the observed scores. The latter is the sum of the variance of the true scores and the error variance:\n\\[\n\\rho_{XX'} = \\frac{\\textrm{true score variance}}{\\textrm{true score variance} + \\textrm{measurement error variance}} = \\frac{\\sigma^2_T}{\\sigma^2_T + \\sigma^2_E}.\n\\]\nRearranging, we get\n\\[\n\\sigma^2_E = \\sigma^2_T\\left(\\frac{1}{\\rho_{XX'}}-1\\right).\n\\]\nNone of these values are known, but they can be estimated based on the sample. Specifically, \\(\\rho_{XX'}\\) can be estimated by a reliability index such as Cronbach’s \\(\\alpha\\) and the sum \\(\\sigma^2_T + \\sigma^2_E\\) can be estimated by computing the variable’s sample variance."
  },
  {
    "objectID": "posts/2020-02-18-measurement-error/index.html#a-simulated-example",
    "href": "posts/2020-02-18-measurement-error/index.html#a-simulated-example",
    "title": "Baby steps in Bayes: Incorporating reliability estimates in regression models",
    "section": "A simulated example",
    "text": "A simulated example\nLet’s first deal with a simulated dataset. The main advantage of analysing simulated data is that you check that what comes out of the model corresponds to what went into the data. In preparing this blog post, I was able to detect an arithmetic error in my model code in this way as one parameter was consistently underestimated. Had I applied the model immediately to the real data set, I wouldn’t have noticed anything wrong. But we’ll deal with real data afterwards.\nUpdate (2023-08-06): When converting this blog from Jekyll/Bootstrap to Quarto, I noticed that the original code used in this blog post, which involved the R package rstan has started to run very slowly. In the present version, I use cmdstanr instead.\nRun these commands to follow along:\n\nlibrary(cmdstanr)  # for fitting Bayesian models, v. 2.32.2\nlibrary(posterior) # for working with posterior distributions\n\n# For drawing scatterplot matrices\nsource(\"https://janhove.github.io/RCode/scatterplot_matrix.R\")\n\n# Set random seed for reproducibility\nset.seed(2020-02-13, kind = \"Mersenne-Twister\")\n\n\nGenerating the construct scores\nThe scenario we’re going to simulate is one in which you have two correlated predictor variables (A and B) and one outcome variable (Z). Unbeknownst to the analyst, Z is causally affected by A but not by B. Moreover, the three variables are measured with some degree of error, but we’ll come to that later. Figure 1 depicts the scenario for which we’re going to simulate data.\n\n\n\n\n\nFigure 1. Graphical causal model for the simulated data. A set of unobserved factors \\(U\\) gives rise to a correlation between A and B. Even though only A causally affects Z, an association between B and Z will be found unless A is controlled for.\n\n\n\n\nThe first thing we need are two correlated predictor variables. I’m going to generate these from a bivariate normal distribution. A has a mean of 3 units and a standard deviation of 1.5 units, and B has mean -4 and standard deviation 0.8 units. The correlation between them is \\(\\rho = 0.73\\). To generate a sample from this bivariate normal distribution, you need to construct the variance-covariance matrix from the standard deviations and correlation, which I do in the code below:\n\n# Generate correlated constructs\nn &lt;- 300\nrho &lt;- 0.73\nmean_A &lt;- 3\nmean_B &lt;- -4\nsd_A &lt;- 1.5\nsd_B &lt;- 0.8\n\n# Given the correlation and the standard deviations,\n# construct the variance-covariance matrix for the constructs like so:\nlatent_covariance_matrix &lt;- rbind(c(sd_A, 0), c(0, sd_B)) %*%\n  rbind(c(1, rho), c(rho, 1)) %*%\n  rbind(c(sd_A, 0), c(0, sd_B))\n\n# Draw data from the multivariate normal distribution:\nconstructs &lt;- MASS::mvrnorm(n = n, mu = c(mean_A, mean_B),\n                            Sigma = latent_covariance_matrix)\n\n# Extract variables from object\nA &lt;- constructs[, 1]\nB &lt;- constructs[, 2]\n\nNext, we need to generate the outcome. In this simulation, Z depends linearly on A but not on B (hence ‘\\(0 \\times B_i\\)’).\n\\[\nZ_i = 2 + 0.7 \\times A_i + 0 \\times B_i + \\varepsilon_i.\n\\]\nThe error term \\(\\varepsilon\\) is drawn from a normal distribution with standard deviation 1.3. Importantly, this error term does not express the measurement error on Z; it is the part of the true score variance in Z that isn’t related to either A or B:\n\\[\n\\varepsilon_i \\sim \\textrm{Normal}(0, 1.3).\n\\]\n\n# Create Z\nintercept &lt;- 2\nslope_A &lt;- 0.7\nslope_B &lt;- 0\nsigma_Z.AB &lt;- 1.3\nZ &lt;- intercept + slope_A*A + slope_B*B + rnorm(n, sd = sigma_Z.AB)\n\nEven though B isn’t causally related to Z, we find that B and Z are correlated thanks to B’s correlation to A.\n\nscatterplot_matrix(cbind(Z, A, B))\n\n\n\n\nFigure 2. B is causally unrelated to Z, but these two variables are still correlated because B correlates with A and A and Z are causally related.\n\n\n\n\nA multiple regression model is able to tease apart the effects of A and B on Z:\n\nsummary(lm(Z ~ A + B))$coefficients\n\n            Estimate Std. Error t value Pr(&gt;|t|)\n(Intercept)   2.1224     0.7130   2.977 3.15e-03\nA             0.6876     0.0706   9.742 1.22e-19\nB             0.0196     0.1340   0.146 8.84e-01\n\n\nThe confound A is significantly related to Z and its estimated regression parameter is close to its true value of 0.70 (\\(\\widehat{\\beta}_A = 0.69 \\pm 0.07\\)). The variable of interest B, by contrast, isn’t significantly related to Z when A has been accounted for (\\(\\widehat{\\beta}_B = 0.02 \\pm 0.13\\)). This is all as it should be.\n\n\nAdding measurement error\nNow let’s add measurement error to all variables. The A values that we actually observe will then be distorted versions of the true A values:\n\\[\n\\textrm{observed } A_i = \\textrm{true } A_i + \\varepsilon_{Ai}.\n\\]\nThe ‘noise’ \\(\\varepsilon_{A}\\) is commonly assumed to be normally distributed:\n\\[\n\\varepsilon_{Ai} \\sim \\textrm{Normal}(0, \\tau_A).\n\\]\nThat said, you can easily imagine situations where the noise likely has a different distribution. For instance, when measurements are measured to the nearest integer (e.g., body weights in kilograms), the noise is likely uniformly distributed (e.g., a reported body weight of 66 kg means that the true body weight lies between 65.5 and 66.5 kg).\nTo make the link with the analysis more transparant, I will express the amount of noise in terms of the variables’ reliabilities. For the confound A, I set the reliability at 0.70. Since A’s standard deviation was 1.5 units, this means that the standard deviation of the noise is \\(\\sqrt{1.5^2\\left(\\frac{1}{0.70} - 1\\right)} = 0.98\\) units. I set the reliability for B, the variable of actual interest at 0.90. Its standard deviation is 0.8, so the standard deviation of the noise is \\(\\sqrt{0.8^2\\left(\\frac{1}{0.90} - 1\\right)} = 0.27\\) units.\n\n# Add measurement error on A and B\nobs_A &lt;- A + rnorm(n = n, sd = sqrt(sd_A^2*(1/0.70 - 1))) # reliability 0.70\nobs_B &lt;- B + rnorm(n = n, sd = sqrt(sd_B^2*(1/0.90 - 1))) # reliability 0.90\n\nThe same logic applies to adding measurement noise to Z. The difficulty here lies in obtaining the population standard deviation (or variance) of Z. I don’t want to just plug in Z’s sample standard deviation since I want to have exact knowledge of the population parameters. While we specified a sigma_Z.AB above, this is not the total population standard deviation of Z: it’s the population standard deviation of Z once Z has been controlled for A and B. To obtain the total standard deviation of Z (here admittedly confusingly labelled \\(\\sigma_Z\\)), we need to add in the variance in Z due to A and B:\n\\[\n\\sigma_Z = \\sqrt{(\\beta_A\\sigma_A)^2 + (\\beta_B\\sigma_B)^2 + 2\\beta_A\\beta_B\\sigma_A\\sigma_B\\rho_{AB} + \\sigma_{Z.AB}^2}.\n\\]\nSince \\(\\beta_B = 0\\), this simplifies to \\(\\sigma_Z = \\sqrt{(\\beta_A\\sigma_A)^2 + \\sigma_{Z.AB}^2}\\), but if you want to simulate your own datasets, the full formula may be useful.\nThe population standard deviation of Z is thus \\(\\sqrt{(0.7 \\times 1.5)^2 + 1.3^2} = 1.67\\). Setting Z’s reliability to 0.70, we find that the standard deviation of the noise is \\(\\sqrt{1.67^2\\left(\\frac{1}{0.70} - 1\\right)} = 1.09\\).\n\n# Measurement error on Z\nsd_Z &lt;- sqrt((slope_A*sd_A)^2 + (0*slope_B)^2 + 2*(slope_A * slope_B * latent_covariance_matrix[1,2]) + sigma_Z.AB^2)\nobs_Z &lt;- Z + rnorm(n = n, sd = sqrt(sd_Z^2*(1/0.70 - 1))) # reliability 0.70\n\nFigure 3 shows the causal diagram for the actually observed simulated data.\n\n\n\n\n\nFigure 3. Graphical causal model for the simulated data. A, B and Z are now unobserved (latent) variables, but so-called ‘descendants’ of them were measured. These reflect their latent constructs imperfectly. Crucially, controlling for obs_A will not entirely control for the confound: the path between obs_B and obs_Z stays open.\n\n\n\n\nAs Figure 4 shows, the observed variables are all correlated with each other.\n\nscatterplot_matrix(cbind(obs_Z, obs_A, obs_B))\n\n\n\n\nFigure 4.\n\n\n\n\nCrucially, controlling for obs_A in a regression model doesn’t entirely eradicate the confound and we find that obs_B is significantly related to obs_Z even after controlling for obs_A (\\(\\widehat{\\beta}_{B_{\\textrm{obs}}} = 0.40 \\pm 0.15\\)). Moreover, the regression model on the observed variables underestimates the strength of the relationship between the true construct scores (\\(\\widehat{\\beta}_{A_{\\textrm{obs}}} = 0.40 \\pm 0.07\\), whereas \\(\\beta_A = 0.7\\)).\n\nsummary(lm(obs_Z ~ obs_A + obs_B))$coefficients\n\n            Estimate Std. Error t value Pr(&gt;|t|)\n(Intercept)    4.498     0.7588    5.93 8.49e-09\nobs_A          0.402     0.0737    5.45 1.05e-07\nobs_B          0.404     0.1470    2.75 6.41e-03\n\n\n\n\nStatistical model\nThe statistical model, written below in Stan code, corresponds to the data generating mechanism above and tries to infer its parameters from the observed data and some prior information.\nThe data block specifies the input that the model should handle. I think this is self-explanatory. Note that the latent variable scores A, B and Z aren’t part of the input as we wouldn’t have directly observed these.\nThe parameters block first defines the parameters needed for the regression model with the unobserved latent variables (the one we used to generate Z). It then defines the parameters needed to generate the true variables scores for A and B as well as the parameters needed to generate the observed scores from the true scores (viz., the true scores themselves and the reliabilities). Note that it is crucial to allow the model to estimate a correlation between A and B, otherwise it won’t ‘know’ that A confounds the B-Z relationship.\nThe transformed parameters block contains, well, transformations of these parameters. For instance, the standard deviations of A and B and the correlation between A and B are used to generate a variance-covariance matrix. Moreover, the standard deviations of the measurement noise are computed using the latent variables’ standard deviation and their reliabilities.\nThe model block, finally, specifies how we think the observed data and the (transformed or untransformed) parameters fit together and what plausible a priori values for the (transformed or untransformed) parameters are. These prior distributions are pretty abstract in this example: we generated context-free data ourselves, so it’s not clear what motivates these priors. The real example to follow will hopefully make more sense in this respect. You’ll notice that I’ve also specified a prior for the reliabilities. The reason is that you typically don’t know the reliability of an observed variable with perfect precision but that you have sample estimate with some inherent uncertainty. The priors reflect this uncertainty. Again, this will become clearer in the real example to follow.\n\nmeas_error_code &lt;- '\ndata {\n  // Number of observations\n  int&lt;lower = 1&gt; N;\n\n  // Observed outcome\n  vector[N] obs_Z;\n\n  // Observed predictors\n  vector[N] obs_A;\n  vector[N] obs_B;\n}\n\nparameters {\n  // Parameters for regression\n  real intercept;\n  real slope_A;\n  real slope_B;\n  real&lt;lower = 0&gt; sigma;\n\n  // Latent predictors (= constructs):\n  // standard deviations and means\n  real&lt;lower = 0&gt; sigma_lat_A;\n  real&lt;lower = 0&gt; sigma_lat_B;\n  row_vector[2] latent_means;\n\n  // Correlation between latent predictors\n  real&lt;lower = -1, upper = 1&gt; latent_rho;\n\n  // Latent variables (true scores)\n  matrix[N, 2] latent_predictors;\n  vector[N] lat_Z; // latent outcome\n\n  // Unknown but estimated reliabilities\n  real&lt;lower = 0, upper = 1&gt; reliability_A;\n  real&lt;lower = 0, upper = 1&gt; reliability_B;\n  real&lt;lower = 0, upper = 1&gt; reliability_Z;\n}\n\ntransformed parameters {\n  vector[N] mu_Z;  // conditional mean of outcome\n  vector[N] lat_A; // latent variables, separated out\n  vector[N] lat_B;\n\n  real error_A; // standard error of measurement\n  real error_B;\n  real error_Z;\n\n  // standard deviations of latent predictors, in matrix form\n  matrix[2, 2] sigma_lat;\n  // correlation and covariance matrix for latent predictors\n  cov_matrix[2] latent_cor;\n  cov_matrix[2] latent_cov;\n\n  // standard deviation of latent outcome\n  real&lt;lower = 0&gt; sigma_lat_Z;\n  sigma_lat_Z = sd(lat_Z);\n\n  // Express measurement error in terms of\n  // standard deviation of constructs and reliability\n  error_A = sqrt(sigma_lat_A^2*(1/reliability_A - 1));\n  error_B = sqrt(sigma_lat_B^2*(1/reliability_B - 1));\n  error_Z = sqrt(sigma_lat_Z^2*(1/reliability_Z - 1));\n\n  // Define diagonal matrix with standard errors of latent variables\n  sigma_lat[1, 1] = sigma_lat_A;\n  sigma_lat[2, 2] = sigma_lat_B;\n  sigma_lat[1, 2] = 0;\n  sigma_lat[2, 1] = 0;\n\n  // Define correlation matrix for latent variables\n  latent_cor[1, 1] = 1;\n  latent_cor[2, 2] = 1;\n  latent_cor[1, 2] = latent_rho;\n  latent_cor[2, 1] = latent_rho;\n\n  // Compute covariance matrix for latent variables\n  latent_cov = sigma_lat * latent_cor * sigma_lat;\n\n  // Extract latent variables from matrix\n  lat_A = latent_predictors[, 1];\n  lat_B = latent_predictors[, 2];\n\n  // Regression model for conditional mean of Z\n  mu_Z = intercept + slope_A*lat_A + slope_B*lat_B;\n}\n\nmodel {\n  // Priors for regression parameters\n  intercept ~ normal(0, 2);\n  slope_A ~ normal(0, 2);\n  slope_B ~ normal(0, 2);\n  sigma ~ normal(0, 2);\n\n  // Prior for latent standard deviations\n  sigma_lat_A ~ normal(0, 2);\n  sigma_lat_B ~ normal(0, 2);\n\n  // Prior for latent means\n  latent_means ~ normal(0, 3);\n\n  // Prior expectation for correlation between latent variables.\n  // Tend towards positive correlation, but pretty vague.\n  latent_rho ~ normal(0.4, 0.3);\n\n  // Prior for reliabilities.\n  // These are estimated with some uncertainty, i.e.,\n  // they are not point values but distributions.\n  reliability_A ~ beta(70, 30);\n  reliability_B ~ beta(90, 10);\n  reliability_Z ~ beta(70, 30);\n\n  // Distribution of latent variable\n  for (i in 1:N) {\n    latent_predictors[i, ] ~ multi_normal(latent_means, latent_cov);\n  }\n\n  // Generate latent outcome\n  lat_Z ~ normal(mu_Z, sigma);\n\n  // Add noise to latent variables\n  obs_A ~ normal(lat_A, error_A);\n  obs_B ~ normal(lat_B, error_B);\n  obs_Z ~ normal(lat_Z, error_Z);\n}\n\n'\n\nPut the data in a list and fit the model:\n\ndata_list &lt;- list(\n  obs_Z = obs_Z,\n  obs_A = obs_A,\n  obs_B = obs_B,\n  N = n\n)\n\n\nmeas_error_model &lt;- cmdstan_model(write_stan_file(meas_error_code))\nmodel_fit &lt;- meas_error_model$sample(\n  data = data_list\n  , seed = 123\n  , chains = 4\n  , parallel_chains = 4\n  , iter_warmup = 1000\n  , iter_sampling = 1000\n  , refresh = 500\n  , max_treedepth = 15\n  , adapt_delta = 0.99\n)\n\nRunning MCMC with 4 parallel chains...\n\nChain 1 Iteration:    1 / 2000 [  0%]  (Warmup) \nChain 2 Iteration:    1 / 2000 [  0%]  (Warmup) \nChain 3 Iteration:    1 / 2000 [  0%]  (Warmup) \nChain 4 Iteration:    1 / 2000 [  0%]  (Warmup) \nChain 4 Iteration:  500 / 2000 [ 25%]  (Warmup) \nChain 3 Iteration:  500 / 2000 [ 25%]  (Warmup) \nChain 1 Iteration:  500 / 2000 [ 25%]  (Warmup) \nChain 2 Iteration:  500 / 2000 [ 25%]  (Warmup) \nChain 4 Iteration: 1000 / 2000 [ 50%]  (Warmup) \nChain 4 Iteration: 1001 / 2000 [ 50%]  (Sampling) \nChain 3 Iteration: 1000 / 2000 [ 50%]  (Warmup) \nChain 3 Iteration: 1001 / 2000 [ 50%]  (Sampling) \nChain 2 Iteration: 1000 / 2000 [ 50%]  (Warmup) \nChain 2 Iteration: 1001 / 2000 [ 50%]  (Sampling) \nChain 1 Iteration: 1000 / 2000 [ 50%]  (Warmup) \nChain 1 Iteration: 1001 / 2000 [ 50%]  (Sampling) \nChain 2 Iteration: 1500 / 2000 [ 75%]  (Sampling) \nChain 4 Iteration: 1500 / 2000 [ 75%]  (Sampling) \nChain 3 Iteration: 1500 / 2000 [ 75%]  (Sampling) \nChain 2 Iteration: 2000 / 2000 [100%]  (Sampling) \nChain 2 finished in 279.7 seconds.\nChain 1 Iteration: 1500 / 2000 [ 75%]  (Sampling) \nChain 4 Iteration: 2000 / 2000 [100%]  (Sampling) \nChain 4 finished in 320.0 seconds.\nChain 3 Iteration: 2000 / 2000 [100%]  (Sampling) \nChain 3 finished in 355.9 seconds.\nChain 1 Iteration: 2000 / 2000 [100%]  (Sampling) \nChain 1 finished in 369.9 seconds.\n\nAll 4 chains finished successfully.\nMean chain execution time: 331.4 seconds.\nTotal execution time: 370.0 seconds.\n\n\n\n\nResults\n\n\n\n\nmodel_fit$summary(\n  variables = c(\"intercept\", \"slope_A\", \"slope_B\", \"sigma\"\n                ,\"sigma_lat_A\", \"sigma_lat_B\", \"sigma_lat_Z\"\n                ,\"latent_means\", \"latent_rho\"\n                , \"reliability_A\", \"reliability_B\", \"reliability_Z\"\n                ,\"error_A\", \"error_B\", \"error_Z\")\n  , \"mean\", \"sd\"\n  , extra_quantiles = ~posterior::quantile2(., probs = c(0.025, 0.975))\n  , \"rhat\"\n)\n\n# A tibble: 16 × 6\n   variable          mean     sd   q2.5  q97.5  rhat\n   &lt;chr&gt;            &lt;num&gt;  &lt;num&gt;  &lt;num&gt;  &lt;num&gt; &lt;num&gt;\n 1 intercept        0.649 1.49   -2.43   3.37   1.02\n 2 slope_A          0.866 0.170   0.564  1.22   1.02\n 3 slope_B         -0.200 0.254  -0.719  0.265  1.02\n 4 sigma            1.17  0.126   0.903  1.40   1.01\n 5 sigma_lat_A      1.40  0.0716  1.26   1.54   1.00\n 6 sigma_lat_B      0.797 0.0353  0.731  0.869  1.00\n 7 sigma_lat_Z      1.59  0.0702  1.45   1.72   1.01\n 8 latent_means[1]  3.07  0.0972  2.87   3.26   1.00\n 9 latent_means[2] -4.02  0.0486 -4.11  -3.92   1.00\n10 latent_rho       0.780 0.0493  0.678  0.871  1.01\n11 reliability_A    0.694 0.0408  0.616  0.773  1.02\n12 reliability_B    0.901 0.0277  0.843  0.947  1.00\n13 reliability_Z    0.699 0.0466  0.600  0.782  1.01\n14 error_A          0.925 0.0710  0.785  1.06   1.01\n15 error_B          0.262 0.0382  0.191  0.338  1.01\n16 error_Z          1.04  0.0872  0.880  1.22   1.01\n\n\nThe model recovers the true parameter values pretty well (Table 1) and, on the basis of this model, you wouldn’t erroneously conclude that B is causally related to Z (see the parameter estimate for slope_B).\n\nTable 1. Comparison of the true parameter values and their estimates. Parameters prefixed with ‘*’ are transformations of other parameters. The estimated reliability parameters aren’t really estimated from the data: they just reflect the prior distributions put on them.\n\n\nParameter\nTrue value\nEstimate\n\n\n\n\nintercept\n2.00\n0.65 ± 1.53\n\n\nslope_A\n0.70\n0.87 ± 0.17\n\n\nslope_B\n0.00\n-0.20 ± 0.25\n\n\nsigma_Z.AB\n1.30\n1.17 ± 0.13\n\n\nsd_A\n1.50\n1.40 ± 0.07\n\n\nsd_B\n0.80\n0.80 ± 0.04\n\n\nmean_A\n3.00\n3.07 ± 0.10\n\n\nmean_B\n-4.00\n-4.02 ± 0.05\n\n\nrho\n0.73\n0.78 ± 0.05\n\n\nreliability_A\n0.70\n0.69 ± 0.04\n\n\nreliability_B\n0.90\n0.90 ± 0.03\n\n\nreliability_Z\n0.70\n0.70 ± 0.05\n\n\n*sd_Z\n1.67\n1.59 ± 0.07\n\n\n*error_A\n0.98\n0.93 ± 0.07\n\n\n*error_B\n0.27\n0.26 ± 0.04\n\n\n*error_Z\n1.09\n1.04 ± 0.09\n\n\n\nIn the previous blog post, I’ve shown that such a model also estimates the latent true variable scores and that these estimates correspond more closely to the actual true variable scores than do the observed variable scores. I’ll skip this step here."
  },
  {
    "objectID": "posts/2020-02-18-measurement-error/index.html#real-life-example-interdependence",
    "href": "posts/2020-02-18-measurement-error/index.html#real-life-example-interdependence",
    "title": "Baby steps in Bayes: Incorporating reliability estimates in regression models",
    "section": "Real-life example: Interdependence",
    "text": "Real-life example: Interdependence\nSatisfied that our model can recover the actual parameter values in scenarios such as those depicted in Figure 3, we now turn to a real-life example of such a situation. The example was already described in the previous blog post; here I’ll just draw the causal model that assumes that reflects the null hypothesis that a child’s Portuguese skills at T2 (PT.T2) don’t contribute to their French skills at T3 (FR.T3), but that due to common factors such as intelligence, form on the day etc. (\\(U\\)), French skills and Portuguese skills at T2 are correlated across children. What is observed are test scores, not the children’s actual skills.\n\n\n\n\n\nFigure 5\n\n\n\n\n\nData\nThe command below is pig-ugly, but allows you to easily read in the data.\n\nskills &lt;- structure(list(\n  Subject = c(\"A_PLF_1\",\"A_PLF_10\",\"A_PLF_12\",\"A_PLF_13\",\"A_PLF_14\",\"A_PLF_15\",\"A_PLF_16\",\"A_PLF_17\",\"A_PLF_19\",\"A_PLF_2\",\"A_PLF_3\",\"A_PLF_4\",\"A_PLF_5\",\"A_PLF_7\",\"A_PLF_8\",\"A_PLF_9\",\"AA_PLF_11\",\"AA_PLF_12\",\"AA_PLF_13\",\"AA_PLF_6\",\"AA_PLF_7\",\"AA_PLF_8\",\"AD_PLF_10\",\"AD_PLF_11\",\"AD_PLF_13\",\"AD_PLF_14\",\"AD_PLF_15\",\"AD_PLF_16\",\"AD_PLF_17\",\"AD_PLF_18\",\"AD_PLF_19\",\"AD_PLF_2\",\"AD_PLF_20\",\"AD_PLF_21\",\"AD_PLF_22\",\"AD_PLF_24\",\"AD_PLF_25\",\"AD_PLF_26\",\"AD_PLF_4\",\"AD_PLF_6\",\"AD_PLF_8\",\"AD_PLF_9\",\"AE_PLF_1\",\"AE_PLF_2\",\"AE_PLF_4\",\"AE_PLF_5\",\"AE_PLF_6\",\"C_PLF_1\",\"C_PLF_16\",\"C_PLF_19\",\"C_PLF_30\",\"D_PLF_1\",\"D_PLF_2\",\"D_PLF_3\",\"D_PLF_4\",\"D_PLF_5\",\"D_PLF_6\",\"D_PLF_7\",\"D_PLF_8\",\"Y_PNF_12\",\"Y_PNF_15\",\"Y_PNF_16\",\"Y_PNF_17\",\"Y_PNF_18\",\"Y_PNF_2\",\"Y_PNF_20\",\"Y_PNF_24\",\"Y_PNF_25\",\"Y_PNF_26\",\"Y_PNF_27\",\"Y_PNF_28\",\"Y_PNF_29\",\"Y_PNF_3\",\"Y_PNF_31\",\"Y_PNF_32\",\"Y_PNF_33\",\"Y_PNF_34\",\"Y_PNF_36\",\"Y_PNF_4\",\"Y_PNF_5\",\"Y_PNF_6\",\"Y_PNF_7\",\"Y_PNF_8\",\"Y_PNF_9\",\"Z_PLF_2\",\"Z_PLF_3\",\"Z_PLF_4\",\"Z_PLF_5\",\"Z_PLF_6\",\"Z_PLF_7\",\"Z_PLF_8\")\n  , FR_T2 = c(0.6842105263,0.4736842105,1,0.4210526316,0.6842105263,0.6842105263,0.8947368421,0.5789473684,0.7368421053,0.7894736842,0.4210526316,0.5263157895,0.3157894737,0.5263157895,0.6842105263,0.8421052632,0.3684210526,0.8421052632,0.7894736842,0.7894736842,0.6842105263,0.6315789474,0.6315789474,0.3684210526,0.4736842105,0.2631578947,0.4736842105,0.9473684211,0.3157894737,0.5789473684,0.2631578947,0.5263157895,0.5263157895,0.7368421053,0.6315789474,0.8947368421,0.6315789474,0.9473684211,0.7368421053,0.6315789474,0.7894736842,0.7894736842,0.4736842105,0.4736842105,0.9473684211,0.7894736842,0.3157894737,0.9473684211,1,0.7368421053,0.5789473684,0.8421052632,0.8421052632,0.7368421053,0.5789473684,0.6842105263,0.4736842105,0.4210526316,0.6842105263,0.8947368421,0.6842105263,0.7368421053,0.5263157895,0.5789473684,0.8947368421,0.7894736842,0.5263157895,0.6315789474,0.3157894737,0.7368421053,0.5789473684,0.6842105263,0.7368421053,0.5789473684,0.7894736842,0.6842105263,0.6315789474,0.6842105263,0.5789473684,0.7894736842,0.5789473684,0.7368421053,0.4736842105,0.8947368421,0.8421052632,0.7894736842,0.6315789474,0.6842105263,0.8947368421,0.6842105263,0.9473684211)\n  , PT_T2 = c(0.7368421053,0.5789473684,0.9473684211,0.5263157895,0.6315789474,0.5789473684,0.9473684211,0.4736842105,0.8421052632,0.5263157895,0.2631578947,0.6842105263,0.3684210526,0.3684210526,0.4736842105,0.8947368421,0.4210526316,0.5263157895,0.8947368421,0.8421052632,0.8947368421,0.8947368421,0.6315789474,0.3684210526,0.0526315789,0.3684210526,0.4210526316,0.9473684211,0.3157894737,0.4736842105,0.3157894737,0.5789473684,0.4736842105,0.7894736842,0.5263157895,0.8947368421,0.6315789474,0.7894736842,0.7368421053,0.5789473684,0.6842105263,0.7368421053,0.3684210526,0.7894736842,0.7368421053,0.4736842105,0.5263157895,1,0.8947368421,0.8947368421,0.4736842105,0.8421052632,1,0.6315789474,0.5263157895,0.5789473684,0.5789473684,0.5789473684,0.5263157895,0.9473684211,0.5263157895,0.6315789474,0.5789473684,0.6315789474,0.9473684211,0.7894736842,0.8421052632,0.5263157895,0.7894736842,0.4736842105,0.6842105263,0.3684210526,0.7894736842,0.7368421053,0.6315789474,0.9473684211,0.4210526316,0.5789473684,0.3684210526,0.8947368421,0.6315789474,0.8421052632,0.5789473684,0.5263157895,0.9473684211,0.8947368421,0.7368421053,0.4736842105,0.8421052632,0.7894736842,0.9473684211)\n  , FR_T3 = c(0.9473684211,0.3157894737,0.9473684211,0.5789473684,0.5789473684,0.6842105263,0.8421052632,0.6842105263,0.7368421053,0.8421052632,0.4210526316,0.5789473684,0.4736842105,0.6842105263,0.5789473684,0.7894736842,0.7368421053,0.7894736842,1,0.8421052632,0.8947368421,0.4210526316,0.8947368421,0.4736842105,0.5263157895,0.4736842105,0.5789473684,1,0.7368421053,0.8421052632,0.2631578947,0.7894736842,0.6842105263,0.8947368421,0.5263157895,0.8947368421,0.6842105263,0.9473684211,0.9473684211,0.5263157895,0.9473684211,0.8421052632,0.4736842105,0.8947368421,0.9473684211,0.7368421053,0.5263157895,0.8421052632,0.9473684211,0.7894736842,0.8947368421,0.8421052632,0.8421052632,0.8947368421,0.5789473684,0.7368421053,0.6842105263,0.4736842105,0.6842105263,0.8947368421,0.4736842105,0.8421052632,0.7894736842,0.5789473684,0.7368421053,0.7894736842,0.8947368421,0.6842105263,0.6842105263,0.9473684211,0.7894736842,0.5263157895,0.7368421053,0.6842105263,0.8421052632,0.7368421053,0.7368421053,0.5789473684,0.4736842105,0.8947368421,0.4210526316,0.8947368421,0.6842105263,1,0.8421052632,0.8421052632,0.6315789474,0.6315789474,0.8947368421,0.6315789474,0.9473684211)\n  , PT_T3 = c(0.8421052632,0.3684210526,0.9473684211,0.3157894737,0.5789473684,0.7894736842,1,0.5263157895,0.8421052632,0.7894736842,0.3157894737,0.6315789474,0.4210526316,0.5263157895,0.6842105263,0.8421052632,0.8947368421,0.6842105263,0.9473684211,0.8947368421,0.9473684211,0.8421052632,0.8421052632,0.5263157895,0.6842105263,0.5263157895,0.8421052632,0.9473684211,0.4210526316,0.7894736842,0.7894736842,0.8421052632,0.7368421053,1,0.6842105263,1,0.7894736842,0.8421052632,0.9473684211,0.6842105263,0.7894736842,0.7894736842,0.3157894737,0.7894736842,NA,0.6315789474,0.6842105263,0.9473684211,1,0.9473684211,0.7368421053,0.8947368421,0.8421052632,0.8421052632,0.5789473684,0.6315789474,0.6315789474,0.8421052632,0.7894736842,0.8421052632,0.5789473684,0.8421052632,0.7368421053,0.6842105263,0.8421052632,0.8421052632,0.9473684211,0.4736842105,0.8421052632,0.7894736842,0.7368421053,0.2105263158,0.7894736842,0.7894736842,0.7368421053,0.6315789474,0.6315789474,0.4210526316,0.6315789474,0.8421052632,0.6842105263,0.9473684211,0.5789473684,0.5263157895,0.7894736842,0.7894736842,0.7894736842,0.6842105263,0.8421052632,0.8421052632,0.8947368421)\n  )\n  , row.names = c(NA, -91L)\n  , class = c(\"tbl_df\",\"tbl\",\"data.frame\")\n)\n\n\n\nStatistical model\nThe only thing that’s changed in the statistical model compared to the example with the simulated data is that I’ve renamed the parameters and that the prior distributions are better motivated. Let’s consider each prior distribution in turn:\n\nintercept ~ normal(0.2, 0.1);: The intercept is the average true French skill score at T3 for children whose true French and Portuguese skill scores at T2 are 0. This is the lowest possible score (the theoretical range of the data is [0, 1]), so we’d expect such children to perform poorly at T3, too. A normal(0.2, 0.1) distribution puts 95% probability on such children having a true French score at T3 between 0 and 0.4.\nslope_FR ~ normal(0.5, 0.25);: This parameter expresses the difference between the average true French skill score at T3 for children with a true French skill score of 1 at T2 (the theoretical maximum) vs. those with a true French skill score of 0 at T2 (the theoretical minimum). This is obviously some value between -1 and 1, and presumably it’s going to be positive. A normal(0.5, 0.25) puts 95% probability on this difference lying between 0 and 1, which I think is reasonable.\nslope_PT ~ normal(0, 0.25);: The slope for Portuguese is bound to be smaller than the one for French. Moreover, it’s not a given that it will be appreciably different from zero. Hence a prior centred on 0 that still gives the data a chance to pull the estimate in either direction.\nsigma ~ normal(0.15, 0.08);: If neither of the T2 variables predicts T3, uncertainty is highest when the mean T3 score is 0.5. Since these scores are bounded between 0 and 1, the standard deviation could not be much higher than 0.20. But French T2 is bound to be a predictor, so let us choose a slightly lower value (0.15).\nlatent_means ~ normal(0.5, 0.1);: These are the prior expectations for the true score means of the T2 variables. 0.5 lies in the middle of the scale; the normal(0.5, 0.1) prior puts 95% probability on these means to lie between 0.3 and 0.7.\nsigma_lat_FR_T2 ~ normal(0, 0.25);, sigma_lat_FR_T2 ~ normal(0, 0.25);: The standard deviations of the latent T2 variables. If these truncated normal distributions put a 95% probability of the latent standard deviations to be lower than 0.50.\nlatent_rho ~ normal(0.4, 0.3);: The a priori expected correlation between the latent variables A and B. These are bound to be positively correlated.\nreliability_FR_T2 ~ beta(100, 100*0.27/0.73); The prior distribution for the reliability of the French T2 variable. Cronbach’s \\(\\alpha\\) for this variable was 0.73 (95% CI: [0.65, 0.78]). This roughly corresponds to a beta(100, 100*0.27/0.73) distribution:\n\n\nqbeta(c(0.025, 0.975), 100, 100*0.27/0.73)\n\n[1] 0.6529105 0.8007296\n\n\n\nreliability_PT_T2 ~ beta(120, 120*0.21/0.79); Similarly, Cronbach’s \\(\\alpha\\) for the Portuguese T2 variable was 0.79 (95% CI: [0.72, 0.84]), which roughly corresponds to a beta(120, 120*0.21/0.79) distribution:\n\n\nqbeta(c(0.025, 0.975), 120, 120*0.21/0.79)\n\n[1] 0.7219901 0.8507814\n\n\n\nreliability_FR_T3 ~ beta(73, 27);: The estimated reliability for the French T3 data was similar to that of the T2 data, so I used the same prior.\n\n\ninterdependence_code &lt;- '\ndata {\n  // Number of observations\n  int&lt;lower = 1&gt; N;\n\n  // Observed outcome\n  vector[N] FR_T3;\n\n  // Observed predictors\n  vector[N] FR_T2;\n  vector[N] PT_T2;\n}\n\nparameters {\n  // Parameters for regression\n  real intercept;\n  real slope_FR;\n  real slope_PT;\n  real&lt;lower = 0&gt; sigma;\n\n  // standard deviations of latent predictors (= constructs)\n  real&lt;lower = 0&gt; sigma_lat_FR_T2;\n  real&lt;lower = 0&gt; sigma_lat_PT_T2;\n  \n  // Means of latent predictors\n  row_vector[2] latent_means;\n\n  // Unknown correlation between latent predictors\n  real&lt;lower = -1, upper = 1&gt; latent_rho;\n\n  // Latent variables\n  matrix[N, 2] latent_predictors;\n  vector[N] lat_FR_T3; // latent outcome\n\n  // Unknown but estimated reliabilities\n  real&lt;lower = 0, upper = 1&gt; reliability_FR_T2;\n  real&lt;lower = 0, upper = 1&gt; reliability_PT_T2;\n  real&lt;lower = 0, upper = 1&gt; reliability_FR_T3;\n}\n\ntransformed parameters {\n  vector[N] mu_FR_T3;  // conditional mean of outcome\n  vector[N] lat_FR_T2; // latent variables, separated out\n  vector[N] lat_PT_T2;\n  real error_FR_T2; // standard error of measurement\n  real error_PT_T2;\n  real error_FR_T3;\n  \n  // standard deviations of latent predictors, in matrix form\n  matrix[2, 2] sigma_lat;\n  \n  // correlation and covariance matrix for latent predictors\n  cov_matrix[2] latent_cor;\n  cov_matrix[2] latent_cov;\n\n  // standard deviation of latent outcome\n  real&lt;lower = 0&gt; sigma_lat_FR_T3;\n  sigma_lat_FR_T3 = sd(lat_FR_T3);\n\n  // Express measurement error in terms of\n  // standard deviation of constructs and reliability\n  error_FR_T2 = sqrt(sigma_lat_FR_T2^2*(1/reliability_FR_T2 - 1));\n  error_PT_T2 = sqrt(sigma_lat_PT_T2^2*(1/reliability_PT_T2 - 1));\n  error_FR_T3 = sqrt(sigma_lat_FR_T3^2*(1/reliability_FR_T3 - 1));\n\n  // Define diagonal matrix with standard errors of latent variables\n  sigma_lat[1, 1] = sigma_lat_FR_T2;\n  sigma_lat[2, 2] = sigma_lat_PT_T2;\n  sigma_lat[1, 2] = 0;\n  sigma_lat[2, 1] = 0;\n\n  // Define correlation matrix for latent variables\n  latent_cor[1, 1] = 1;\n  latent_cor[2, 2] = 1;\n  latent_cor[1, 2] = latent_rho;\n  latent_cor[2, 1] = latent_rho;\n\n  // Compute covariance matrix for latent variables\n  latent_cov = sigma_lat * latent_cor * sigma_lat;\n\n  // Extract latent variables from matrix\n  lat_FR_T2 = latent_predictors[, 1];\n  lat_PT_T2 = latent_predictors[, 2];\n\n  // Regression model for conditional mean of Z\n  mu_FR_T3 = intercept + slope_FR*lat_FR_T2 + slope_PT*lat_PT_T2;\n}\n\nmodel { \n  // Priors for regression parameters \n  intercept ~ normal(0.2, 0.1); \n  slope_FR ~ normal(0.5, 0.25); \n  slope_PT ~ normal(0, 0.25); \n  sigma ~ normal(0.15, 0.08); \n\n  // Prior for latent means \n  latent_means ~ normal(0.5, 0.1); \n\n  // Prior for latent standard deviations \n  sigma_lat_FR_T2 ~ normal(0, 0.25); \n  sigma_lat_PT_T2 ~ normal(0, 0.25); \n\n  // Prior expectation for correlation between latent variables. \n  latent_rho ~ normal(0.4, 0.3); \n\n  // Prior for reliabilities. \n  // These are estimated with some uncertainty, i.e., \n  // they are not point values but distributions. \n  reliability_FR_T2 ~ beta(100, 100*0.27/0.73); \n  reliability_PT_T2 ~ beta(120, 120*0.21/0.79); \n  reliability_FR_T3 ~ beta(100, 100*0.27/0.73); \n\n  // Distribution of latent variable \n  for (i in 1:N) { \n    latent_predictors[i, ] ~ multi_normal(latent_means, latent_cov); \n  } \n\n  // Generate latent outcome \n  lat_FR_T3 ~ normal(mu_FR_T3, sigma); \n\n  // Measurement model \n  FR_T2 ~ normal(lat_FR_T2, error_FR_T2); \n  PT_T2 ~ normal(lat_PT_T2, error_PT_T2); \n  FR_T3 ~ normal(lat_FR_T3, error_FR_T3); \n} \n' \n\n\ndata_list &lt;- list(\n  FR_T2 = skills$FR_T2,\n  PT_T2 = skills$PT_T2,\n  FR_T3 = skills$FR_T3,\n  N = length(skills$FR_T3)\n)\n\ninterdependence_model &lt;- cmdstan_model(write_stan_file(interdependence_code))\ninterdependence_fit &lt;- interdependence_model$sample(\n  data = data_list\n  , seed = 42\n  , chains = 4\n  , parallel_chains = 4\n  , iter_warmup = 2000\n  , iter_sampling = 6000\n  , refresh = 1000\n  , max_treedepth = 15\n  , adapt_delta = 0.9999\n)\n\nRunning MCMC with 4 parallel chains...\n\nChain 1 Iteration:    1 / 8000 [  0%]  (Warmup) \nChain 2 Iteration:    1 / 8000 [  0%]  (Warmup) \nChain 3 Iteration:    1 / 8000 [  0%]  (Warmup) \nChain 4 Iteration:    1 / 8000 [  0%]  (Warmup) \nChain 1 Iteration: 1000 / 8000 [ 12%]  (Warmup) \nChain 4 Iteration: 1000 / 8000 [ 12%]  (Warmup) \nChain 2 Iteration: 1000 / 8000 [ 12%]  (Warmup) \nChain 3 Iteration: 1000 / 8000 [ 12%]  (Warmup) \nChain 1 Iteration: 2000 / 8000 [ 25%]  (Warmup) \nChain 1 Iteration: 2001 / 8000 [ 25%]  (Sampling) \nChain 4 Iteration: 2000 / 8000 [ 25%]  (Warmup) \nChain 4 Iteration: 2001 / 8000 [ 25%]  (Sampling) \nChain 2 Iteration: 2000 / 8000 [ 25%]  (Warmup) \nChain 2 Iteration: 2001 / 8000 [ 25%]  (Sampling) \nChain 3 Iteration: 2000 / 8000 [ 25%]  (Warmup) \nChain 3 Iteration: 2001 / 8000 [ 25%]  (Sampling) \nChain 1 Iteration: 3000 / 8000 [ 37%]  (Sampling) \nChain 4 Iteration: 3000 / 8000 [ 37%]  (Sampling) \nChain 1 Iteration: 4000 / 8000 [ 50%]  (Sampling) \nChain 4 Iteration: 4000 / 8000 [ 50%]  (Sampling) \nChain 2 Iteration: 3000 / 8000 [ 37%]  (Sampling) \nChain 1 Iteration: 5000 / 8000 [ 62%]  (Sampling) \nChain 4 Iteration: 5000 / 8000 [ 62%]  (Sampling) \nChain 4 Iteration: 6000 / 8000 [ 75%]  (Sampling) \nChain 1 Iteration: 6000 / 8000 [ 75%]  (Sampling) \nChain 2 Iteration: 4000 / 8000 [ 50%]  (Sampling) \nChain 3 Iteration: 3000 / 8000 [ 37%]  (Sampling) \nChain 4 Iteration: 7000 / 8000 [ 87%]  (Sampling) \nChain 1 Iteration: 7000 / 8000 [ 87%]  (Sampling) \nChain 4 Iteration: 8000 / 8000 [100%]  (Sampling) \nChain 4 finished in 261.0 seconds.\nChain 1 Iteration: 8000 / 8000 [100%]  (Sampling) \nChain 1 finished in 271.4 seconds.\nChain 2 Iteration: 5000 / 8000 [ 62%]  (Sampling) \nChain 2 Iteration: 6000 / 8000 [ 75%]  (Sampling) \nChain 3 Iteration: 4000 / 8000 [ 50%]  (Sampling) \nChain 2 Iteration: 7000 / 8000 [ 87%]  (Sampling) \nChain 2 Iteration: 8000 / 8000 [100%]  (Sampling) \nChain 2 finished in 384.6 seconds.\nChain 3 Iteration: 5000 / 8000 [ 62%]  (Sampling) \nChain 3 Iteration: 6000 / 8000 [ 75%]  (Sampling) \nChain 3 Iteration: 7000 / 8000 [ 87%]  (Sampling) \nChain 3 Iteration: 8000 / 8000 [100%]  (Sampling) \nChain 3 finished in 656.3 seconds.\n\nAll 4 chains finished successfully.\nMean chain execution time: 393.3 seconds.\nTotal execution time: 656.4 seconds.\n\n\n\n\nResults\n\ninterdependence_fit$summary(\n  variables = c(\"intercept\", \"slope_FR\", \"slope_PT\"\n                , \"sigma\", \"latent_rho\")\n  , \"mean\", \"sd\"\n  , extra_quantiles = ~posterior::quantile2(., probs = c(0.025, 0.975))\n  , \"rhat\"\n)\n\n# A tibble: 5 × 6\n  variable     mean     sd    q2.5 q97.5  rhat\n  &lt;chr&gt;       &lt;num&gt;  &lt;num&gt;   &lt;num&gt; &lt;num&gt; &lt;num&gt;\n1 intercept  0.189  0.0529  0.0848 0.293  1.00\n2 slope_FR   0.712  0.154   0.407  1.01   1.00\n3 slope_PT   0.107  0.138  -0.165  0.378  1.00\n4 sigma      0.0721 0.0182  0.0331 0.106  1.00\n5 latent_rho 0.812  0.0781  0.646  0.950  1.00\n\n\nUnsurprisingly, the model confidently finds a link between French skills at T2 and at T3, even on the level of the unobserved true scores (\\(\\widehat{\\beta}_{\\textrm{French}} = 0.71 \\pm 0.15\\)). But more importantly, the evidence for an additional effect of Portuguese skills at T2 on French skills at T3 is flimsy (\\(\\widehat{\\beta}_{\\textrm{Portuguese}} = 0.11 \\pm 0.14\\)). The latent T2 variables are estimated to correlate strongly at \\(\\widehat{\\rho} = 0.81 \\pm 0.08\\). These results don’t change much when a flat prior on \\(\\rho\\) is specified (this can be accomplished by not specifying any prior at all for \\(\\rho\\)). Compared to the model in the previous blog post (Table 2), little has changed. The only appreciable difference is that the estimate for sigma is lower. The reason is that, unlike the previous model, the current model partitions the variance in the French T3 scores into true score variance and measurement error variance. In this model, sigma captures the true score variance that isn’t accounted for by T2 skills, whereas in the previous model, sigma captured the total variance that wasn’t accounted for by T2 skills. But other than that, the current model doesn’t represent a huge change from the previous one.\n\nTable 2. Comparison of parameter estimates between the model fitted in this blog post and the model fitted in the previous one. In the previous model, the outcome was assumed to be perfectly reliable.\n\n\nParameter\nCurrent estimate\nPrevious estimate\n\n\n\n\nintercept\n0.19 ± 0.05\n0.19 ± 0.05\n\n\nslope_FR\n0.71 ± 0.15\n0.71 ± 0.16\n\n\nslope_PT\n0.11 ± 0.14\n0.10 ± 0.14\n\n\nsigma\n0.07 ± 0.02\n0.12 ± 0.01\n\n\nlatent_rho\n0.81 ± 0.08\n0.81 ± 0.08\n\n\n\nA couple of things still remain to be done. First, the French test at T3 was the same as the one at T2 so it’s likely that the measurement errors on both scores won’t be completely independent of one another. I’d like to find out how correlated measurement errors affect the parameter estimates. Second, I’d like to get started with prior and posterior predictive checks: the former to check if the priors give rise to largely possible data patterns, and the latter to check if the full model tends to generate data sets similar to the one actually observed."
  },
  {
    "objectID": "posts/2020-02-18-measurement-error/index.html#references",
    "href": "posts/2020-02-18-measurement-error/index.html#references",
    "title": "Baby steps in Bayes: Incorporating reliability estimates in regression models",
    "section": "References",
    "text": "References\nBerthele, Raphael and Jan Vanhove. 2017. What would disprove interdependence? Lessons learned from a study on biliteracy in Portuguese heritage language speakers in Switzerland. International Journal of Bilingual Education and Bilingualism.\nBrunner, Jerry and Peter C. Austin. 2009. Inflation of Type I error rate in multiple regression when independent variables are measured with error. Canadian Journal of Statistics 37(1). 33–46.\nWestfall, Jacob and Tal Yarkoni. 2016. Statistically controlling for confounding constructs is harder than you think.. PLOS ONE 11(3). e0152719."
  },
  {
    "objectID": "posts/2020-02-18-measurement-error/index.html#software-versions",
    "href": "posts/2020-02-18-measurement-error/index.html#software-versions",
    "title": "Baby steps in Bayes: Incorporating reliability estimates in regression models",
    "section": "Software versions",
    "text": "Software versions\nPlease note that I reran the code on this page on August 6, 2023.\n\ndevtools::session_info()\n\n─ Session info ───────────────────────────────────────────────────────────────\n setting  value\n version  R version 4.3.1 (2023-06-16)\n os       Ubuntu 22.04.2 LTS\n system   x86_64, linux-gnu\n ui       X11\n language en_US\n collate  en_US.UTF-8\n ctype    en_US.UTF-8\n tz       Europe/Zurich\n date     2023-08-06\n pandoc   3.1.1 @ /usr/lib/rstudio/resources/app/bin/quarto/bin/tools/ (via rmarkdown)\n\n─ Packages ───────────────────────────────────────────────────────────────────\n package        * version date (UTC) lib source\n abind            1.4-5   2016-07-21 [1] CRAN (R 4.3.1)\n backports        1.4.1   2021-12-13 [1] CRAN (R 4.3.0)\n boot             1.3-28  2021-05-03 [4] CRAN (R 4.2.0)\n cachem           1.0.6   2021-08-19 [2] CRAN (R 4.2.0)\n callr            3.7.3   2022-11-02 [1] CRAN (R 4.3.1)\n checkmate        2.2.0   2023-04-27 [1] CRAN (R 4.3.1)\n cli              3.6.1   2023-03-23 [1] CRAN (R 4.3.0)\n cmdstanr       * 0.6.0   2023-08-02 [1] local\n colorspace       2.1-0   2023-01-23 [1] CRAN (R 4.3.0)\n crayon           1.5.2   2022-09-29 [1] CRAN (R 4.3.1)\n curl             5.0.1   2023-06-07 [1] CRAN (R 4.3.1)\n dagitty        * 0.3-1   2021-01-21 [1] CRAN (R 4.3.1)\n devtools         2.4.5   2022-10-11 [1] CRAN (R 4.3.1)\n digest           0.6.29  2021-12-01 [2] CRAN (R 4.2.0)\n distributional   0.3.2   2023-03-22 [1] CRAN (R 4.3.1)\n dplyr            1.1.2   2023-04-20 [1] CRAN (R 4.3.0)\n ellipsis         0.3.2   2021-04-29 [2] CRAN (R 4.2.0)\n evaluate         0.15    2022-02-18 [2] CRAN (R 4.2.0)\n fansi            1.0.4   2023-01-22 [1] CRAN (R 4.3.1)\n farver           2.1.1   2022-07-06 [1] CRAN (R 4.3.0)\n fastmap          1.1.0   2021-01-25 [2] CRAN (R 4.2.0)\n fs               1.5.2   2021-12-08 [2] CRAN (R 4.2.0)\n generics         0.1.3   2022-07-05 [1] CRAN (R 4.3.0)\n ggplot2          3.4.2   2023-04-03 [1] CRAN (R 4.3.0)\n glue             1.6.2   2022-02-24 [2] CRAN (R 4.2.0)\n gtable           0.3.3   2023-03-21 [1] CRAN (R 4.3.0)\n htmltools        0.5.5   2023-03-23 [1] CRAN (R 4.3.0)\n htmlwidgets      1.6.2   2023-03-17 [1] CRAN (R 4.3.1)\n httpuv           1.6.11  2023-05-11 [1] CRAN (R 4.3.1)\n jsonlite         1.8.7   2023-06-29 [1] CRAN (R 4.3.1)\n knitr            1.39    2022-04-26 [2] CRAN (R 4.2.0)\n later            1.3.1   2023-05-02 [1] CRAN (R 4.3.1)\n lifecycle        1.0.3   2022-10-07 [1] CRAN (R 4.3.0)\n magrittr         2.0.3   2022-03-30 [1] CRAN (R 4.3.0)\n MASS             7.3-60  2023-05-04 [4] CRAN (R 4.3.1)\n memoise          2.0.1   2021-11-26 [2] CRAN (R 4.2.0)\n mime             0.10    2021-02-13 [2] CRAN (R 4.0.2)\n miniUI           0.1.1.1 2018-05-18 [1] CRAN (R 4.3.1)\n munsell          0.5.0   2018-06-12 [1] CRAN (R 4.3.0)\n pillar           1.9.0   2023-03-22 [1] CRAN (R 4.3.0)\n pkgbuild         1.4.2   2023-06-26 [1] CRAN (R 4.3.1)\n pkgconfig        2.0.3   2019-09-22 [2] CRAN (R 4.2.0)\n pkgload          1.3.2.1 2023-07-08 [1] CRAN (R 4.3.1)\n posterior      * 1.4.1   2023-03-14 [1] CRAN (R 4.3.1)\n prettyunits      1.1.1   2020-01-24 [2] CRAN (R 4.2.0)\n processx         3.8.2   2023-06-30 [1] CRAN (R 4.3.1)\n profvis          0.3.8   2023-05-02 [1] CRAN (R 4.3.1)\n promises         1.2.0.1 2021-02-11 [1] CRAN (R 4.3.1)\n ps               1.7.5   2023-04-18 [1] CRAN (R 4.3.1)\n purrr            1.0.1   2023-01-10 [1] CRAN (R 4.3.0)\n R6               2.5.1   2021-08-19 [2] CRAN (R 4.2.0)\n Rcpp             1.0.11  2023-07-06 [1] CRAN (R 4.3.1)\n remotes          2.4.2   2021-11-30 [2] CRAN (R 4.2.0)\n rlang            1.1.1   2023-04-28 [1] CRAN (R 4.3.0)\n rmarkdown        2.21    2023-03-26 [1] CRAN (R 4.3.0)\n rstudioapi       0.14    2022-08-22 [1] CRAN (R 4.3.0)\n scales           1.2.1   2022-08-20 [1] CRAN (R 4.3.0)\n sessioninfo      1.2.2   2021-12-06 [2] CRAN (R 4.2.0)\n shiny            1.7.4.1 2023-07-06 [1] CRAN (R 4.3.1)\n stringi          1.7.12  2023-01-11 [1] CRAN (R 4.3.1)\n stringr          1.5.0   2022-12-02 [1] CRAN (R 4.3.0)\n tensorA          0.36.2  2020-11-19 [1] CRAN (R 4.3.1)\n tibble           3.2.1   2023-03-20 [1] CRAN (R 4.3.0)\n tidyselect       1.2.0   2022-10-10 [1] CRAN (R 4.3.0)\n urlchecker       1.0.1   2021-11-30 [1] CRAN (R 4.3.1)\n usethis          2.2.2   2023-07-06 [1] CRAN (R 4.3.1)\n utf8             1.2.3   2023-01-31 [1] CRAN (R 4.3.1)\n V8               4.3.0   2023-04-08 [1] CRAN (R 4.3.0)\n vctrs            0.6.3   2023-06-14 [1] CRAN (R 4.3.0)\n xfun             0.39    2023-04-20 [1] CRAN (R 4.3.0)\n xtable           1.8-4   2019-04-21 [1] CRAN (R 4.3.1)\n yaml             2.3.5   2022-02-21 [2] CRAN (R 4.2.0)\n\n [1] /home/jan/R/x86_64-pc-linux-gnu-library/4.3\n [2] /usr/local/lib/R/site-library\n [3] /usr/lib/R/site-library\n [4] /usr/lib/R/library\n\n──────────────────────────────────────────────────────────────────────────────"
  },
  {
    "objectID": "posts/2019-11-28-scatterplot-matrix/index.html",
    "href": "posts/2019-11-28-scatterplot-matrix/index.html",
    "title": "Adjusting for a covariate in cluster-randomised experiments",
    "section": "",
    "text": "This is just a quick blog post to share a function with which you can draw scatterplot matrices.\nScatterplot matrices are useful for displaying the intercorrelation between several continuous variables. Based on the help page for the pairs() function in R, I put together a function for quickly drawing scatterplot matrices that also show the Pearson correlation coefficients for the bivariate relationships as well as the number of observations that go into them: scatterplot_matrix(). Here’s how you’d use it:\n\n# Read in the function\nsource(\"https://janhove.github.io/RCode/scatterplot_matrix.R\")\n\n# Example: airquality is a built-in dataset in R\ndata(airquality)\n\nscatterplot_matrix(\n  x = subset(airquality, select = c(Ozone, Wind, Temp, Solar.R, Month)),\n  labels = c(\"Ozone\", \"Wind\", \"Temperature\", \"Solar radiation\", \"Month\")\n)\n\n\n\n\nUse:\n\nx contains the vectors (= columns in a dataset) containing the continuous variables you want to plot in the order you want to plot them. To the extent possible, try to arrange the variables in such an order that the earlier variables are more likely to be influenced by the later variables than vice versa. I’m hardly an expert on meteorology, so the order in which I put the variables may not be optimal – but Month is evidently more likely to influence Temperature than vice versa, so put Month somewhere after Temp. Similarly, if you collected your participants’ age, L1 vocabulary skills, and their results on a cognate translation test in L2, put age last, L1 vocabulary skills second and the translation test results first.\nlabels contains the readable names of the variables, in that same order. If you leave out this argument, the column names will serve as labels.\n\nOutput:\n\nMain diagonal: Histograms for each variable as well as the number of available data points for that variable.\nUpper triangle: Scatterplots for the bivariate relationships between the variables, with a nonlinear scatterplot smoother. In the example above, the scatterplot in the first row, third column, shows the relationship between the Temperature (the third variable, on the x-axis) and Ozone (the first variable, on the y-axis). The scatterplot in the second row, fifth column, shows the relationship between Month (the fifth variable, on the x-axis) and Wind (the second variable, on the y-axis).\nLower triangle: Pearson correlation coefficients for the bivariate relationships between the variables and the number of observations on which it is based. In the example above, the correlation coefficient in the third row, first column, concerns the relationship between the Temperature and Ozone (the first variable). The correlation coefficient in the second row, fifth column, shows the relationship between Month (the fifth variable) and Wind (the second variable).\n\nFunctions you can use instead of this one are pairscor.fnc() in the languageR package, and ggscatmat() and ggpairs() in the GGally package."
  },
  {
    "objectID": "posts/2019-04-11-assumptions-relevance/index.html",
    "href": "posts/2019-04-11-assumptions-relevance/index.html",
    "title": "Before worrying about model assumptions, think about model relevance",
    "section": "",
    "text": "Beginning analysts tend to be overly anxious about the assumptions of their statistical models. This observation is the point of departure of my tutorial Checking the assumptions of your statistical model without getting paranoid, but it’s probably too general. It’d be more accurate to say that beginning analysts who e-mail me about possible assumption violations and who read tutorials on statistics are overly anxious about model assumptions. (Of course, there are beginning as well as seasoned researchers who are hardly ever worry about model assumptions, but they’re unlikely to read papers and blog posts about model assumptions.)\nSome anxiety about assumptions is desired if it results in more careful analyses. But if it leads researchers to abandon attempts to model their data, to resort to arcane modelling techniques with little added value, or to reject outright the results of other researchers’ modelling attempts because they didn’t resort to some arcane model, it is counterproductive. I suspect that part of what causes anxiety about assumptions is that these assumptions tend to be interpreted as mathematical requirements that, if violated, vacate any inferential guarantees the model may offer. Here, I will take a different perspective: often, the main problem when some model assumptions are clearly violated is not that the inferences won’t be approximately correct but rather that they may not be as relevant."
  },
  {
    "objectID": "posts/2019-04-11-assumptions-relevance/index.html#the-general-linear-models-assumptions",
    "href": "posts/2019-04-11-assumptions-relevance/index.html#the-general-linear-models-assumptions",
    "title": "Before worrying about model assumptions, think about model relevance",
    "section": "The general linear model’s assumptions",
    "text": "The general linear model’s assumptions\nThe general linear model fitted using ordinary least squares (which includes Student’s t test, ANOVA, and linear regression) makes four assumptions: linearity, homoskedasticity (constant variance), normality, and independence. It’s possible to simulate scenarios in which violations of any of these assumptions utterly invalidate the inferences gleaned from the model. But I suspect that in the vast majority of cases in which violations of the first three assumptions (linearity, homoskedasticity, normality) invalidate inferences, the main problem is that these inferences – even if they were technically correct – wouldn’t be too interesting to begin with. (Violations of the fourth assumption (independence) are less easily identifiable in terms of model relevance but can seriously affect inferences, as I’ll reiterate at the end of this blog post for good measure.)\nFigure 1 illustrates the linearity, homoskedasticity and normality assumptions in the context of a linear regression model with one predictor.\n\n\n\n\n\nFigure 1. Illustration of the linearity, homoskedasticity and normality assumptions for a linear regression model with one continuous predictor. The blue regression lines connects the fitted y-values at each x-value and is assumed to be straight. The red density curves visualise that the model assumes the y-data at each x-value to be normally distributed with the same variance at different x-values.\n\n\n\n\nFirst, the general linear model assumes that the relationships between the outcome and any continuous predictors are linear; in Figure 1, the straight regression line adequately captures the trend in the data. (You can add non-linear transformations of the predictors, too, but then the model will assume that the transformed predictors are linearly related to the outcome. I’m using the term ‘linearity assumption’ to mean ‘the model captures the trend in the data’.)\nSecond, the general linear model assumes that the residuals were all sampled from the same distribution. The three red curves in Figure 1 show the assumed distribution of the residuals at three predictor values; the ‘constant variance’ assumption entails that these distributions all have the same width as they do in Figure 1.\nThird, the general linear model assumes that the residuals were all sampled from normal distributions. Strictly speaking, t- and F-statistics – and the p-values derived from them – depend on this assumption.\nA useful way of looking at these three assumptions is as follows: According to the model, if you took a large sample of observations with a predictor value of 0.2, you’d find that their outcome values are normally distributed, that the mean outcome value will be about 1.2, and that the standard deviation of the outcome values will be about 0.5. Similarly, according to the model, if you took a large sample of observations with a predictor value of 0.5, you’d also find that their outcome values are normally distributed and have a standard deviation of 0.5, but this time with a mean value of 2.4. And for a large sample of observations with a predictor value of 0.8, you’d again expect to find a normal distribution of outcome values with a standard deviation of 0.5 but with a mean of 3.6. In other words, for each predictor value, the model expects to find a normal distribution of outcomes with the same spread but with a different mean; the location of the mean is captured by the regression line."
  },
  {
    "objectID": "posts/2019-04-11-assumptions-relevance/index.html#model-assumptions-are-always-violated",
    "href": "posts/2019-04-11-assumptions-relevance/index.html#model-assumptions-are-always-violated",
    "title": "Before worrying about model assumptions, think about model relevance",
    "section": "Model assumptions are always violated",
    "text": "Model assumptions are always violated\nIn real-life examples (as opposed to computer simulations), the linearity, homoskedasticity and normality assumptions are pretty much guaranteed to be violated. Any non-tautological relationship is much more likely to be ever so slightly nonlinear than it is to be perfectly linear, even if floor and ceiling effects aren’t at play: There are just so many more ways in which a relationship can be nonlinear than it can be linear. Similarly, the odds against the distribution of outcome values having a different mean but the exact same standard deviation at different predictor values must be staggering. And finally, normal distributions range from negative to positive infinity and can take any real value, whereas actual data have theoretical or practical minimum and maximum values and don’t have infinite precision (e.g., they’re rounded to the nearest millisecond).\nRealistically, then, model assumptions are known to be violated. But the general linear model wouldn’t enjoy such widespread use if even the slightest of assumption violations jeopardised the validity of the inferences gleaned from it. Model assumptions are violated to some degree, and as a rule of thumb, I suggest that the moment you should seriously start to worry about violated assumptions is when you realise that the model you’ve fitted is answering isn’t the most relevant questions that your data suggest."
  },
  {
    "objectID": "posts/2019-04-11-assumptions-relevance/index.html#assumptions-are-mostly-a-matter-of-model-relevance",
    "href": "posts/2019-04-11-assumptions-relevance/index.html#assumptions-are-mostly-a-matter-of-model-relevance",
    "title": "Before worrying about model assumptions, think about model relevance",
    "section": "Assumptions are mostly a matter of model relevance",
    "text": "Assumptions are mostly a matter of model relevance\nThe most egregious assumption violations aren’t problematic primarily because they may invalidate your inferences; they’re problematic because you’re asking your model to answer a question that’s probably not the most relevant. The examples below illustrate what I mean.\n\nNonlinearities\nFigure 2 shows how well the participants in my Ph.D. study (Vanhove 2014) could guess the meaning of words in an unknown but related language against their age. If you fit a linear model on these data or if you compute a correlation coefficient, you’d obtain a nonsignificant result, even though there’s quite clearly some association between age and performance on the task.\n\n\n\n\n\nFigure 2. The plot on the left shows the participants’ performance on a linguistic task according to their age. It’s possible to fit a linear regression to these data (cf. the blue regression line), but the scatterplot strongly suggests that the age-related development on this task is non-linear. The plot on the right is a diagnostic plot for this model; the blue line is a nonlinear scatterplot smoother.\n\n\n\n\nSome may prefer to categorise this nonlinearity under the heading of violated model assumptions. But I think this may be confusing. The problem here isn’t that the model (or the correlation coefficient) doesn’t the answer it was asked correctly; it’s that, having seen this scatterplot, researchers should realise that they’re not asking the question they probably ought to be asking. To elaborate, the linear regression model asks “What’s the form of the linear relationship between the two variables?”, and the correlation coefficient asks “What’s the strength of their linear relationship?”. Both methods provide correct answers to these questions: There isn’t any linear relationship. The error that some researchers make is to leave out the word “linear” and conclude that there isn’t any relationship between the two variables without having looked at the data. This conclusion would be bogus, but that isn’t the model’s fault – let alone the data’s. Having seen this scatterplot or having seen the linear model’s diagnostic plot at the latest, analysts should realise that by fitting a straight line to these data they would gloss over a salient aspect of the relationship, and they should resort to tools that are capable of better capturing the nonlinearity (e.g., generalised additive models).\n\n\nNon-normal residuals\nStrictly speaking, normality is required to compute t-distribution-based confidence intervals and p-values for parameter estimates. However, the larger the sample, the less important this assumption is. This is why Gelman and Hill (2007) list normality as the least important assumption in regression modelling. Unfortunately, such statements are easily misinterpreted as meaning that you can run regression models willy-nilly without regard to the distribution of the data. Again, I think the relevance perspective is useful. An ordinary least-squares model fits mean trends, that is, the regression lines tries to capture the mean of the outcome distribution for any combination of predictor values. But the mean is just a tool to capture a distribution’s central tendency. One nice aspect of normal distribution is that the mean perfectly captures their central tendency and does so more efficiently than other measures of central tendency (e.g., median and mode).\nConsider, by contrast, the data in Figure 3 (data from Klein et al., 2014, ‘Gambling study’, University of Florida sample). A linear model ran on these data (corresponding to a Student’s t-test) would characterise the difference in the means between the two groups, and it would do so pretty accurately: When I checked the normality-based inferential results (standard errors and confidence intervals) using a nonparametric bootstrap that does not assume normality, I obtained very similar results. Instead, the main problem is that these means don’t approximately capture the central tendency in the groups – they represent pretty atypical values: the vast majority of outcome values lie below the group means. So again, the linear model would correctly answer a question that, once you’ve seen the data, you should realise isn’t too relevant.\n\n\n\n\n\nFigure 3. The plot on the left shows how the data are distributed in each group; the blue star indicate the group means. The diagnostic plot on the right shows what should’ve been obvious from inspecting the plot on the left: the residuals are strongly right-skewed. This indicates that the group means don’t capture the data’s central tendencies very well.\n\n\n\n\nInstead of fitting a linear model to these data (or running a t-test), it would make sense to look into sensible transformations for these data (which is what Klein et al. [2014] did, incidentally), to find a better measure of the data’s central tendencies (e.g., the median), or both. These solutions have their drawbacks (transformed outcome data are more difficult to interpret; the median doesn’t scale up as nicely as the mean does to more complex data), but fitting a straightforward but mostly irrelevant model doesn’t strike me as a solution, either.\nThere are, of course, situations in which the residuals are distributed non-normally but where the mean trend may actually be of interest. For instance, the outcome variable may have some predetermined lower and/or upper bound so that the residuals don’t extend as far out as a normally distributed variable would, or the outcome variable may be fairy discrete because of rounding. In cases such as these, your inferences about the mean trend would generally be quite correct and relevant. There are, however, two problems you may face in such cases:\n\nIf you ask your model to generate a prediction for the mean outcome value for a given combination of predictor values, this mean outcome may lie outside the range of possible outcome values. For instance, if your outcome variable is bounded between 0 and 20, you may still end up with predicted mean values below 0 or above 20 for some combinations of predictor values.\nIf you ask your model to generate the distribution of the outcome values (not just their mean) for a given combination of predictor values, none of the generated values may actually be possible. For instance, even if your outcome variable is measured to the nearest integer, your model will generate real-valued predictions. More generally, your model will generate normally distributed outcome values even if the analyst knows that the outcome distribution couldn’t possibly be normal.\n\nIf you use your model to these ends, then non-normally distributed data may be a cause for concern. But if you don’t, there’s no need to get too anxious just yet.\n\n\nNon-constant variance\nFinally, let’s take a look at the “constant variance” (or “homoskedasticity”) assumption. Violations of this assumption imply that the width of the red curves in Figure 1 varies according to some predictor variable or according to the outcome variable. Figure 4 shows a regression example with simulated data (I don’t have one with real data). The regression line correctly captures the mean trend in the data, and the mean is also a relevant measure of the data’s central tendency. However, since the scatter around the regression line isn’t approximately equal along the predictor variable, the model’s estimated standard errors and confidence intervals may be off, and if asked to generate the distribution of the outcome values for a given predictor value, the model will overestimate the spread of this distribution for low predictor values and underestimate it for high ones.\n\n\n\n\n\nFigure 4. The spread around the regression line increases with the predictor variable. (simulated data)\n\n\n\n\nBut rather than immediately starting to worry about incorrect standard errors and how to fix them when looking at a diagnostic plot like in Figure 4, I think analysts’ first reaction typically ought to be “Huh? That’s interesting – why does the spread increase along the predictor variable?” Perhaps the increasing spread is interesting in its own right rather than just a nuisance to be taken care off when estimating the mean trend.\nFigure 5 shows another example, this time in the context of a group comparison. There is a pronounced difference between the spreads in Groups 1 and 2, but the mean trend seems to be quite relevant. In fact, incorrect standard errors are not a worry in a two-group comparison because Welch’ t-test (the default t-test in R) takes into account between-group variance differences. However, I still think that researchers’ reaction when seeing plots like these typically ought to be one of wonder (“What could have caused these differences in variance?”) rather of contentment (“Ah well, our standard errors are estimated correctly.”).\n\n\n\n\n\nFigure 5. The scatter is considerably greater in Group 2 than it is in Group 1. (simulated data)"
  },
  {
    "objectID": "posts/2019-04-11-assumptions-relevance/index.html#but-dont-ignore-dependencies",
    "href": "posts/2019-04-11-assumptions-relevance/index.html#but-dont-ignore-dependencies",
    "title": "Before worrying about model assumptions, think about model relevance",
    "section": "But don’t ignore dependencies",
    "text": "But don’t ignore dependencies\nViolations of the independence assumption are difficult to diagnose, but even minor violations of this assumption can wreak havoc on your inferential results. So once you’re satisfied that your model is relevant, worry about violations of this assumption first."
  },
  {
    "objectID": "posts/2019-04-11-assumptions-relevance/index.html#wrapping-up",
    "href": "posts/2019-04-11-assumptions-relevance/index.html#wrapping-up",
    "title": "Before worrying about model assumptions, think about model relevance",
    "section": "Wrapping up",
    "text": "Wrapping up\n\nWorry about the relevance of your model first. Can it answer the question you’re actually interested in?\nIf you look at the data’s distribution or at a diagnostic plot and you go “Huh, that’s strange/interesting!”, don’t start to look for a more complicated tool that will yield more accurate inferential results. Instead, think about what may be causing the pattern, and if it is scientifically relevant, look into modelling the pattern in its own right.\nIf you look at a diagnostic plot and you’re not quite sure whether an assumption is strongly violated, this is likely because the pattern isn’t salient. And if the pattern isn’t too salient, it’s presumably not too interesting, and its effects on your inferences will probably be fairly minor.\nDon’t ignore dependencies in your data. These are difficult to diagnose if you’re not familiar with the study’s design."
  },
  {
    "objectID": "posts/2019-04-11-assumptions-relevance/index.html#references",
    "href": "posts/2019-04-11-assumptions-relevance/index.html#references",
    "title": "Before worrying about model assumptions, think about model relevance",
    "section": "References",
    "text": "References\nAndrew Gelman & Jennifer Hill. 2007. Data analysis using regression and multilevel/hierarchical models. Cambridge, UK: Cambridge University Press.\nKlein, Richard A., Kate A. Ratliff, Michelangelo Vianello, Reginald B. Adams Jr., Štěpán Bahník, Michael J. Bernstein, Konrad Bocian et al. 2014. Investigating variation in replicability: A ‘many labs’ replication project. Social Psychology 45(3). 142–152.\nJan Vanhove. 2014. Receptive multilingualism across the lifespan: Cognitive and linguistic factors in cognate guessing. Ph.D. thesis. University of Fribourg."
  },
  {
    "objectID": "posts/2020-01-21-statistical-control-measurement-error/index.html",
    "href": "posts/2020-01-21-statistical-control-measurement-error/index.html",
    "title": "Baby steps in Bayes: Accounting for measurement error on a control variable",
    "section": "",
    "text": "In observational studies, it is customary to account for confounding variables by including measurements of them in the statistical model. This practice is referred to as statistically controlling for the confounding variables. An underappreciated problem is that if the confounding variables were measured imperfectly, then statistical control will be imperfect as well, and the confound won’t be eradicated entirely (see Berthele & Vanhove 2017; Brunner & Austin 2009; Westfall & Yarkoni 2016) (see also Controlling for confounding variables in correlational research: Four caveats).\nThis blog post details my efforts to specify a Bayesian model in which the measurement error on the confounding variable was taken into account. The ultimate aim was to obtain more honest estimates of the impact of the confounding variable and the variable of actual interest on the outcome. First, I’ll discuss a simulated example to demonstrate the consequences of measurement error on statistical control and what a model needs to do to appropriately take measurement error into account. Then I apply the insights gained on a real-life study in applied linguistics.\nI will preface all of this with the disclaimer that I don’t consider myself an expert in the techniques discussed below; one reason for writing this blog is to solicit feedback from readers more knowledgeable than I am."
  },
  {
    "objectID": "posts/2020-01-21-statistical-control-measurement-error/index.html#preliminaries",
    "href": "posts/2020-01-21-statistical-control-measurement-error/index.html#preliminaries",
    "title": "Baby steps in Bayes: Accounting for measurement error on a control variable",
    "section": "Preliminaries",
    "text": "Preliminaries\nUpdate (2023-08-06): When converting this blog from Jekyll/Bootstrap to Quarto, I noticed that the original code used in this blog post, which involved the R package rstan has started to run very slowly. In the present version, I use cmdstanr instead.\nIf you want to follow along, you need the following R packages/settings:\n\nlibrary(tidyverse)\n\nlibrary(cmdstanr)  # for fitting Bayesian models, v. 2.32.2\nlibrary(posterior) # for working with posterior distributions\n\n# For drawing scatterplot matrices\nsource(\"https://janhove.github.io/RCode/scatterplot_matrix.R\")\n\n# Set random seed for reproducibility\nset.seed(2020-01-21, kind = \"Mersenne-Twister\")\n\nYou’ll also need the MASS package, but you don’t need to load it."
  },
  {
    "objectID": "posts/2020-01-21-statistical-control-measurement-error/index.html#demonstration-measurement-error-messes-up-statistical-control",
    "href": "posts/2020-01-21-statistical-control-measurement-error/index.html#demonstration-measurement-error-messes-up-statistical-control",
    "title": "Baby steps in Bayes: Accounting for measurement error on a control variable",
    "section": "Demonstration: Measurement error messes up statistical control",
    "text": "Demonstration: Measurement error messes up statistical control\nLet’s first illustrate the problem that measurement error causes for statistical control using simulated data. That way, we know what goes into the data and what we hope a model should take out of it.\nThe scenario I want to focus on is the following. You are pretty sure that a given construct A causally affects a variable Z. You are, however, interested in finding out if another construct B also affects Z. You can’t manipulate any of the variables, so you have to make do with an observational study. Unfortunately, A and B are likely to be correlated. Let’s simulate some data to make this more concrete:\n\n500 datapoints (n)\nConstructs A and B are correlated at \\(\\rho = 0.73\\).\nConstructs A and B are normally distributed with standard deviations of 1.5 (sd_A) and 0.8 (sd_B), respectively. The means of these normal distributions are 3 and -4, respectively.\n\nThe numbers in the list above aren’t special; I just wanted to make sure the model I will specify further down below isn’t restricted to assuming that the constructs are distributed normally with mean 0 and standard deviation 1.\n\n# Generate correlated constructs\nn &lt;- 500\nrho &lt;- 0.73\nsd_A &lt;- 1.5\nsd_B &lt;- 0.8\n\n# Given the correlation and the standard deviations,\n# construct the covariance matrix for the constructs like so:\nlatent_covariance_matrix &lt;- rbind(c(sd_A, 0), c(0, sd_B)) %*% \n  rbind(c(1, rho), c(rho, 1)) %*% \n  rbind(c(sd_A, 0), c(0, sd_B))\n\n# Draw data from the multivariate normal distribution:\nconstructs &lt;- MASS::mvrnorm(n = n, mu = c(3, -4)\n                            , Sigma = latent_covariance_matrix)\nA &lt;- constructs[, 1]\nB &lt;- constructs[, 2]\n\nFor the purposes of this simulation, I’ll generate data for Z that are affected by A but not by B:\n\n# A influences Z, B doesn't\nZ &lt;- 2 + 0.7*A + rnorm(n, sd = 1.3)\n\nAs Figure 1 shows, B and Z are correlated, even though neither influences the other. This is because of their link with A.\n\nscatterplot_matrix(cbind(Z, A, B))\n\n\n\n\nFigure 1. Even though variable Z isn’t causally affected by construct B, there exists a considerable correlation between B and Z. This correlation exists because Z is causally affected by construct A, which is correlated with B.\n\n\n\n\nIn situations such as these, researchers typically include both A and B as predictors in a model with Z as the outcome. And this works: we find a significant relationship between A and Z, but not between B and Z. Moreover, all estimated parameters are in the vicinity of their true values, as specified in the simulation.\n\nsummary(lm(Z ~ A + B))$coefficients\n\n            Estimate Std. Error t value Pr(&gt;|t|)\n(Intercept)    2.881     0.5439    5.30 1.78e-07\nA              0.609     0.0546   11.14 6.93e-26\nB              0.146     0.1025    1.43 1.54e-01\n\n\nBut in real life, the situation is more complicated. When researchers “statistically control” for a possible confound, they’re usually interested in controlling for the confounding construct rather than for any one measurement of this construct. For instance, when teasing apart the influences of L2 vocabulary knowledge and L2 morphosyntactic knowledge on L2 speaking fluency, researchers don’t actually want to control for the learners performance on this or that vocabulary test: they want to control for L2 vocabulary knowledge itself. One would hope that the vocabulary test gives a good indication of the learners’ vocabulary knowledge, but it’s understood that their performance will be affected by other factors as well (e.g., form on the day, luck with guessing, luck with the words occurring in the test etc.).\nSo let’s add some noise (measurement error) to constructs A and B. Here I express the measurement error in terms of the reliability of the instruments used to measure the constructs: If \\(\\sigma_A\\) is the standard deviation of the unobserved construct scores and \\(r_{AA'}\\) is the reliability of the measurement instrument, then the standard deviation of the measurement error is \\(\\sqrt{\\frac{\\sigma_A^2}{r_{AA'}} - \\sigma_A^2}\\). For the purposes of this demonstration, I’m going to specify that construct A was measured with ‘okay’ reliability (0.70), whereas construct B was measured with exceptional reliability (0.95):\n\nobs_A &lt;- A + rnorm(n = n, sd = sqrt(sd_A^2/0.70 - sd_A^2))\nobs_B &lt;- B + rnorm(n = n, sd = sqrt(sd_B^2/0.95 - sd_B^2))\n\nCrucially, if we include the observed values obs_A and obs_B as predictors in a model with Z as the outcome, we find that the parameter for obs_B is significant—even though there is no causal link between B and Z:\n\nsummary(lm(Z ~ obs_A + obs_B))$coefficients\n\n            Estimate Std. Error t value Pr(&gt;|t|)\n(Intercept)    4.842     0.4557   10.63 6.68e-24\nobs_A          0.364     0.0418    8.71 4.36e-17\nobs_B          0.456     0.0911    5.01 7.66e-07\n\n\nDescriptively, this is perfectly fine: You do indeed now know more about Z if you take into account obs_B in addition to obs_A. But you take this to interpret that the construct of B can explain variation in Z over and beyond that which can be explained by the construct of A, this would be a mistake.\nConceptually, what has happened is that since obs_A imperfectly reflects construct A, including obs_A in the model control for construct A imperfectly."
  },
  {
    "objectID": "posts/2020-01-21-statistical-control-measurement-error/index.html#fitting-a-model-that-takes-measurement-into-account",
    "href": "posts/2020-01-21-statistical-control-measurement-error/index.html#fitting-a-model-that-takes-measurement-into-account",
    "title": "Baby steps in Bayes: Accounting for measurement error on a control variable",
    "section": "Fitting a model that takes measurement into account",
    "text": "Fitting a model that takes measurement into account\nBelow is the Stan code I used for fitting the simulated data. The model takes as its input the three observed variables (obs_A, obs_B and Z). Information about the reliability of obs_A and obs_B is also provided in the form of a prior distribution on reliability_A and reliability_B. Specifically, it’s assumed that the reliability coefficient for obs_A is drawn from a beta(30, 10)-distribution. This assigns a 95% probability to the reliability coefficient lying between roughly 0.61 and 0.87. obs_B is assumed to be measured more reliably, as encoded by a beta(95, 5)-distribution, which assigns a 95% probability to the reliability coefficient lying between 0.90 and 0.98.\nImportantly, as I noted in some earlier explorations, the model has to take into account the possibility that the constructs of A and B are correlated. I specified a prior vaguely expecting a positive correlation but that wouldn’t find correlations close to or below zero to be too surprising either. Priors on the other parameters are pretty vague; I find it difficult to come up with reasonable priors in context-free examples.\n\nmeas_error_code &lt;- '\ndata { \n  // Number of observations\n  int&lt;lower = 1&gt; N;      \n  // Observed outcome\n  vector[N] Z;         \n  // Observed predictors\n  vector[N] obs_A;\n  vector[N] obs_B;\n}\nparameters {\n  // Parameters for regression\n  real intercept;  \n  real slope_A;\n  real slope_B;\n  real&lt;lower = 0&gt; sigma; \n  \n  // standard deviations of latent variables (= constructs)\n  real&lt;lower = 0&gt; sigma_lat_A; \n  real&lt;lower = 0&gt; sigma_lat_B;\n  \n  // Unknown but estimated reliabilities\n  real&lt;lower = 0, upper = 1&gt; reliability_A;\n  real&lt;lower = 0, upper = 1&gt; reliability_B;\n  \n  // Means of latent predictors\n  row_vector[2] latent_means;\n  \n  // Unknown correlation between latent predictors\n  real&lt;lower = -1, upper = 1&gt; latent_rho;\n  \n  // Latent variables\n  matrix[N, 2] latent_variables;\n} \ntransformed parameters {\n  vector[N] mu_Z;  // conditional mean of outcome\n  vector[N] lat_A; // latent variables, separated out\n  vector[N] lat_B;\n\n  real error_A; // standard error of measurement\n  real error_B;\n \n  // standard deviations of latent variables, in matrix form\n  matrix[2, 2] sigma_lat; \n  // correlation and covariance matrix for latent variables \n  cov_matrix[2] latent_cor; \n  cov_matrix[2] latent_cov;\n  \n  // Standardised slopes for A and B\n  real slope_A_std;\n  real slope_B_std;\n  \n  // Express measurement error in terms of \n  // standard deviation of constructs and reliability\n  error_A = sqrt(sigma_lat_A^2/reliability_A - sigma_lat_A^2);\n  error_B = sqrt(sigma_lat_B^2/reliability_B - sigma_lat_B^2);\n  \n  // Define diagonal matrix with standard errors of latent variables\n  sigma_lat[1, 1] = sigma_lat_A;\n  sigma_lat[2, 2] = sigma_lat_B;\n  sigma_lat[1, 2] = 0;\n  sigma_lat[2, 1] = 0;\n\n  // Define correlation matrix for latent variables\n  latent_cor[1, 1] = 1;\n  latent_cor[2, 2] = 1;\n  latent_cor[1, 2] = latent_rho;\n  latent_cor[2, 1] = latent_rho;\n  \n  // Compute covariance matrix for latent variables\n  latent_cov = sigma_lat * latent_cor * sigma_lat;\n  \n  // Extract latent variables from matrix\n  lat_A = latent_variables[, 1];\n  lat_B = latent_variables[, 2];\n  \n  // Regression model\n  mu_Z = intercept + slope_A*lat_A + slope_B*lat_B;\n  \n  // Standardised regression parameters\n  slope_A_std = slope_A * sigma_lat_A;\n  slope_B_std = slope_B * sigma_lat_B;\n}\nmodel {\n  // Priors for regression parameters\n  intercept ~ normal(0, 2);\n  slope_A ~ normal(0, 2);\n  slope_B ~ normal(0, 2);\n  sigma ~ normal(0, 2);\n  \n  // Prior for reliabilities\n  reliability_A ~ beta(30, 10); // assume this has been estimated using some metric to be \n                                // roughly 0.78, but with considerable uncertainty\n  reliability_B ~ beta(95, 5); // assume this has been estimated using exceptional reliability\n  \n  // Prior for latent means\n  latent_means ~ normal(0, 3);\n\n  // Prior for latent standard deviations\n  sigma_lat_A ~ normal(0, 2);\n  sigma_lat_B ~ normal(0, 2);\n  \n  // Prior expectation for correlation between latent variables:\n  // tend towards positive rho\n  latent_rho ~ normal(0.4, 0.3);\n  \n  // Distribution of latent variable\n  for (i in 1:N) {\n    latent_variables[i, ] ~ multi_normal(latent_means, latent_cov);\n  }\n  \n  // Measurement model\n  obs_A ~ normal(lat_A, error_A);\n  obs_B ~ normal(lat_B, error_B);\n  \n  // Generate outcome\n  Z ~ normal(mu_Z, sigma);\n}\n'\n\nLet’s put the data into a Stan-friendly list and fit the model:\n\ndata_list &lt;- list(\n  Z = Z,\n  obs_A = obs_A,\n  obs_B = obs_B,\n  N = n\n)\n\n\nmeas_error_model &lt;- cmdstan_model(write_stan_file(meas_error_code))\nmodel_fit &lt;- meas_error_model$sample(\n  data = data_list\n  , seed = 123\n  , chains = 4\n  , parallel_chains = 4\n  , iter_warmup = 2000\n  , iter_sampling = 2000\n  , refresh = 500\n  , max_treedepth = 15\n  , adapt_delta = 0.95\n)\n\nRunning MCMC with 4 parallel chains...\n\nChain 1 Iteration:    1 / 4000 [  0%]  (Warmup) \nChain 2 Iteration:    1 / 4000 [  0%]  (Warmup) \nChain 3 Iteration:    1 / 4000 [  0%]  (Warmup) \nChain 4 Iteration:    1 / 4000 [  0%]  (Warmup) \nChain 1 Iteration:  500 / 4000 [ 12%]  (Warmup) \nChain 4 Iteration:  500 / 4000 [ 12%]  (Warmup) \nChain 3 Iteration:  500 / 4000 [ 12%]  (Warmup) \nChain 2 Iteration:  500 / 4000 [ 12%]  (Warmup) \nChain 1 Iteration: 1000 / 4000 [ 25%]  (Warmup) \nChain 3 Iteration: 1000 / 4000 [ 25%]  (Warmup) \nChain 4 Iteration: 1000 / 4000 [ 25%]  (Warmup) \nChain 2 Iteration: 1000 / 4000 [ 25%]  (Warmup) \nChain 1 Iteration: 1500 / 4000 [ 37%]  (Warmup) \nChain 3 Iteration: 1500 / 4000 [ 37%]  (Warmup) \nChain 4 Iteration: 1500 / 4000 [ 37%]  (Warmup) \nChain 2 Iteration: 1500 / 4000 [ 37%]  (Warmup) \nChain 1 Iteration: 2000 / 4000 [ 50%]  (Warmup) \nChain 1 Iteration: 2001 / 4000 [ 50%]  (Sampling) \nChain 3 Iteration: 2000 / 4000 [ 50%]  (Warmup) \nChain 3 Iteration: 2001 / 4000 [ 50%]  (Sampling) \nChain 4 Iteration: 2000 / 4000 [ 50%]  (Warmup) \nChain 4 Iteration: 2001 / 4000 [ 50%]  (Sampling) \nChain 2 Iteration: 2000 / 4000 [ 50%]  (Warmup) \nChain 2 Iteration: 2001 / 4000 [ 50%]  (Sampling) \nChain 1 Iteration: 2500 / 4000 [ 62%]  (Sampling) \nChain 3 Iteration: 2500 / 4000 [ 62%]  (Sampling) \nChain 2 Iteration: 2500 / 4000 [ 62%]  (Sampling) \nChain 3 Iteration: 3000 / 4000 [ 75%]  (Sampling) \nChain 4 Iteration: 2500 / 4000 [ 62%]  (Sampling) \nChain 1 Iteration: 3000 / 4000 [ 75%]  (Sampling) \nChain 2 Iteration: 3000 / 4000 [ 75%]  (Sampling) \nChain 3 Iteration: 3500 / 4000 [ 87%]  (Sampling) \nChain 1 Iteration: 3500 / 4000 [ 87%]  (Sampling) \nChain 4 Iteration: 3000 / 4000 [ 75%]  (Sampling) \nChain 3 Iteration: 4000 / 4000 [100%]  (Sampling) \nChain 3 finished in 283.6 seconds.\nChain 2 Iteration: 3500 / 4000 [ 87%]  (Sampling) \nChain 1 Iteration: 4000 / 4000 [100%]  (Sampling) \nChain 1 finished in 311.5 seconds.\nChain 4 Iteration: 3500 / 4000 [ 87%]  (Sampling) \nChain 2 Iteration: 4000 / 4000 [100%]  (Sampling) \nChain 2 finished in 330.5 seconds.\nChain 4 Iteration: 4000 / 4000 [100%]  (Sampling) \nChain 4 finished in 349.7 seconds.\n\nAll 4 chains finished successfully.\nMean chain execution time: 318.8 seconds.\nTotal execution time: 349.9 seconds.\n\n\nI’ve turn off warning notifications for this blog post, but I did receive this one:\n\nWarning: 4 of 4 chains had an E-BFMI less than 0.2. See https://mc-stan.org/misc/warnings for details.\n\nThe mc-stan website does indeed contain some advice, but I’m going to ignore this warning for the time being and get on with the blog post.\nThat said, all estimated parameters are pretty much on the money. Importantly, include the estimated slope for the B construct (slope_B: -0.09, with a 95% credible interval of [-0.66, 0.33]). Notice, too, that the model was able to figure out the correlation between the latent constructs A and B (latent_rho).\n\nmodel_fit$summary(\n  variables = c(\"intercept\", \"slope_A\", \"slope_B\", \"sigma\"\n                ,\"sigma_lat_A\", \"sigma_lat_B\"\n                , \"latent_means\", \"latent_rho\"\n                , \"slope_A_std\", \"slope_B_std\"\n                , \"reliability_A\", \"reliability_B\")\n  , \"mean\", \"sd\"\n  , extra_quantiles = ~posterior::quantile2(., probs = c(0.025, 0.975))\n  , \"rhat\"\n)\n\n# A tibble: 13 × 6\n   variable          mean     sd   q2.5  q97.5  rhat\n   &lt;chr&gt;            &lt;num&gt;  &lt;num&gt;  &lt;num&gt;  &lt;num&gt; &lt;num&gt;\n 1 intercept        1.25  1.63   -2.38   3.92   1.01\n 2 slope_A          0.782 0.185   0.483  1.20   1.02\n 3 slope_B         -0.137 0.277  -0.750  0.316  1.01\n 4 sigma            1.21  0.0742  1.05   1.35   1.01\n 5 sigma_lat_A      1.55  0.0850  1.39   1.72   1.01\n 6 sigma_lat_B      0.828 0.0279  0.775  0.883  1.00\n 7 latent_means[1]  2.90  0.0830  2.74   3.07   1.00\n 8 latent_means[2] -4.01  0.0370 -4.08  -3.93   1.00\n 9 latent_rho       0.775 0.0465  0.680  0.861  1.01\n10 slope_A_std      1.21  0.249   0.792  1.76   1.01\n11 slope_B_std     -0.114 0.230  -0.620  0.262  1.01\n12 reliability_A    0.705 0.0618  0.593  0.829  1.01\n13 reliability_B    0.950 0.0209  0.903  0.984  1.01\n\n\nTo get some sense of what the model is doing, I’m going to extract the posterior distributions for the latent construct scores. These are the model’s guesses of which scores the simulated participants would have had if there had been no measurement error. These guesses are based on the information we’ve fed the model, including the observed variables, the relationships among them, and their probable reliability. I’m just going to work with the means of these posterior distributions, but there can be substantial uncertainty about the model’s guesses.\n\nest_lat_A &lt;- model_fit$draws(\"lat_A\", format = \"draws_matrix\")\nest_lat_B &lt;- model_fit$draws(\"lat_B\", format = \"draws_matrix\")\n\n\ndf_variables &lt;- tibble(\n  Z = Z,\n  obs_A = obs_A,\n  obs_B = obs_B,\n  est_A = apply(est_lat_A, 2, mean),\n  est_B = apply(est_lat_B, 2, mean)\n)\n\nFigure 2 shows the relationships among the three variables and shows shrinkage at work. For the variables about whose actual values there is uncertainty (viz., A and B), the model reckons that extreme values are caused by a combination of skill (or lack thereof) as well as good (or bad) luck. Accordingly, it adjusts these values towards the bulk of the data. In doing so, it takes into account both the correlation that we ‘expected’ between A and B as well as the possible relationship between A and B on the one hand and Z on the other. For A, the adjustments are fairly large because this variable was assumed to be measured with considerable error. For B, the adjustments are smaller. Z, finally, was assumed to be measured without error and so no adjustments are required.\n\npar(mfrow = c(2, 2))\n\n# Z vs. A\nplot(Z ~ obs_A, df_variables, pch = 1,\n     xlab = \"A\", ylab = \"Z\")\npoints(Z ~ est_A, df_variables, pch = 16)\narrows(x0 = df_variables$obs_A, x1 = df_variables$est_A,\n       y0 = df_variables$Z,\n       col = \"grey80\", length = 0)\n\n# Z vs. B\nplot(Z ~ obs_B, df_variables, pch = 1,\n     xlab = \"B\", ylab = \"Z\")\npoints(Z ~ est_B, df_variables, pch = 16)\narrows(x0 = df_variables$obs_B, x1 = df_variables$est_B,\n       y0 = df_variables$Z,\n       col = \"grey80\", length = 0)\n\n# B vs. A\nplot(obs_B ~ obs_A, df_variables, pch = 1,\n     xlab = \"A\", ylab = \"B\")\npoints(est_B ~ est_A, df_variables, pch = 16)\narrows(x0 = df_variables$obs_A, x1 = df_variables$est_A,\n       y0 = df_variables$obs_B, y1 = df_variables$est_B,\n       col = \"grey80\", length = 0)\npar(mfrow = c(1, 1))\n\n\n\n\nFigure 2. The relationships among the three variables. The empty circles represent the variables as they were observed. The black circles represent, for each observation, the mean of the model’s guesses as to what the value would have been if there hadn’t been any measurement error.\n\n\n\n\nIn statistics at least, shrinkage is generally a good thing: The shrunken values (i.e., the model’s guesses) lie, on average, closer to the true but unobserved values than the observed values do. This is clearly the case for variable A:\n\nmean(abs(A - obs_A))\n\n[1] 0.724\n\nmean(abs(A - df_variables$est_A))\n\n[1] 0.518\n\n\nFor variable B, the difference is negligible seeing as this variable was measured with exceptional reliability:\n\nmean(abs(B - obs_B))\n\n[1] 0.149\n\nmean(abs(B - df_variables$est_B))\n\n[1] 0.146"
  },
  {
    "objectID": "posts/2020-01-21-statistical-control-measurement-error/index.html#a-real-life-example-linguistic-interdependence",
    "href": "posts/2020-01-21-statistical-control-measurement-error/index.html#a-real-life-example-linguistic-interdependence",
    "title": "Baby steps in Bayes: Accounting for measurement error on a control variable",
    "section": "A real-life example: Linguistic interdependence",
    "text": "A real-life example: Linguistic interdependence\nFor the simulated data, the model seemed to work okay, so let’s turn to a real-life example. I’ll skip the theoretical background, but several studies in applied linguistics have tried to find out if knowledge in a ‘heritage language’ contributes to the development of the societal language (For more information about such research, see Berthele & Lambelet (2017), Vanhove & Berthele (2017) and Berthele & Vanhove (2017)). In a typical research design, researchers collect data on a group of pupils’ language skills in their heritage language as well as in their societal languages at the beginning of the school year. Then, at the end of the school year, they collect similar data. Unsurprisingly, pupils with relatively good societal language skills at the beginning of the year are still relatively good at the end. But what is sometimes also observed is that heritage language proficiency at the first data collection is a predictor of societal language proficiency at the second data collection, even after taking into account societal language proficiency at the first data collection.\nIt’s tempting but premature to interpret such findings as evidence for a beneficial effect of heritage language skills on the development of societal language proficiency. The reason is that (a) societal and heritage language proficiency are bound to be correlated at the first data collection due to factors such as intelligence, testwiseness, form on the day etc., and (b) language proficiency is invariably measured with error. This is true of heritage language proficiency, but most importantly, it’s true of the variable that is “statistically controlled for”, i.e., societal language proficiency. Consequently, it’s likely that an off-the-shelf statistical model undercorrects for the role of societal language proficiency and overestimates the role of heritage language profiency.\nSo let’s fit a model that takes measurement error into account.\n\nData and question\nThe data we’re going to analyse are a subset of those analysed by Vanhove & Berthele (2017) and Berthele & Vanhove (2017). We have data on 91 pupils with French as their societal language and Portuguese as their heritage language. The study consisted of three data collections (and many more pupils), but we’re just going to analyse the reading proficiency data collected during waves 2 and 3 here.\nThe full datasets are are available as an R package from https://github.com/janhove/helascot, but copy-paste the command below into R to work with the reduced dataset we’ll work with here.\n\nskills &lt;- structure(list(\n  Subject = c(\"A_PLF_1\",\"A_PLF_10\",\"A_PLF_12\",\"A_PLF_13\",\"A_PLF_14\",\"A_PLF_15\",\"A_PLF_16\",\"A_PLF_17\",\"A_PLF_19\",\"A_PLF_2\",\"A_PLF_3\",\"A_PLF_4\",\"A_PLF_5\",\"A_PLF_7\",\"A_PLF_8\",\"A_PLF_9\",\"AA_PLF_11\",\"AA_PLF_12\",\"AA_PLF_13\",\"AA_PLF_6\",\"AA_PLF_7\",\"AA_PLF_8\",\"AD_PLF_10\",\"AD_PLF_11\",\"AD_PLF_13\",\"AD_PLF_14\",\"AD_PLF_15\",\"AD_PLF_16\",\"AD_PLF_17\",\"AD_PLF_18\",\"AD_PLF_19\",\"AD_PLF_2\",\"AD_PLF_20\",\"AD_PLF_21\",\"AD_PLF_22\",\"AD_PLF_24\",\"AD_PLF_25\",\"AD_PLF_26\",\"AD_PLF_4\",\"AD_PLF_6\",\"AD_PLF_8\",\"AD_PLF_9\",\"AE_PLF_1\",\"AE_PLF_2\",\"AE_PLF_4\",\"AE_PLF_5\",\"AE_PLF_6\",\"C_PLF_1\",\"C_PLF_16\",\"C_PLF_19\",\"C_PLF_30\",\"D_PLF_1\",\"D_PLF_2\",\"D_PLF_3\",\"D_PLF_4\",\"D_PLF_5\",\"D_PLF_6\",\"D_PLF_7\",\"D_PLF_8\",\"Y_PNF_12\",\"Y_PNF_15\",\"Y_PNF_16\",\"Y_PNF_17\",\"Y_PNF_18\",\"Y_PNF_2\",\"Y_PNF_20\",\"Y_PNF_24\",\"Y_PNF_25\",\"Y_PNF_26\",\"Y_PNF_27\",\"Y_PNF_28\",\"Y_PNF_29\",\"Y_PNF_3\",\"Y_PNF_31\",\"Y_PNF_32\",\"Y_PNF_33\",\"Y_PNF_34\",\"Y_PNF_36\",\"Y_PNF_4\",\"Y_PNF_5\",\"Y_PNF_6\",\"Y_PNF_7\",\"Y_PNF_8\",\"Y_PNF_9\",\"Z_PLF_2\",\"Z_PLF_3\",\"Z_PLF_4\",\"Z_PLF_5\",\"Z_PLF_6\",\"Z_PLF_7\",\"Z_PLF_8\")\n  , FR_T2 = c(0.6842105263,0.4736842105,1,0.4210526316,0.6842105263,0.6842105263,0.8947368421,0.5789473684,0.7368421053,0.7894736842,0.4210526316,0.5263157895,0.3157894737,0.5263157895,0.6842105263,0.8421052632,0.3684210526,0.8421052632,0.7894736842,0.7894736842,0.6842105263,0.6315789474,0.6315789474,0.3684210526,0.4736842105,0.2631578947,0.4736842105,0.9473684211,0.3157894737,0.5789473684,0.2631578947,0.5263157895,0.5263157895,0.7368421053,0.6315789474,0.8947368421,0.6315789474,0.9473684211,0.7368421053,0.6315789474,0.7894736842,0.7894736842,0.4736842105,0.4736842105,0.9473684211,0.7894736842,0.3157894737,0.9473684211,1,0.7368421053,0.5789473684,0.8421052632,0.8421052632,0.7368421053,0.5789473684,0.6842105263,0.4736842105,0.4210526316,0.6842105263,0.8947368421,0.6842105263,0.7368421053,0.5263157895,0.5789473684,0.8947368421,0.7894736842,0.5263157895,0.6315789474,0.3157894737,0.7368421053,0.5789473684,0.6842105263,0.7368421053,0.5789473684,0.7894736842,0.6842105263,0.6315789474,0.6842105263,0.5789473684,0.7894736842,0.5789473684,0.7368421053,0.4736842105,0.8947368421,0.8421052632,0.7894736842,0.6315789474,0.6842105263,0.8947368421,0.6842105263,0.9473684211)\n  , PT_T2 = c(0.7368421053,0.5789473684,0.9473684211,0.5263157895,0.6315789474,0.5789473684,0.9473684211,0.4736842105,0.8421052632,0.5263157895,0.2631578947,0.6842105263,0.3684210526,0.3684210526,0.4736842105,0.8947368421,0.4210526316,0.5263157895,0.8947368421,0.8421052632,0.8947368421,0.8947368421,0.6315789474,0.3684210526,0.0526315789,0.3684210526,0.4210526316,0.9473684211,0.3157894737,0.4736842105,0.3157894737,0.5789473684,0.4736842105,0.7894736842,0.5263157895,0.8947368421,0.6315789474,0.7894736842,0.7368421053,0.5789473684,0.6842105263,0.7368421053,0.3684210526,0.7894736842,0.7368421053,0.4736842105,0.5263157895,1,0.8947368421,0.8947368421,0.4736842105,0.8421052632,1,0.6315789474,0.5263157895,0.5789473684,0.5789473684,0.5789473684,0.5263157895,0.9473684211,0.5263157895,0.6315789474,0.5789473684,0.6315789474,0.9473684211,0.7894736842,0.8421052632,0.5263157895,0.7894736842,0.4736842105,0.6842105263,0.3684210526,0.7894736842,0.7368421053,0.6315789474,0.9473684211,0.4210526316,0.5789473684,0.3684210526,0.8947368421,0.6315789474,0.8421052632,0.5789473684,0.5263157895,0.9473684211,0.8947368421,0.7368421053,0.4736842105,0.8421052632,0.7894736842,0.9473684211)\n  , FR_T3 = c(0.9473684211,0.3157894737,0.9473684211,0.5789473684,0.5789473684,0.6842105263,0.8421052632,0.6842105263,0.7368421053,0.8421052632,0.4210526316,0.5789473684,0.4736842105,0.6842105263,0.5789473684,0.7894736842,0.7368421053,0.7894736842,1,0.8421052632,0.8947368421,0.4210526316,0.8947368421,0.4736842105,0.5263157895,0.4736842105,0.5789473684,1,0.7368421053,0.8421052632,0.2631578947,0.7894736842,0.6842105263,0.8947368421,0.5263157895,0.8947368421,0.6842105263,0.9473684211,0.9473684211,0.5263157895,0.9473684211,0.8421052632,0.4736842105,0.8947368421,0.9473684211,0.7368421053,0.5263157895,0.8421052632,0.9473684211,0.7894736842,0.8947368421,0.8421052632,0.8421052632,0.8947368421,0.5789473684,0.7368421053,0.6842105263,0.4736842105,0.6842105263,0.8947368421,0.4736842105,0.8421052632,0.7894736842,0.5789473684,0.7368421053,0.7894736842,0.8947368421,0.6842105263,0.6842105263,0.9473684211,0.7894736842,0.5263157895,0.7368421053,0.6842105263,0.8421052632,0.7368421053,0.7368421053,0.5789473684,0.4736842105,0.8947368421,0.4210526316,0.8947368421,0.6842105263,1,0.8421052632,0.8421052632,0.6315789474,0.6315789474,0.8947368421,0.6315789474,0.9473684211)\n  , PT_T3 = c(0.8421052632,0.3684210526,0.9473684211,0.3157894737,0.5789473684,0.7894736842,1,0.5263157895,0.8421052632,0.7894736842,0.3157894737,0.6315789474,0.4210526316,0.5263157895,0.6842105263,0.8421052632,0.8947368421,0.6842105263,0.9473684211,0.8947368421,0.9473684211,0.8421052632,0.8421052632,0.5263157895,0.6842105263,0.5263157895,0.8421052632,0.9473684211,0.4210526316,0.7894736842,0.7894736842,0.8421052632,0.7368421053,1,0.6842105263,1,0.7894736842,0.8421052632,0.9473684211,0.6842105263,0.7894736842,0.7894736842,0.3157894737,0.7894736842,NA,0.6315789474,0.6842105263,0.9473684211,1,0.9473684211,0.7368421053,0.8947368421,0.8421052632,0.8421052632,0.5789473684,0.6315789474,0.6315789474,0.8421052632,0.7894736842,0.8421052632,0.5789473684,0.8421052632,0.7368421053,0.6842105263,0.8421052632,0.8421052632,0.9473684211,0.4736842105,0.8421052632,0.7894736842,0.7368421053,0.2105263158,0.7894736842,0.7894736842,0.7368421053,0.6315789474,0.6315789474,0.4210526316,0.6315789474,0.8421052632,0.6842105263,0.9473684211,0.5789473684,0.5263157895,0.7894736842,0.7894736842,0.7894736842,0.6842105263,0.8421052632,0.8421052632,0.8947368421)\n  )\n  , row.names = c(NA, -91L)\n  , class = c(\"tbl_df\",\"tbl\",\"data.frame\")\n)\n\nWe’re going to fit the French reading scores at the third data collection (FR_T3) in terms of the French and Portuguese reading scores at the second data collection (FR_T2 and PT_T2). Figure 3 shows the observed variables. Note that all values are bounded between 0 and 1, where 1 was the highest possible result.\n\nscatterplot_matrix(skills %&gt;% select(FR_T3, FR_T2, PT_T2))\n\n\n\n\nFigure 3. The relationships between the French proficiency scores at T3, the French proficiency scores at T2 and the Portuguese profiency scores at T2.\n\n\n\n\nFitting an off-the-shelf regression model, we find that PT_T2 is significantly related to FR_T3, even when accounting for FR_T2.\n\nsummary(lm(FR_T3 ~ FR_T2 + PT_T2, skills))$coefficients\n\n            Estimate Std. Error t value Pr(&gt;|t|)\n(Intercept)    0.266     0.0520    5.11 1.84e-06\nFR_T2          0.506     0.0990    5.11 1.84e-06\nPT_T2          0.196     0.0868    2.26 2.60e-02\n\n\nLastly, as reported by Pestana et al. (2017), the reliability of the French reading test at T2 was estimated to be 0.73, with a 95% confidence interval of [0.65, 0.78]. For Portuguese at T2, the reliability was estimated to be 0.79, with a 95% confidence interval of [0.72, 0.84]. This is information we can feed to the model. (For French at T3, the estimated reliability coefficient was 0.73, 95% CI: [0.65, 0.79], but for now, we’re not going to model the measurement error on the outcome variable.)\n\n\nModel\nThe model specified below is essentially the same as the model for the simulated example, but with more informed priors.\nThe reliability estimates for the French T2 and Portuguese T2 variables were incorporated by means of prior distributions.\n\nFor French T2, I put a beta(73, 27) prior on the reliability coefficient, which assigns a 95% probability of the reliablity coefficient lying between 0.64 and 0.81. This doesn’t exactly correspond to the estimated reliability coefficient’s confidence interval, but I think it’s close enough.\nFor Portuguese T2, I put a beta(79, 21) prior on the reliability coefficient, which assigns a 95% probability of the reliablity coefficient lying between 0.71 and 0.86.\n\nOther prior distributions reflect the fact that the predictor and the outcome data were restricted to the [0, 1] range and some common knowledge. The rationale for them is explained in the comments sprinkled throughout the code.\n\ninterdependence_code &lt;- '\ndata { \n  // Number of observations\n  int&lt;lower = 1&gt; N;      \n  // Observed outcome\n  vector[N] FR_T3;         \n  // Observed predictors\n  vector[N] FR_T2;\n  vector[N] PT_T2;\n}\nparameters {\n  // Parameters for regression\n  real intercept;  \n  real slope_FR;\n  real slope_PT;\n  real&lt;lower = 0&gt; sigma; \n  \n  // standard deviations of latent variables\n  real&lt;lower = 0&gt; sigma_lat_FR_T2; \n  real&lt;lower = 0&gt; sigma_lat_PT_T2;\n  \n  // Unknown but estimated reliabilities\n  real&lt;lower = 0, upper = 1&gt; reliability_FR_T2;\n  real&lt;lower = 0, upper = 1&gt; reliability_PT_T2;\n  \n  // Means of latent predictors\n  row_vector[2] latent_means;\n  \n  // Unknown correlation between latent predictors\n  real&lt;lower = -1, upper = 1&gt; latent_rho;\n  \n  // Latent variables\n  matrix[N, 2] latent_variables;\n} \ntransformed parameters {\n  vector[N] mu_FR_T3;  // conditional mean of outcome\n  vector[N] lat_FR_T2; // latent variables, separated out\n  vector[N] lat_PT_T2;\n  real error_FR_T2; // standard error of measurement\n  real error_PT_T2;\n  matrix[2, 2] sigma_lat; // standard deviations of latent variables, in matrix form\n  cov_matrix[2] latent_cor; // correlation and covariance matrix for latent variables \n  cov_matrix[2] latent_cov;\n  real slope_FR_std;\n  real slope_PT_std;\n  \n  // Express measurement error in terms of \n  // standard deviation of constructs and reliability\n  error_FR_T2 = sqrt(sigma_lat_FR_T2^2/reliability_FR_T2 - sigma_lat_FR_T2^2);\n  error_PT_T2 = sqrt(sigma_lat_PT_T2^2/reliability_PT_T2 - sigma_lat_PT_T2^2);\n  \n  // Define diagonal matrix with standard errors of latent variables\n  sigma_lat[1, 1] = sigma_lat_FR_T2;\n  sigma_lat[2, 2] = sigma_lat_PT_T2;\n  sigma_lat[1, 2] = 0;\n  sigma_lat[2, 1] = 0;\n\n  // Define correlation matrix for latent variables\n  latent_cor[1, 1] = 1;\n  latent_cor[2, 2] = 1;\n  latent_cor[1, 2] = latent_rho;\n  latent_cor[2, 1] = latent_rho;\n  \n  // Compute covariance matrix for latent variables\n  latent_cov = sigma_lat * latent_cor * sigma_lat;\n  \n  // Extract latent variables from matrix\n  lat_FR_T2 = latent_variables[, 1];\n  lat_PT_T2 = latent_variables[, 2];\n  \n  // Regression model\n  mu_FR_T3 = intercept + slope_FR*lat_FR_T2 + slope_PT*lat_PT_T2;\n  \n  // Standardised regression parameters\n  slope_FR_std = slope_FR * sigma_lat_FR_T2;\n  slope_PT_std = slope_PT * sigma_lat_PT_T2;\n}\nmodel {\n  // Priors for regression parameters; these essentially\n  // reflect the fact that the data lie in the [0, 1] interval.\n  // The intercept is for FR_T2 and PT_T2 = 0, so a pupil with\n  // poor scores at T2. So probably pretty low.\n  intercept ~ normal(0.20, 0.10);\n  // French at T2 is bound to positively predict French at T3,\n  // but the slope can at most be 1 (bounded data).\n  slope_FR ~ normal(0.50, 0.25);\n  // Portuguese at T2 is not necessarily a positive predictor of\n  // French at T3, given French at T2. So centre around 0.\n  slope_PT ~ normal(0, 0.25);\n  // If neither of the T2 variables predicts T3, uncertainty\n  // is highest when the mean T3 score is 0.5. Since these scores\n  // are bounded between 0 and 1, the standard deviation could not \n  // be much higher than 0.20. But French T2 is bound to be a predictor,\n  // so let us choose a slighlty lower value.\n  sigma ~ normal(0.15, 0.08);\n  \n  // Prior reliabilities\n  reliability_FR_T2 ~ beta(73, 27); // alpha = 0.73, 95% CI [0.65, 0.78], \n                                    // which is roughly beta(73, 27)\n  reliability_PT_T2 ~ beta(79, 21);  // alpha = 0.79, 95% CI [0.72, 0.84]\n  \n  // Prior expectation for latent means, viz., the means\n  // for French and Portuguese T2. These are going to be in the 0.4-0.6\n  // range.\n  latent_means ~ normal(0.50, 0.10);\n  \n  // Prior expectation for correlation between latent variables:\n  // tend towards positive rho; pretty vague;\n  latent_rho ~ normal(0.4, 0.3);\n  \n  // Distribution of latent variable\n  for (i in 1:N) {\n    latent_variables[i, ] ~ multi_normal(latent_means, latent_cov);\n  }\n  \n  // Measurement model\n  FR_T2 ~ normal(lat_FR_T2, error_FR_T2);\n  PT_T2 ~ normal(lat_PT_T2, error_PT_T2);\n  \n  // Generate outcome\n  FR_T3 ~ normal(mu_FR_T3, sigma);\n}\n'\n\n\ndata_list &lt;- list(\n  FR_T2 = skills$FR_T2,\n  PT_T2 = skills$PT_T2,\n  FR_T3 = skills$FR_T3,\n  N = nrow(skills)\n)\n\n\ninterdependence_model &lt;- cmdstan_model(write_stan_file(interdependence_code))\ninterdependence_fit &lt;- interdependence_model$sample(\n  data = data_list\n  , seed = 42\n  , chains = 4\n  , parallel_chains = 4\n  , iter_warmup = 2000\n  , iter_sampling = 6000\n  , refresh = 1000\n  , max_treedepth = 12\n  , adapt_delta = 0.99\n)\n\nRunning MCMC with 4 parallel chains...\n\nChain 1 Iteration:    1 / 8000 [  0%]  (Warmup) \nChain 2 Iteration:    1 / 8000 [  0%]  (Warmup) \nChain 3 Iteration:    1 / 8000 [  0%]  (Warmup) \nChain 4 Iteration:    1 / 8000 [  0%]  (Warmup) \nChain 4 Iteration: 1000 / 8000 [ 12%]  (Warmup) \nChain 1 Iteration: 1000 / 8000 [ 12%]  (Warmup) \nChain 2 Iteration: 1000 / 8000 [ 12%]  (Warmup) \nChain 3 Iteration: 1000 / 8000 [ 12%]  (Warmup) \nChain 4 Iteration: 2000 / 8000 [ 25%]  (Warmup) \nChain 4 Iteration: 2001 / 8000 [ 25%]  (Sampling) \nChain 1 Iteration: 2000 / 8000 [ 25%]  (Warmup) \nChain 1 Iteration: 2001 / 8000 [ 25%]  (Sampling) \nChain 2 Iteration: 2000 / 8000 [ 25%]  (Warmup) \nChain 2 Iteration: 2001 / 8000 [ 25%]  (Sampling) \nChain 3 Iteration: 2000 / 8000 [ 25%]  (Warmup) \nChain 3 Iteration: 2001 / 8000 [ 25%]  (Sampling) \nChain 4 Iteration: 3000 / 8000 [ 37%]  (Sampling) \nChain 1 Iteration: 3000 / 8000 [ 37%]  (Sampling) \nChain 2 Iteration: 3000 / 8000 [ 37%]  (Sampling) \nChain 3 Iteration: 3000 / 8000 [ 37%]  (Sampling) \nChain 4 Iteration: 4000 / 8000 [ 50%]  (Sampling) \nChain 1 Iteration: 4000 / 8000 [ 50%]  (Sampling) \nChain 2 Iteration: 4000 / 8000 [ 50%]  (Sampling) \nChain 3 Iteration: 4000 / 8000 [ 50%]  (Sampling) \nChain 4 Iteration: 5000 / 8000 [ 62%]  (Sampling) \nChain 1 Iteration: 5000 / 8000 [ 62%]  (Sampling) \nChain 2 Iteration: 5000 / 8000 [ 62%]  (Sampling) \nChain 3 Iteration: 5000 / 8000 [ 62%]  (Sampling) \nChain 1 Iteration: 6000 / 8000 [ 75%]  (Sampling) \nChain 4 Iteration: 6000 / 8000 [ 75%]  (Sampling) \nChain 2 Iteration: 6000 / 8000 [ 75%]  (Sampling) \nChain 3 Iteration: 6000 / 8000 [ 75%]  (Sampling) \nChain 1 Iteration: 7000 / 8000 [ 87%]  (Sampling) \nChain 2 Iteration: 7000 / 8000 [ 87%]  (Sampling) \nChain 4 Iteration: 7000 / 8000 [ 87%]  (Sampling) \nChain 3 Iteration: 7000 / 8000 [ 87%]  (Sampling) \nChain 1 Iteration: 8000 / 8000 [100%]  (Sampling) \nChain 1 finished in 119.3 seconds.\nChain 2 Iteration: 8000 / 8000 [100%]  (Sampling) \nChain 2 finished in 119.8 seconds.\nChain 4 Iteration: 8000 / 8000 [100%]  (Sampling) \nChain 4 finished in 120.4 seconds.\nChain 3 Iteration: 8000 / 8000 [100%]  (Sampling) \nChain 3 finished in 124.1 seconds.\n\nAll 4 chains finished successfully.\nMean chain execution time: 120.9 seconds.\nTotal execution time: 124.2 seconds.\n\n\nAfter tweaking the adapt_delta and max_treedepth parameters and letting the model run for a sufficient number of iterations, it converged without errors or warnings.\n\n\nResults\nCompared to the off-the-shelf lm() model, the model that takes measurement errors and correlated predictors into account estimates the parameter for FR_T2 to be higher and that of PT_T2 to be lower. It is also more uncertain about these parameters than the off-the-shelf model, I think appropriately so. The slope for PT_T2, which is the important bit, is now estimated to be 0.10, with a 95% credible interval of [-0.17, 0.38]. So when you take measurement error into account, you find that there is less evidence for a beneficial effect of heritage language proficiency on the development of societal language proficiency that you could otherwise have thought. Moreover, the model estimates that the correlation between FR_T2 and PT_T2 is in the vicinity of 0.81 rather than 0.67, once measurement error has been accounted for.\n\ninterdependence_fit$summary(\n  variables = c(\"intercept\", \"slope_FR\", \"slope_PT\", \"sigma\"\n                , \"sigma_lat_FR_T2\", \"sigma_lat_PT_T2\"\n                , \"latent_means\", \"latent_rho\"\n                , \"reliability_FR_T2\", \"reliability_PT_T2\")\n  , \"mean\", \"sd\"\n  , extra_quantiles = ~posterior::quantile2(., probs = c(0.025, 0.975))\n  , \"rhat\"\n)\n\n# A tibble: 11 × 6\n   variable           mean     sd    q2.5 q97.5  rhat\n   &lt;chr&gt;             &lt;num&gt;  &lt;num&gt;   &lt;num&gt; &lt;num&gt; &lt;num&gt;\n 1 intercept         0.191 0.0536  0.0869 0.297  1.00\n 2 slope_FR          0.712 0.156   0.405  1.02   1.00\n 3 slope_PT          0.105 0.140  -0.169  0.382  1.00\n 4 sigma             0.116 0.0114  0.0941 0.139  1.00\n 5 sigma_lat_FR_T2   0.155 0.0122  0.133  0.181  1.00\n 6 sigma_lat_PT_T2   0.182 0.0141  0.157  0.212  1.00\n 7 latent_means[1]   0.653 0.0183  0.617  0.689  1.00\n 8 latent_means[2]   0.642 0.0210  0.601  0.683  1.00\n 9 latent_rho        0.813 0.0783  0.643  0.949  1.00\n10 reliability_FR_T2 0.750 0.0392  0.670  0.823  1.00\n11 reliability_PT_T2 0.790 0.0402  0.706  0.863  1.00\n\n\nConsiderable shrinkage is observed for both predictors (Figure 4); keep in mind that we didn’t account for measurement error on the outcome. The plots in the top row shows how predictor values that are pretty low given a certain outcome value are shifted toward the right and how predictor values that are pretty high given a certain outcome value get shifted toward the left. The plot in the bottom row additionally shows that the model has learnt, among other things, that if a participant has a high PT_T2 score but a low FR_T2 score, that the former is probably an overestimate and the latter is likely an underestimate. So it adjusts both values towards the centre.\n\nlat_FR_T2 &lt;- interdependence_fit$draws(\"lat_FR_T2\", format = \"draws_matrix\")\nlat_PT_T2 &lt;- interdependence_fit$draws(\"lat_PT_T2\", format = \"draws_matrix\")\n\ndf_variables &lt;- data.frame(\n  FR_T3 = skills$FR_T3,\n  FR_T2 = skills$FR_T2,\n  PT_T2 = skills$PT_T2,\n  est_FR_T2 = apply(lat_FR_T2, 2, mean),\n  est_PT_T2 = apply(lat_PT_T2, 2, mean)\n)\n\npar(mfrow = c(2, 2))\nplot(FR_T3 ~ FR_T2, data = df_variables, pch = 1)\npoints(FR_T3 ~ est_FR_T2, data = df_variables, pch = 16)\narrows(x0 = df_variables$FR_T2, x1 = df_variables$est_FR_T2,\n       y0 = df_variables$FR_T3, y1 = df_variables$FR_T3,\n       col = \"grey60\", length = 0.1)\n\nplot(FR_T3 ~ PT_T2, data = df_variables, pch = 1)\npoints(FR_T3 ~ est_PT_T2, data = df_variables, pch = 16)\narrows(x0 = df_variables$PT_T2, x1 = df_variables$est_PT_T2,\n       y0 = df_variables$FR_T3, y1 = df_variables$FR_T3,\n       col = \"grey60\", length = 0.1)\n\nplot(PT_T2 ~ FR_T2, data = df_variables, pch = 1)\npoints(est_PT_T2 ~ est_FR_T2, data = df_variables, pch = 16)\narrows(x0 = df_variables$FR_T2, x1 = df_variables$est_FR_T2,\n       y0 = df_variables$PT_T2, y1 = df_variables$est_PT_T2,\n       col = \"grey60\", length = 0.1)\n\n\n\n\nFigure 4. Relationships among the predictors and the outcome. The empty circles represent the variables as they were observed. The black circles represent, for each observation, the mean of the model’s guesses as to what the value would have been if there hadn’t been any measurement error."
  },
  {
    "objectID": "posts/2020-01-21-statistical-control-measurement-error/index.html#caveats",
    "href": "posts/2020-01-21-statistical-control-measurement-error/index.html#caveats",
    "title": "Baby steps in Bayes: Accounting for measurement error on a control variable",
    "section": "Caveats",
    "text": "Caveats\n\nI’m just trying to figure out this stuff, so this blog posts comes with absolutely no warranties.\nWe only took into account reliability and we took validity for granted. In the real-life example, this means that we acknowledged that the French reading test measured French reading proficiency imperfectly — but we did assume that it, deep down, measured French reading proficiency. You can easily imagine a test that measures something highly reliably, but that still doesn’t reflect what it’s supposed to measure well. (For instance, a highly reliable trivia quiz about the Belgian national football team under the reign of Georges Leekens that’s used to represent general intelligence.) Taking measurement error (in terms of unreliability) into account doesn’t fix possible validity problems."
  },
  {
    "objectID": "posts/2020-01-21-statistical-control-measurement-error/index.html#software-versions",
    "href": "posts/2020-01-21-statistical-control-measurement-error/index.html#software-versions",
    "title": "Baby steps in Bayes: Accounting for measurement error on a control variable",
    "section": "Software versions",
    "text": "Software versions\nPlease note that I reran the code on this page on August 6, 2023.\n\ndevtools::session_info()\n\n─ Session info ───────────────────────────────────────────────────────────────\n setting  value\n version  R version 4.3.1 (2023-06-16)\n os       Ubuntu 22.04.2 LTS\n system   x86_64, linux-gnu\n ui       X11\n language en_US\n collate  en_US.UTF-8\n ctype    en_US.UTF-8\n tz       Europe/Zurich\n date     2023-08-06\n pandoc   3.1.1 @ /usr/lib/rstudio/resources/app/bin/quarto/bin/tools/ (via rmarkdown)\n\n─ Packages ───────────────────────────────────────────────────────────────────\n package        * version date (UTC) lib source\n abind            1.4-5   2016-07-21 [1] CRAN (R 4.3.1)\n backports        1.4.1   2021-12-13 [1] CRAN (R 4.3.0)\n cachem           1.0.6   2021-08-19 [2] CRAN (R 4.2.0)\n callr            3.7.3   2022-11-02 [1] CRAN (R 4.3.1)\n checkmate        2.2.0   2023-04-27 [1] CRAN (R 4.3.1)\n cli              3.6.1   2023-03-23 [1] CRAN (R 4.3.0)\n cmdstanr       * 0.6.0   2023-08-02 [1] local\n colorspace       2.1-0   2023-01-23 [1] CRAN (R 4.3.0)\n crayon           1.5.2   2022-09-29 [1] CRAN (R 4.3.1)\n data.table       1.14.8  2023-02-17 [1] CRAN (R 4.3.0)\n devtools         2.4.5   2022-10-11 [1] CRAN (R 4.3.1)\n digest           0.6.29  2021-12-01 [2] CRAN (R 4.2.0)\n distributional   0.3.2   2023-03-22 [1] CRAN (R 4.3.1)\n dplyr          * 1.1.2   2023-04-20 [1] CRAN (R 4.3.0)\n ellipsis         0.3.2   2021-04-29 [2] CRAN (R 4.2.0)\n evaluate         0.15    2022-02-18 [2] CRAN (R 4.2.0)\n fansi            1.0.4   2023-01-22 [1] CRAN (R 4.3.1)\n farver           2.1.1   2022-07-06 [1] CRAN (R 4.3.0)\n fastmap          1.1.0   2021-01-25 [2] CRAN (R 4.2.0)\n forcats        * 1.0.0   2023-01-29 [1] CRAN (R 4.3.0)\n fs               1.5.2   2021-12-08 [2] CRAN (R 4.2.0)\n generics         0.1.3   2022-07-05 [1] CRAN (R 4.3.0)\n ggplot2        * 3.4.2   2023-04-03 [1] CRAN (R 4.3.0)\n glue             1.6.2   2022-02-24 [2] CRAN (R 4.2.0)\n gtable           0.3.3   2023-03-21 [1] CRAN (R 4.3.0)\n hms              1.1.3   2023-03-21 [1] CRAN (R 4.3.0)\n htmltools        0.5.5   2023-03-23 [1] CRAN (R 4.3.0)\n htmlwidgets      1.6.2   2023-03-17 [1] CRAN (R 4.3.1)\n httpuv           1.6.11  2023-05-11 [1] CRAN (R 4.3.1)\n jsonlite         1.8.7   2023-06-29 [1] CRAN (R 4.3.1)\n knitr            1.39    2022-04-26 [2] CRAN (R 4.2.0)\n later            1.3.1   2023-05-02 [1] CRAN (R 4.3.1)\n lifecycle        1.0.3   2022-10-07 [1] CRAN (R 4.3.0)\n lubridate      * 1.9.2   2023-02-10 [1] CRAN (R 4.3.0)\n magrittr         2.0.3   2022-03-30 [1] CRAN (R 4.3.0)\n MASS             7.3-60  2023-05-04 [4] CRAN (R 4.3.1)\n matrixStats      1.0.0   2023-06-02 [1] CRAN (R 4.3.1)\n memoise          2.0.1   2021-11-26 [2] CRAN (R 4.2.0)\n mime             0.10    2021-02-13 [2] CRAN (R 4.0.2)\n miniUI           0.1.1.1 2018-05-18 [1] CRAN (R 4.3.1)\n munsell          0.5.0   2018-06-12 [1] CRAN (R 4.3.0)\n pillar           1.9.0   2023-03-22 [1] CRAN (R 4.3.0)\n pkgbuild         1.4.2   2023-06-26 [1] CRAN (R 4.3.1)\n pkgconfig        2.0.3   2019-09-22 [2] CRAN (R 4.2.0)\n pkgload          1.3.2.1 2023-07-08 [1] CRAN (R 4.3.1)\n posterior      * 1.4.1   2023-03-14 [1] CRAN (R 4.3.1)\n prettyunits      1.1.1   2020-01-24 [2] CRAN (R 4.2.0)\n processx         3.8.2   2023-06-30 [1] CRAN (R 4.3.1)\n profvis          0.3.8   2023-05-02 [1] CRAN (R 4.3.1)\n promises         1.2.0.1 2021-02-11 [1] CRAN (R 4.3.1)\n ps               1.7.5   2023-04-18 [1] CRAN (R 4.3.1)\n purrr          * 1.0.1   2023-01-10 [1] CRAN (R 4.3.0)\n R6               2.5.1   2021-08-19 [2] CRAN (R 4.2.0)\n Rcpp             1.0.11  2023-07-06 [1] CRAN (R 4.3.1)\n readr          * 2.1.4   2023-02-10 [1] CRAN (R 4.3.0)\n remotes          2.4.2   2021-11-30 [2] CRAN (R 4.2.0)\n rlang            1.1.1   2023-04-28 [1] CRAN (R 4.3.0)\n rmarkdown        2.21    2023-03-26 [1] CRAN (R 4.3.0)\n rstudioapi       0.14    2022-08-22 [1] CRAN (R 4.3.0)\n scales           1.2.1   2022-08-20 [1] CRAN (R 4.3.0)\n sessioninfo      1.2.2   2021-12-06 [2] CRAN (R 4.2.0)\n shiny            1.7.4.1 2023-07-06 [1] CRAN (R 4.3.1)\n stringi          1.7.12  2023-01-11 [1] CRAN (R 4.3.1)\n stringr        * 1.5.0   2022-12-02 [1] CRAN (R 4.3.0)\n tensorA          0.36.2  2020-11-19 [1] CRAN (R 4.3.1)\n tibble         * 3.2.1   2023-03-20 [1] CRAN (R 4.3.0)\n tidyr          * 1.3.0   2023-01-24 [1] CRAN (R 4.3.0)\n tidyselect       1.2.0   2022-10-10 [1] CRAN (R 4.3.0)\n tidyverse      * 2.0.0   2023-02-22 [1] CRAN (R 4.3.1)\n timechange       0.2.0   2023-01-11 [1] CRAN (R 4.3.0)\n tzdb             0.4.0   2023-05-12 [1] CRAN (R 4.3.0)\n urlchecker       1.0.1   2021-11-30 [1] CRAN (R 4.3.1)\n usethis          2.2.2   2023-07-06 [1] CRAN (R 4.3.1)\n utf8             1.2.3   2023-01-31 [1] CRAN (R 4.3.1)\n vctrs            0.6.3   2023-06-14 [1] CRAN (R 4.3.0)\n withr            2.5.0   2022-03-03 [2] CRAN (R 4.2.0)\n xfun             0.39    2023-04-20 [1] CRAN (R 4.3.0)\n xtable           1.8-4   2019-04-21 [1] CRAN (R 4.3.1)\n yaml             2.3.5   2022-02-21 [2] CRAN (R 4.2.0)\n\n [1] /home/jan/R/x86_64-pc-linux-gnu-library/4.3\n [2] /usr/local/lib/R/site-library\n [3] /usr/lib/R/site-library\n [4] /usr/lib/R/library\n\n──────────────────────────────────────────────────────────────────────────────"
  },
  {
    "objectID": "posts/2017-09-14-comparable-stimulus-sets/index.html",
    "href": "posts/2017-09-14-comparable-stimulus-sets/index.html",
    "title": "Creating comparable sets of stimuli",
    "section": "",
    "text": "When designing a study, you sometimes have a pool of candidate stimuli (words, sentences, texts, images etc.) that is too large to present to each participant in its entirety. If you want data for all or at least most stimuli, a possible solution is to split up the pool of stimuli into sets of overseeable size and assign each participant to one of the different sets. Ideally, you’d want the different sets to be as comparable as possible with respect to a number of relevant characteristics so that each participant is exposed to about the same diversity of stimuli during the task. For instance, when presenting individual words to participants, you may want for each participant to be confronted with a similar distribution of words in terms of their frequency, length, and number of adverbs. In this blog post I share some R code that I used to split up two types of stimuli into sets that are comparable with respect to one or several variables—in the hopes that you can easily adapt them for your own needs.\nFair warning: There’s lots of R code below, but it’s mainly a matter of copy-pasting the same lines and changing one or two things. If the R is poorly organised on your screen, you can copy-paste it into RStudio."
  },
  {
    "objectID": "posts/2017-09-14-comparable-stimulus-sets/index.html#creating-sets-that-are-identical-with-respect-to-one-categorical-variable",
    "href": "posts/2017-09-14-comparable-stimulus-sets/index.html#creating-sets-that-are-identical-with-respect-to-one-categorical-variable",
    "title": "Creating comparable sets of stimuli",
    "section": "Creating sets that are identical with respect to one categorical variable",
    "text": "Creating sets that are identical with respect to one categorical variable\nI have a list of 144 sentences that I want to use as stimuli in an experiment. I categorised these sentences into five different-sized plausibility categories myself. The categories themselves don’t matter here, but before running the experiment itself I want to make sure that my categorisation agrees with people’s intuitions about the plausibility of these sentences. To that end, I will run a validation study in which people rate their plausibility. The way the validation study is designed, it would take about half an hour to rate the plausibility of all 144 sentences. This isn’t that long, but when I can’t pay the participants, I prefer to keep the task’s duration at 10 minutes or so. To that end, I split up the pool of 144 sentences into 3 sets of 48 each. But rather than constructing these sets completely at random, I ensured that the plausibility categories (as I had defined them) were distributed identically in each set, so that each participant would be confronted with roughly the same variation in terms of the sentences’ plausibility. This is just a matter of randomly splitting up each of the categories into 3 sets, but the code below automises this.\nFirst, you need a dataframe that lists the stimuli and their categories. Below, I just create this dataframe in R, but you can also read in a spreadsheet with the candidate stimuli.\n\n# I'll be using the tidyverse suite throughout.\n# Use 'install.packages(\"tidyverse\")' if you don't have it installed.\nlibrary(tidyverse)\n\n── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──\n✔ dplyr     1.1.2     ✔ readr     2.1.4\n✔ forcats   1.0.0     ✔ stringr   1.5.0\n✔ ggplot2   3.4.2     ✔ tibble    3.2.1\n✔ lubridate 1.9.2     ✔ tidyr     1.3.0\n✔ purrr     1.0.1     \n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\nℹ Use the conflicted package (&lt;http://conflicted.r-lib.org/&gt;) to force all conflicts to become errors\n\n# Create a dataframe with the sentence IDs and each sentence's category:\nstimuli &lt;- data.frame(ID = 1:144,\n                      Category = rep(x = c(\"PNR\", \"INR\", \"PR\", \"IR\", \"SYM\"),\n                                     times = c(24, 18, 18, 36, 48)))\n\n# Number of sentences per category in the entire pool\ntable(stimuli$Category)\n\n\nINR  IR PNR  PR SYM \n 18  36  24  18  48 \n\n\nIn this case, the numbers of stimuli per category are all divisible by the number of sets (3), but the code below will also work when this isn’t the case (e.g., when the number of INR items would be 16 and the number of IR items 38). In such cases, the algorithm will create comparable equal-sized sets and a handful of items will remain unassigned. You can try this yourself by changing the number of stimuli per category above, or by setting n_sets below to, say, 5.\n\n# Get number of categories\ncategories &lt;- unique(stimuli$Category)\n# Set number of desired sets\nn_sets &lt;- 5\n\n# Creata a column that will specify the set the stimulus was assigned to\nstimuli$Set &lt;- NA\n\n# For each category...\nfor (i in 1:length(categories)) {\n  # Get number of stimuli in this category\n  n_in_category &lt;- stimuli %&gt;% \n    filter(., Category == categories[i]) %&gt;% \n    nrow()\n  \n  # Throw warning if n_in_category isn't divisible by n_sets\n  if ((n_in_category %% n_sets) &gt; 0) {\n    warning(paste(\"The number of items in category \", categories[i], \" is \", n_in_category, \n               \", which isn't divisible by \", n_sets, \". \", \n               n_in_category %% n_sets, \" items in this category will remained unassigned.\\n\",\n               sep = \"\"))\n  }\n  \n  # Assign items randomly to sets (equal number of items per set).\n  # If n_in_category isn't divisible by n_sets, mod(n_in_category, n_sets) items will\n  # remain unassigned ('NA').\n  stimuli$Set[stimuli$Category == categories[i]] &lt;- c(rep(LETTERS[1:n_sets], \n                                                          each = floor(n_in_category/n_sets)), \n                                                      rep(NA, \n                                                          times = n_in_category %% n_sets)) %&gt;% \n    sample()\n}\n\nWarning: The number of items in category PNR is 24, which isn't divisible by 5. 4 items in this category will remained unassigned.\n\n\nWarning: The number of items in category INR is 18, which isn't divisible by 5. 3 items in this category will remained unassigned.\n\n\nWarning: The number of items in category PR is 18, which isn't divisible by 5. 3 items in this category will remained unassigned.\n\n\nWarning: The number of items in category IR is 36, which isn't divisible by 5. 1 items in this category will remained unassigned.\n\n\nWarning: The number of items in category SYM is 48, which isn't divisible by 5. 3 items in this category will remained unassigned.\n\n\nThe column Set now specifies which set each item was assigned to, and all sets have the same distribution of Category.\n\nhead(stimuli)\n\n  ID Category  Set\n1  1      PNR    B\n2  2      PNR    B\n3  3      PNR    B\n4  4      PNR &lt;NA&gt;\n5  5      PNR    C\n6  6      PNR    D\n\nxtabs(~ Set + Category, stimuli)\n\n   Category\nSet INR IR PNR PR SYM\n  A   3  7   4  3   9\n  B   3  7   4  3   9\n  C   3  7   4  3   9\n  D   3  7   4  3   9\n  E   3  7   4  3   9\n\n\nIf the numbers of stimuli per category aren’t all divisible by the number of sets, some stimuli will remained unassigned. If you want to have data on all stimuli, you could randomly assign these remaining stimuli to the sets. The sets won’t be identically distributed with regard to the categories, but depending on what you want, it may be good enough."
  },
  {
    "objectID": "posts/2017-09-14-comparable-stimulus-sets/index.html#creating-similar-sets-with-respect-to-several-categorical-variables",
    "href": "posts/2017-09-14-comparable-stimulus-sets/index.html#creating-similar-sets-with-respect-to-several-categorical-variables",
    "title": "Creating comparable sets of stimuli",
    "section": "Creating similar sets with respect to several categorical variables",
    "text": "Creating similar sets with respect to several categorical variables\nFor another study, we had 1,065 short texts for which we wanted to collect human lexical richness ratings. Some of the texts were narrative, others argumentative; some were written by bilingual children and others by children in a monolingual control group; and some were written when the children were 8, 9 or 10 years old. You can’t expect volunteers to rate 1,065 texts, so we decided to split them up into 20 sets of 53 texts each. (In reality, each set contained only 50 texts as we excluded a bunch of very short texts.) The sets were constructed so as to be maximally similar in terms of narrative vs. argumentative texts, texts written by bi- vs. monolingual children, and texts written by 8-, 9- and 10-year-olds. Below is the commented R code used to construct them.\nFirst read in a dataframe containing the TextIDs and info regarding bi- vs. monolingual (ControlGroup), TextType and the children’s age (Time: 1, 2, 3). The dataframe for this example is available from http://homeweb.unifr.ch/VanhoveJ/Pub/Data/french_texts.csv.\n\ntexts &lt;- read.csv(\"http://homeweb.unifr.ch/VanhoveJ/Pub/Data/french_texts.csv\"\n                  , stringsAsFactors = TRUE)\nsummary(texts)\n\n     TextID     ControlGroup    TextType        Time      \n Min.   :   1   Mode :logical   arg :559   Min.   :1.000  \n 1st Qu.: 267   FALSE:632       narr:506   1st Qu.:1.000  \n Median : 533   TRUE :433                  Median :2.000  \n Mean   : 533                              Mean   :2.009  \n 3rd Qu.: 799                              3rd Qu.:3.000  \n Max.   :1065                              Max.   :3.000  \n\n\nHere’s how many texts there are for each combination of the three variables:\n\nxtabs(~ ControlGroup + TextType + Time, texts)\n\n, , Time = 1\n\n            TextType\nControlGroup arg narr\n       FALSE 113  100\n       TRUE   78   50\n\n, , Time = 2\n\n            TextType\nControlGroup arg narr\n       FALSE 109  108\n       TRUE   82   74\n\n, , Time = 3\n\n            TextType\nControlGroup arg narr\n       FALSE 101  101\n       TRUE   76   73\n\n\nThere are 2×2×3 = 12 combinations of these three variables. Create a new variable that specifies which of these combinations each text belongs to:\n\n# Paste the grouping variables together\ntexts$Combined &lt;- paste(texts$ControlGroup, texts$TextType, texts$Time, \n                        sep = \"\")\n\n# Breakdown of texts per combination\ntable(texts$Combined)\n\n\n FALSEarg1  FALSEarg2  FALSEarg3 FALSEnarr1 FALSEnarr2 FALSEnarr3   TRUEarg1 \n       113        109        101        100        108        101         78 \n  TRUEarg2   TRUEarg3  TRUEnarr1  TRUEnarr2  TRUEnarr3 \n        82         76         50         74         73 \n\n\nTo divide up these texts evenly into 20 sets, we’ll need at least the following numbers of texts per combination per batch:\n\nfloor(table(texts$Combined)/20)\n\n\n FALSEarg1  FALSEarg2  FALSEarg3 FALSEnarr1 FALSEnarr2 FALSEnarr3   TRUEarg1 \n         5          5          5          5          5          5          3 \n  TRUEarg2   TRUEarg3  TRUEnarr1  TRUEnarr2  TRUEnarr3 \n         4          3          2          3          3 \n\n\nThat is, we’ll need 5 FALSEarg1 texts, 3 TRUEarg2 texts etc. per set. These numbers add up to 48 in this case, which means that 20×48 = 960 out of 1,065 texts can be already be assigned by applying the code from the previous example. Below I only changed the name of the dataframe (stimuli became texts) and of the column containing the categories (Category became Combined).\n\n# Get category names\ncategories &lt;- unique(texts$Combined)\n# Set desired number of sets\nn_sets &lt;- 20\n# Create column that'll contain the set names\ntexts$Set &lt;- NA\n\n# For each category...\nfor (i in 1:length(categories)) {\n  # Get number of stimuli in this category\n  n_in_category &lt;- texts %&gt;% \n    filter(., Combined == categories[i]) %&gt;% \n    nrow()\n  \n  # Throw warning if n_in_category isn't divisible by n_sets\n  if ((n_in_category %% n_sets) &gt; 0) {\n    warning(paste(\"The number of items in category \", categories[i], \" is \", n_in_category, \n                  \", which isn't divisible by \", n_sets, \". \", \n                  n_in_category %% n_sets, \" items in this category will remained unassigned.\\n\",\n                  sep = \"\"))\n  }\n  \n  # Assign items randomly to sets (equal number of items per set).\n  # If n_in_category isn't divisible by n_sets, mod(n_in_category, n_sets) items will\n  # remain unassigned ('NA').\n  texts$Set[texts$Combined == categories[i]] &lt;- c(rep(LETTERS[1:n_sets], \n                                                      each = floor(n_in_category/n_sets)), \n                                                  rep(NA, \n                                                      times = n_in_category %% n_sets)) %&gt;% \n    sample()\n}\n\nWarning: The number of items in category FALSEarg1 is 113, which isn't divisible by 20. 13 items in this category will remained unassigned.\n\n\nWarning: The number of items in category FALSEarg2 is 109, which isn't divisible by 20. 9 items in this category will remained unassigned.\n\n\nWarning: The number of items in category FALSEnarr2 is 108, which isn't divisible by 20. 8 items in this category will remained unassigned.\n\n\nWarning: The number of items in category FALSEarg3 is 101, which isn't divisible by 20. 1 items in this category will remained unassigned.\n\n\nWarning: The number of items in category FALSEnarr3 is 101, which isn't divisible by 20. 1 items in this category will remained unassigned.\n\n\nWarning: The number of items in category TRUEarg1 is 78, which isn't divisible by 20. 18 items in this category will remained unassigned.\n\n\nWarning: The number of items in category TRUEarg2 is 82, which isn't divisible by 20. 2 items in this category will remained unassigned.\n\n\nWarning: The number of items in category TRUEarg3 is 76, which isn't divisible by 20. 16 items in this category will remained unassigned.\n\n\nWarning: The number of items in category TRUEnarr1 is 50, which isn't divisible by 20. 10 items in this category will remained unassigned.\n\n\nWarning: The number of items in category TRUEnarr2 is 74, which isn't divisible by 20. 14 items in this category will remained unassigned.\n\n\nWarning: The number of items in category TRUEnarr3 is 73, which isn't divisible by 20. 13 items in this category will remained unassigned.\n\n\nNow each set contains 48 texts and all sets are identically distributed with regard to each combination of the three structural variables. This leaves 105 texts unassigned. For this project, it wasn’t so important that the distributions of the structural variables TextType, ControlGroup and Time were exactly identical in each set, just close enough.\nIn order to assign the remaining texts to sets, we loosened up the constraints slightly: to each set, we added as many texts as possible while ensuring that the joint distribution of ControlGroup and Time would be identical across the sets; we didn’t insist on equality in terms of TextType any more.\n\n# Combine ControlGroup and Time info.\ntexts$Combined &lt;- paste(texts$ControlGroup, texts$Time,\n                         sep = \"\")\n\n# Retain only unassigned texts\ntexts_2b_assigned &lt;- texts %&gt;% filter(is.na(Set))\n\n# One 'TRUE1' and one 'TRUE3' will be assigned to each set\n# (fewer than 20 texts remaining for the other combinations):\nfloor(table(texts_2b_assigned$Combined)/20)\n\n\nFALSE1 FALSE2 FALSE3  TRUE1  TRUE2  TRUE3 \n     0      0      0      1      0      1 \n\n\nWe can again copy-paste and slightly adapt the code above. I’ve commented all changes:\n\ncategories &lt;- unique(texts$Combined)\n\nfor (i in 1:length(categories)) {\n  # Get number of UNASSIGNED stimuli in this category\n  n_in_category &lt;- texts %&gt;% \n    # only the unassigned stimuli\n    filter(is.na(Set)) %&gt;% \n    filter(., Combined == categories[i]) %&gt;% \n    nrow()\n  \n  if ((n_in_category %% n_sets) &gt; 0) {\n    warning(paste(\"The number of items in category \", categories[i], \" is \", n_in_category, \n                  \", which isn't divisible by \", n_sets, \". \", \n                  n_in_category %% n_sets, \" items in this category will remained unassigned.\\n\",\n                  sep = \"\"))\n  }\n\n  # Note the additional selector (\"is.na(texts$Set)\"): we only want to assign\n  # the stimuli that haven't been assigned yet.\n  texts$Set[is.na(texts$Set) & texts$Combined == categories[i]] &lt;- c(rep(LETTERS[1:n_sets], \n                                                                          each = floor(n_in_category/n_sets)), \n                                                                      rep(NA, \n                                                                          times = n_in_category %% n_sets)) %&gt;% \n    sample()\n}\n\nWarning: The number of items in category FALSE1 is 13, which isn't divisible by 20. 13 items in this category will remained unassigned.\n\n\nWarning: The number of items in category FALSE2 is 17, which isn't divisible by 20. 17 items in this category will remained unassigned.\n\n\nWarning: The number of items in category FALSE3 is 2, which isn't divisible by 20. 2 items in this category will remained unassigned.\n\n\nWarning: The number of items in category TRUE1 is 28, which isn't divisible by 20. 8 items in this category will remained unassigned.\n\n\nWarning: The number of items in category TRUE2 is 16, which isn't divisible by 20. 16 items in this category will remained unassigned.\n\n\nWarning: The number of items in category TRUE3 is 29, which isn't divisible by 20. 9 items in this category will remained unassigned.\n\n\nWith 20×2 = 40 additional texts assigned, all sets now contain 50 stimuli and 65 texts remain unassigned. To assign as many of the remaining texts as possible, we can again loosen the constraints and only insist that all sets be identical in terms of their ControlGroup distribution (for instance). Apart from the first line, the code is the same as above:\n\n# Combine ControlGroup\ntexts$Combined &lt;- paste(texts$ControlGroup, sep = \"\")\n\ncategories &lt;- unique(texts$Combined)\n\nfor (i in 1:length(categories)) {\n  # Get number of UNASSIGNED stimuli in this category\n  n_in_category &lt;- texts %&gt;% \n    # only the unassigned stimuli\n    filter(is.na(Set)) %&gt;% \n    filter(., Combined == categories[i]) %&gt;% \n    nrow()\n  \n  if ((n_in_category %% n_sets) &gt; 0) {\n    warning(paste(\"The number of items in category \", categories[i], \" is \", n_in_category, \n                  \", which isn't divisible by \", n_sets, \". \", \n                  n_in_category %% n_sets, \" items in this category will remained unassigned.\\n\",\n                  sep = \"\"))\n  }\n\n  # Note the additional selector (\"is.na(texts$Set)\"): we only want to assign\n  # the stimuli that haven't been assigned yet.\n  texts$Set[is.na(texts$Set) & texts$Combined == categories[i]] &lt;- c(rep(LETTERS[1:n_sets], \n                                                                          each = floor(n_in_category/n_sets)), \n                                                                      rep(NA, \n                                                                          times = n_in_category %% n_sets)) %&gt;% \n    sample()\n}\n\nWarning: The number of items in category FALSE is 32, which isn't divisible by 20. 12 items in this category will remained unassigned.\n\n\nWarning: The number of items in category TRUE is 33, which isn't divisible by 20. 13 items in this category will remained unassigned.\n\n\nNow each set contains 52 texts. The distribution of ControlGroup is the same in each set; the distributions of TextType and Time are similar but not identical across sets.\n\ntable(texts$Set)\n\n\n A  B  C  D  E  F  G  H  I  J  K  L  M  N  O  P  Q  R  S  T \n52 52 52 52 52 52 52 52 52 52 52 52 52 52 52 52 52 52 52 52 \n\n\nIf you want, you can randomly assign 20 of the remaining 25 texts to a set; 5 texts will remain unassigned if we insist on each set having the same number of texts:\n\n# Get number of unallocated stmuli\nunallocated &lt;- sum(is.na(texts$Set))\n\n# How many stimuli will remain unallocated?\nremain_unallocated &lt;- unallocated %% n_sets\n\n# Allocate whatever texts we still can allocate randomly\nallocations &lt;- c(rep(LETTERS[1:n_sets], length.out = unallocated - remain_unallocated), \n                 rep(NA, remain_unallocated))\n\n# randomise these\nallocations &lt;- sample(allocations)\n\n# add them to data frame\ntexts$Set[is.na(texts$Set)] &lt;- allocations\n\nThis way, each set contains 53 texts:\n\ntable(texts$Set)\n\n\n A  B  C  D  E  F  G  H  I  J  K  L  M  N  O  P  Q  R  S  T \n53 53 53 53 53 53 53 53 53 53 53 53 53 53 53 53 53 53 53 53 \n\n\nWhile the sets aren’t with respect to the structural variables ControlGroup, TextType and Time, they are highly similar:\n\ntexts |&gt; \n  filter(!is.na(Set)) |&gt; \n  group_by(Set) |&gt; \n  summarise(\n    Arg = mean(TextType == \"arg\")\n    , Control = mean(ControlGroup)\n    , Time1 = mean(Time == 1)\n    , Time2 = mean(Time == 2)\n    , Time3 = mean(Time == 3)\n  ) |&gt; \n  pivot_longer(\n    cols = Arg:Time3\n    , names_to = \"Variable\"\n    , values_to = \"Proportion\"\n    ) |&gt; \n  ggplot(aes(x = Variable, y = Proportion,\n             label = Set)) +\n  geom_text(position = position_jitter(width = 0.3, height = 0)) +\n  ylim(0, 1)"
  },
  {
    "objectID": "posts/2017-09-14-comparable-stimulus-sets/index.html#dealing-with-numerical-variables",
    "href": "posts/2017-09-14-comparable-stimulus-sets/index.html#dealing-with-numerical-variables",
    "title": "Creating comparable sets of stimuli",
    "section": "Dealing with numerical variables",
    "text": "Dealing with numerical variables\nI haven’t dealt with numerical variables yet (e.g., constructing word sets that are similar in terms of their corpus frequency), but I think a reasonable way of going about it would be to bin the numerical variable (e.g., convert it to a variable with 10 categories) and apply the functions above. If too many stimuli remain unallocated, the remaining stimuli could be allocated using a wider bin size.\nThis was probably a bit messy with all the R code, but I hope some of you found it useful!"
  },
  {
    "objectID": "posts/2017-09-14-comparable-stimulus-sets/index.html#software-versions",
    "href": "posts/2017-09-14-comparable-stimulus-sets/index.html#software-versions",
    "title": "Creating comparable sets of stimuli",
    "section": "Software versions",
    "text": "Software versions\n\ndevtools::session_info()\n\n─ Session info ───────────────────────────────────────────────────────────────\n setting  value\n version  R version 4.3.1 (2023-06-16)\n os       Ubuntu 22.04.2 LTS\n system   x86_64, linux-gnu\n ui       X11\n language en_US\n collate  en_US.UTF-8\n ctype    en_US.UTF-8\n tz       Europe/Zurich\n date     2023-08-07\n pandoc   3.1.1 @ /usr/lib/rstudio/resources/app/bin/quarto/bin/tools/ (via rmarkdown)\n\n─ Packages ───────────────────────────────────────────────────────────────────\n package     * version date (UTC) lib source\n cachem        1.0.6   2021-08-19 [2] CRAN (R 4.2.0)\n callr         3.7.3   2022-11-02 [1] CRAN (R 4.3.1)\n cli           3.6.1   2023-03-23 [1] CRAN (R 4.3.0)\n colorspace    2.1-0   2023-01-23 [1] CRAN (R 4.3.0)\n crayon        1.5.2   2022-09-29 [1] CRAN (R 4.3.1)\n devtools      2.4.5   2022-10-11 [1] CRAN (R 4.3.1)\n digest        0.6.29  2021-12-01 [2] CRAN (R 4.2.0)\n dplyr       * 1.1.2   2023-04-20 [1] CRAN (R 4.3.0)\n ellipsis      0.3.2   2021-04-29 [2] CRAN (R 4.2.0)\n evaluate      0.15    2022-02-18 [2] CRAN (R 4.2.0)\n fansi         1.0.4   2023-01-22 [1] CRAN (R 4.3.1)\n farver        2.1.1   2022-07-06 [1] CRAN (R 4.3.0)\n fastmap       1.1.0   2021-01-25 [2] CRAN (R 4.2.0)\n forcats     * 1.0.0   2023-01-29 [1] CRAN (R 4.3.0)\n fs            1.5.2   2021-12-08 [2] CRAN (R 4.2.0)\n generics      0.1.3   2022-07-05 [1] CRAN (R 4.3.0)\n ggplot2     * 3.4.2   2023-04-03 [1] CRAN (R 4.3.0)\n glue          1.6.2   2022-02-24 [2] CRAN (R 4.2.0)\n gtable        0.3.3   2023-03-21 [1] CRAN (R 4.3.0)\n hms           1.1.3   2023-03-21 [1] CRAN (R 4.3.0)\n htmltools     0.5.5   2023-03-23 [1] CRAN (R 4.3.0)\n htmlwidgets   1.6.2   2023-03-17 [1] CRAN (R 4.3.1)\n httpuv        1.6.11  2023-05-11 [1] CRAN (R 4.3.1)\n jsonlite      1.8.7   2023-06-29 [1] CRAN (R 4.3.1)\n knitr         1.39    2022-04-26 [2] CRAN (R 4.2.0)\n labeling      0.4.2   2020-10-20 [1] CRAN (R 4.3.0)\n later         1.3.1   2023-05-02 [1] CRAN (R 4.3.1)\n lifecycle     1.0.3   2022-10-07 [1] CRAN (R 4.3.0)\n lubridate   * 1.9.2   2023-02-10 [1] CRAN (R 4.3.0)\n magrittr      2.0.3   2022-03-30 [1] CRAN (R 4.3.0)\n memoise       2.0.1   2021-11-26 [2] CRAN (R 4.2.0)\n mime          0.10    2021-02-13 [2] CRAN (R 4.0.2)\n miniUI        0.1.1.1 2018-05-18 [1] CRAN (R 4.3.1)\n munsell       0.5.0   2018-06-12 [1] CRAN (R 4.3.0)\n pillar        1.9.0   2023-03-22 [1] CRAN (R 4.3.0)\n pkgbuild      1.4.2   2023-06-26 [1] CRAN (R 4.3.1)\n pkgconfig     2.0.3   2019-09-22 [2] CRAN (R 4.2.0)\n pkgload       1.3.2.1 2023-07-08 [1] CRAN (R 4.3.1)\n prettyunits   1.1.1   2020-01-24 [2] CRAN (R 4.2.0)\n processx      3.8.2   2023-06-30 [1] CRAN (R 4.3.1)\n profvis       0.3.8   2023-05-02 [1] CRAN (R 4.3.1)\n promises      1.2.0.1 2021-02-11 [1] CRAN (R 4.3.1)\n ps            1.7.5   2023-04-18 [1] CRAN (R 4.3.1)\n purrr       * 1.0.1   2023-01-10 [1] CRAN (R 4.3.0)\n R6            2.5.1   2021-08-19 [2] CRAN (R 4.2.0)\n Rcpp          1.0.11  2023-07-06 [1] CRAN (R 4.3.1)\n readr       * 2.1.4   2023-02-10 [1] CRAN (R 4.3.0)\n remotes       2.4.2   2021-11-30 [2] CRAN (R 4.2.0)\n rlang         1.1.1   2023-04-28 [1] CRAN (R 4.3.0)\n rmarkdown     2.21    2023-03-26 [1] CRAN (R 4.3.0)\n rstudioapi    0.14    2022-08-22 [1] CRAN (R 4.3.0)\n scales        1.2.1   2022-08-20 [1] CRAN (R 4.3.0)\n sessioninfo   1.2.2   2021-12-06 [2] CRAN (R 4.2.0)\n shiny         1.7.4.1 2023-07-06 [1] CRAN (R 4.3.1)\n stringi       1.7.12  2023-01-11 [1] CRAN (R 4.3.1)\n stringr     * 1.5.0   2022-12-02 [1] CRAN (R 4.3.0)\n tibble      * 3.2.1   2023-03-20 [1] CRAN (R 4.3.0)\n tidyr       * 1.3.0   2023-01-24 [1] CRAN (R 4.3.0)\n tidyselect    1.2.0   2022-10-10 [1] CRAN (R 4.3.0)\n tidyverse   * 2.0.0   2023-02-22 [1] CRAN (R 4.3.1)\n timechange    0.2.0   2023-01-11 [1] CRAN (R 4.3.0)\n tzdb          0.4.0   2023-05-12 [1] CRAN (R 4.3.0)\n urlchecker    1.0.1   2021-11-30 [1] CRAN (R 4.3.1)\n usethis       2.2.2   2023-07-06 [1] CRAN (R 4.3.1)\n utf8          1.2.3   2023-01-31 [1] CRAN (R 4.3.1)\n vctrs         0.6.3   2023-06-14 [1] CRAN (R 4.3.0)\n withr         2.5.0   2022-03-03 [2] CRAN (R 4.2.0)\n xfun          0.39    2023-04-20 [1] CRAN (R 4.3.0)\n xtable        1.8-4   2019-04-21 [1] CRAN (R 4.3.1)\n yaml          2.3.5   2022-02-21 [2] CRAN (R 4.2.0)\n\n [1] /home/jan/R/x86_64-pc-linux-gnu-library/4.3\n [2] /usr/local/lib/R/site-library\n [3] /usr/lib/R/site-library\n [4] /usr/lib/R/library\n\n──────────────────────────────────────────────────────────────────────────────"
  },
  {
    "objectID": "posts/2014-10-28-assessing-differences-of-significance/index.html",
    "href": "posts/2014-10-28-assessing-differences-of-significance/index.html",
    "title": "Assessing differences of significance",
    "section": "",
    "text": "When it comes to finding a powerful descriptive title for a research paper, it’s hard to top Gelman and Stern’s The difference between “significant” and “not significant” is not itself statistically significant. Yet, students and experienced researchers routinely draw substantial conclusions from effects being significant in one condition but not in the other.\nLet’s say you recruit two groups – creatively named Group 1 and Group 2 – for an experiment. In a between-subjects design, half of the participants in each group are tested in a control condition (Condition A) and half in an experimental condition (Condition B). In Group 1, you observe what you’ve hoped for: a significant difference between Conditions A and B. In Group 2, however, the effect of Condition isn’t significant. Since you’re a creative researcher, your brain shifts into high gear and you can readily come up with a handful of interesting explanations as to why the effect can be observed in one group but not in the other. Perhaps the effect is moderated by the participants’ linguistic or cultural backgrounds, or perhaps the effect disappears in older participants?\nBut let’s pause for a moment. As Gelman & Stern’s title says, “the difference between ‘significant’ and ‘not significant’ is not itself statistically significant”. I take it that most researchers readily accept this when the p-values involved hover around the 0.05 threshold. For instance, with 25 observations, a correlation coefficient of 0.40 is statistically significant (p = 0.048), whereas a correlation coefficient of 0.39 isn’t (p = 0.061) – but it’s readily appreciated that the tiny difference between 0.39 and 0.40 probably isn’t meaningful. After all, 0.05 is an arbitrary threshold. But as Gelman & Stern explain with enviable clarity, even more dramatic differences aren’t necessarily significant either.\nI think that researchers – both junior and more experienced ones – routinely read too much into comparisons of significance. To be more specific, I think they play the ‘moderator’ card too quickly and don’t truly consider the more prosaic alternative: chance. Obviously, the problem isn’t restricted to language-related research (see, e.g. Nieuwenhuis et al. 2011 on neuroscience). In this post, I discuss how to test whether the difference between a significant and a non-significant result really is significant for three types of tests: t-tests, correlation coefficients, and χ²-tests."
  },
  {
    "objectID": "posts/2014-10-28-assessing-differences-of-significance/index.html#differences-between-mean-differences",
    "href": "posts/2014-10-28-assessing-differences-of-significance/index.html#differences-between-mean-differences",
    "title": "Assessing differences of significance",
    "section": "Differences between mean differences",
    "text": "Differences between mean differences\nExample 1: Two groups of 60 participants each are recruited. Half of the participants in each group are assigned to the control condition (Condition A), and half to the experimental condition (Condition B). The participants’ score on a continuous dependent variable is then compared between the conditions. In Group 1, the participants in Condition B (n = 30, M = 2.0, SD = 1.7) significantly outperform those in Condition A (n = 30, M = 1.0, SD = 1.6; t(58) = 2.35, p = 0.02). In Group 2, by contrast, the difference between participants in Condition B (n = 30, M = 1.6, SD = 1.8) and those in Condition A (n = 30, M = 1.2, SD = 1.7) isn’t even approximately significant (t(58) = 0.88, p = 0.38).\n\n\n\n\n\nIn this example, researchers familiar with ANOVA will probably recognise that we can’t jump to conclusions about moderator variables just yet: we need to establish whether the effect of Condition is significantly different in the two groups. In other words, we need to investigate the interaction between Condition and Group. The (fictitious) data for this example are stored in the dataframe df1, and the significance of the interaction term can easily be computed in R by running the anova command on a linear model with Group, Condition and their interaction as predictors.\n\nsummary(df1)\n\n    Outcome            Group          Condition \n Min.   :-2.6731   Group 1:60   Condition A:60  \n 1st Qu.: 0.1625   Group 2:60   Condition B:60  \n Median : 1.5222                                \n Mean   : 1.4500                                \n 3rd Qu.: 2.7186                                \n Max.   : 5.7476                                \n\nanova(lm(Outcome ~ Group * Condition, df1))\n\nAnalysis of Variance Table\n\nResponse: Outcome\n                 Df Sum Sq Mean Sq F value  Pr(&gt;F)  \nGroup             1   0.30   0.300  0.1036 0.74810  \nCondition         1  14.70  14.700  5.0777 0.02611 *\nGroup:Condition   1   2.70   2.700  0.9326 0.33619  \nResiduals       116 335.82   2.895                  \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nThe effect of Condition, while significant in Group 1 and not significant in Group 2, does not statistically differ between the groups (F(1, 116) = 0.9, p = 0.34). Of course, that doesn’t mean that it doesn’t differ from group to group (‘not significant’ ≠ ‘no effect’), but we can straightforwardly account for the observed data without invoking moderator variables."
  },
  {
    "objectID": "posts/2014-10-28-assessing-differences-of-significance/index.html#differences-between-correlation-coefficients",
    "href": "posts/2014-10-28-assessing-differences-of-significance/index.html#differences-between-correlation-coefficients",
    "title": "Assessing differences of significance",
    "section": "Differences between correlation coefficients",
    "text": "Differences between correlation coefficients\nIn linguistic research, we’re often more interested in comparing the strength of the relationship between two (more or less) continuous variables measured in different samples (e.g. language learners at different levels; different language families etc.). In this kind of enterprise, substantial conclusions are often drawn on the basis of significant vs. non-significant correlation coefficients (i.e. r).\n\nIndependent correlations\nExample 2: We recruit two groups of 30 participants each (e.g. a group of French-speaking learners of German and a group of Dutch-speaking learners of German). All participants complete two tasks (e.g. an English vocabulary test and a German vocabulary test), and we want to establish whether performance on the two tasks is correlated in the two groups.\nIn the first group, we observe a significant correlation between the two variables (r = 0.50, p = 0.005), whereas the correlation isn’t remotely significant in the second group (r = 0.10, p = 0.60).\n\n\n\n\n\nIt’s tempting to think that such a large difference must be significant, but confidence intervals around correlation coefficients are ridiculously large. For instance, the 95% confidence interval of r in Group 1 is [0.17, 0.73], and that in Group 2 [-0.27, 0.44], so the confidence intervals overlap considerably. We need to test directly for the difference in the correlation coefficients.\nIn this example, we compare correlation coefficients between groups, i.e. the correlations are independent of one another. Note that the fact that the variables are named identically in the two groups is of no consequence. This is important since correlations can also be dependent, in which case a different analysis is needed (see below). The significance of the difference between two correlation coefficients is merely a function of (a) the correlation coefficients and (b) the sample sizes. In R, the test for such a comparison is implemented in the r.test() function in the psych package (to download it, type install.packages(\"psych\") at the command prompt). For this test, which is based on the Fisher z-transformation, r.test() takes the arguments n (sample size in Group 1), n2 (sample size in Group 2), r12 (first correlation coefficient) and r34 (second correlation coefficient).\n\npsych::r.test(n = 30, r12 = 0.50, n2 = 30, r34 = 0.10)\n\nCorrelation tests \nCall:psych::r.test(n = 30, r12 = 0.5, r34 = 0.1, n2 = 30)\nTest of difference between two independent correlations \n z value 1.65    with probability  0.1\n\n\nEven though thedifference between the correlation coefficients for both groups is quite large (r = 0.50 vs. 0.10), this difference isn’t itself statistically significant (z = 1.65, p = 0.10).\n\n\nDependent correlations\nExample 3: Now imagine that rather than recruiting two groups of participants, we recruited just one group of 30 participants (e.g. Flemish learners of German). Each participant completes three tasks (e.g. an English, a French and a German vocabulary test), and we’re interested in comparing which of the two first tasks (English or French) is the better predictor of performance on the third task (German).\nWe observe a significant correlation between (say) English and German test performance (r = 0.50, p = 0.005) on the one hand, and a non-significant correlation between (say) French and German test performance (r = 0.10, p = 0.60) on the other hand.\n\n\n\n\n\nThis time, however, the correlations aren’t independent of one another: since the participants took all three tests, it’s likely that the two ‘independent’ variables correlate with one another, too. In this example, the correlation between English and French vocabulary test performance is r = 0.70. This dependence needs to be taken into account when testing the significance of the difference between r = 0.50 vs. 0.10. We can use the same r.test() function as above but now we have to specify r12 (correlation between independent variable 1 and the dependent variable), r13 (correlation between independent variable 2 and the dependent variable) as well as r23 (the intercorrelation between the independent variables):\n\npsych::r.test(n = 30, r12 = 0.50, r13 = 0.10, r23 = 0.70)\n\nCorrelation tests \nCall:[1] \"r.test(n =  30 ,  r12 =  0.5 ,  r23 =  0.7 ,  r13 =  0.1 )\"\nTest of difference between two correlated  correlations \n t value 3.38    with probability &lt; 0.0022\n\n\nIn this case, the difference between r = 0.50 vs. 0.10 is significant (t(28) = 3.38, p = 0.002). The outcome of this test crucially depends on the intercorrelation between the two independent variables, e.g.:\n\npsych::r.test(n = 30, r12 = 0.50, r13 = 0.10, r23 = 0.1)\n\nCorrelation tests \nCall:[1] \"r.test(n =  30 ,  r12 =  0.5 ,  r23 =  0.1 ,  r13 =  0.1 )\"\nTest of difference between two correlated  correlations \n t value 1.76    with probability &lt; 0.09\n\npsych::r.test(n = 30, r12 = 0.50, r13 = 0.10, r23 = 0.8)\n\nCorrelation tests \nCall:[1] \"r.test(n =  30 ,  r12 =  0.5 ,  r23 =  0.8 ,  r13 =  0.1 )\"\nTest of difference between two correlated  correlations \n t value 4.64    with probability &lt; 7.9e-05\n\n\nIncidentally, the above also applies when comparing the strength of the relationship between one indepedent variable and two dependent variables. In addition, the intercorrelation should be taken into account even if it isn’t significant.\nFor a more detailed treatment of comparing correlation coefficients, see Olkin & Finn’s (1995) Correlation redux."
  },
  {
    "objectID": "posts/2014-10-28-assessing-differences-of-significance/index.html#differences-between-proportion-differences",
    "href": "posts/2014-10-28-assessing-differences-of-significance/index.html#differences-between-proportion-differences",
    "title": "Assessing differences of significance",
    "section": "Differences between proportion differences",
    "text": "Differences between proportion differences\nExample 4: We recruit two groups of 120 participants each and randomly assign half of the participants in each group to Condition A and half to Condition B. We present each participant with a dilemma, to which they must answer by either “yes” or “no”.\nIn Group 1, we find a significant effect of Condition: of the 60 participants in Condition A, 35 answered “yes”, whereas only 20 participants answered “yes” in Condition B (χ²(1) = 7.6, p = 0.006). In Group 2, 30 out of 60 participants in Condition A responded positively, and 23 out of 60 participants in Condition B did likewise – a non-significant difference (χ²(1) = 1.7, p = 0.20).\n\n\n\n\n\nAs in the examples above, it would be premature to start looking for moderator variables just yet. What we need to investigate is the interaction between Condition and Group. To that end, let’s first reconstruct the summary table with the number of positive and negative answers per Condition per Group (this, incidentally, can often be done even if you don’t have access to the raw data):\n\npositive &lt;- c(35, 30, 20, 23)\nnegative &lt;- c(25, 30, 40, 37)\ncondition &lt;- c(\"A\", \"A\", \"B\", \"B\")\ngroup &lt;- c(\"1\", \"2\", \"1\", \"2\")\ndf4 &lt;- data.frame(positive = positive,\n                  negative = negative,\n                  condition = factor(condition),\n                  group = factor(group))\ndf4\n\n  positive negative condition group\n1       35       25         A     1\n2       30       30         A     2\n3       20       40         B     1\n4       23       37         B     2\n\n\nWe have all the information we need to compute the significance of the interaction: we can combine the columns with the positive and negative responses and feed it to a logistic regression model as the dependent variable; the independent variables are Condition and Group, which are allowed to interact with one another. (Sociolinguists already know a form of logistic regression (complemented with stepwise variable selection) as ‘variable rules analysis’ or Varbrul.) We first construct the model (a logistic/binomial generalized linear model, hence glm()), and then compute an analysis of deviance table for the three terms in the model (two main effects and the interaction). (Analysis of deviance is like ANOVA but for generalized models; here, we ask R to compute χ²-tests.)\n\nmod.glm &lt;- glm(cbind(positive, negative) ~ condition * group, \n               data = df4, \n               family = \"binomial\")\nanova(mod.glm, test = \"Chisq\")\n\nAnalysis of Deviance Table\n\nModel: binomial, link: logit\n\nResponse: cbind(positive, negative)\n\nTerms added sequentially (first to last)\n\n                Df Deviance Resid. Df Resid. Dev Pr(&gt;Chi)   \nNULL                                3     9.3634            \ncondition        1   8.1968         2     1.1666 0.004196 **\ngroup            1   0.0697         1     1.0968 0.791759   \ncondition:group  1   1.0968         0     0.0000 0.294960   \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nWhile the effect of Condition is significant in Group 1 and not in Group 2, the Condition effect isn’t significant different between Groups 1 and 2 (χ²(1) = 1.1, p = 0.29)."
  },
  {
    "objectID": "posts/2014-10-28-assessing-differences-of-significance/index.html#summary",
    "href": "posts/2014-10-28-assessing-differences-of-significance/index.html#summary",
    "title": "Assessing differences of significance",
    "section": "Summary",
    "text": "Summary\nMost quantitatively-oriented researchers are familiar with investigating interaction effects when comparing mean differences. For the same reason that we’re not content with computing two t-tests and comparing their p-values, comparing the p-values associated with correlation coefficients or χ²-tests doesn’t suffice. It’s questionable to draw strong conclusions merely on the basis of an effect being significant in one condition/group/study and not in the other – even when the p-values differ substantially."
  },
  {
    "objectID": "posts/2014-10-28-assessing-differences-of-significance/index.html#software-versions",
    "href": "posts/2014-10-28-assessing-differences-of-significance/index.html#software-versions",
    "title": "Assessing differences of significance",
    "section": "Software versions",
    "text": "Software versions\n\ndevtools::session_info()\n\n─ Session info ───────────────────────────────────────────────────────────────\n setting  value\n version  R version 4.3.1 (2023-06-16)\n os       Ubuntu 22.04.3 LTS\n system   x86_64, linux-gnu\n ui       X11\n language en_US\n collate  en_US.UTF-8\n ctype    en_US.UTF-8\n tz       Europe/Zurich\n date     2023-08-26\n pandoc   3.1.1 @ /usr/lib/rstudio/resources/app/bin/quarto/bin/tools/ (via rmarkdown)\n\n─ Packages ───────────────────────────────────────────────────────────────────\n package     * version date (UTC) lib source\n cachem        1.0.6   2021-08-19 [2] CRAN (R 4.2.0)\n callr         3.7.3   2022-11-02 [1] CRAN (R 4.3.1)\n cannonball    0.1.1   2023-06-21 [1] Github (janhove/cannonball@fe70eff)\n cli           3.6.1   2023-03-23 [1] CRAN (R 4.3.0)\n crayon        1.5.2   2022-09-29 [1] CRAN (R 4.3.1)\n devtools      2.4.5   2022-10-11 [1] CRAN (R 4.3.1)\n digest        0.6.29  2021-12-01 [2] CRAN (R 4.2.0)\n ellipsis      0.3.2   2021-04-29 [2] CRAN (R 4.2.0)\n evaluate      0.15    2022-02-18 [2] CRAN (R 4.2.0)\n fastmap       1.1.0   2021-01-25 [2] CRAN (R 4.2.0)\n fs            1.5.2   2021-12-08 [2] CRAN (R 4.2.0)\n glue          1.6.2   2022-02-24 [2] CRAN (R 4.2.0)\n htmltools     0.5.5   2023-03-23 [1] CRAN (R 4.3.0)\n htmlwidgets   1.6.2   2023-03-17 [1] CRAN (R 4.3.1)\n httpuv        1.6.11  2023-05-11 [1] CRAN (R 4.3.1)\n jsonlite      1.8.7   2023-06-29 [1] CRAN (R 4.3.1)\n knitr         1.39    2022-04-26 [2] CRAN (R 4.2.0)\n later         1.3.1   2023-05-02 [1] CRAN (R 4.3.1)\n lattice       0.21-8  2023-04-05 [4] CRAN (R 4.3.0)\n lifecycle     1.0.3   2022-10-07 [1] CRAN (R 4.3.0)\n magrittr      2.0.3   2022-03-30 [1] CRAN (R 4.3.0)\n MASS          7.3-60  2023-05-04 [4] CRAN (R 4.3.1)\n memoise       2.0.1   2021-11-26 [2] CRAN (R 4.2.0)\n mime          0.10    2021-02-13 [2] CRAN (R 4.0.2)\n miniUI        0.1.1.1 2018-05-18 [1] CRAN (R 4.3.1)\n mnormt        2.1.1   2022-09-26 [1] CRAN (R 4.3.1)\n nlme          3.1-162 2023-01-31 [4] CRAN (R 4.2.2)\n pkgbuild      1.4.2   2023-06-26 [1] CRAN (R 4.3.1)\n pkgload       1.3.2.1 2023-07-08 [1] CRAN (R 4.3.1)\n prettyunits   1.1.1   2020-01-24 [2] CRAN (R 4.2.0)\n processx      3.8.2   2023-06-30 [1] CRAN (R 4.3.1)\n profvis       0.3.8   2023-05-02 [1] CRAN (R 4.3.1)\n promises      1.2.0.1 2021-02-11 [1] CRAN (R 4.3.1)\n ps            1.7.5   2023-04-18 [1] CRAN (R 4.3.1)\n psych         2.3.6   2023-06-21 [1] CRAN (R 4.3.1)\n purrr         1.0.1   2023-01-10 [1] CRAN (R 4.3.0)\n R6            2.5.1   2021-08-19 [2] CRAN (R 4.2.0)\n Rcpp          1.0.11  2023-07-06 [1] CRAN (R 4.3.1)\n remotes       2.4.2   2021-11-30 [2] CRAN (R 4.2.0)\n rlang         1.1.1   2023-04-28 [1] CRAN (R 4.3.0)\n rmarkdown     2.21    2023-03-26 [1] CRAN (R 4.3.0)\n rstudioapi    0.14    2022-08-22 [1] CRAN (R 4.3.0)\n sessioninfo   1.2.2   2021-12-06 [2] CRAN (R 4.2.0)\n shiny         1.7.4.1 2023-07-06 [1] CRAN (R 4.3.1)\n stringi       1.7.12  2023-01-11 [1] CRAN (R 4.3.1)\n stringr       1.5.0   2022-12-02 [1] CRAN (R 4.3.0)\n urlchecker    1.0.1   2021-11-30 [1] CRAN (R 4.3.1)\n usethis       2.2.2   2023-07-06 [1] CRAN (R 4.3.1)\n vctrs         0.6.3   2023-06-14 [1] CRAN (R 4.3.0)\n xfun          0.39    2023-04-20 [1] CRAN (R 4.3.0)\n xtable        1.8-4   2019-04-21 [1] CRAN (R 4.3.1)\n yaml          2.3.5   2022-02-21 [2] CRAN (R 4.2.0)\n\n [1] /home/jan/R/x86_64-pc-linux-gnu-library/4.3\n [2] /usr/local/lib/R/site-library\n [3] /usr/lib/R/site-library\n [4] /usr/lib/R/library\n\n──────────────────────────────────────────────────────────────────────────────"
  },
  {
    "objectID": "posts/2018-07-27-bayesian-two-breakpoint-model/index.html",
    "href": "posts/2018-07-27-bayesian-two-breakpoint-model/index.html",
    "title": "Baby steps in Bayes: Piecewise regression with two breakpoints",
    "section": "",
    "text": "In this follow-up to the blog post Baby steps in Bayes: Piecewise regression, I’m going to try to model the relationship between two continuous variables using a piecewise regression with not one but two breakpoints. (The rights to the movie about the first installment are still up for grabs, incidentally.)\nThe kind of relationship I want to model is plotted in Figure 1. According to some applied linguists, relationships similar to these underpin some aspects of language learning, but we don’t need to go into those discussions here.\nUpdate (2023-08-06): When I reran the code in this blog post using newer versions of Stan and R, I ran into serious convergence issues. I have pulled down this blog post till I figure out how to fix these.\n\n\n\n\n\nFigure 1. The underlying form of the x-y relationship is characterised by two breakpoints. The function’s three pieces are connected to each other."
  },
  {
    "objectID": "posts/2015-07-17-covariate-adjustment-logistic-regression/index.html",
    "href": "posts/2015-07-17-covariate-adjustment-logistic-regression/index.html",
    "title": "Covariate adjustment in logistic regression — and some counterintuitive findings",
    "section": "",
    "text": "Including sensible covariates is a good idea when analysing continuous experimental data, but when I learnt that its benefits may not carry entirely carry over to the analysis of binary data, I wasn’t sure that I’d fully understood the implications. This post summarises the results of some simulations that I ran to learn more about the usefulness of covariates when analysing binary experimental data."
  },
  {
    "objectID": "posts/2015-07-17-covariate-adjustment-logistic-regression/index.html#background",
    "href": "posts/2015-07-17-covariate-adjustment-logistic-regression/index.html#background",
    "title": "Covariate adjustment in logistic regression — and some counterintuitive findings",
    "section": "Background",
    "text": "Background\nMy previous discussions of the usefulness of including covariates when analysing randomised experiments (here and here) dealt with the case in which the dependent variable is continuous and could be analysed in a linear regression model. In a nutshell, including sensible covariates in such an analysis increases precision and power and does not bias the estimates of the treatment effect.\nI mostly deal with binary dependent variables (e.g. presence v. absence or correct v. incorrect), however, which can be analysed in logistic regression models. According to Robinson and Jewell (1991), the benefits of covariate adjustment in linear regression don’t fully apply to logistic regression models. I wasn’t sure whether I had fully understood their take-home points (reading math-heavy papers means reading selectively for me), though. As I am currently planning a number of randomised experiments with binary outcome variables, I’d like to know whether it’d be useful to extract a number of additional variables that are known to related to this outcome but that aren’t of primary interest to me. To this end, I ran a couple of simulations that probe the effects of covariate adjustment in logistic regressions on the effect size estimate, its standard error and the statistical power for finding a treatment effect. I typically analyse my data using logistic mixed-effects models (see Jaeger 2008), but as a first step, I investigated the effects of covariate adjustment in ‘normal’ logistic regression models, i.e. models without random intercepts and random slopes."
  },
  {
    "objectID": "posts/2015-07-17-covariate-adjustment-logistic-regression/index.html#set-up-a-simple-experiment",
    "href": "posts/2015-07-17-covariate-adjustment-logistic-regression/index.html#set-up-a-simple-experiment",
    "title": "Covariate adjustment in logistic regression — and some counterintuitive findings",
    "section": "Set-up: A simple experiment",
    "text": "Set-up: A simple experiment\nThe set-up of the simulation is based on the one reported in my first blog post. The narrative is as follows. Eighty German-speaking participants (the number of participants can be adjusted using the n parameter in the simulation) are randomly assigned to two equal-sized groups. Half of the participants are told something about Dutch grapheme-to-phoneme correspondences (say that Dutch  is pronounced /u/ and not /ø/, as a native German speaker might assume; experimental group). The other participants aren’t told anything (control group). The participants are then asked to translate a single Dutch (e.g. schoen) word into German. If they correctly translate the word as Schuh ‘shoe’, their answer is scored as correct, otherwise it is scored as incorrect, i.e. we have a binary dependent variable. The question is to what extent translation accuracy differs between the two groups.\nHowever, the participants differ in their knowledge of English, which may also affect their translation accuracy. Would it be worth the effort to give the participants an English-language test in order to be able to take this covariate (English skills) into account during the analysis, even though we’re not really interested in this variable?"
  },
  {
    "objectID": "posts/2015-07-17-covariate-adjustment-logistic-regression/index.html#skippable-settings",
    "href": "posts/2015-07-17-covariate-adjustment-logistic-regression/index.html#skippable-settings",
    "title": "Covariate adjustment in logistic regression — and some counterintuitive findings",
    "section": "Skippable: Settings",
    "text": "Skippable: Settings\nI assumed that the accuracy variable (correct vs. incorrect) was generated from a binomial distribution. The probability of a correct translation was specified as:\n\np.correct = plogis(-0.5 + b.condition * condition + b.covariate * covariate + rnorm(80, sd.lat))\n\nIn words, condition (experimental v. control) has an effect (in log-odds) that is equal to b.condition (i.e. the slope of condition in log-odds), the covariate has an effect (in log-odds) equal to b.covariate and participants could differ randomly in their underlying ‘translation skills’, which is captured by the rnorm term – the sd.lat parameter specifies the standard deviation of this normal distribution.\nFor the simulation, the covariate (English test performance) is specified to be uniformly distributed between -1 and 1, whereas the parameters b.condition, b.covariate and sd.lat are systematically varied. For each combination of parameters, 10,000 (update (2023-08-27): 3,000) datasets were generated that were analysed in two logistic models: one without and one with the covariate. From each model, I culled the estimate of the treatment effect and its standard error. Additionally, the p-value for the treatment effect in each model was computed twice: once based on Wald’s z-test and once based on the χ² test in a (sequential) analysis of deviance in which the treatment effect was added last.\nUpdate (2023-08-27): I’ve run the simulation again but with only 3,000 datasets per parameter combination. The plots from the original blog post were redrawn using ggplot2 instead of base R."
  },
  {
    "objectID": "posts/2015-07-17-covariate-adjustment-logistic-regression/index.html#results",
    "href": "posts/2015-07-17-covariate-adjustment-logistic-regression/index.html#results",
    "title": "Covariate adjustment in logistic regression — and some counterintuitive findings",
    "section": "Results",
    "text": "Results\n\nEstimates of the treatment effect\nThe first question is whether the logistic models are able to correctly estimate the treatment effect underlying the simulated data (b.condition). For this comparison, b.condition was fixed at 1.6, meaning that participants in the treatment group would be roughly 5 times (4.9530324) more likely to provide a correct translation than those in the control group. The parameter b.covariate, i.e. the underlying slope parameter of linking the covariate to the outcome in log-odds space, varied between 0 and 4 (0, 0.5, 1, 1.5 etc.). The parameter sd.lat, i.e. the standard deviation of the unexplained variability in the latent ‘translation skill’ variable in log-odds space, also varied between 0 and 4 in the same fashion.\nFigure 1 shows the average estimated treatment effect for the covariate-adjusted (blue) and unadjusted (red) logistic models:\n\n\n\n\n\nFigure 1 shows the models’ mean estimation of the treatment effect according to the strength of the relationship between the dependent variable and the covariate and the standard deviation of the residual variability in the latent skill. The underlying treatment effect was fixed at b.condition = 1.6 (dashed line). Red: the estimate for the logistic model without the covariate. Blue: the estimate for the logistic model with the covariate.\n\n\n\n\nI noticed four things when looking at these graphs:\n\nBoth the adjusted and unadjusted models massively underestimate the true treatment effect (dashed line at 1.6) in the presence of substantial residual variability in the latent skill unaccounted for by the model. The treatment estimate seems to be biased towards zero, which is new to me.\nAs the variability in the latent skill increases due to a stronger effect of the covariate, the unadjusted model (blue) performs increasingly more poorly relative to the model that adjusts for the covariate effect (red). This can be understood in terms of point 1: The variability in the latent skill increases when b.covariate increases. Since the unadjusted model does not account for this increase whereas the adjusted model does, the bias towards zero affects the unadjusted model.\nThe treatment effect is slightly but systematically overestimated when all the variability in the latent skill is accounted for by variables in the model: When sd.lat is 0, the adjusted model always yields estimates that are slightly higher than 1.6, and the unadjusted model similarly yields an estimate that is slightly too high when both sd.lat and b.covariate are 0.\nAdjusting for an irrelevant covariate (b.covariate = 0) does not noticeably affect the treatment estimate.\n\nWhen the goal is to estimate a treatment effect, then, covariate adjustment seems useful but cannot be counted on to yield an unbiased of the treatment effect.\n\n\nStandard errors of the treatment effect\nRobinson and Jewell (1991) noted that adjusting for a covariate always decreases precision. To verify whether I’d understood this correctly, I computed the mean standard error of the treatment estimate for each set of 10.000 models. The results are shown in Figure 2:\n\n\n\n\n\nFigure 2 shows the models’ mean standard error of the treatment effect according to the strength of the relationship between the dependent variable and the covariate and the standard deviation of the residual variability in the latent skill. The underlying treatment effect was fixed at b.condition = 1.6. Red: the estimate for the logistic model without the covariate. Blue: the estimate for the logistic model with the covariate.\n\n\n\n\nTwo things were striking:\n\nThe standard error decreases as the unexplained variability in the latent skill increases. This, too, is counter-intuitive and new to me.\nThe standard error is consistently larger when adjusting for a covariate, even when the covariate is important. I assume it is this that Robinson and Jewell (1991) mean when they state that adjusting for covariates reduces precision of the treatment effect estimate.\n\nSo the bias towards zero caused by variability in the latent skill that is unaccounted for in the statistical model and which would give rise to reduced statistical power is accompanied by smaller standard errors, which would increase statistical power. Furthermore, the absolute estimates of the treatment effect are larger in adjusted models than in unadjusted models (which would yield greater statistical power), but on the other hand, the standard errors are larger in these models, too (which would reduce statistical power).\nThe obvious question is whether adjusting for covariates increases statistical power giving these two opposite forces.\n\n\nStatistical power\nFor these simulations, I computed the proportion of the models that returned a significant (p &lt; 0.05) treatment effect according to a Wald z-test and a χ²-test in a sequential analysis of deviance. The differences in power between the Wald z-test and the χ²-test were largely negligible, and only the power for the χ²-tests is reported.\nFigure 3 shows how the study’s power varies as a function of the impact of the covariate and the underlying treatment effect:\n\n\n\n\n\nFigure 3 shows the models’ power for rejecting the null hypothesis of ‘no treatment’ according to the strength of the relationship between the dependent variable and the covariate and the underlying treatment effect. The residual variability in the latent skill was fixed at sd.lat = 1. Red: the estimate for the logistic model without the covariate. Blue: the estimate for the logistic model with the covariate.\n\n\n\n\nI noticed three things:\n\nUnsurprisingly, power increases as the treatment effect increases.\nAdjusting for a covariate increases power when the covariate is strongly correlated with the outcome variable. It only has a negligible effect when the relationship is low, hower.\nWith 80 participants, the loss of one degree of freedom for modelling an unimportant covariate (b.covariate = 0) doesn’t affect the study’s power. For small samples, this may not be the case, though I haven’t run simulations to test this intuition.\n\nIn terms of power, then, adjusting for a covariate in a logistic model doesn’t hurt. Whether the benefit of including a covariate in the analysis outweighs the effort to collect these data may be debatable when the relationship between the covariate and the outcome isn’t too strong, however.\nFigure 4 shows how power varies according to unaccounted variability in the latent skill:\n\n\n\n\n\nFigure 4 shows the models’ power for rejecting the null hypothesis of ‘no treatment’ according to the strength of the relationship between the dependent variable and the covariate and the standard deviation of the residual variability in the latent skill. The underlying treatment effect was fixed at b.condition = 1.6. Red: the estimate for the logistic model without the covariate. Blue: the estimate for the logistic model with the covariate.\n\n\n\n\nAgain, two points are noteworthy:\n\nAdjusting for a covariate is especially useful if this covariate accounts for most of the variability in the latent skill (low sd.lat values).\nWhile unmodelled variability in the latent skill reduces both the treatment estimate and its standard error, the overall effect is a reduction in power."
  },
  {
    "objectID": "posts/2015-07-17-covariate-adjustment-logistic-regression/index.html#conclusions",
    "href": "posts/2015-07-17-covariate-adjustment-logistic-regression/index.html#conclusions",
    "title": "Covariate adjustment in logistic regression — and some counterintuitive findings",
    "section": "Conclusions",
    "text": "Conclusions\nAll in all, covariate adjustment seems beneficial in terms of power and ‘accuracy’ (but not precision!) of the treatment effect in logistic models. That said, estimates of treatment effects seem bound to be underestimations when not all information relevant to the underlying data generating process can be brought under control, even in a randomized experiment. I don’t fully grasp the intuition behind these findings, but being aware of them is a first step.\nWhether the effort of collecting a covariate so that it can be included in the model is worth it in terms of effort, time and cost would seem to depend on its potential to explain between-subjects differences that aren’t linked to the experimental condition.\nMost of this was new and surprising to me, so I can’t guarantee that something hasn’t horribly gone wrong in my simulations. Please let me know if you spot an error! For a next post, I plan to take a look at covariate adjustment in logistic mixed-effects regression."
  },
  {
    "objectID": "posts/2015-07-17-covariate-adjustment-logistic-regression/index.html#r-code",
    "href": "posts/2015-07-17-covariate-adjustment-logistic-regression/index.html#r-code",
    "title": "Covariate adjustment in logistic regression — and some counterintuitive findings",
    "section": "R code",
    "text": "R code\nsimulate.logistic generates one data point each for n participants. The probability of a success (in log-odds) is a function of the participants’ covariate scores (slope parameter: b.covariate), the condition to which they were randomly assigned (b.condition) as well as normally distributed unexplained factors (with a standard deviation of sd.lat). Two models are computed for this dataset with n observations: one without and one with a covariate. For each model, the estimated treatment effect, its standard error and two p-values are computed.\n\nsimulate.logistic &lt;- function(n, b.covariate = 2, b.condition = 1, sd.lat = 1) {\n  \n  covariate &lt;- runif(n, -1, 1)\n  condition &lt;- sample(c(rep(1, n/2), rep(0, n/2)))\n  outcome.logodds &lt;- -0.5 + b.covariate*covariate + b.condition*condition + rnorm(n, sd = sd.lat)\n  outcome.binary &lt;- factor(rbinom(size = 1, n = n, prob = plogis(outcome.logodds)))\n  \n  # Model without covariate\n  mod.nocov &lt;- glm(outcome.binary ~ factor(condition), family = \"binomial\")\n  # Estimate of condition effect\n  estimate.nocov &lt;- summary(mod.nocov)$coefficients[2, 1]\n  # Standard error of estimate\n  sterror.nocov &lt;- summary(mod.nocov)$coefficients[2, 2]\n  # P-value of Wald z\n  p.z.nocov &lt;- summary(mod.nocov)$coefficients[2, 4]\n  # P-value of analysis of deviance\n  p.x2.nocov &lt;- anova(mod.nocov, test = \"Chisq\")[2, 5]\n  \n  # Model with covariates\n  mod.cov &lt;- glm(outcome.binary ~ covariate + factor(condition), family = \"binomial\")\n  # Estimate of condition effect\n  estimate.cov &lt;- summary(mod.cov)$coefficients[3, 1]\n  # Standard error of estimate\n  sterror.cov &lt;- summary(mod.cov)$coefficients[3, 2]\n  # P-value of Wald z\n  p.z.cov &lt;- summary(mod.cov)$coefficients[3, 4]\n  # P-value of sequential analysis of deviance\n  p.x2.cov &lt;- anova(mod.cov, test = \"Chisq\")[3, 5]\n  \n  return(list(estimate.nocov, estimate.cov,\n              sterror.nocov, sterror.cov,\n              p.z.nocov, p.z.cov,\n              p.x2.nocov, p.x2.cov))\n}\n\nreplicate.logistic takes the function simulate.logistic and runs it a large number of times (runs). It then returns the average slope and standard error for each modelling approach as well as their estimated power.\n\nreplicate.logistic &lt;- function(runs = 1000,\n                               n = 100,\n                               b.covariate = 2,\n                               b.condition = 1,\n                               sd.lat = 1) {\n  \n  # run simulate.logistic() a number of times\n  sims &lt;- replicate(runs, simulate.logistic(n, b.covariate, b.condition, sd.lat))\n  \n  # Compute average slope\n  slope.nocov = mean(unlist(sims[1, ]))\n  slope.cov = mean(unlist(sims[2, ]))\n  \n  # Compute average standard error\n  se.nocov = mean(unlist(sims[3, ]))\n  se.cov = mean(unlist(sims[4, ]))\n  \n  # Compute average power Wald\n  power.z.nocov = mean(unlist(sims[5, ]) &lt;= 0.05)\n  power.z.cov = mean(unlist(sims[6, ]) &lt;= 0.05)\n  \n  # Compute average power analysis of deviance\n  power.x2.nocov = mean(unlist(sims[7, ]) &lt;= 0.05)\n  power.x2.cov = mean(unlist(sims[8, ]) &lt;= 0.05)\n  \n  # Spit it all out\n  return(list(slope.nocov = slope.nocov, \n              slope.cov = slope.cov, \n              se.nocov = se.nocov, \n              se.cov = se.cov, \n              power.z.nocov = power.z.nocov, \n              power.z.cov = power.z.cov, \n              power.x2.nocov = power.x2.nocov, \n              power.x2.cov = power.x2.cov))\n}\n\nreplicate.logistic is then run 10,000 (update: 3,000) times for a combination of b.covariate and b.condition values. For this simulation, n is fixed at 80 and sd.lat at 1. For this, I use the mcmapply() function in the parallel package:\n\n# This tabulates all relevant combinations of b.covariate and b.condition\ngrid &lt;- expand.grid(b.covariate = seq(0, 4, 0.5),\n                    b.condition = seq(0, 2, 0.2))\n\nlibrary(parallel)\n# Run replicate.logistic 3,000 times for every combination of b.covariate and b.condition contained in 'grid'\n# I'm not sure whether this works on Mac or Windows; perhaps use mapply instead of mcmapply.\nsimulatedResults &lt;- mcmapply(replicate.logistic,\n                             b.covariate = grid$b.covariate,\n                             b.condition = grid$b.condition,\n                             # set fixed parameters\n                             MoreArgs = list(runs = 3000,\n                                             n = 80, sd.lat = 1),\n                             # distribute work over CPU cores\n                             mc.cores = detectCores())\n# Output results (transposed for clarity)\nsimulatedResults &lt;- cbind(grid, data.frame(t(simulatedResults)))\n\n# Unlist columns and save\nlibrary(tidyverse)\nsimulatedResults |&gt; \n  mutate(across(where(is.list), unlist)) |&gt; \n  write_csv(\"simulatedResults.csv\")\n\nI used the simulatedResults data for Figure 3. For Figures 1, 2 and 4, I varied b.covariate and sd.lat and fixed b.condition at 1.6:\n\ngrid2 &lt;- expand.grid(b.covariate = seq(0, 4, 0.5),\n                     sd.lat = seq(0, 4, 0.5))\nsimulatedResults2 &lt;- mcmapply(replicate.logistic,\n                              b.covariate = grid2$b.covariate,\n                              sd.lat = grid2$sd.lat,\n                              # set fixed parameters\n                              MoreArgs = list(runs = 3000,\n                                              n = 80, b.condition = 1.6),\n                              # distribute work over CPU cores\n                              mc.cores = detectCores())\n# Output results (transposed for clarity)\nsimulatedResults2 &lt;- cbind(grid2, data.frame(t(simulatedResults2)))\n\n# Unlist columns and save\nsimulatedResults2 |&gt; \n  mutate(across(where(is.list), unlist)) |&gt; \n  write_csv(\"simulatedResults2.csv\")"
  },
  {
    "objectID": "posts/2015-07-17-covariate-adjustment-logistic-regression/index.html#software-versions",
    "href": "posts/2015-07-17-covariate-adjustment-logistic-regression/index.html#software-versions",
    "title": "Covariate adjustment in logistic regression — and some counterintuitive findings",
    "section": "Software versions",
    "text": "Software versions\n\ndevtools::session_info()\n\n─ Session info ───────────────────────────────────────────────────────────────\n setting  value\n version  R version 4.3.1 (2023-06-16)\n os       Ubuntu 22.04.3 LTS\n system   x86_64, linux-gnu\n ui       X11\n language en_US\n collate  en_US.UTF-8\n ctype    en_US.UTF-8\n tz       Europe/Zurich\n date     2023-08-27\n pandoc   3.1.1 @ /usr/lib/rstudio/resources/app/bin/quarto/bin/tools/ (via rmarkdown)\n\n─ Packages ───────────────────────────────────────────────────────────────────\n package     * version date (UTC) lib source\n bit           4.0.5   2022-11-15 [1] CRAN (R 4.3.0)\n bit64         4.0.5   2020-08-30 [1] CRAN (R 4.3.0)\n cachem        1.0.6   2021-08-19 [2] CRAN (R 4.2.0)\n callr         3.7.3   2022-11-02 [1] CRAN (R 4.3.1)\n cli           3.6.1   2023-03-23 [1] CRAN (R 4.3.0)\n colorspace    2.1-0   2023-01-23 [1] CRAN (R 4.3.0)\n crayon        1.5.2   2022-09-29 [1] CRAN (R 4.3.1)\n devtools      2.4.5   2022-10-11 [1] CRAN (R 4.3.1)\n digest        0.6.29  2021-12-01 [2] CRAN (R 4.2.0)\n dplyr       * 1.1.2   2023-04-20 [1] CRAN (R 4.3.0)\n ellipsis      0.3.2   2021-04-29 [2] CRAN (R 4.2.0)\n evaluate      0.15    2022-02-18 [2] CRAN (R 4.2.0)\n fansi         1.0.4   2023-01-22 [1] CRAN (R 4.3.1)\n farver        2.1.1   2022-07-06 [1] CRAN (R 4.3.0)\n fastmap       1.1.0   2021-01-25 [2] CRAN (R 4.2.0)\n forcats     * 1.0.0   2023-01-29 [1] CRAN (R 4.3.0)\n fs            1.5.2   2021-12-08 [2] CRAN (R 4.2.0)\n generics      0.1.3   2022-07-05 [1] CRAN (R 4.3.0)\n ggplot2     * 3.4.2   2023-04-03 [1] CRAN (R 4.3.0)\n glue          1.6.2   2022-02-24 [2] CRAN (R 4.2.0)\n gtable        0.3.3   2023-03-21 [1] CRAN (R 4.3.0)\n hms           1.1.3   2023-03-21 [1] CRAN (R 4.3.0)\n htmltools     0.5.5   2023-03-23 [1] CRAN (R 4.3.0)\n htmlwidgets   1.6.2   2023-03-17 [1] CRAN (R 4.3.1)\n httpuv        1.6.11  2023-05-11 [1] CRAN (R 4.3.1)\n jsonlite      1.8.7   2023-06-29 [1] CRAN (R 4.3.1)\n knitr         1.39    2022-04-26 [2] CRAN (R 4.2.0)\n labeling      0.4.2   2020-10-20 [1] CRAN (R 4.3.0)\n later         1.3.1   2023-05-02 [1] CRAN (R 4.3.1)\n lifecycle     1.0.3   2022-10-07 [1] CRAN (R 4.3.0)\n lubridate   * 1.9.2   2023-02-10 [1] CRAN (R 4.3.0)\n magrittr      2.0.3   2022-03-30 [1] CRAN (R 4.3.0)\n memoise       2.0.1   2021-11-26 [2] CRAN (R 4.2.0)\n mime          0.10    2021-02-13 [2] CRAN (R 4.0.2)\n miniUI        0.1.1.1 2018-05-18 [1] CRAN (R 4.3.1)\n munsell       0.5.0   2018-06-12 [1] CRAN (R 4.3.0)\n pillar        1.9.0   2023-03-22 [1] CRAN (R 4.3.0)\n pkgbuild      1.4.2   2023-06-26 [1] CRAN (R 4.3.1)\n pkgconfig     2.0.3   2019-09-22 [2] CRAN (R 4.2.0)\n pkgload       1.3.2.1 2023-07-08 [1] CRAN (R 4.3.1)\n prettyunits   1.1.1   2020-01-24 [2] CRAN (R 4.2.0)\n processx      3.8.2   2023-06-30 [1] CRAN (R 4.3.1)\n profvis       0.3.8   2023-05-02 [1] CRAN (R 4.3.1)\n promises      1.2.0.1 2021-02-11 [1] CRAN (R 4.3.1)\n ps            1.7.5   2023-04-18 [1] CRAN (R 4.3.1)\n purrr       * 1.0.1   2023-01-10 [1] CRAN (R 4.3.0)\n R6            2.5.1   2021-08-19 [2] CRAN (R 4.2.0)\n Rcpp          1.0.11  2023-07-06 [1] CRAN (R 4.3.1)\n readr       * 2.1.4   2023-02-10 [1] CRAN (R 4.3.0)\n remotes       2.4.2   2021-11-30 [2] CRAN (R 4.2.0)\n rlang         1.1.1   2023-04-28 [1] CRAN (R 4.3.0)\n rmarkdown     2.21    2023-03-26 [1] CRAN (R 4.3.0)\n rstudioapi    0.14    2022-08-22 [1] CRAN (R 4.3.0)\n scales        1.2.1   2022-08-20 [1] CRAN (R 4.3.0)\n sessioninfo   1.2.2   2021-12-06 [2] CRAN (R 4.2.0)\n shiny         1.7.4.1 2023-07-06 [1] CRAN (R 4.3.1)\n stringi       1.7.12  2023-01-11 [1] CRAN (R 4.3.1)\n stringr     * 1.5.0   2022-12-02 [1] CRAN (R 4.3.0)\n tibble      * 3.2.1   2023-03-20 [1] CRAN (R 4.3.0)\n tidyr       * 1.3.0   2023-01-24 [1] CRAN (R 4.3.0)\n tidyselect    1.2.0   2022-10-10 [1] CRAN (R 4.3.0)\n tidyverse   * 2.0.0   2023-02-22 [1] CRAN (R 4.3.1)\n timechange    0.2.0   2023-01-11 [1] CRAN (R 4.3.0)\n tzdb          0.4.0   2023-05-12 [1] CRAN (R 4.3.0)\n urlchecker    1.0.1   2021-11-30 [1] CRAN (R 4.3.1)\n usethis       2.2.2   2023-07-06 [1] CRAN (R 4.3.1)\n utf8          1.2.3   2023-01-31 [1] CRAN (R 4.3.1)\n vctrs         0.6.3   2023-06-14 [1] CRAN (R 4.3.0)\n vroom         1.6.3   2023-04-28 [1] CRAN (R 4.3.0)\n withr         2.5.0   2022-03-03 [2] CRAN (R 4.2.0)\n xfun          0.39    2023-04-20 [1] CRAN (R 4.3.0)\n xtable        1.8-4   2019-04-21 [1] CRAN (R 4.3.1)\n yaml          2.3.5   2022-02-21 [2] CRAN (R 4.2.0)\n\n [1] /home/jan/R/x86_64-pc-linux-gnu-library/4.3\n [2] /usr/local/lib/R/site-library\n [3] /usr/lib/R/site-library\n [4] /usr/lib/R/library\n\n──────────────────────────────────────────────────────────────────────────────"
  },
  {
    "objectID": "posts/2023-03-07-julia-breakpoint-regression/index.html",
    "href": "posts/2023-03-07-julia-breakpoint-regression/index.html",
    "title": "Adjusting to Julia: Piecewise regression",
    "section": "",
    "text": "In this fourth installment of Adjusting to Julia, I will at long last analyse some actual data. One of the first posts on this blog was Calibrating p-values in ‘flexible’ piecewise regression models. In that post, I fitted a piecewise regression to a dataset comprising the ages at which a number of language learners started learning a second language (age of acquisition, AOA) and their scores on a grammaticality judgement task (GJT) in that second language. A piecewise regression is a regression model in which the slope of the function relating the predictor (here: AOA) to the outcome (here: GJT) changes at some value of the predictor, the so-called breakpoint. The problem, however, was that I didn’t specify the breakpoint beforehand but pick the breakpoint that minimised the model’s deviance. This increased the probability that I would find that the slope before and after the breakpoint differed, even if they in fact were the same. In the blog post I wrote almost nine years ago, I sought to recalibrate the p-value for the change in slope by running a bunch of simulations in R. In this blog post, I’ll do the same, but in Julia."
  },
  {
    "objectID": "posts/2023-03-07-julia-breakpoint-regression/index.html#the-data-set",
    "href": "posts/2023-03-07-julia-breakpoint-regression/index.html#the-data-set",
    "title": "Adjusting to Julia: Piecewise regression",
    "section": "The data set",
    "text": "The data set\nWe’ll work with the data from the North America study conducted by DeKeyser et al. (2010). If you want to follow along, you can download this dataset here and save it to a subdirectory called data in your working directory.\nWe need the DataFrames, CSV and StatsPlots packages in order to read in the CSV with the dataset as a data frame and draw some basic graphs.\n\nusing DataFrames, CSV, StatsPlots\n\nd = CSV.read(\"data/dekeyser2010.csv\", DataFrame);\n\n@df d plot(:AOA, :GJT\n           , seriestype = :scatter\n           , legend = :none\n           , xlabel = \"AOA\"\n           , ylabel = \"GJT\")\n\n\n\n\n  \n    \n  \n\n\n\n  \n    \n  \n\n\n\n  \n    \n  \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nThe StatsPlots package uses the @df macro to specify that the variables in the plot() function can be found in the data frame provided just after it (i.e., d)."
  },
  {
    "objectID": "posts/2023-03-07-julia-breakpoint-regression/index.html#two-regression-models",
    "href": "posts/2023-03-07-julia-breakpoint-regression/index.html#two-regression-models",
    "title": "Adjusting to Julia: Piecewise regression",
    "section": "Two regression models",
    "text": "Two regression models\nLet’s fit two regression models to this data set using the GLM package. The first model, lm1, is a simple regression model with AOA as the predictor and GJT as the outcome. The syntax should be self-explanatory:\n\nusing GLM \n\nlm1 = lm(@formula(GJT ~ AOA), d);\ncoeftable(lm1)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nCoef.\nStd. Error\nt\nPr(&gt;\nt\n)\n\n\n\n\n(Intercept)\n190.409\n3.90403\n48.77\n&lt;1e-57\n182.63\n198.188\n\n\nAOA\n-1.21798\n0.105139\n-11.58\n&lt;1e-17\n-1.42747\n-1.00848\n\n\n\n\n\nWe can visualise this model by plotting the data in a scatterplot and adding the model predictions to it like so. I use begin and end to force Julia to only produce a single plot.\n\nd[!, \"prediction\"] = predict(lm1);\n\nbegin\n@df d plot(:AOA, :GJT\n           , seriestype = :scatter\n           , legend = :none\n           , xlabel = \"AOA\"\n           , ylabel = \"GJT\");\n@df d plot!(:AOA, :prediction\n            , seriestype = :line)\nend\n\n\n\n\n  \n    \n  \n\n\n\n  \n    \n  \n\n\n\n  \n    \n  \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nOur second model will incorporate an ‘elbow’ in the regression line at a given breakpoint – a piecewise regression model. For a breakpoint bp, we need to create a variable since_bp that encodes how many years beyond this breakpoint the participants’ AOA values are. If an AOA value is lower than the breakpoint, the corresponding since_bp value is just 0. The add_breakpoint() value takes a dataset containing an AOA variable and adds a variable called since_bp to it.\n\nfunction add_breakpoint(data, bp)\n    data[!, \"since_bp\"] = max.(0, data[!, \"AOA\"] .- bp);\nend;\n\nTo add the since_bp variable for a breakpoint at age 12 to our dataset d, we just run this function. Note that in Julia, arguments are not copied when they are passed to a function. That is, the add_breakpoint() function changes the dataset; it does not create a changed copy of the dataset like R would:\n\n# changes d!\nadd_breakpoint(d, 12);\nprint(d);\n\n76×4 DataFrame\n Row │ AOA    GJT    prediction  since_bp \n     │ Int64  Int64  Float64     Int64    \n─────┼────────────────────────────────────\n   1 │    59    151     118.548        47\n   2 │     9    182     179.447         0\n   3 │    51    127     128.292        39\n   4 │    58    113     119.766        46\n   5 │    27    157     157.523        15\n   6 │    11    188     177.011         0\n   7 │    17    125     169.703         5\n   8 │    57    138     120.984        45\n   9 │    10    171     178.229         0\n  10 │    14    168     173.357         2\n  11 │    20    174     166.049         8\n  12 │    34    149     148.997        22\n  13 │    19    155     167.267         7\n  14 │    54    149     124.638        42\n  15 │    63    107     113.676        51\n  16 │    71    104     103.932        59\n  17 │    24    176     161.177        12\n  18 │    16    143     170.921         4\n  19 │    22    133     163.613        10\n  20 │    48    113     131.946        36\n  21 │    17    171     169.703         5\n  22 │    20    144     166.049         8\n  23 │    44    151     136.818        32\n  24 │    24    182     161.177        12\n  25 │    56    113     122.202        44\n  26 │     5    197     184.319         0\n  27 │    71    114     103.932        59\n  28 │    36    170     146.561        24\n  29 │    57    115     120.984        45\n  30 │    45    115     135.6          33\n  31 │    56    118     122.202        44\n  32 │    44    118     136.818        32\n  33 │    23    155     162.395        11\n  34 │    18    186     168.485         6\n  35 │    42    132     139.254        30\n  36 │    54    116     124.638        42\n  37 │    14    169     173.357         2\n  38 │    47    131     133.164        35\n  39 │     8    196     180.665         0\n  40 │    24    122     161.177        12\n  41 │    52    148     127.074        40\n  42 │    27    188     157.523        15\n  43 │    11    198     177.011         0\n  44 │    18    174     168.485         6\n  45 │    48    150     131.946        36\n  46 │    31    158     152.651        19\n  47 │    49    131     130.728        37\n  48 │    48    131     131.946        36\n  49 │    15    180     172.139         3\n  50 │    49    113     130.728        37\n  51 │    23    167     162.395        11\n  52 │    10    193     178.229         0\n  53 │    20    164     166.049         8\n  54 │    24    183     161.177        12\n  55 │    35    118     147.779        23\n  56 │    36    136     146.561        24\n  57 │    44    115     136.818        32\n  58 │    49    141     130.728        37\n  59 │    15    181     172.139         3\n  60 │    12    193     175.793         0\n  61 │    53    140     125.856        41\n  62 │    16    153     170.921         4\n  63 │    54    110     124.638        42\n  64 │     9    163     179.447         0\n  65 │    25    174     159.959        13\n  66 │    27    169     157.523        15\n  67 │    18    179     168.485         6\n  68 │    26    143     158.741        14\n  69 │    22    162     163.613        10\n  70 │    50    128     129.51         38\n  71 │    42    119     139.254        30\n  72 │     5    197     184.319         0\n  73 │    14    168     173.357         2\n  74 │    39    132     142.908        27\n  75 │    56    140     122.202        44\n  76 │    12    182     175.793         0\n\n\nSince we don’t know what the best breakpoint is, we’re going to estimate it from the data. For each integer in a given range (minbp through maxbp), we’re going to fit a piecewise regression model with that integer as the breakpoint. We’ll then pick the breakpoint that minimises the deviance of the fit (i.e., the sum of squared differences between the model fit and the actual outcome). The fit_piecewise() function takes care of this. It outputs both the best fitting piecewise regression model and the breakpoint used for this model.\n\nfunction fit_piecewise(data, minbp, maxbp)\n  min_deviance = Inf\n  best_model = nothing\n  best_bp = 0\n  current_model = nothing\n  \n  for bp in minbp:maxbp\n    add_breakpoint(data, bp)\n    current_model = lm(@formula(GJT ~ AOA + since_bp), data)\n    if deviance(current_model) &lt; min_deviance\n      min_deviance = deviance(current_model)\n      best_model = current_model\n      best_bp = bp\n    end\n  end\n  \n  return best_model, best_bp\nend;\n\nLet’s now apply this function to our dataset. The estimated breakpoint is at age 16, and the model coefficients are shown below:\n\nlm2 = fit_piecewise(d, 6, 20);\n# the first output is the model itself, the second the breakpoint used\ncoeftable(lm2[1])\nlm2[2]\n\n16\n\n\nLet’s visualise this model by drawing a scatterplot and adding the regression fit to it. While we’re at it, we might as well add a 95% confidence band around the regression fit.\n\nadd_breakpoint(d, 16);\npredictions = predict(lm2[1], d;\n                      interval = :confidence,\n                      level = 0.95);\nd[!, \"prediction\"] = predictions[!, \"prediction\"];\nd[!, \"lower\"] = predictions[!, \"lower\"];\nd[!, \"upper\"] = predictions[!, \"upper\"];\n\nbegin\n@df d plot(:AOA, :GJT\n           , seriestype = :scatter\n           , legend = :none\n           , xlabel = \"AOA\"\n           , ylabel = \"GJT\"\n      );\n@df d plot!(:AOA, :prediction\n            , seriestype = :line\n            , ribbon = (:prediction .- :lower, \n                        :upper .- :prediction)\n      )\nend\n\n\n\n\n  \n    \n  \n\n\n\n  \n    \n  \n\n\n\n  \n    \n  \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nWe could run an \\(F\\)-test for the model comparison like below, but the \\(p\\)-value corresponds to the \\(p\\)-value for the since_bp value, anyway:\n\nftest(lm1.model, lm2[1].model);\n\nBut there’s a problem: This \\(p\\)-value can’t be taken at face value. By looping through different possible breakpoint and then picking the one that worked best for our dataset, we’ve increased our chances of finding some pattern in the data even if nothing is going on. So we need to recalibrate the \\(p\\)-value we’ve obtained."
  },
  {
    "objectID": "posts/2023-03-07-julia-breakpoint-regression/index.html#recalibrating-the-p-value",
    "href": "posts/2023-03-07-julia-breakpoint-regression/index.html#recalibrating-the-p-value",
    "title": "Adjusting to Julia: Piecewise regression",
    "section": "Recalibrating the p-value",
    "text": "Recalibrating the p-value\nOur strategy is as follows. We will generate a fairly large number of datasets similar to d but of which we know that there isn’t any breakpoint in the GJT/AOA relationship. We will do this by simulating new GJT values from the simple regression model fitted above (lm1). We will then apply the fit_piecewise() function to each of these datasets, using the same minbp and maxbp values as before and obtain the \\(p\\)-value associated with each model. We will then compute the proportion of the \\(p\\)-value so obtained that is lower than the \\(p\\)-value from our original model, i.e., 0.0472.\nI wasn’t able to find a Julia function similar to R’s simulate() that simulates a new outcome variable based on a linear regression model. But such a function is easy enough to put together:\n\nusing Distributions\n\nfunction simulate_outcome(null_model)\n  resid_distr = Normal(0, dispersion(null_model.model))\n  prediction = fitted(null_model)\n  new_outcome = prediction + rand(resid_distr, length(prediction))\n  return new_outcome\nend;\n\nThe one_run() function generates a single new outcome vector, overwrites the GJT variable in our dataset with it, and then applies the fit_piecewise() function to the dataset, returning the \\(p\\)-value of the best-fitting piecewise regression model.\n\nfunction one_run(data, null_model, min_bp, max_bp)\n  new_outcome = simulate_outcome(null_model)\n  data[!, \"GJT\"] = new_outcome\n  best_model = fit_piecewise(data, min_bp, max_bp)\n  pval = coeftable(best_model[1]).cols[4][3]\n  return pval\nend;\n\nFinally, the generate_p_distr() function runs the one_run() function a large number of times and output the \\(p\\)-values generated.\n\nfunction generate_p_distr(data, null_model, min_bp, max_bp, n_runs)\n  pvals = [one_run(data, null_model, min_bp, max_bp) for _ in 1:n_runs]\n  return pvals\nend;\n\nOur simulation will consist of 25,000 runs, and in each run, 16 regression models will be fitted, for a total of 400,000 models. On my machine, this takes less than 20 seconds (i.e., less than 50 microseconds per model).\n\nn_runs = 25_000;\npvals = generate_p_distr(d, lm1, 6, 20, n_runs);\n\nFor about 11–12% of the datasets in which no breakpoint governed the data, the fit_piecewise() procedure returned a \\(p\\)-value of 0.0472 or lower. So our original \\(p\\)-value of 0.0472 ought to be recalibrated to about 0.12.\n\nsum(pvals .&lt;= 0.0472) / n_runs\n\n0.11388"
  },
  {
    "objectID": "posts/2023-02-23-julia-tea-tasting/index.html",
    "href": "posts/2023-02-23-julia-tea-tasting/index.html",
    "title": "Adjusting to Julia: Tea tasting",
    "section": "",
    "text": "In this third blog post in which I try my hand at the Julia language, I’ll tackle a slight variation of an old problem – Fisher’s tea tasting lady – both analytically and using a brute-force simulation."
  },
  {
    "objectID": "posts/2023-02-23-julia-tea-tasting/index.html#the-lady-tasting-tea",
    "href": "posts/2023-02-23-julia-tea-tasting/index.html#the-lady-tasting-tea",
    "title": "Adjusting to Julia: Tea tasting",
    "section": "The lady tasting tea",
    "text": "The lady tasting tea\nIn The Design of Experiments, Ronald A. Fisher explained the Fisher exact test using the following example. Imagine that a lady claims she can taste the difference between cups of tea in which the tea was poured into the cup first and then milk was added, and cups of tea in which the milk was poured first and then the tea was added. A sceptic might put the lady to the test and prepare eight cups of tea – four with tea to which milk was added, and four with milk to which tea was added. (Yuck to both, by the way.) The lady is presented with these in a random order and is asked to identify those four cups with tea to which milk was added. Now, if the lady has no discriminatory ability whatever, there is only a 1-in-70 chance she identifies all four cups correctly. This is because there are 70 ways of picking four cups out of eight, and only one of these ways is correct. In Julia:\n\nbinomial(8, 4)\n\n70\n\n\nWe can now imagine a slight variation on this experiment. If the lady identifies all four cups correctly, we choose to believe she has the purported discriminatory ability. If she identifies two or fewer cups correctly, we remain sceptical. But she identifies three out of four cups correctly, we prepare another eight cups of tea and give her another chance under the same conditions.\nWe can ask two questions about this new procedure:\n\nWith which probability will we believe the lady if she, in fact, does not have any discriminatory ability?\nHow many rounds of tea tasting will we need on average before the experiment terminates?\n\nIn the following, I’ll share both analytical and a simulation-based answers to these questions."
  },
  {
    "objectID": "posts/2023-02-23-julia-tea-tasting/index.html#analytical-solution",
    "href": "posts/2023-02-23-julia-tea-tasting/index.html#analytical-solution",
    "title": "Adjusting to Julia: Tea tasting",
    "section": "Analytical solution",
    "text": "Analytical solution\nUnder the null hypothesis of no discriminatory ability, the number of correctly identified cups in any one draw (\\(X\\)) follows a hypergeometric distribution with parameters \\(N = 8\\) (total), \\(K = 4\\) (successes) and \\(n = 4\\) (draws), i.e., [ X (8, 4, 4). ] In any given round, the subject fails the test if she only identifies 0, 1 or 2 cups correctly. Under the null hypothesis, the probability of this happening is given by \\(p = \\mathbb{P}(X \\leq 2)\\), the value of which we can determine using the cumulative mass function of the Hypergeometric(8, 4, 4) distribution. We suspend judgement on the subject’s discriminatory abilities if she identifies exactly three cups correctly, in which case she has another go. Under the null hypothesis, the probability of this happening in any given round is given by \\(q = \\mathbb{P}(X = 3)\\), the value of which can be determined using the probability mass function of the Hypergeometric(8, 4, 4) distribution.\nWith those probabilities in hand, we can now compute the probability that the subject fails the experiment in a specific round, assuming the null hypothesis is correct. In the first round, she will fail the experiment with probability \\(p\\). In order to fail in the second round, she needed to have advanced to the second round, which happens with probability \\(q\\), and then fail in that round, which happens with probability \\(p\\). That is, she will fail in the second round with probability \\(pq\\). To fail in the third round, she needed to advance to the third round, which happens with probability \\(q^2\\) and then fail in the third round, which happens with probability \\(p\\). That is, she will fail in the third round with probability \\(pq^2\\). Etc. etc. The probability that she will fail somewhere in the experiment if the null hypothesis is true, then, is given by \\[\n\\sum_{i = 1}^{\\infty}pq^{i-1} = \\sum_{i = 0}^{\\infty}pq^i = \\frac{p}{1-q},\n\\] where the first equality is just a matter of shifting the index and the second equality holds because the expression is a geometric series.\nLet’s compute the final results using Julia. The following loads the DataFrames and Distributions packages and then defines d as the Hypergeometric(8, 4, 4) distribution. Note that in Julia, the parameters for the Hypergeometric distribution aren’t \\(N\\) (total), \\(K\\) (successes) and \\(n\\) (draws), but rather \\(k\\) (successes), \\(N-k\\) (failures) and \\(n\\) (draws); see the documentation. We then read off the values for \\(p\\) and \\(q\\) from the cumulative mass function and probability mass function, respectively:\n\nusing Distributions\nd = Hypergeometric(4, 4, 4);\np = cdf(d, 2);\nq = pdf(d, 3);\n\nThe probability that the subject will fail the experiment if she does indeed not have the purported discriminatory ability is now easily computed:\n\np / (1 - q)\n\n0.9814814814814815\n\n\nThe next question is how many rounds we expect the experiment to carry on for if the null hypothesis is true. At each round, the probability that the experiment terminates in that round is given by \\(1 - q\\). From the geometric distribution, we know that we then on average need \\(1/(1-q)\\) attempts before the first terminating event occurs:\n\n1 / (1 - q)\n\n1.2962962962962965\n\n\nIn sum, if the subject lacks any discriminatory ability, there is only a 1.85% chance that she will pass the test, and on average, the experiment will run for 1.3 rounds."
  },
  {
    "objectID": "posts/2023-02-23-julia-tea-tasting/index.html#brute-force-solution",
    "href": "posts/2023-02-23-julia-tea-tasting/index.html#brute-force-solution",
    "title": "Adjusting to Julia: Tea tasting",
    "section": "Brute-force solution",
    "text": "Brute-force solution\nFirst, we define a function experiment() that runs the experiment once. In essence, we have an urn that contains four correct identifications (true) and four incorrect identifications (false). From this urn, we sample() four identifications without replacement.\nNote, incidentally, that Julia functions can take both positional arguments and keyword arguments. In the sample() command below, both urn and 4 are passed as positional arguments, and you’d have to read the documentation to figure out which argument specifies what. The keyword arguments are separated from the positional arguments by a semi-colon and are identified with the parameter’s name.\nNext, we count the number of trues in our pick using sum(). Depending on how many trues there are in pick, we terminate the experiment, returning false if we remain sceptic and true if we choose to believe the lady, or we run the experiment for one more round. The number of attempts are tallied and output as well.\n\nfunction experiment(attempt = 1)\n    urn = [false, false, false, false, true, true, true, true]\n    pick = sample(urn, 4; replace = false)\n    number = sum(pick)\n    if number &lt;= 2\n        return false, attempt\n    elseif number &gt;= 4\n        return true, attempt\n    else\n      return experiment(attempt + 1)\n    end\nend;\n\nA single run of experiment() could produce the following output:\n\nexperiment()\n\n(false, 1)\n\n\nNext, we write a function simulate() that runs the experiment() a large number of times, and outputs both whether each experiment() led to us believe the lady or remaining sceptical, and how many round each experiment() took. These results are tabulated in a DataFrame – just because. Of note, Julia supports list comprehension that Python users will be familiar with. I use this feature here both the run the experiment a large number of times as well as to parse the output.\n\nusing DataFrames\n\nfunction simulate(runs = 10000)\n    results = [experiment() for _ in 1:runs]\n    success = [results[i][1] for i in 1:runs]\n    attempts = [results[i][2] for i in 1:runs]\n    d = DataFrame(Successful = success, Attempts = attempts)\n    return d\nend;\n\nLet’s swing for the fences and run this experiment a million times. Like in Python, we can make large numbers easier to parse by inserting underscores in them:\n\nruns = 1_000_000;\n\nUsing the @time macro, we can check how long it take for this simulation to finish.\n\n@time d = simulate(runs)\n\n  0.359740 seconds (4.07 M allocations: 361.334 MiB, 14.82% gc time, 35.07% compilation time)\n\n\n1000000×2 DataFrame999975 rows omitted\n\n\n\nRow\nSuccessful\nAttempts\n\n\n\nBool\nInt64\n\n\n\n\n1\nfalse\n1\n\n\n2\nfalse\n1\n\n\n3\nfalse\n1\n\n\n4\nfalse\n1\n\n\n5\nfalse\n1\n\n\n6\nfalse\n2\n\n\n7\nfalse\n3\n\n\n8\nfalse\n2\n\n\n9\nfalse\n1\n\n\n10\nfalse\n1\n\n\n11\nfalse\n1\n\n\n12\nfalse\n1\n\n\n13\nfalse\n2\n\n\n⋮\n⋮\n⋮\n\n\n999989\nfalse\n1\n\n\n999990\nfalse\n1\n\n\n999991\nfalse\n1\n\n\n999992\nfalse\n4\n\n\n999993\nfalse\n1\n\n\n999994\nfalse\n1\n\n\n999995\nfalse\n1\n\n\n999996\nfalse\n1\n\n\n999997\nfalse\n1\n\n\n999998\nfalse\n1\n\n\n999999\nfalse\n1\n\n\n1000000\nfalse\n1\n\n\n\n\n\n\nOn my machine then, running this simulation takes less than a second. Note that 60% of this time is compilation time. (Update: When migrating my blog to Quarto, I reran this code using a new Julia version (1.9.1). Now the code runs faster.) Indeed, if we run the function another time, i.e., after it’s been compiled, the run time drops to about 0.3 seconds (Update: 0.2 seconds now.):\n\n@time d2 = simulate(runs)\n\n  0.209087 seconds (3.89 M allocations: 348.982 MiB, 16.29% gc time)\n\n\n1000000×2 DataFrame999975 rows omitted\n\n\n\nRow\nSuccessful\nAttempts\n\n\n\nBool\nInt64\n\n\n\n\n1\nfalse\n1\n\n\n2\nfalse\n1\n\n\n3\nfalse\n1\n\n\n4\nfalse\n1\n\n\n5\nfalse\n1\n\n\n6\nfalse\n2\n\n\n7\nfalse\n2\n\n\n8\nfalse\n1\n\n\n9\nfalse\n1\n\n\n10\nfalse\n1\n\n\n11\nfalse\n2\n\n\n12\nfalse\n1\n\n\n13\nfalse\n1\n\n\n⋮\n⋮\n⋮\n\n\n999989\nfalse\n3\n\n\n999990\nfalse\n3\n\n\n999991\nfalse\n1\n\n\n999992\nfalse\n1\n\n\n999993\nfalse\n1\n\n\n999994\nfalse\n1\n\n\n999995\nfalse\n1\n\n\n999996\nfalse\n1\n\n\n999997\nfalse\n1\n\n\n999998\nfalse\n2\n\n\n999999\nfalse\n1\n\n\n1000000\nfalse\n1\n\n\n\n\n\n\nUsing describe(), we see that this simulation – which doesn’t ‘know’ anything about hypergeometric and geometric distributions, produces the same answers that we arrived at by analytical means: There’s a 1.86% chance that we end up believing the lady even if she has no discriminatory ability whatsoever. And if she doesn’t have any discriminatory ability, we’ll need 1.3 rounds on average before terminating the experiment:\n\ndescribe(d)\n\n2×7 DataFrame\n\n\n\nRow\nvariable\nmean\nmin\nmedian\nmax\nnmissing\neltype\n\n\n\nSymbol\nFloat64\nInteger\nFloat64\nInteger\nInt64\nDataType\n\n\n\n\n1\nSuccessful\n0.018533\nfalse\n0.0\ntrue\n0\nBool\n\n\n2\nAttempts\n1.29611\n1\n1.0\n10\n0\nInt64\n\n\n\n\n\n\nThe slight discrepancy between the simulation-based results and the analytical ones are just due to randomness. Below is a quick way for constructing 95% confidence intervals around both of our simulation-based estimates, and the analytical solutions fall within both intervals.\n\nmeans = mean.(eachcol(d))\n\n2-element Vector{Float64}:\n 0.018533\n 1.296105\n\n\n\nses = std.(eachcol(d)) / sqrt(runs)\n\n2-element Vector{Float64}:\n 0.00013486862533794173\n 0.0006198767726106645\n\n\n\nupr = means + 1.96*ses\n\n2-element Vector{Float64}:\n 0.018797342505662368\n 1.297319958474317\n\n\n\nlwr = means - 1.96*ses\n\n2-element Vector{Float64}:\n 0.018268657494337634\n 1.2948900415256832"
  },
  {
    "objectID": "posts/2015-03-03-more-selective-approach/index.html",
    "href": "posts/2015-03-03-more-selective-approach/index.html",
    "title": "A more selective approach to reporting statistics",
    "section": "",
    "text": "Daniël Lakens recently argued that, when reporting statistics in a research paper, ‘the more the merrier’, since reporting several descriptive and inferential statistics facilitates the aggregation of research results. In this post, I argue that there are other factors to be considered that can tip the scale in favour of a more selective approach to reporting statistics.\nBefore I present my case, I’d like to stress that Daniël and I agree on the key issue: we both advocate making data freely available whenever possible so that researchers can scrutinise, learn from and elaborate on each others’ findings.\nThat said, his suggestion to report more statistics presents me with an opportunity to argue for the precise opposite. It is not my intention to offer a blanket recommendation about which statistics to report and which not, but rather to highlight two factors that should inform this decision:\nI’ll briefly discuss these two factors before turning to Daniël’s specific example."
  },
  {
    "objectID": "posts/2015-03-03-more-selective-approach/index.html#the-intelligibility-factor",
    "href": "posts/2015-03-03-more-selective-approach/index.html#the-intelligibility-factor",
    "title": "A more selective approach to reporting statistics",
    "section": "The intelligibility factor",
    "text": "The intelligibility factor\nResearch papers tend to contain too many significance tests, many of which are irrelevant to the authors’ research question (e.g. here and here). While such superfluous tests needn’t affect the validity of the study, they can make its results unintelligible, especially to readers without a strong quantitative background who might not see the forest for the trees.\nI’m forced to make ample use of hedging here (‘can’, ‘might’) since I don’t know of any studies that have actually investigated how the number of statistical tests affects the communicative value of research papers. That said, I’m similarly concerned that presenting a plethora of descriptive and inferential statistics for a single comparison could likewise deepen the gap between quantitatively minded researchers and the rest: those in the know can either interpret all statistics or know enough to be comfortable with ignoring the rest; others, however, may lack these heuristics, figuring that they’re simply unable to make much sense of several of the numbers reported.\nThis, of course, is an unverified assumption, but one that warrants some thought. The implication would be that we should think twice about reporting statistics that carry little or no new or relevant information, especially if these statistics aren’t part and parcel of most readers’ repertoires."
  },
  {
    "objectID": "posts/2015-03-03-more-selective-approach/index.html#highlighting-the-take-home-message",
    "href": "posts/2015-03-03-more-selective-approach/index.html#highlighting-the-take-home-message",
    "title": "A more selective approach to reporting statistics",
    "section": "Highlighting the take-home message",
    "text": "Highlighting the take-home message\nResults sections should ideally highlight the striking features of the data in light of the research questions. Reporting fewer tests would certainly contribute to this as this makes it easier to separate the wheat from the chaff. Carefully considering how best to present the results and the tests that have been carried out would, too.\nTo be clear, I am not saying that running several tests or transforming your data in several ways and then picking whichever test happened to confirm your hypothesis is a viable alternative. What I mean is that, before actually running any statistics, you try to think through possible representations of the data, pick the most concise and informative one, and then present it."
  },
  {
    "objectID": "posts/2015-03-03-more-selective-approach/index.html#a-look-at-daniëls-example",
    "href": "posts/2015-03-03-more-selective-approach/index.html#a-look-at-daniëls-example",
    "title": "A more selective approach to reporting statistics",
    "section": "A look at Daniël’s example",
    "text": "A look at Daniël’s example\nAdmittedly, these two considerations are as yet rather vague. Helpfully, Daniël provided an example of how he would write up the results of a Stroop task conducted in one of his psychology classes and he has challenged me to come up with a better way of reporting this:\n\nThe mean reaction times of 95 participants in the Congruent condition (M = 14.88, SD = 4.50) was smaller than the mean of participants in the Incongruent condition (M = 23.69, SD = 5.41, dependent measures correlate r = 0.30). The average difference between conditions is -8.81 seconds (SD = 5.91), 95% CI = [-10.01;-7.61], t(94) = -14.54, p &lt; 0.001, Hedges’ g = -1.76, 95% CI [-2.12;-1.42]. The data are logBF10 = 52.12 times more probable under the alternative hypothesis than under the null hypothesis.\n\nI’ll first try to spell out my thoughts when I first read this example. I’ll then share my thoughts about I might go about reporting these Stroop data.\n\nThe main rationale for reporting several of these statistics is that they can then be included in meta-analyses. But the 95% confidence interval, the t-test and its associated p-value, standardised effect size (Hedges’ g) and its 95% confidence interval as well as the Bayes factor (logBF10) are all direct functions of the descriptive statistics that were provided. Surely anyone intending to run a meta-analysis should be able to derive such measures from the statistics provided.\nFrom my patchy knowledge of Bayesian statistics, I would think that the Bayes factor only makes sense when comparing two specific models (the null hypothesis and the alternative hypothesis). How is the alternative hypothesis specified in this case? I assumed that this Bayes factor was computed using the default settings in BayesFactor package for R, but I wouldn’t think that this knowledge is readily available to most readers. Perhaps it would be useful to make the competing Bayesian prior distributions more explicit first. That said, if the prior distributions are made scrutable, I’m not against reporting Bayesian statistics per se, and I like the idea of using them in an intelligible sentence (but should we remember the ‘52.12’ or rather a 1 followed by 52 zeros, i.e. 1 × 10^52?).\nI’ve written about standardised effect sizes (here: Hedges’ g) before, and I don’t think this is one of the cases where they contribute meaningful information. Since I don’t think Hedges’ g is common currency amongst most of my potential readers (your situation may be different), I’d axe it. I’ll address the question whether not reporting such standardised effect sizes endangers the potential for comparisons between studies below.\nEven though Daniël makes the case for reporting more statistics, this example only features a fraction of the statistics that could reasonably have been reported (medians, MADs, number of participants who show a Stroop effect vs those who don’t; Wilcoxon signed rank test, sign test, permutation tests, regressions, perhaps some machine-learning algorithms I haven’t thought of etc.), illustrating that we necessarily have to exercise selectivity when reporting statistics.\nIf the reading times weren’t measured with centisecond accuracy, keeping only two significant figures may be better.\nIs the Stroop effect best thought of as a 9-second average difference between reading times in the congruent and the incongruent condition? (I’d lose the minus sign, too, but now I’m splitting hairs.) To elaborate, do we conceive of the Stroop effect as an additive shift (i.e. take your reading time in the congruent condition, add the Stroop effect and there you have your reading time in the incongruent condition, give or take an error term) or would we rather think of it as a multiplicative effect (i.e. take your reading time in the congruent condition, multiply by the Stroop effect and out comes your reading time in the incongruent condition, give or take an error term).\n\nThis may sound like nit-picking, but whether we think of the Stroop effect as an additive or a multiplicative effect should affect how we present the results. I’m not on top of the Stroop literature, but Uttl and Graf (1997) discuss a study that nicely illustrates why the difference is important. A Stroop task was administered to middle-old and old-old participants. The middle-old participants showed a difference of 495 ms per item between the congruent (592 ms/item) and the incongruent condition (1087 ms/item), whereas old-old participants showed an effect of 710 ms per item (854 vs 1564 ms). This could be interpreted as evidence for an Age-by-Condition interaction. But if instead of calculating differences in reading times, we calculate ratios of reading times, we find that both groups show a multiplicative Stroop effect of 1.8: to arrive at the reading time in the incongruent condition, you have to multiply the reading time in the congruent condition by 1.8 (i.e. add 80%) in both cases.\nI’m not an expert on the Stroop task, but I think that, in light of such findings, the Stroop effect is best understood as a multiplicative effect. But I might be wrong. Daniël’s data, which he was kind enough to share, don’t really provide compelling evidence for either position.\nNote, incidentally, that if we compute Stroop ratios rather than differences, we can directly compare studies without needing to bother with ‘standardised’ effect sizes: the reading time ratios can straigthforwardly be compared between studies.\nAnother way to standardise the Stroop effect so that it can be compared across studies would be to divide the total reading times by the number of items (seconds needed per item) or their reciprocal, i.e. speeds (number of items read per second).\n\nRegardless of whether the Stroop effect is best thought of as additive or multiplicative, a detailed reporting based on the average effect can obfuscate some important trends that would seem relevant for meta-analytical purposes. Such patterns may be worth highlighting even if they weren’t of primary importance to the researchers themselves.\n\nDaniël has kindly provided me with his dataset so that I can illustrate what I mean. In the two plots below, I’ve plotted the Stroop effect (left: difference scores, right: ratio scores) against the participants’ reading times in the congruent (baseline) condition. The dashed horizontal lines indicate the expected effect if there weren’t any Stroop effect. Since all but five out of 95 participants show a Stroop effect, we’d already be pretty confident about the existence of the effect, and the inferential statistics are an afterthought at best.\n\nMore interestingly, we can discern a negative pattern in these data. This is likely due to regression to the mean (slow participants sped up relative to fast participants). But this negative pattern could give rise to spurious contradictions between studies that only report averages (be it in great detail). For instance, if another study were to sample participants with faster reading times in the congruent condition and find a much more larger average Stroop effect than Daniël did, the two findings aren’t necessarily in contradiction given this negative trend.\nThere’s no doubt that there’s a Stroop effect in these data (90 out of 95 participants show it), but such systematic variation in the Stroop effect forces us to think about whether it makes sense to report a single average Stroop effect. I’m not in a position to answer this question, but in any case, I think it would make sense to highlight this pattern."
  },
  {
    "objectID": "posts/2015-03-03-more-selective-approach/index.html#an-alternative-write-up",
    "href": "posts/2015-03-03-more-selective-approach/index.html#an-alternative-write-up",
    "title": "A more selective approach to reporting statistics",
    "section": "An alternative write-up",
    "text": "An alternative write-up\nAssuming that both conditions contained 30 items and that I’d have to report a global average Stroop effect, my summary of these data might read something like this:\n\nAcross the 95 participants, the mean reading time per item in the incongruent condition (M = 0.79 s, SD = 0.18 s) was 1.7 ± 0.5 (mean ± SD) times as large as in the congruent condition (M = 0.50 s, SD = 0.15 s). We note, however, that the effect seems to vary systematically with the baseline reading times such that slow readers in the congruent condition showed smaller or no Stroop effects (see Figure X).\n\n\nI’ve assumed that the average (multiplicative) effect is what is of interest here, so that is what I have highlighted both in my description and in the graph.\nIf for some reason, I couldn’t share the dataset, I’d probably also report the intercorrelation between the two dependent measures. I’d probably also add a one-sample t-test on the individual participants’ Stroop ratios (with H0: μ = 1) to anticipate any reviewer comments, though I think the ‘1.7 ± 0.5’ is pretty convincing, especially considering the graph.\nIf the negative trend is of further importance, though, a regression model (in which Incongruent is modeled as a function of Congruent centred at its central tendency) could be more useful than a comparisons of means. The intercept of such a model would indicate the Stroop effect for participant with an average reading time in the congruent condition, whereas the slope would indicate how the effect varies as a function of the baseline reading times.\nThese considerations are specific to this example, but I think considering each type of data in its own right makes sense. And obviously, this write-up could no doubt be improved upon."
  },
  {
    "objectID": "posts/2015-03-03-more-selective-approach/index.html#wrapping-up",
    "href": "posts/2015-03-03-more-selective-approach/index.html#wrapping-up",
    "title": "A more selective approach to reporting statistics",
    "section": "Wrapping up",
    "text": "Wrapping up\nI don’t want to give off the impression that my write-ups are exemplary. When re-reading older drafts or published papers, I more often than not wonder whether I could’ve done a better job at presenting my results. But my worry is always that I may have reported too much information – not too little.\nI don’t want to overstate the extent to which I beg to disagree with Daniël’s blog post. We both advocate ‘open science’ and we both want to communicate our results as optimally as possible. I just think that this entails reporting fewer rather than more statistics. Perhaps this is because I, as well as most people I work with and write for, haven’t had a formal education in statistics and too often had to skip utterly unintelligible results sections as a result.\nIn any case – whether or not my arguments apply when writing to audiences with strong quantitative backgrounds – I hope that my write-up at least illustrates that a simple graph can highlight striking patterns that a t-test, even when reported in great detail, just can’t."
  },
  {
    "objectID": "posts/2015-03-03-more-selective-approach/index.html#acknowledgements-and-further-reading",
    "href": "posts/2015-03-03-more-selective-approach/index.html#acknowledgements-and-further-reading",
    "title": "A more selective approach to reporting statistics",
    "section": "Acknowledgements and further reading",
    "text": "Acknowledgements and further reading\nThanks to Daniel Ezra Johnson for his comments on an earlier draft.\nMany of the points discussed here were inspired by Robert P. Abelson’s Statistics as principled argument and William S. Cleveland’s Visualizing data."
  },
  {
    "objectID": "posts/2021-06-29-posttreatment/index.html",
    "href": "posts/2021-06-29-posttreatment/index.html",
    "title": "The consequences of controlling for a post-treatment variable",
    "section": "",
    "text": "Let’s say you want to find out if a pedagogical intervention boosts learners’ conversational skills in L2 French. You’ve learnt that including a well-chosen control variable in your analysis can work wonders in terms of statistical power and precision, so you decide to administer a French vocabulary test to your participants in order to include their score on this test in your analyses as a covariate. But if you administer the vocabulary test after the intervention, it’s possible that the vocabulary scores are themselves affected by the intervention as well. If this is indeed the case, you may end up doing more harm than good. In this blog post, I will take a closer look at five general cases where controlling for such a ‘post-treatment’ variable is harmful.\nIn the following, x and y refer to the independent and dependent variable of interest, respectively, i.e., x would correspond to the intervention and y to the L2 French conversational skills in our example. z refers to the post-treatment variable, i.e., the French vocabulary scores in our example. x is a binary variable, y and z are continuous. Since z is a post-treatment variable, it’s possible that it is itself influenced directly or indirectly by x. In the five cases examined below, this is indeed the case.\nI’ve included all R code as I think running simulations like the ones below are a useful way to learn research design and statistics. If you’re just interested in the upshot, just ignore the code snippets :)"
  },
  {
    "objectID": "posts/2021-06-29-posttreatment/index.html#case-1-x-affects-both-y-and-z-y-and-z-dont-affect-each-other.",
    "href": "posts/2021-06-29-posttreatment/index.html#case-1-x-affects-both-y-and-z-y-and-z-dont-affect-each-other.",
    "title": "The consequences of controlling for a post-treatment variable",
    "section": "Case 1: x affects both y and z; y and z don’t affect each other.",
    "text": "Case 1: x affects both y and z; y and z don’t affect each other.\nIn the first case, x affects both y and z, but z and y don’t influence each other.\n\n\n\n\n\nFigure 1.1. The causal links between x, y and z in Case 1.\n\n\n\n\nIn this case, controlling for z doesn’t bias the estimate for the causal influence of x on y. It does, however, reduce the precision of these estimates. To appreciate this, let’s simulate some data. The function case1() defined in the next code snippet generates a dataset corersponding to Case 1. The parameter beta_xy specifies the coefficient of the influence of x on y; the goal of the analysis is to estimate the value of this parameter from the data. The parameter beta_xz similarly specifies the coefficient of the influence of x on z. Estimating the latter coefficient isn’t a goal of the analysis, since z is merely a control variable.\n\ncase1 &lt;- function(n_per_group, beta_xy = 1, beta_xz = 1.5) {\n  # Create x (n_per_group 0s and n_per_group 1s)\n  x &lt;- rep(c(0, 1), each = n_per_group)\n  \n  # x affects y; 'rnorm' just adds some random noise to the observations.\n  # In a DAG, this noise corresponds to the influence of other variables that\n  # didn't need to be plotted.\n  y &lt;- beta_xy*x + rnorm(2*n_per_group)\n  \n  # x affects z\n  z &lt;- beta_xz*x + rnorm(2*n_per_group)\n  \n  # Create data frame\n  dfr &lt;- data.frame(x = as.factor(x), y, z)\n  \n  # Add info: z above or below median?\n  dfr$z_split &lt;- factor(ifelse(dfr$z &gt; median(dfr$z), \"above\", \"below\"))\n  \n  # Return data frame\n  dfr\n}\n\nUse this function to create a dataset with 100 participants per group:\n\ndf_case1 &lt;- case1(n_per_group = 100)\n# Type 'df_case1' to inspect.\n\nA graphical analysis that doesn’t take the control variable z into account reveals a roughly one-point difference between the two conditions, which is as it should be.\n\nlibrary(tidyverse)\nggplot(data = df_case1,\n       aes(x = x, y = y)) +\n  geom_boxplot(outlier.shape = NA) +\n  geom_point(position = position_jitter(width = 0.2), pch = 1)\n\n\n\n\nFigure 1.2. Graphical analysis without the covariate for Case 1.\n\n\n\n\nA linear model is able to retrieve the beta_xy coefficient, which was set at 1, well enough (\\(\\widehat{\\beta_{xy}} = 1.03 \\pm 0.13\\)).\n\nsummary(lm(y ~ x, df_case1))$coefficient\n\n            Estimate Std. Error t value Pr(&gt;|t|)\n(Intercept)  -0.0333     0.0932  -0.357 7.21e-01\nx1            1.0432     0.1319   7.911 1.77e-13\n\n\nAlternatively, we could analyse these data while taking the control variable into account. The graphical analysis in Figure 3 achieves this by splitting up the control variable at its median and plotting the two subset separately. This is statistically suboptimal, but it makes the visualisation easier to grok. Here we also find a roughly one-point difference between the two conditions in each panel, which suggests that controlling for z won’t induce any bias.\n\nggplot(data = df_case1,\n       aes(x = x, y = y)) +\n  geom_boxplot(outlier.shape = NA) +\n  geom_point(position = position_jitter(width = 0.2), pch = 1) +\n  facet_wrap(~ z_split)\n\n\n\n\nFigure 1.3. Graphical analysis with the covariate (median split) for Case 1.\n\n\n\n\nThe linear model is again able to retrieve the coefficient of interest well enough (\\(\\widehat{\\beta_{xy}} = 1.04 \\pm 0.16\\)), though with a slightly wider standard error.\n\nsummary(lm(y ~ x + z, df_case1))$coefficient\n\n            Estimate Std. Error t value Pr(&gt;|t|)\n(Intercept)  -0.0346     0.0935  -0.371 7.11e-01\nx1            1.0866     0.1698   6.398 1.12e-09\nz            -0.0254     0.0625  -0.407 6.85e-01\n\n\nOf course, it’s difficult to draw any firm conclusions about the analysis of a single simulated dataset. To see that in this general case, the coefficient of interest is indeed estimated without bias but with decreased precision, let’s generate 5,000 such datasets and analyse them with and without taking the control variable into account. The function sim_case1() defined below runs these analyses; the ggplot call plots the estimates for the \\(\\beta_{xy}\\) parameter. As the caption to Figure 4 explains, this simulation confirms what we observed above.\n\n# Another function. This one takes the function case1(),\n# runs it nruns (here: 1000) times and extracts estimates\n# from two analyses per generated dataset.\nsim_case1 &lt;- function(nruns = 5000, n_per_group = 100, beta_xy = 1, beta_xz = 1.5) {\n  est_without &lt;- vector(\"double\", length = nruns)\n  est_with &lt;- vector(\"double\", length = nruns)\n  \n  for (i in 1:nruns) {\n    # Generate data\n    d &lt;- case1(n_per_group = n_per_group, beta_xy = beta_xy, beta_xz = beta_xz)\n    \n    # Analyse (in regression model) without covariate and extract estimate\n    est_without[[i]] &lt;- coef(lm(y ~ x, data = d))[[2]]\n    \n    # Analyse with covariate, extract estimate\n    est_with[[i]] &lt;- coef(lm(y ~ x + z, data = d))[[2]]\n  }\n  \n  # Output data frame with results\n  results &lt;- data.frame(\n    simulation = rep(1:nruns, 2),\n    covariate = rep(c(\"with covariate\", \"without covariate\"), each = nruns),\n    estimate = c(est_with, est_without)\n  )\n}\n\n# Run function and plot results\nresults_sim_case1 &lt;- sim_case1()\nggplot(data = results_sim_case1,\n       aes(x = estimate)) +\n  geom_histogram(fill = \"lightgrey\", colour = \"black\", bins = 20) +\n  geom_vline(xintercept = 1, linetype = \"dashed\") +\n  facet_wrap(~ covariate)\n\n\n\n\nFigure 1.4. In Case 1, the distribution of the parameter estimates is centred around the correct value both when the control variable is taken into account and when it isn’t. The distribution is wider when taking the control variable into account, however, i.e., the estimates are less precise when taking the control variable into account than when not taking it into account.\n\n\n\n\nThe estimate for the \\(\\beta_{xy}\\) parameter is unbiased in both analyses, but the analysis with the covariate offers less rather than more precision: The standard deviation of the distribution of parameter estimates increases from 0.14 to 0.18:\n\nresults_sim_case1 %&gt;% \n  group_by(covariate) %&gt;% \n  summarise(mean_est = mean(estimate),\n            sd_est = sd(estimate))\n\n# A tibble: 2 × 3\n  covariate         mean_est sd_est\n  &lt;chr&gt;                &lt;dbl&gt;  &lt;dbl&gt;\n1 with covariate       0.999  0.176\n2 without covariate    0.998  0.140"
  },
  {
    "objectID": "posts/2021-06-29-posttreatment/index.html#case-2-x-affects-y-which-in-turn-affects-z.",
    "href": "posts/2021-06-29-posttreatment/index.html#case-2-x-affects-y-which-in-turn-affects-z.",
    "title": "The consequences of controlling for a post-treatment variable",
    "section": "Case 2: x affects y, which in turn affects z.",
    "text": "Case 2: x affects y, which in turn affects z.\nIn the second case, x affects y directly, and y in turns affects z.\n\n\n\n\n\nFigure 2.1. The causal links between x, y and z in Case 2.\n\n\n\n\nThis time, controlling for z biases the estimates for the \\(\\beta_{xy}\\) parameter. To see this, let’s again simulate and analyse some data.\n\ncase2 &lt;- function(n_per_group, beta_xy = 1, beta_yz = 1.5) {\n  x &lt;- rep(c(0, 1), each = n_per_group)\n  y &lt;- beta_xy*x + rnorm(2*n_per_group)\n  \n  # y affects z\n  z &lt;- beta_yz*y + rnorm(2*n_per_group)\n  \n  dfr &lt;- data.frame(x = as.factor(x), y, z)\n  dfr$z_split &lt;- factor(ifelse(dfr$z &gt; median(dfr$z), \"above\", \"below\"))\n  \n  dfr\n}\n\ndf_case2 &lt;- case2(n_per_group = 100)\n\nWhen the data are analyses without taking the control variable into account, we obtain the following result:\n\nggplot(data = df_case2,\n       aes(x = x, y = y)) +\n  geom_boxplot(outlier.shape = NA) +\n  geom_point(position = position_jitter(width = 0.2), pch = 1)\n\n\n\n\nFigure 2.2. Graphical analysis without the covariate for Case 2.\n\n\n\n\nThis isn’t quite as close to a one-point difference as in the previous example, but as we’ll see below that’s merely due to the randomness inherent in these simulations. The linear model yields a parameter estimate of \\(\\widehat{\\beta_{xy}} = 0.76 \\pm 0.14\\).\n\nsummary(lm(y ~ x, df_case2))$coefficient\n\n            Estimate Std. Error t value Pr(&gt;|t|)\n(Intercept)    0.295      0.095    3.11 2.15e-03\nx1             0.684      0.134    5.09 8.33e-07\n\n\nWhen we take the control variable into account, however, the difference between the two groups defined by x becomes smaller:\n\nggplot(data = df_case2,\n       aes(x = x, y = y)) +\n  geom_boxplot(outlier.shape = NA) +\n  geom_point(position = position_jitter(width = 0.2), pch = 1) +\n  facet_wrap(~ z_split)\n\n\n\n\nFigure 2.3. Graphical analysis with the covariate for Case 2.\n\n\n\n\nThe linear model now yields a parameter estimate of \\(\\widehat{\\beta_{xy}} = 0.18 \\pm 0.08\\), which is considerably farther from the actual parameter value of 1.\n\nsummary(lm(y ~ x + z, df_case2))$coefficient\n\n            Estimate Std. Error t value Pr(&gt;|t|)\n(Intercept)   0.0653     0.0540    1.21 2.28e-01\nx1            0.1638     0.0788    2.08 3.89e-02\nz             0.4642     0.0221   21.03 3.05e-52\n\n\nThe larger-scale simulation shows that the analysis with the covariate is indeed biased if you want to estimate the causal influence of x on y.\n\n# Change beta_xz to beta_xy compared to the previous case\nsim_case2 &lt;- function(nruns = 5000, n_per_group = 100, beta_xy = 1, beta_yz = 1.5) {\n  est_without &lt;- vector(\"double\", length = nruns)\n  est_with &lt;- vector(\"double\", length = nruns)\n  \n  for (i in 1:nruns) {\n    d &lt;- case2(n_per_group = n_per_group, beta_xy = beta_xy, beta_yz = beta_yz)\n    est_without[[i]] &lt;- coef(lm(y ~ x, data = d))[[2]]\n    est_with[[i]] &lt;- coef(lm(y ~ x + z, data = d))[[2]]\n  }\n  \n  results &lt;- data.frame(\n    simulation = rep(1:nruns, 2),\n    covariate = rep(c(\"with covariate\", \"without covariate\"), each = nruns),\n    estimate = c(est_with, est_without)\n  )\n}\n\nresults_sim_case2 &lt;- sim_case2()\nggplot(data = results_sim_case2,\n       aes(x = estimate)) +\n  geom_histogram(fill = \"lightgrey\", colour = \"black\", bins = 20) +\n  geom_vline(xintercept = 1, linetype = \"dashed\") +\n  facet_wrap(~ covariate)\n\n\n\n\nFigure 2.4. In Case 2, the distribution of the parameter estimates is centred around the correct value both when the control variable isn’t taken into account but it is strongly biased when this control variable is taken into account, i.e., the analysis with the covariate yields biased estimates.\n\n\n\n\nThe fact that the distribution of the parameter estimates is narrower when taking the covariate into account is completely immaterial, since these estimates are estimating the wrong quantity.\n\nresults_sim_case2 %&gt;% \n  group_by(covariate) %&gt;% \n  summarise(mean_est = mean(estimate),\n            sd_est = sd(estimate))\n\n# A tibble: 2 × 3\n  covariate         mean_est sd_est\n  &lt;chr&gt;                &lt;dbl&gt;  &lt;dbl&gt;\n1 with covariate       0.309 0.0851\n2 without covariate    1.00  0.143"
  },
  {
    "objectID": "posts/2021-06-29-posttreatment/index.html#case-3-x-and-y-both-affect-z.-x-also-affects-y.",
    "href": "posts/2021-06-29-posttreatment/index.html#case-3-x-and-y-both-affect-z.-x-also-affects-y.",
    "title": "The consequences of controlling for a post-treatment variable",
    "section": "Case 3: x and y both affect z. x also affects y.",
    "text": "Case 3: x and y both affect z. x also affects y.\nNow z is affected by both x and y. x still affects y, though. Taking the covariate into account again yields biased estimates.\n\n\n\n\n\nFigure 3.1. The causal links between x, y and z in Case 3.\n\n\n\n\nSame procedure as last year, James.\n\ncase3 &lt;- function(n_per_group, beta_xy = 1, beta_xz = 1.5, beta_yz = 1.5) {\n  x &lt;- rep(c(0, 1), each = n_per_group)\n  y &lt;- beta_xy*x + rnorm(2*n_per_group)\n  \n  # x and y affect z\n  z &lt;- beta_xz*x + beta_yz*y + rnorm(2*n_per_group)\n  \n  dfr &lt;- data.frame(x = as.factor(x), y, z)\n  dfr$z_split &lt;- factor(ifelse(dfr$z &gt; median(dfr$z), \"above\", \"below\"))\n\n  dfr\n}\n\ndf_case3 &lt;- case3(n_per_group = 100)\n\n\nggplot(data = df_case3,\n       aes(x = x, y = y)) +\n  geom_boxplot(outlier.shape = NA) +\n  geom_point(position = position_jitter(width = 0.2), pch = 1)\n\n\n\n\nFigure 3.2. Graphical analysis without the covariate for Case 3.\n\n\n\n\nAgain, the analysis without the control variable yields a reasonably accurate estimate of the true parameter value of 1 (\\(\\widehat{\\beta_{xy}} = 1.07 \\pm 0.15\\)).\n\nsummary(lm(y ~ x, df_case3))$coefficient\n\n            Estimate Std. Error t value Pr(&gt;|t|)\n(Intercept)   -0.101      0.106  -0.953 3.42e-01\nx1             1.047      0.150   6.992 4.07e-11\n\n\nWhen we take the control variable into account, however, the difference between the two groups defined by x becomes smaller:\n\nggplot(data = df_case3,\n       aes(x = x, y = y)) +\n  geom_boxplot(outlier.shape = NA) +\n  geom_point(position = position_jitter(width = 0.2), pch = 1) +\n  facet_wrap(~ z_split)\n\n\n\n\nFigure 3.3. Graphical analysis with the covariate for Case 3.\n\n\n\n\nThe linear model now yields a parameter estimate of \\(\\widehat{\\beta_{xy}} = -0.31 \\pm 0.11\\), which is considerably farther from the actual parameter value of 1 and even has the wrong sign.\n\nsummary(lm(y ~ x + z, df_case3))$coefficient\n\n            Estimate Std. Error t value Pr(&gt;|t|)\n(Intercept)  0.00596     0.0580   0.103 9.18e-01\nx1          -0.31746     0.1033  -3.074 2.41e-03\nz            0.46047     0.0213  21.629 6.28e-54\n\n\nFor the sake of completeness, let’s run this simulation 5,000 times, too.\n\nsim_case3 &lt;- function(nruns = 5000, n_per_group = 100, beta_xy = 1, beta_xz = 1.5, beta_yz = 1.5) {\n  est_without &lt;- vector(\"double\", length = nruns)\n  est_with &lt;- vector(\"double\", length = nruns)\n  \n  for (i in 1:nruns) {\n    d &lt;- case3(n_per_group = n_per_group, beta_xy = beta_xy, beta_xz = beta_xz, beta_yz = beta_yz)\n    est_without[[i]] &lt;- coef(lm(y ~ x, data = d))[[2]]\n    est_with[[i]] &lt;- coef(lm(y ~ x + z, data = d))[[2]]\n  }\n\n  results &lt;- data.frame(\n    simulation = rep(1:nruns, 2),\n    covariate = rep(c(\"with covariate\", \"without covariate\"), each = nruns),\n    estimate = c(est_with, est_without)\n  )\n}\n\nresults_sim_case3 &lt;- sim_case3()\nggplot(data = results_sim_case3,\n       aes(x = estimate)) +\n  geom_histogram(fill = \"lightgrey\", colour = \"black\", bins = 20) +\n  geom_vline(xintercept = 1, linetype = \"dashed\") +\n  facet_wrap(~ covariate)\n\n\n\n\nFigure 3.4. In Case 3, too, the distribution of the parameter estimates is centred around the correct value when the control variable isn’t taken into account but it is strongly biased when this control variable is taken into account, i.e., the analysis with the covariate yields biased estimates.\n\n\n\n\nThe fact that the distribution of the parameter estimates is narrower when taking the covariate into account is completely immaterial, since these estimates are estimating the wrong quantity.\n\nresults_sim_case3 %&gt;% \n  group_by(covariate) %&gt;% \n  summarise(mean_est = mean(estimate),\n            sd_est = sd(estimate))\n\n# A tibble: 2 × 3\n  covariate         mean_est sd_est\n  &lt;chr&gt;                &lt;dbl&gt;  &lt;dbl&gt;\n1 with covariate      -0.382  0.105\n2 without covariate    1.00   0.145"
  },
  {
    "objectID": "posts/2021-06-29-posttreatment/index.html#case-4-x-affects-z-both-x-and-z-influence-y.",
    "href": "posts/2021-06-29-posttreatment/index.html#case-4-x-affects-z-both-x-and-z-influence-y.",
    "title": "The consequences of controlling for a post-treatment variable",
    "section": "Case 4: x affects z; both x and z influence y.",
    "text": "Case 4: x affects z; both x and z influence y.\nThat is, x influences both y and z, but z also influences y. Let \\(\\beta_{xy}\\) be the direct effect of x on y, \\(\\beta_{xz}\\) the effect of x on z and \\(\\beta_{zy}\\) the effect of z on y. Then the total effect of x on y is \\(\\beta_{xy} + \\beta_{xz}\\times\\beta_{zy}\\).\n\n\n\n\n\nFigure 4.1. The causal links between x, y and z in Case 4.\n\n\n\n\nUsing the defaults in the following function, the total effect of x on y is \\(1 + 1.5\\times 0.5 = 1.75\\). If this doesn’t make immediate sense, consider what a change of one unit in x causes downstream: A one-unit increase in x directly increases y by 1. It also increases z by 1.5. But a one-unit increase in z causes an increase of 0.5 in y as well, so a 1.5-unit increase in z causes an additional increase of 0.75 in y. So in total, a one-unit increase in x causes a 1.75-point increase in y.\n\ncase4 &lt;- function(n_per_group, beta_xy = 1, beta_xz = 1.5, beta_zy = 0.5) {\n  x &lt;- rep(c(0, 1), each = n_per_group)\n  \n  # x affects z\n  z &lt;- beta_xz*x + rnorm(2*n_per_group)\n  \n  # x and z affect y\n  y &lt;- beta_xy*x + beta_zy*z + rnorm(2*n_per_group)\n  \n  dfr &lt;- data.frame(x = as.factor(x), y, z)\n  \n  dfr$z_split &lt;- factor(ifelse(dfr$z &gt; median(dfr$z), \"above\", \"below\"))\n  \n  dfr\n}\n\ndf_case4 &lt;- case4(n_per_group = 100)\n\n\nggplot(data = df_case4,\n       aes(x = x, y = y)) +\n  geom_boxplot(outlier.shape = NA) +\n  geom_point(position = position_jitter(width = 0.2), pch = 1)\n\n\n\n\nFigure 4.2. Graphical analysis without the covariate for Case 4.\n\n\n\n\nAgain, the analysis without the control variable yields a reasonably accurate estimate of the true total influence of x on y of 1.75 (!) (\\(\\widehat{\\beta_{xy}} = 1.67 \\pm 0.16\\)).\n\nsummary(lm(y ~ x, df_case4))$coefficient\n\n            Estimate Std. Error t value Pr(&gt;|t|)\n(Intercept)     0.19      0.114    1.66 9.79e-02\nx1              1.49      0.161    9.21 4.69e-17\n\n\nWhen we take the control variable into account, however, the difference between the two groups defined by x becomes smaller:\n\nggplot(data = df_case4,\n       aes(x = x, y = y)) +\n  geom_boxplot(outlier.shape = NA) +\n  geom_point(position = position_jitter(width = 0.2), pch = 1) +\n  facet_wrap(~ z_split)\n\n\n\n\nFigure 4.3. Graphical analysis with the covariate for Case 4.\n\n\n\n\nThe linear model now yields a parameter estimate of \\(\\widehat{\\beta_{xy}} = 1.11 \\pm 0.17\\). This analysis correctly estimates the direct effect of x on y (i.e., without the additional causal link between x on y through z). This may be interesting in its own right, but the analysis addresses a question different from `What's the causal influence ofxony`?’’\n\nsummary(lm(y ~ x + z, df_case4))$coefficient\n\n            Estimate Std. Error t value Pr(&gt;|t|)\n(Intercept)   0.0464     0.1077   0.431 6.67e-01\nx1            0.9863     0.1702   5.797 2.65e-08\nz             0.4694     0.0777   6.041 7.52e-09\n\n\nFor the sake of completeness, let’s run this simulation 5,000 times, too.\n\nsim_case4 &lt;- function(nruns = 5000, n_per_group = 100, beta_xy = 1, beta_xz = 1.5, beta_zy = 0.5) {\n  est_without &lt;- vector(\"double\", length = nruns)\n  est_with &lt;- vector(\"double\", length = nruns)\n  \n  for (i in 1:nruns) {\n    d &lt;- case4(n_per_group = n_per_group, beta_xy = beta_xy, beta_xz = beta_xz, beta_zy = beta_zy)\n    est_without[[i]] &lt;- coef(lm(y ~ x, data = d))[[2]]\n    est_with[[i]] &lt;- coef(lm(y ~ x + z, data = d))[[2]]\n  }\n  \n  results &lt;- data.frame(\n    simulation = rep(1:nruns, 2),\n    covariate = rep(c(\"with covariate\", \"without covariate\"), each = nruns),\n    estimate = c(est_with, est_without)\n  )\n}\n\nresults_sim_case4 &lt;- sim_case4()\nggplot(data = results_sim_case4,\n       aes(x = estimate)) +\n  geom_histogram(fill = \"lightgrey\", colour = \"black\", bins = 20) +\n  geom_vline(xintercept = 1.75, linetype = \"dashed\") +\n  facet_wrap(~ covariate)\n\n\n\n\nFigure 4.4. In Case 4, the analysis without the covariate correctly estimates the total causal influence that x has on y, while the analysis with the covariate correctly estimates the direct causal effect of x on y. Either may be relevant, but you have to know which!\n\n\n\n\n\nresults_sim_case4 %&gt;% \n  group_by(covariate) %&gt;% \n  summarise(mean_est = mean(estimate),\n            sd_est = sd(estimate))\n\n# A tibble: 2 × 3\n  covariate         mean_est sd_est\n  &lt;chr&gt;                &lt;dbl&gt;  &lt;dbl&gt;\n1 with covariate       0.998  0.177\n2 without covariate    1.75   0.157"
  },
  {
    "objectID": "posts/2021-06-29-posttreatment/index.html#case-5-x-and-z-affect-y-x-and-z-dont-affect-each-other.",
    "href": "posts/2021-06-29-posttreatment/index.html#case-5-x-and-z-affect-y-x-and-z-dont-affect-each-other.",
    "title": "The consequences of controlling for a post-treatment variable",
    "section": "Case 5: x and z affect y; x and z don’t affect each other.",
    "text": "Case 5: x and z affect y; x and z don’t affect each other.\nIn the final general case, x and z both affect y, but x and z don’t affect each other. That is, z isn’t affected by the intervention in any way and so functions like a pre-treatment control variable would. The result is an increase in statistical precision. This is the only of the five cases examined in which the control variable has added value for the purposes of estimated the causal influence of x on y.\n\n\n\n\n\nFigure 5.1. The causal links between x, y and z in Case 5.\n\n\n\n\nUsing the defaults in the following function, the total effect of x on y is \\(1 + 1.5\\times 0.5 = 1.75\\). If this doesn’t make immediate sense, consider what a change of one unit in x causes downstream: A one-unit increase in x directly increases y by 1. It also increases z by 1.5. But a one-unit increase in z causes an increase of 0.5 in y as well, so a 1.5-unit increase in z causes an additional increase of 0.75 in y. So in total, a one-unit increase in x causes a 1.75-point increase in y.\n\ncase5 &lt;- function(n_per_group, beta_xy = 1, beta_zy = 1.5) {\n  x &lt;- rep(c(0, 1), each = n_per_group)\n  \n  # Create z\n  z &lt;- rnorm(2*n_per_group)\n  \n  # x and z affect y\n  y &lt;- beta_xy*x + beta_zy*z + rnorm(2*n_per_group)\n  \n  dfr &lt;- data.frame(x = as.factor(x), y, z)\n  dfr$z_split &lt;- factor(ifelse(dfr$z &gt; mean(dfr$z), \"above\", \"below\"))\n  \n  dfr\n}\n\ndf_case5 &lt;- case5(n_per_group = 100)\n\n\nggplot(data = df_case5,\n       aes(x = x, y = y)) +\n  geom_boxplot(outlier.shape = NA) +\n  geom_point(position = position_jitter(width = 0.2), pch = 1)\n\n\n\n\nFigure 5.2. Graphical analysis without the covariate for Case 5.\n\n\n\n\nAgain, the analysis without the control variable yields an estimate within one standard error of the true parameter value of 1 (\\(\\widehat{\\beta_{xy}} = 0.76 \\pm 0.24\\)).\n\nsummary(lm(y ~ x, df_case5))$coefficient\n\n            Estimate Std. Error t value Pr(&gt;|t|)\n(Intercept)   0.0201      0.172   0.117   0.9068\nx1            0.6818      0.243   2.807   0.0055\n\n\n\nggplot(data = df_case5,\n       aes(x = x, y = y)) +\n  geom_boxplot(outlier.shape = NA) +\n  geom_point(position = position_jitter(width = 0.2), pch = 1) +\n  facet_wrap(~ z_split)\n\n\n\n\nFigure 5.3. Graphical analysis with the covariate for Case 5.\n\n\n\n\nThe linear model now yields a parameter estimate of \\(\\widehat{\\beta_{xy}} = 1.08 \\pm 0.15\\), with is also a reasonable estimate but with a smaller standard error.\n\nsummary(lm(y ~ x + z, df_case5))$coefficient\n\n            Estimate Std. Error t value Pr(&gt;|t|)\n(Intercept) -0.00183     0.1022 -0.0179 9.86e-01\nx1           1.05971     0.1459  7.2626 8.62e-12\nz            1.44502     0.0759 19.0298 1.67e-46\n\n\nFor the sake of completeness, let’s run this simulation 5,000 times, too.\n\nsim_case5 &lt;- function(nruns = 1000, n_per_group = 100, beta_xy = 1, beta_zy = 1.5) {\n  est_without &lt;- vector(\"double\", length = nruns)\n  est_with &lt;- vector(\"double\", length = nruns)\n  \n  for (i in 1:nruns) {\n    d &lt;- case5(n_per_group = n_per_group, beta_xy = beta_xy, beta_zy = beta_zy)\n    est_without[[i]] &lt;- coef(lm(y ~ x, data = d))[[2]]\n    est_with[[i]] &lt;- coef(lm(y ~ x + z, data = d))[[2]]\n  }\n\n  results &lt;- data.frame(\n    simulation = rep(1:nruns, 2),\n    covariate = rep(c(\"with covariate\", \"without covariate\"), each = nruns),\n    estimate = c(est_with, est_without)\n  )\n}\n\nresults_sim_case5 &lt;- sim_case5()\nggplot(data = results_sim_case5,\n       aes(x = estimate)) +\n  geom_histogram(fill = \"lightgrey\", colour = \"black\", bins = 20) +\n  geom_vline(xintercept = 1, linetype = \"dashed\") +\n  facet_wrap(~ covariate)\n\n\n\n\nFigure 5.4. In Case 5, both analyses are centred around the correct value, i.e., both are unbiased. The analysis with the control variable yields a narrower distribution of estimates, however, i.e., it is more precise.\n\n\n\n\n\nresults_sim_case5 %&gt;% \n  group_by(covariate) %&gt;% \n  summarise(mean_est = mean(estimate),\n            sd_est = sd(estimate))\n\n# A tibble: 2 × 3\n  covariate         mean_est sd_est\n  &lt;chr&gt;                &lt;dbl&gt;  &lt;dbl&gt;\n1 with covariate       1.00   0.139\n2 without covariate    0.995  0.249"
  },
  {
    "objectID": "posts/2021-06-29-posttreatment/index.html#conclusion",
    "href": "posts/2021-06-29-posttreatment/index.html#conclusion",
    "title": "The consequences of controlling for a post-treatment variable",
    "section": "Conclusion",
    "text": "Conclusion\nWhen a control variable is collected after the intervention took place, it is possible that it is directly or indirectly affected by the intervention. If this is indeed the case, including the control variable in the analysis may yield biased estimates or decrease rather than increase the precision of the estimates. In designed experiments, the solution to this problem is evident: collect the control variable before the intervention takes place. If this isn’t possible, you had better be pretty sure that the control variable isn’t a post-treatment variable. More generally, throwing predictor variables into a statistical model in the hopes that this will improve the analysis is a dreadful idea."
  },
  {
    "objectID": "posts/2021-06-29-posttreatment/index.html#software-versions",
    "href": "posts/2021-06-29-posttreatment/index.html#software-versions",
    "title": "The consequences of controlling for a post-treatment variable",
    "section": "Software versions",
    "text": "Software versions\nPlease note that I reran the code on this page on August 6, 2023.\n\ndevtools::session_info()\n\n─ Session info ───────────────────────────────────────────────────────────────\n setting  value\n version  R version 4.3.1 (2023-06-16)\n os       Ubuntu 22.04.2 LTS\n system   x86_64, linux-gnu\n ui       X11\n language en_US\n collate  en_US.UTF-8\n ctype    en_US.UTF-8\n tz       Europe/Zurich\n date     2023-08-06\n pandoc   3.1.1 @ /usr/lib/rstudio/resources/app/bin/quarto/bin/tools/ (via rmarkdown)\n\n─ Packages ───────────────────────────────────────────────────────────────────\n package        * version date (UTC) lib source\n abind            1.4-5   2016-07-21 [1] CRAN (R 4.3.1)\n backports        1.4.1   2021-12-13 [1] CRAN (R 4.3.0)\n boot             1.3-28  2021-05-03 [4] CRAN (R 4.2.0)\n cachem           1.0.6   2021-08-19 [2] CRAN (R 4.2.0)\n callr            3.7.3   2022-11-02 [1] CRAN (R 4.3.1)\n checkmate        2.2.0   2023-04-27 [1] CRAN (R 4.3.1)\n cli              3.6.1   2023-03-23 [1] CRAN (R 4.3.0)\n cmdstanr       * 0.6.0   2023-08-02 [1] local\n coda             0.19-4  2020-09-30 [1] CRAN (R 4.3.1)\n codetools        0.2-19  2023-02-01 [4] CRAN (R 4.2.2)\n colorspace       2.1-0   2023-01-23 [1] CRAN (R 4.3.0)\n crayon           1.5.2   2022-09-29 [1] CRAN (R 4.3.1)\n curl             5.0.1   2023-06-07 [1] CRAN (R 4.3.1)\n dagitty        * 0.3-1   2021-01-21 [1] CRAN (R 4.3.1)\n devtools         2.4.5   2022-10-11 [1] CRAN (R 4.3.1)\n digest           0.6.29  2021-12-01 [2] CRAN (R 4.2.0)\n distributional   0.3.2   2023-03-22 [1] CRAN (R 4.3.1)\n dplyr          * 1.1.2   2023-04-20 [1] CRAN (R 4.3.0)\n ellipsis         0.3.2   2021-04-29 [2] CRAN (R 4.2.0)\n evaluate         0.15    2022-02-18 [2] CRAN (R 4.2.0)\n fansi            1.0.4   2023-01-22 [1] CRAN (R 4.3.1)\n farver           2.1.1   2022-07-06 [1] CRAN (R 4.3.0)\n fastmap          1.1.0   2021-01-25 [2] CRAN (R 4.2.0)\n forcats        * 1.0.0   2023-01-29 [1] CRAN (R 4.3.0)\n fs               1.5.2   2021-12-08 [2] CRAN (R 4.2.0)\n generics         0.1.3   2022-07-05 [1] CRAN (R 4.3.0)\n ggplot2        * 3.4.2   2023-04-03 [1] CRAN (R 4.3.0)\n glue             1.6.2   2022-02-24 [2] CRAN (R 4.2.0)\n gridExtra        2.3     2017-09-09 [1] CRAN (R 4.3.0)\n gtable           0.3.3   2023-03-21 [1] CRAN (R 4.3.0)\n hms              1.1.3   2023-03-21 [1] CRAN (R 4.3.0)\n htmltools        0.5.5   2023-03-23 [1] CRAN (R 4.3.0)\n htmlwidgets      1.6.2   2023-03-17 [1] CRAN (R 4.3.1)\n httpuv           1.6.11  2023-05-11 [1] CRAN (R 4.3.1)\n inline           0.3.19  2021-05-31 [1] CRAN (R 4.3.1)\n jsonlite         1.8.7   2023-06-29 [1] CRAN (R 4.3.1)\n knitr            1.39    2022-04-26 [2] CRAN (R 4.2.0)\n labeling         0.4.2   2020-10-20 [1] CRAN (R 4.3.0)\n later            1.3.1   2023-05-02 [1] CRAN (R 4.3.1)\n lattice          0.21-8  2023-04-05 [4] CRAN (R 4.3.0)\n lifecycle        1.0.3   2022-10-07 [1] CRAN (R 4.3.0)\n loo              2.6.0   2023-03-31 [1] CRAN (R 4.3.1)\n lubridate      * 1.9.2   2023-02-10 [1] CRAN (R 4.3.0)\n magrittr         2.0.3   2022-03-30 [1] CRAN (R 4.3.0)\n MASS             7.3-60  2023-05-04 [4] CRAN (R 4.3.1)\n matrixStats      1.0.0   2023-06-02 [1] CRAN (R 4.3.1)\n memoise          2.0.1   2021-11-26 [2] CRAN (R 4.2.0)\n mime             0.10    2021-02-13 [2] CRAN (R 4.0.2)\n miniUI           0.1.1.1 2018-05-18 [1] CRAN (R 4.3.1)\n munsell          0.5.0   2018-06-12 [1] CRAN (R 4.3.0)\n mvtnorm          1.2-2   2023-06-08 [1] CRAN (R 4.3.1)\n pillar           1.9.0   2023-03-22 [1] CRAN (R 4.3.0)\n pkgbuild         1.4.2   2023-06-26 [1] CRAN (R 4.3.1)\n pkgconfig        2.0.3   2019-09-22 [2] CRAN (R 4.2.0)\n pkgload          1.3.2.1 2023-07-08 [1] CRAN (R 4.3.1)\n posterior        1.4.1   2023-03-14 [1] CRAN (R 4.3.1)\n prettyunits      1.1.1   2020-01-24 [2] CRAN (R 4.2.0)\n processx         3.8.2   2023-06-30 [1] CRAN (R 4.3.1)\n profvis          0.3.8   2023-05-02 [1] CRAN (R 4.3.1)\n promises         1.2.0.1 2021-02-11 [1] CRAN (R 4.3.1)\n ps               1.7.5   2023-04-18 [1] CRAN (R 4.3.1)\n purrr          * 1.0.1   2023-01-10 [1] CRAN (R 4.3.0)\n R6               2.5.1   2021-08-19 [2] CRAN (R 4.2.0)\n Rcpp             1.0.11  2023-07-06 [1] CRAN (R 4.3.1)\n RcppParallel     5.1.7   2023-02-27 [1] CRAN (R 4.3.1)\n readr          * 2.1.4   2023-02-10 [1] CRAN (R 4.3.0)\n remotes          2.4.2   2021-11-30 [2] CRAN (R 4.2.0)\n rethinking     * 2.31    2023-08-02 [1] Github (rmcelreath/rethinking@2f01a9c)\n rlang            1.1.1   2023-04-28 [1] CRAN (R 4.3.0)\n rmarkdown        2.21    2023-03-26 [1] CRAN (R 4.3.0)\n rstan          * 2.26.22 2023-08-01 [1] local\n rstudioapi       0.14    2022-08-22 [1] CRAN (R 4.3.0)\n scales           1.2.1   2022-08-20 [1] CRAN (R 4.3.0)\n sessioninfo      1.2.2   2021-12-06 [2] CRAN (R 4.2.0)\n shape            1.4.6   2021-05-19 [1] CRAN (R 4.3.1)\n shiny            1.7.4.1 2023-07-06 [1] CRAN (R 4.3.1)\n StanHeaders    * 2.26.27 2023-06-14 [1] CRAN (R 4.3.1)\n stringi          1.7.12  2023-01-11 [1] CRAN (R 4.3.1)\n stringr        * 1.5.0   2022-12-02 [1] CRAN (R 4.3.0)\n tensorA          0.36.2  2020-11-19 [1] CRAN (R 4.3.1)\n tibble         * 3.2.1   2023-03-20 [1] CRAN (R 4.3.0)\n tidyr          * 1.3.0   2023-01-24 [1] CRAN (R 4.3.0)\n tidyselect       1.2.0   2022-10-10 [1] CRAN (R 4.3.0)\n tidyverse      * 2.0.0   2023-02-22 [1] CRAN (R 4.3.1)\n timechange       0.2.0   2023-01-11 [1] CRAN (R 4.3.0)\n tzdb             0.4.0   2023-05-12 [1] CRAN (R 4.3.0)\n urlchecker       1.0.1   2021-11-30 [1] CRAN (R 4.3.1)\n usethis          2.2.2   2023-07-06 [1] CRAN (R 4.3.1)\n utf8             1.2.3   2023-01-31 [1] CRAN (R 4.3.1)\n V8               4.3.0   2023-04-08 [1] CRAN (R 4.3.0)\n vctrs            0.6.3   2023-06-14 [1] CRAN (R 4.3.0)\n withr            2.5.0   2022-03-03 [2] CRAN (R 4.2.0)\n xfun             0.39    2023-04-20 [1] CRAN (R 4.3.0)\n xtable           1.8-4   2019-04-21 [1] CRAN (R 4.3.1)\n yaml             2.3.5   2022-02-21 [2] CRAN (R 4.2.0)\n\n [1] /home/jan/R/x86_64-pc-linux-gnu-library/4.3\n [2] /usr/local/lib/R/site-library\n [3] /usr/lib/R/site-library\n [4] /usr/lib/R/library\n\n──────────────────────────────────────────────────────────────────────────────"
  },
  {
    "objectID": "posts/2016-10-31-why-preregister/index.html",
    "href": "posts/2016-10-31-why-preregister/index.html",
    "title": "The Centre for Open Science’s Preregistration Challenge: Why it’s relevant and some recommended background reading",
    "section": "",
    "text": "This blog post is an edited version of a mail I sent round to my colleagues at the various language and linguistics departments in Fribourg. Nothing in this post is new per se, but I haven’t seen much discussion of these issues among linguists, applied linguists and bilingualism researchers.\nI’d like to point you to an initiative of the Center for Open Science: the $1,000,000 Preregistration Challenge. The basic idea is to foster research transparency by offering a monetary reward to researchers who’ve outlined their study design and planned analyses in advance and report the results of these analyses in the report.\nI’m not affiliated with this organisation, but I do think both it and its initiative are important developments. For those interested in knowing why I think so, I’ve written a brief text below that includes links to more detailed articles or examples; if you prefer reference lists, there’s one of those down below. Most of articles were written by and for psychologists, but I reckon pretty much all of it applies equally to research in linguistics and language learning.\nEver wondered why the literature is filled to the brim with statistically significant results – often mutually contradictory – even though we know that you shouldn’t be able to find so many of them even if everyone’s theories were correct? A big part of the answer is that researchers enjoy a great degree of flexibility in terms of running their studies, analysing their data, and interpreting the findings, and – wittingly or unwittingly – use this flexibility to increase their chances of finding a significant result.\nAll of this, incidentally, leaves aside false findings due to errors in research design and data analysis, including common ones such as reading too much into the difference between significant and non-significant results, taking the results of an analysis that ‘statistically controls for’ confounding variables at face value, or carving up continuous variables into groups, as well as mistakes in reporting, and flat-out fraud.\nScientific thoroughness is a virtue, but such flexibility messes up statistical inferences in a big way, and the odds that a statistically significant finding represents a fluke sky-rockets. As this flexibility almost always remains undisclosed, perusers of the scholarly literature have no way of calibrating their expectations of what p &lt; 0.05 means in terms of providing support for a theory, and budding researchers may find themselves scratching their heads wondering why everyone seems to ‘achieve’ significance but they can’t.\nPreregistering your study – i.e., writing down, to the extent possible, what you’ll do in your study and how you’ll analyse the data – and then following through on these decisions won’t solve all of these problems. But it’ll help researchers and readers to distinguish more clearly between planned and post-hoc decisions, which will in turn allow them to calibrate their interpretation of the results more accurately."
  },
  {
    "objectID": "posts/2016-10-31-why-preregister/index.html#references",
    "href": "posts/2016-10-31-why-preregister/index.html#references",
    "title": "The Centre for Open Science’s Preregistration Challenge: Why it’s relevant and some recommended background reading",
    "section": "References",
    "text": "References\nBrown, Nicholas J. L. & James A. J. Heathers. 2016. The GRIM test: A simple technique detects numerous anomalies in the reporting of results in psychology. Social Psychological & Personality Science.\nDe Groot, Adriaan D. 2014. The meaning of “significance”” for different types of research. Translated and annotated by Eric-Jan Wagenmakers, Denny Borsboom, Josine Verhagen, Rogier Kievit, Marjan Bakker, Angelique Cramer, Dora Matzke, Don Mellenbergh, and Han L. J. van der Maas. Acta Psychologica 148. 188-194.\nElson, Malte. FlexibleMeasures.com.\nGelman, Andrew & Eric Loken. 2013. The garden of forking paths: Why multiple comparisons can be a problem, even when there is no “fishing expedition” or “\\(p\\)-hacking” and the research hypothesis was posited ahead of time..\nIoannidis, John P. A. 2005. Why most published research findings are false. PLOS Medicine 2. e124.\nJohn, Leslie K., George Loewenstein & Drazen Prelec. 2012. Measuring the prevalence of questionable research practices with incentives for truth telling. Psychological Science 23. 524-532.\nKerr, Norbert L. 1998. HARKing: Hypothesizing After the Results are Known. Personality and Social Psychology Review 2. 196-217.\nOpen Science Collaboration. 2015. Estimating the reproducibility of psychological science. Science 349.\nSedlmeier, Peter & Gerd Gigerenzer. 1989. Do studies of statistical power have an effect on the power of studies? Psychological Bulletin 105. 309-316.\nSimmons, Joseph P., Leif D. Nelson & Uri Simonsohn. 2011. False-positive psychology: Undisclosed flexibility in data collection and analysis allows presenting anything as significant. Psychological Science 22. 1359-1366.\nSterling, Theodore D. 1959. Publication decisions and their possible effects on inferences drawn from tests of significance–or vice versa. Journal of the American Statistical Association 54. 30-34.\nSterling, Theodore D., W. L. Rosenbaum & J. J. Weinkam. 1995. Publication decisions revisited: The effect of the outcome of statistical tests on the decision to publish and vice versa. The American Statistician 49. 108-112."
  },
  {
    "objectID": "posts/2014-09-26-balance-tests/index.html",
    "href": "posts/2014-09-26-balance-tests/index.html",
    "title": "Silly significance tests: Balance tests",
    "section": "",
    "text": "It’s something of a pet peeve of mine that your average research paper contains way too many statistical tests. I’m all in favour of reporting research and analyses meticulously, but it’d make our papers easier to read – and ultimately more impactful – if we were to cut down on silly, superfluous significance tests. Under scrutiny today: balance tests.\nThe term ‘silly significance test’ in the title comes from Abelson’s Statistics as principled argument and refers to tests that don’t contribute anything to a research report other than making it harder to read. I’d distinguish at least four kinds of ‘silly tests’ that we can largely do without – in this post, I focus on balance tests in randomised experiments."
  },
  {
    "objectID": "posts/2014-09-26-balance-tests/index.html#what-are-balance-tests",
    "href": "posts/2014-09-26-balance-tests/index.html#what-are-balance-tests",
    "title": "Silly significance tests: Balance tests",
    "section": "What are balance tests?",
    "text": "What are balance tests?\nBalance tests, also called randomisation checks, are a ubiquitous type of significance test. As an example of a balance test, consider a researcher who wants to compare a new vocabulary learning method to an established one. She randomly assigns forty participants to either the control group (established method) or the experimental group (new method). After four weeks, she tests the participants’ vocabulary knowledge – and let’s pretend she finds a significant difference in favour of the experimental group (e.g. t(38) = 2.7, p = 0.01).\nTo pre-empt criticisms that the difference between the two groups could be due to factors other than the learning method, the conscientious research also runs a t-test to verify that the control and experimental group don’t differ significantly in terms of age as well as a Χ²-test to check whether the proportion of men and women in approximately the same in both groups. The goal of these tests is to enable the researcher to argue that the randomisation gave rise to groups that are balanced with respect to these variables and that the observed difference between the two groups therefore can’t be due to these possible confounds. If a balance test comes out significant, the researcher could be tempted to run another analysis with the possible confound as a covariate."
  },
  {
    "objectID": "posts/2014-09-26-balance-tests/index.html#why-are-they-superfluous",
    "href": "posts/2014-09-26-balance-tests/index.html#why-are-they-superfluous",
    "title": "Silly significance tests: Balance tests",
    "section": "Why are they superfluous?",
    "text": "Why are they superfluous?\nReasonable though this strategy may appear to be, balance tests are problematic in several respects. The following list is based on a paper by Mutz and Pemantle (2013). This discussion gives you the short version; for a somewhat more detailed treatment of balanced tests geared towards applied linguists, see the first section of my (as yet unpublished) paper on analysing randomised controlled interventions.\nUpdate (2023-08-26): The Mutz and Pemantle paper has now been published as Mutz, Pemantle and Pham (2019). My own paper has now been published as Vanhove (2015).\n\nBalance tests are uninformative…\nStatistical tests are used to draw inferences about a population rather than about a specific sample. Sure, it’s possible to end up with 3 men in the experimental group and 14 in the control group; a Χ²-test would then produce a significance result. But would we seriously go on to argue that men are more likely to end up in the control group than in the experimental group? Of course not! We know men were equally likely to end up in the control or experimental group because we randomly assigned all participants to the conditions – we know the null hypothesis (no difference between the groups) is true with respect to this variable. Consequently, every significant balance test is a false alarm that has come about due to sheer randomness. A balance test can’t tell us anything we don’t know already.\n\n\n… as well as unnecessary…\nResearchers that concede the first point may go on to argue that their use of balance tests isn’t intended to make inferences about populations, but to give an idea about the magnitude of the unbalancedness between the groups. However, perfectly balanced groups are not a prerequisite for making valid statistical inferences. Thus, balance tests are also unnecessary.\nTo elaborate on this point, randomisation guarantees that all possible confounds – both the ones we had and those we hadn’t thought of – are balanced out on average, i.e. in the infamous long run. A given randomisation may give rise to an experimental group that is older or that has more women, or has a higher proportion of MasterChef fans – and, yes, unbalancedness with respect to such variables could conceivably give rise to a larger or smaller observed treatment effect in any given study. But the distribution of such fluke findings follows the laws of probability. The p-value for the main analysis already takes such flukes into account and doesn’t need to be ‘corrected’ for an unbalanced distribution of possible confounding variables.\n\n\n… and they invalidate significance tests.\nSince p-values have their precise meaning when no balance test is carried out, it follows that they don’t have their precise meaning when a balanced test is carried out. p-values are conditional probability statements (‘the probability of observing data at least this extreme if the null hypothesis were true’), but by using balance tests, we add a condition to this statement: ‘the probability of observing data at least this extreme if the null hypothesis were true and if the balance test yields a particular result’. This doesn’t seem like much, but it is a form of data-dependent analysis, which invalidates significance tests. (For a more general discussion of data-dependent analyses, see Gelman & Loken 2013.)\nA demonstration of this point seems in order. I ran a small simulation (R code below) of the following scenario. We want to test an experimental effect and randomly assign 40 participants to an experimental or a control condition. The participants vary in age between 20 and 40. The age variable doesn’t interest us so much, but it is linearly related to the outcome variable. However, the treatment effect is 0, i.e. the null hypothesis is true. Our analytical strategy is as follows: We run a significance test on the age variable to see whether the experimental and control groups are ‘balanced’ in terms of age. If this test comes back non-significant, we conclude that we have balanced groups and run a t-test on the outcome variable; if the test comes back significant, we run an ANCOVA with age as a covariate. I simulated this scenario 10,000 times and compared the distribution of the p-values for the treatment effect resulting from this ‘conditional’ analytical strategy with those provided by t-tests and ANCOVAs that were run regardless of the outcome of the balance test. The histograms below show the distributions of p-values for these three testing strategies.\n\n\n\n\n\nSince the null hypothesis is true in this case, the distribution of p-values should be uniform, i.e. all bars should be roughly equally as high. This is the case in the left and middle histograms, showing that p-values are correctly distributed when the analysis is not affected by balance tests. Put simply, p-values have their intended meaning in these cases. The right histogram shows that low p-values are too rare when the analysis is affected by balance tests: the test of the treatment effect is too conservative, i.e. its p-value doesn’t have its intended meaning.\nRecent blogs and articles have highlighted the fact that data-dependent analysis yields anti-conservative p-values, i.e. that it is too likely to observe a significant effect where none exists (e.g. Gelman & Loken 2013 and Simmons et al. 2011). It may therefore seem strange to highlight that data-dependent analysis can produce overconservative results, too. My main point, however, is that balance tests produce inaccurate results that can easily be avoided – regardless of the direction of the error. That said, overconservatism has a practical downside as well, namely lower power: it’s less likely to observe a statistically significant effect when the effect does in fact exist. The following histograms show the p-value distribution when there is a relatively small effect (see also R code below).\n\n\n\n\n\nClearly, the ANCOVA-only strategy wins hands-down in terms of power, whereas the balance test strategy doesn’t even compare favourably to the t-test-only approach."
  },
  {
    "objectID": "posts/2014-09-26-balance-tests/index.html#solutions",
    "href": "posts/2014-09-26-balance-tests/index.html#solutions",
    "title": "Silly significance tests: Balance tests",
    "section": "Solutions",
    "text": "Solutions\nThe solution to simple: just don’t use balance tests. They clutter up the research report and don’t have any obvious advantages when analysing randomised experiments. When there are good reasons to assume that a covariate affects the results, it’s a good idea to include it in the main analysis regardless of whether the experimental and control groups are balanced with respect to this variable. In fact, Mutz and Pemantle (2013) show that including a covariate in the analysis is slightly more effective when the groups are in fact balanced. (Update (2023-08-26): See Mutz et al. (2019) for the published paper.) While this post is strictly on randomised experiments, I would think that this is also the most sensible strategy when analysing non-randomised quasi-experiments.\nAlternatively, it can make sense to consider the covariate in the design of the study, i.e. before randomising (see the part in my analysis paper on ‘blocking’, pp. 6-7). (Update (2023-08-26): See Vanhove (2015) for the published paper.)"
  },
  {
    "objectID": "posts/2014-09-26-balance-tests/index.html#r-code",
    "href": "posts/2014-09-26-balance-tests/index.html#r-code",
    "title": "Silly significance tests: Balance tests",
    "section": "R code",
    "text": "R code\n\nsimulate.balance &lt;- function(effect = 0, covar.effect = 0) {\n  age &lt;- seq(from = 20, to = 40, length.out = 40)\n  condition &lt;- sample(c(rep(0, 20), rep(1, 20)), replace = FALSE)\n  var &lt;- covar.effect*age + effect*condition + rnorm(40, sd=2)\n  \n  # t-test regardless of balancedness\n  test1 &lt;- t.test(var ~ factor(condition), var.equal = TRUE)$p.value\n  \n  # balance test\n  check &lt;- t.test(age ~ factor(condition), var.equal = TRUE)$p.value\n  # conditional analysis\n  if (check &lt;= 0.05) {\n    test2 &lt;- anova(lm(var ~ age + factor(condition)))$'Pr(&gt;F)'[2]\n  } else {\n    test2 &lt;- test1\n  }\n  \n  # ancova regardless of balancedness\n  test3 &lt;- anova(lm(var ~ age + factor(condition)))$'Pr(&gt;F)'[2]\n  \n  return(list(test1 = test1,\n              test2 = test2,\n              test3 = test3,\n              check = check))\n}\nsims &lt;- replicate(10000, simulate.balance(effect = 0, covar.effect = 1))\n\nSettings for histograms above: effect = 0 and effect = 1."
  },
  {
    "objectID": "posts/2014-09-26-balance-tests/index.html#software-versions",
    "href": "posts/2014-09-26-balance-tests/index.html#software-versions",
    "title": "Silly significance tests: Balance tests",
    "section": "Software versions",
    "text": "Software versions\n\ndevtools::session_info()\n\n─ Session info ───────────────────────────────────────────────────────────────\n setting  value\n version  R version 4.3.1 (2023-06-16)\n os       Ubuntu 22.04.3 LTS\n system   x86_64, linux-gnu\n ui       X11\n language en_US\n collate  en_US.UTF-8\n ctype    en_US.UTF-8\n tz       Europe/Zurich\n date     2023-08-26\n pandoc   3.1.1 @ /usr/lib/rstudio/resources/app/bin/quarto/bin/tools/ (via rmarkdown)\n\n─ Packages ───────────────────────────────────────────────────────────────────\n package     * version date (UTC) lib source\n cachem        1.0.6   2021-08-19 [2] CRAN (R 4.2.0)\n callr         3.7.3   2022-11-02 [1] CRAN (R 4.3.1)\n cli           3.6.1   2023-03-23 [1] CRAN (R 4.3.0)\n codetools     0.2-19  2023-02-01 [4] CRAN (R 4.2.2)\n crayon        1.5.2   2022-09-29 [1] CRAN (R 4.3.1)\n devtools      2.4.5   2022-10-11 [1] CRAN (R 4.3.1)\n digest        0.6.29  2021-12-01 [2] CRAN (R 4.2.0)\n ellipsis      0.3.2   2021-04-29 [2] CRAN (R 4.2.0)\n evaluate      0.15    2022-02-18 [2] CRAN (R 4.2.0)\n fastmap       1.1.0   2021-01-25 [2] CRAN (R 4.2.0)\n fs            1.5.2   2021-12-08 [2] CRAN (R 4.2.0)\n glue          1.6.2   2022-02-24 [2] CRAN (R 4.2.0)\n htmltools     0.5.5   2023-03-23 [1] CRAN (R 4.3.0)\n htmlwidgets   1.6.2   2023-03-17 [1] CRAN (R 4.3.1)\n httpuv        1.6.11  2023-05-11 [1] CRAN (R 4.3.1)\n jsonlite      1.8.7   2023-06-29 [1] CRAN (R 4.3.1)\n knitr         1.39    2022-04-26 [2] CRAN (R 4.2.0)\n later         1.3.1   2023-05-02 [1] CRAN (R 4.3.1)\n lifecycle     1.0.3   2022-10-07 [1] CRAN (R 4.3.0)\n magrittr      2.0.3   2022-03-30 [1] CRAN (R 4.3.0)\n memoise       2.0.1   2021-11-26 [2] CRAN (R 4.2.0)\n mime          0.10    2021-02-13 [2] CRAN (R 4.0.2)\n miniUI        0.1.1.1 2018-05-18 [1] CRAN (R 4.3.1)\n pkgbuild      1.4.2   2023-06-26 [1] CRAN (R 4.3.1)\n pkgload       1.3.2.1 2023-07-08 [1] CRAN (R 4.3.1)\n prettyunits   1.1.1   2020-01-24 [2] CRAN (R 4.2.0)\n processx      3.8.2   2023-06-30 [1] CRAN (R 4.3.1)\n profvis       0.3.8   2023-05-02 [1] CRAN (R 4.3.1)\n promises      1.2.0.1 2021-02-11 [1] CRAN (R 4.3.1)\n ps            1.7.5   2023-04-18 [1] CRAN (R 4.3.1)\n purrr         1.0.1   2023-01-10 [1] CRAN (R 4.3.0)\n R6            2.5.1   2021-08-19 [2] CRAN (R 4.2.0)\n Rcpp          1.0.11  2023-07-06 [1] CRAN (R 4.3.1)\n remotes       2.4.2   2021-11-30 [2] CRAN (R 4.2.0)\n rlang         1.1.1   2023-04-28 [1] CRAN (R 4.3.0)\n rmarkdown     2.21    2023-03-26 [1] CRAN (R 4.3.0)\n rstudioapi    0.14    2022-08-22 [1] CRAN (R 4.3.0)\n sessioninfo   1.2.2   2021-12-06 [2] CRAN (R 4.2.0)\n shiny         1.7.4.1 2023-07-06 [1] CRAN (R 4.3.1)\n stringi       1.7.12  2023-01-11 [1] CRAN (R 4.3.1)\n stringr       1.5.0   2022-12-02 [1] CRAN (R 4.3.0)\n urlchecker    1.0.1   2021-11-30 [1] CRAN (R 4.3.1)\n usethis       2.2.2   2023-07-06 [1] CRAN (R 4.3.1)\n vctrs         0.6.3   2023-06-14 [1] CRAN (R 4.3.0)\n xfun          0.39    2023-04-20 [1] CRAN (R 4.3.0)\n xtable        1.8-4   2019-04-21 [1] CRAN (R 4.3.1)\n yaml          2.3.5   2022-02-21 [2] CRAN (R 4.2.0)\n\n [1] /home/jan/R/x86_64-pc-linux-gnu-library/4.3\n [2] /usr/local/lib/R/site-library\n [3] /usr/lib/R/site-library\n [4] /usr/lib/R/library\n\n──────────────────────────────────────────────────────────────────────────────"
  },
  {
    "objectID": "posts/2018-04-25-graphical-model-checking/index.html",
    "href": "posts/2018-04-25-graphical-model-checking/index.html",
    "title": "Checking model assumptions without getting paranoid",
    "section": "",
    "text": "Statistical models come with a set of assumptions, and violations of these assumptions can render irrelevant or even invalid the inferences drawn from these models. It is important, then, to verify that your model’s assumptions are at least approximately tenable for your data. To this end, statisticians commonly recommend that you check the distribution of your model’s residuals (i.e., the difference between your actual data and the model’s fitted values) graphically. An excellent piece of advice that, unfortunately, causes some students to become paranoid and see violated assumptions everywhere they look. This blog post is for them.\nIn the following, I’ll outline the assumptions of the simple linear regression and how they can be checked visually without succumbing to paranoia. t-tests, ANOVA and ANCOVA all make essentially the same assumptions. These assumptions can be checked in the same way as illustrated below, but doing so requires that they are rewritten as regression models. For instance, if you want to test whether the mean of a variable called \\(Y\\) varies between group encoded in variable \\(X\\) using a t-test, you’d ordinarily do this in R using the following syntax:\nt.test(Y ~ X, data = dat, var.equal = TRUE)\nBut you’ll get the same results if you rewrite this as a linear model:\nlm(Y ~ X, data = dat)\nTo make use of the techniques below, you need to use the second notation."
  },
  {
    "objectID": "posts/2018-04-25-graphical-model-checking/index.html#assumptions-of-the-simple-linear-regression-model",
    "href": "posts/2018-04-25-graphical-model-checking/index.html#assumptions-of-the-simple-linear-regression-model",
    "title": "Checking model assumptions without getting paranoid",
    "section": "Assumptions of the simple linear regression model",
    "text": "Assumptions of the simple linear regression model\nA simple linear regression model relates one continuous predictor to a continuous outcome in terms of a straight line and some random scatter about that line. Its key assumptions are the following:\n\nA straight line adequately captures the trend in the data. The model will still gladly estimate the form of the linear relationship in the data even if this relationship isn’t approximately linear, but its estimates won’t be as relevant.\nThe residuals are independent of one another. While this extremely important assumption can sometimes be checked, in most cases you’ll need to know how the data were collected to know whether it is tenable. For this reason, I won’t go into this assumption.\nThe amount of scatter of the residuals doesn’t vary with the predictor or (equivalently) with the fitted values for the outcome.\nIf you compute p-values or construct confidence intervals based on t-distributions, you tacitly assume that the residuals are drawn from a normal distribution. This assumption isn’t that important inasmuch as the Central Limit Theorem tends to kick in for reasonably-sized samples. Perhaps more importantly in such cases, the regression line captures the conditional means of \\(y\\) given \\(x\\) (e.g., the estimated mean for the outcome when the predictor value is 15). But if the distribution of the residuals is bimodal, skewed or otherwise wonky, the mean may not be such a relevant summary of the distribution lying behind it."
  },
  {
    "objectID": "posts/2018-04-25-graphical-model-checking/index.html#assumption-checks-and-sampling-error",
    "href": "posts/2018-04-25-graphical-model-checking/index.html#assumption-checks-and-sampling-error",
    "title": "Checking model assumptions without getting paranoid",
    "section": "Assumption checks and sampling error",
    "text": "Assumption checks and sampling error\nWhen checking model assumptions, it’s important to keep sampling error in mind. Even if the residuals were actually drawn from a normal distribution, their distribution needn’t look normal in any given sample, as the figure below shows. Similarly, the residuals in a given sample may seem like they don’t have constant variance even if they actually do in the population, and a relationship may seem somewhat nonlinear in a sample even if it’s perfectly linear in the population at large.\n\n\n\n\n\nBut what’s important isn’t whether the normality, constant variance and linearity exist in the sample, but whether they exist in the population at large. Now it’s pretty difficult to tell conclusively whether this is indeed the case based on a smallish sample. But what you can do is check whether you could plausibly find the same degree of nonlinearity, non-constant variance and nonnormality that you observe in your sample even if they actually don’t occur in the population. Buja et al. (2009) and Majumder et al. (2013) introduce a visual inferencing procedure that allows you to judge whether the residual patterns in your sample are cause for concern or whether they are in fact to be expected even if everything were peachy. Below I illustrate this procedure (also see an earlier blog post for another example of visual inferencing: A purely graphical explanation of p-values)."
  },
  {
    "objectID": "posts/2018-04-25-graphical-model-checking/index.html#an-example",
    "href": "posts/2018-04-25-graphical-model-checking/index.html#an-example",
    "title": "Checking model assumptions without getting paranoid",
    "section": "An example",
    "text": "An example\nI’m going to use an old dataset of mine to fit a simple linear regression model between an outcome (TotCorrect, which in principle can take integer values between 0 and 45) and a predictor (WST, which takes integer values between 0 and 42) using R. The data can be read in directly from my webspace.\n\n# Read in data\ndat &lt;- read.csv(\"http://homeweb.unifr.ch/VanhoveJ/Pub/Data/VanhoveBerthele2015.csv\",\n                na.strings = \"-9999\")\n\n# Only retain complete cases\ndat &lt;- dat[complete.cases(dat), ]\n\n# Fit regression model\nwst.lm &lt;- lm(TotCorrect ~ WST, data = dat)\n\nIf you want to inspect the estimated coefficients and their standard errors, run the command summary(wst.lm). Here I’ll skip right to the assumption checks.\n\nChecking for non-linearity\nI fitted the relationship between WST and TotCorrect linearly. This assumption is typically checked by inspecting whether the model’s residuals still show some trend when plotted against the model’s fitted values. If the relationship is linear, there shouldn’t be no such trend. However, due to random variation, you’re bound to find some pattern in the residuals if you squint hard enough. To help analysts evaluate whether such patterns are compatible with a truly linear relationship tainted by some random variation, Buja et al. (2009) and Majumder et al. (2013) suggest the following procedure:\n\n\nFit your model.\n\n\nSimulate new outcome data from your model. In our example, the estimated intercept is about 7.83 and the estimated slope parameter for the WST variable is about 0.29.\n\n# Estimated coefficients\ncoef(wst.lm)\n\n(Intercept)         WST \n  7.8321310   0.2860069 \n\n\nFor a participant with a WST value of 13, the fitted TotCorrect value is consequently \\(7.83 + 0.29 \\times 13 = 11.6\\). To simulate a new datum for such a participant, this fitted value (\\(11.6\\)) is combined with a random number drawn from a normal distribution whose mean is 0 and whose standard deviation equals the model estimate for the standard deviation of the residuals – in our case about 4.81.\n\n# Estimated standard deviation of residuals\nsigma(wst.lm)\n\n[1] 4.814969\n\n\nSuch new outcome data are simulated a couple of times for each observation in the dataset. (This can be achieved quickly using the simulate() function.)\nImportantly, the new outcome data were simulated from a regression model that assumes that the relationship between the predictor and the outcome is linear, that the residuals are randomly sampled from one and the same normal distribution and so have constant variance. In a word, all assumptions we want to test are literally true in the simulated data.\n\n\nNow refit your model to the simulated datasets and extract the residuals from each model.\n\n\nDraw diagnostic plots for both your actual model and the models fitted on the simulated datasets (for which the model’s assumptions are literally true) and jumble up the different plots.\n\n\nIf you can’t distinguish the diagnostic plot based on your actual data from the ones based on the simulated datasets, it doesn’t suggest that your model violates the assumption that you were testing.\n\n\nWe’ll carry out these steps in R, of course; the nullabor package is helpful for this.\nThe visual inferencing procedure is based on randomly generated data so if you want to obtain the same results as in this blog post, be sure to run the set.seed() command below.\n\n# Set seed for generating random numbers\nset.seed(2018-04-25)\n\n# Load the tidyverse packages;\n#  use install.packages(\"tidyverse\")\n#  to install if needed.\nlibrary(tidyverse)\n\n# Load the nullabor package (for visual inferencing);\n#  use install.packages(\"nullabor\")\n#  to install if needed.\nlibrary(nullabor)\n\nThen construct a data frame that contains the real data and the real model’s residuals and fitted values. Importantly, the column with the residuals should be called .resid and the one with the fitted values .fitted, otherwise the next steps won’t work.\n\nreal_data &lt;- data.frame(dat,\n                        .resid = residuals(wst.lm),\n                        .fitted = fitted(wst.lm))\n\nNow create a new object (here parade) that combines the newly created real_data with (by default) 19 similar but simulated datasets based on the same model. This is accomplished using nullabor’s convenience function null_lm() (for generating the simulated datasets) and the lineup() function (for combining the real and simulated datasets and shuffling their positions).\n\nparade &lt;- lineup(null_lm(TotCorrect ~ WST), real_data)\n\ndecrypt(\"sFZM ldbd 32 PN43b3N2 ux\")\n\n\nWe’ll come back to this “decrypt” message in a minute. First let’s plot the relevant diagnostic plots to check for remaining trends in the residuals:\n\nggplot(parade,\n       aes(x = .fitted,             # plot residuals against\n           y = .resid)) +           #  fitted values\n  geom_point(shape = 1) +           # plot values as points\n  geom_smooth(formula = \"y ~ x\") +  # add a scatterplot smoother (optional)\n  facet_wrap(~ .sample)             # separate panels for each dataset\n\n`geom_smooth()` using method = 'loess'\n\n\n\n\n\nIn your view, which plot shows the strongest residual trend?\nAfter you’ve made up your mind, copy-paste decrypt(\"sFZM ldbd 32 PN43b3N2 ux\") (the output of the lineup() command above) to the R prompt. If this confirms your answer, then perhaps you ought to consider reading up on models that can take into account nonlinear patterns. But if you thought another plot was more conspicuous, then that shows that randomness alone can plausibly account for any departures from linearity in your actual data.\n(In case you already knew beforehand what the diagnostic plot for your model looks like, just mail this type of lineup to a couple of colleagues of yours and ask them to identify the most conspicuous pattern.)\n\n\nChecking for non-constant variance\nTo check for non-constant variance (heteroskedasticity), the idea is exactly the same as before. The only thing that changes is the diagnostic plot. We’ll create a new lineup with new simulated datasets, though.\n\nparade &lt;- lineup(null_lm(TotCorrect ~ WST), real_data)\n\ndecrypt(\"sFZM ldbd 32 PN43b3N2 5J\")\n\n\nTo check for heteroskedasticity, statisticians typically plot the square roots of the absolute residuals against the fitted values against the fitted data. If the variance around the regression line is constant, then you’d expect a horizontal trend line in this type of plot.\n\nggplot(parade,\n       aes(x = .fitted, \n           y = sqrt(abs(.resid)))) +\n  geom_point(shape = 1) +\n  geom_smooth(formula = \"y ~ x\") +\n  facet_wrap(~ .sample)\n\n`geom_smooth()` using method = 'loess'\n\n\n\n\n\nAgain, ask yourself which plot looks most conspicuously heteroskedastic? Then check your answer with decrypt(). If you guessed right, then perhaps you need to consider brushing up on methods that can take into account heteroskedasticity, but otherwise, you can conclude the residual plot doesn’t give you much reason to doubt that the residuals are homoskedastic.\n\n\nChecking for non-normality\nThe same procedure as every year, James. A useful diagnostic plot for checking for non-normal residuals is the quantile-quantile plot. If the residuals are distributed normally, the points in such plots fall on a diagonal line.\n\nparade &lt;- lineup(null_lm(TotCorrect ~ WST), real_data)\n\ndecrypt(\"sFZM ldbd 32 PN43b3N2 5x\")\n\nggplot(parade,\n       aes(sample = TotCorrect)) +\n  stat_qq() +\n  facet_wrap(~ .sample)\n\n\n\n\nAgain, decide which pattern is most conspicuous and check your answer. If the normality assumption is clearly violated, you need to ask yourself whether the degree of normality is such that it would affect your inferences. If you want to play it safe, you can try computing p-values and constructing confidence intervals using a bootstrap procedure that doesn’t assume normality, for instance.\n\n\nImpossible data\nThe technique illustrated above is also useful to make it clearer for yourself what you know about your data but your model doesn’t. Let’s generate a new lineup:\n\nparade &lt;- lineup(null_lm(TotCorrect ~ WST), real_data)\n\ndecrypt(\"sFZM ldbd 32 PN43b3N2 5O\")\n\n\nNow draw scatterplots as you would:\n\nggplot(parade,\n       aes(x = WST, \n           y = TotCorrect)) +\n  geom_point(shape = 1) +\n  geom_smooth(formula = \"y ~ x\") +\n  facet_wrap(~ .sample)\n\n`geom_smooth()` using method = 'loess'\n\n\n\n\n\nTake a closer look at each of these plots. Do you find patterns or data points that are just impossible given what you know about how the data were collected?\nSeveral of these plots contain TotCorrect values below 0, but such values are impossible. I’m not saying categorically that this is a problem, but it does show that the model doesn’t ‘know’ everything about the problem at hand that I do know.\nLet’s try this once more. Instead of drawing scatterplots, I visualise the distribution of the outcome variable using empirical cumulative density plots.\n\nparade &lt;- lineup(null_lm(TotCorrect ~ WST), real_data)\n\ndecrypt(\"sFZM ldbd 32 PN43b3N2 5t\")\n\nggplot(parade,\n       aes(TotCorrect)) +\n  stat_ecdf(geom = \"step\") +\n  geom_vline(xintercept = 0, linetype = 2) +\n  facet_wrap(~ .sample)\n\n\n\n\nWhat you’ll notice here is that there’s one empirical cumulative density plot that’s considerably more step-like than the others. This is the plot for our actual data and it reflects the fact that our actual outcome variable is measured in integers (0, 1, 2, …) rather than decimals. This is something I know, but clearly the model doesn’t and so it happily generates non-integer values for the outcome. This doesn’t dramatically affect the inferences from the model, but it still goes to show that fairly run-of-the-mill models such as these don’t encode everything the analyst knows about the problem at hand."
  },
  {
    "objectID": "posts/2018-04-25-graphical-model-checking/index.html#an-important-caveat",
    "href": "posts/2018-04-25-graphical-model-checking/index.html#an-important-caveat",
    "title": "Checking model assumptions without getting paranoid",
    "section": "An important caveat",
    "text": "An important caveat\nSaying that a diagnostic plot is still compatible with the model’s assumptions doesn’t mean that the diagnostic plot confirms that the model’s assumptions are true. This is particulary important if you’re dealing with small samples: Assumptions such as normality are more important yet harder to verify in small samples."
  },
  {
    "objectID": "posts/2018-04-25-graphical-model-checking/index.html#conclusion",
    "href": "posts/2018-04-25-graphical-model-checking/index.html#conclusion",
    "title": "Checking model assumptions without getting paranoid",
    "section": "Conclusion",
    "text": "Conclusion\nIf your diagnostic plots suggest departures from linearity, homoskedasticity and normality, don’t panic: Such deviations may just be due to randomness. Draw a lineup of plots with both simulated and real data, and see if you can guess which plot show the real data: If you can’t, your diagnostic plot is in fact compatible with the model’s assumptions. If you don’t trust your own judgement, just ask a colleague to identify the most different plot in the lineup."
  },
  {
    "objectID": "posts/2018-04-25-graphical-model-checking/index.html#references",
    "href": "posts/2018-04-25-graphical-model-checking/index.html#references",
    "title": "Checking model assumptions without getting paranoid",
    "section": "References",
    "text": "References\nBuja, Andreas, Dianne Cook, Heike Hofmann, Michael Lawrence, Eun-Kyung Lee, Deborah F. Swayne & Hadley Wickham. 2009. Statistical inference for exploratory data analysis and model diagnostics. Philosophical Transactions of the Royal Society A 367(1906). 4361-4383. doi:10.1098/rsta.2009.0120\nMajumder, Mahbubul, Heike Hofmann & Dianne Cook. 2013. Validation of visual statistical inference, applied to linear models. Journal of the American Statistical Association 108(503). 942-956. doi:10.1080/01621459.2013.808157"
  },
  {
    "objectID": "posts/2018-04-25-graphical-model-checking/index.html#software-versions",
    "href": "posts/2018-04-25-graphical-model-checking/index.html#software-versions",
    "title": "Checking model assumptions without getting paranoid",
    "section": "Software versions",
    "text": "Software versions\nPlease note that I reran the code on this page on August 6, 2023.\n\ndevtools::session_info()\n\n─ Session info ───────────────────────────────────────────────────────────────\n setting  value\n version  R version 4.3.1 (2023-06-16)\n os       Ubuntu 22.04.2 LTS\n system   x86_64, linux-gnu\n ui       X11\n language en_US\n collate  en_US.UTF-8\n ctype    en_US.UTF-8\n tz       Europe/Zurich\n date     2023-08-06\n pandoc   3.1.1 @ /usr/lib/rstudio/resources/app/bin/quarto/bin/tools/ (via rmarkdown)\n\n─ Packages ───────────────────────────────────────────────────────────────────\n package     * version date (UTC) lib source\n cachem        1.0.6   2021-08-19 [2] CRAN (R 4.2.0)\n callr         3.7.3   2022-11-02 [1] CRAN (R 4.3.1)\n class         7.3-22  2023-05-03 [4] CRAN (R 4.3.1)\n cli           3.6.1   2023-03-23 [1] CRAN (R 4.3.0)\n cluster       2.1.4   2022-08-22 [4] CRAN (R 4.2.1)\n colorspace    2.1-0   2023-01-23 [1] CRAN (R 4.3.0)\n crayon        1.5.2   2022-09-29 [1] CRAN (R 4.3.1)\n DEoptimR      1.1-0   2023-07-12 [1] CRAN (R 4.3.1)\n devtools      2.4.5   2022-10-11 [1] CRAN (R 4.3.1)\n digest        0.6.29  2021-12-01 [2] CRAN (R 4.2.0)\n diptest       0.76-0  2021-05-04 [1] CRAN (R 4.3.1)\n dplyr       * 1.1.2   2023-04-20 [1] CRAN (R 4.3.0)\n ellipsis      0.3.2   2021-04-29 [2] CRAN (R 4.2.0)\n evaluate      0.15    2022-02-18 [2] CRAN (R 4.2.0)\n fansi         1.0.4   2023-01-22 [1] CRAN (R 4.3.1)\n farver        2.1.1   2022-07-06 [1] CRAN (R 4.3.0)\n fastmap       1.1.0   2021-01-25 [2] CRAN (R 4.2.0)\n flexmix       2.3-19  2023-03-16 [1] CRAN (R 4.3.1)\n forcats     * 1.0.0   2023-01-29 [1] CRAN (R 4.3.0)\n fpc           2.2-10  2023-01-07 [1] CRAN (R 4.3.1)\n fs            1.5.2   2021-12-08 [2] CRAN (R 4.2.0)\n generics      0.1.3   2022-07-05 [1] CRAN (R 4.3.0)\n ggplot2     * 3.4.2   2023-04-03 [1] CRAN (R 4.3.0)\n glue          1.6.2   2022-02-24 [2] CRAN (R 4.2.0)\n gtable        0.3.3   2023-03-21 [1] CRAN (R 4.3.0)\n hms           1.1.3   2023-03-21 [1] CRAN (R 4.3.0)\n htmltools     0.5.5   2023-03-23 [1] CRAN (R 4.3.0)\n htmlwidgets   1.6.2   2023-03-17 [1] CRAN (R 4.3.1)\n httpuv        1.6.11  2023-05-11 [1] CRAN (R 4.3.1)\n jsonlite      1.8.7   2023-06-29 [1] CRAN (R 4.3.1)\n kernlab       0.9-32  2023-01-31 [1] CRAN (R 4.3.1)\n knitr         1.39    2022-04-26 [2] CRAN (R 4.2.0)\n labeling      0.4.2   2020-10-20 [1] CRAN (R 4.3.0)\n later         1.3.1   2023-05-02 [1] CRAN (R 4.3.1)\n lattice       0.21-8  2023-04-05 [4] CRAN (R 4.3.0)\n lifecycle     1.0.3   2022-10-07 [1] CRAN (R 4.3.0)\n lubridate   * 1.9.2   2023-02-10 [1] CRAN (R 4.3.0)\n magrittr      2.0.3   2022-03-30 [1] CRAN (R 4.3.0)\n MASS          7.3-60  2023-05-04 [4] CRAN (R 4.3.1)\n Matrix        1.6-0   2023-07-08 [4] CRAN (R 4.3.1)\n mclust        6.0.0   2022-10-31 [1] CRAN (R 4.3.1)\n memoise       2.0.1   2021-11-26 [2] CRAN (R 4.2.0)\n mgcv          1.9-0   2023-07-11 [4] CRAN (R 4.3.1)\n mime          0.10    2021-02-13 [2] CRAN (R 4.0.2)\n miniUI        0.1.1.1 2018-05-18 [1] CRAN (R 4.3.1)\n modeltools    0.2-23  2020-03-05 [1] CRAN (R 4.3.1)\n moments       0.14.1  2022-05-02 [1] CRAN (R 4.3.1)\n munsell       0.5.0   2018-06-12 [1] CRAN (R 4.3.0)\n nlme          3.1-162 2023-01-31 [4] CRAN (R 4.2.2)\n nnet          7.3-19  2023-05-03 [4] CRAN (R 4.3.1)\n nullabor    * 0.3.9   2020-02-25 [1] CRAN (R 4.3.1)\n pillar        1.9.0   2023-03-22 [1] CRAN (R 4.3.0)\n pkgbuild      1.4.2   2023-06-26 [1] CRAN (R 4.3.1)\n pkgconfig     2.0.3   2019-09-22 [2] CRAN (R 4.2.0)\n pkgload       1.3.2.1 2023-07-08 [1] CRAN (R 4.3.1)\n prabclus      2.3-2   2020-01-08 [1] CRAN (R 4.3.1)\n prettyunits   1.1.1   2020-01-24 [2] CRAN (R 4.2.0)\n processx      3.8.2   2023-06-30 [1] CRAN (R 4.3.1)\n profvis       0.3.8   2023-05-02 [1] CRAN (R 4.3.1)\n promises      1.2.0.1 2021-02-11 [1] CRAN (R 4.3.1)\n ps            1.7.5   2023-04-18 [1] CRAN (R 4.3.1)\n purrr       * 1.0.1   2023-01-10 [1] CRAN (R 4.3.0)\n R6            2.5.1   2021-08-19 [2] CRAN (R 4.2.0)\n Rcpp          1.0.11  2023-07-06 [1] CRAN (R 4.3.1)\n readr       * 2.1.4   2023-02-10 [1] CRAN (R 4.3.0)\n remotes       2.4.2   2021-11-30 [2] CRAN (R 4.2.0)\n rlang         1.1.1   2023-04-28 [1] CRAN (R 4.3.0)\n rmarkdown     2.21    2023-03-26 [1] CRAN (R 4.3.0)\n robustbase    0.99-0  2023-06-16 [1] CRAN (R 4.3.1)\n rstudioapi    0.14    2022-08-22 [1] CRAN (R 4.3.0)\n scales        1.2.1   2022-08-20 [1] CRAN (R 4.3.0)\n sessioninfo   1.2.2   2021-12-06 [2] CRAN (R 4.2.0)\n shiny         1.7.4.1 2023-07-06 [1] CRAN (R 4.3.1)\n stringi       1.7.12  2023-01-11 [1] CRAN (R 4.3.1)\n stringr     * 1.5.0   2022-12-02 [1] CRAN (R 4.3.0)\n tibble      * 3.2.1   2023-03-20 [1] CRAN (R 4.3.0)\n tidyr       * 1.3.0   2023-01-24 [1] CRAN (R 4.3.0)\n tidyselect    1.2.0   2022-10-10 [1] CRAN (R 4.3.0)\n tidyverse   * 2.0.0   2023-02-22 [1] CRAN (R 4.3.1)\n timechange    0.2.0   2023-01-11 [1] CRAN (R 4.3.0)\n tzdb          0.4.0   2023-05-12 [1] CRAN (R 4.3.0)\n urlchecker    1.0.1   2021-11-30 [1] CRAN (R 4.3.1)\n usethis       2.2.2   2023-07-06 [1] CRAN (R 4.3.1)\n utf8          1.2.3   2023-01-31 [1] CRAN (R 4.3.1)\n vctrs         0.6.3   2023-06-14 [1] CRAN (R 4.3.0)\n withr         2.5.0   2022-03-03 [2] CRAN (R 4.2.0)\n xfun          0.39    2023-04-20 [1] CRAN (R 4.3.0)\n xtable        1.8-4   2019-04-21 [1] CRAN (R 4.3.1)\n yaml          2.3.5   2022-02-21 [2] CRAN (R 4.2.0)\n\n [1] /home/jan/R/x86_64-pc-linux-gnu-library/4.3\n [2] /usr/local/lib/R/site-library\n [3] /usr/lib/R/site-library\n [4] /usr/lib/R/library\n\n──────────────────────────────────────────────────────────────────────────────"
  },
  {
    "objectID": "posts/2017-06-26-continuous-interactions/index.html",
    "href": "posts/2017-06-26-continuous-interactions/index.html",
    "title": "Fitting interactions between continuous variables",
    "section": "",
    "text": "Splitting up continuous variables is generally a bad idea. In terms of statistical efficiency, the popular practice of dichotomising continuous variables at their median is comparable to throwing out a third of the dataset. Moreover, statistical models based on split-up continuous variables are prone to misinterpretation: threshold effects are easily read into the results when, in fact, none exist. Splitting up, or ‘binning’, continuous variables, then, is something to avoid. But what if you’re interested in how the effect of one continuous predictor varies according to the value of another continuous predictor? In other words, what if you’re interested in the interaction between two continuous predictors? Binning one of the predictors seems appealing since it makes the model easier to interpret. However, as I’ll show in this blog post, it’s fairly straightforward to fit and interpret interactions between continuous predictors."
  },
  {
    "objectID": "posts/2017-06-26-continuous-interactions/index.html#data",
    "href": "posts/2017-06-26-continuous-interactions/index.html#data",
    "title": "Fitting interactions between continuous variables",
    "section": "Data",
    "text": "Data\nTo keep things easy, I will use a simulated dataset with two continuous predictors (pred1 and pred2) and one continuous outcome. The dataset is available here.\n\ndf &lt;- read.csv(\"http://homeweb.unifr.ch/VanhoveJ/Pub/Data/example_interaction.csv\")"
  },
  {
    "objectID": "posts/2017-06-26-continuous-interactions/index.html#exploratory-plots",
    "href": "posts/2017-06-26-continuous-interactions/index.html#exploratory-plots",
    "title": "Fitting interactions between continuous variables",
    "section": "Exploratory plots",
    "text": "Exploratory plots\nFirst, let’s draw some quick graphs to get a sense of how the predictors and the outcome are related.\n\n# 3 plots next to each other\npar(mfrow = c(1, 3))\n\n# Intercorrelation pred1/pred2\nplot(pred2 ~ pred1, df)\nlines(lowess(df$pred1, df$pred2), col = \"red\")\n\n# Pred1 vs. outcome\nplot(outcome ~ pred1, df)\nlines(lowess(df$pred1, df$outcome), col = \"red\")\n\n# Pred2 vs. outcome\nplot(outcome ~ pred2, df)\nlines(lowess(df$pred2, df$outcome), col = \"red\")\n\n\n\n\nLeft: The predictors are negatively correlated with each other. Middle: pred1 is negatively correlated with the outcome. Right: pred2 seems to be nonlinearly correlated with the outcome.\n\n\n\n# Normal plotting again\npar(mfrow = c(1, 1))\n\nWhen we suspect there’s an interaction between two predictors, it’s useful to draw conditioning plots as well. For these plots, the dataset is split up into a number of overlapping equal-sized regions defined by a conditioning variable, and the relationship between the predictor of interest and the outcome within each region is plotted.\n\n# coplot's in the lattice package\nlibrary(lattice)\ncoplot(outcome ~ pred1 | pred2, data = df, \n       number = 4, rows = 1,\n       panel = panel.smooth)\n\n\n\n\nThe effect of pred1 on the outcome in four regions defined by pred2. The negative relationship grows stronger as pred2 increases (from left to right).\n\n\n\n\n\ncoplot(outcome ~ pred2 | pred1, data = df, \n       number = 4, rows = 1,\n       panel = panel.smooth)\n\n\n\n\nThe effect of pred2 on the outcome in four regions defined by pred1. The relationship may be positive for small values of pred1, but becomes negative for larger values (from left to right)."
  },
  {
    "objectID": "posts/2017-06-26-continuous-interactions/index.html#fitting-the-model",
    "href": "posts/2017-06-26-continuous-interactions/index.html#fitting-the-model",
    "title": "Fitting interactions between continuous variables",
    "section": "Fitting the model",
    "text": "Fitting the model\nI find it easiest to fit the interaction between two continuous variables as a wiggly regression surface. A wiggly regression surface is the generalisation of a wiggly curve, such as the one in Figure 3 in this earlier blog post, into two dimensions. The advantage of fitting a wiggly surface rather than a plane is that we don’t have to assume that the interaction is linear. Rather, the shape of the surface will be estimated from the data. To fit such wiggly surfaces, we need the gam() function in the mgcv package. The wiggly regression surface is fitted by feeding the two predictors to a te() function within the gam() call.\n\n# install.packages(\"mgcv\")\nlibrary(mgcv)\n\nLoading required package: nlme\n\n\nThis is mgcv 1.9-0. For overview type 'help(\"mgcv-package\")'.\n\nm1 &lt;- gam(outcome ~ te(pred1, pred2), data = df)\n\nA numerical summary of the model can be obtained using summary(), but as you can see, this doesn’t offer much in the way of interpretable results.\n\nsummary(m1)\n\n\nFamily: gaussian \nLink function: identity \n\nFormula:\noutcome ~ te(pred1, pred2)\n\nParametric coefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)  92.7686     0.7505   123.6   &lt;2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nApproximate significance of smooth terms:\n                  edf Ref.df     F p-value    \nte(pred1,pred2) 4.629  5.406 27.97  &lt;2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nR-sq.(adj) =  0.427   Deviance explained = 44.1%\nGCV = 115.91  Scale est. = 112.65    n = 200\n\n\nTo understand what the wiggly regression surface looks like, you need to plot it. This can be done using the vis.gam() function. The two plots below show the same regression surface: once as a three-dimensional plot and once as a contour plot. The latter can be read like a topographic map of hilly terrain: the contour lines connect points on the surface with the same height, and the closer the contour lines are to each other, the steeper the surface is.\n\n# 3-d graph\nvis.gam(m1, plot.type = \"persp\", color = \"terrain\", \n        theta = 135, # changes the perspective\n        main = \"\")\n\n\n\n\n\n# A contour plot\nvis.gam(m1, plot.type = \"contour\", color = \"terrain\",\n        main = \"\")\n\n\n\n\nWhat can be gleaned from these plots? First of all, the outcome variable is largest for low pred1 and pred2 values. Second, pred1 and pred2 both have strong negative effects when the respective other variable is large, but their effect is quite weak when the other variable is low.\nThe contour lines are useful to quantify this. Going along the y-axis for a low pred1 value (say, 2.5), we only cross one contour line, and we seem to climb slightly. Going along the y-axis for a large pred1 value (say, 12.5), we cross 8 contour lines and we descend considerably, from around 100 on the outcome variable to below 30. Similarly, going along the x-axis for a low pred2 value (say, 0.2), we descend slightly (perhaps 20 units). Going alongside the x-axis for a large pred2 value (say, 0.8), we cross 10 contour lines. The effect of pred1, then, depends on the value of pred2 and vice versa. Or in other words, pred1 and pred2 interact with each other."
  },
  {
    "objectID": "posts/2017-06-26-continuous-interactions/index.html#improved-visualisation",
    "href": "posts/2017-06-26-continuous-interactions/index.html#improved-visualisation",
    "title": "Fitting interactions between continuous variables",
    "section": "Improved visualisation",
    "text": "Improved visualisation\nI find it useful to plot a contour plot with the wiggly regression surface with the interaction alongside a contour plot with a regression surface without an interaction. I find this helps me to better understand what the interaction consists of. This is particularly helpful when the one or two of the predictors are nonlinearly related to the outcome. To draw the second contour plot, first fit a model without an interaction. By feeding each predictor to a separate te() function, potentially nonlinear regression curves are estimated for both predictors.\n\nm2 &lt;- gam(outcome ~ te(pred1) + te(pred2), data = df)\n\nThen plot both contour plots alongside each other. Here I also set the too.far parameter. This way, region of the graph for which no combinations of predictor variables are available are not plotted.\n\n# Two plots next to each other\npar(mfrow = c(1, 2))\nvis.gam(m2, plot.type = \"contour\", color = \"terrain\", \n        too.far = 0.1, main = \"without interaction\")\nvis.gam(m1, plot.type = \"contour\", color = \"terrain\", \n        too.far = 0.1, main = \"with interaction\")\n\n\n\npar(mfrow = c(1, 1))"
  },
  {
    "objectID": "posts/2017-06-26-continuous-interactions/index.html#decomposing-a-regression-surface-into-main-effects-and-an-interaction",
    "href": "posts/2017-06-26-continuous-interactions/index.html#decomposing-a-regression-surface-into-main-effects-and-an-interaction",
    "title": "Fitting interactions between continuous variables",
    "section": "Decomposing a regression surface into main effects and an interaction",
    "text": "Decomposing a regression surface into main effects and an interaction\nIt may be useful to ‘decompose’ the regression surface into the main effects associated with each predictor and the interaction itself. This can be done by fitting the main effects and the interaction in separate ti() (not te()) functions. This fits non-linear main effects, which is what we want: this way, the interaction term doesn’t absorb any nonlinearities not modelled by the linear main effects.\n\nm3 &lt;- gam(outcome ~ ti(pred1) + ti(pred2) + # main effects\n                    ti(pred1, pred2),       # interaction\n          data = df)\nsummary(m3)\n\n\nFamily: gaussian \nLink function: identity \n\nFormula:\noutcome ~ ti(pred1) + ti(pred2) + ti(pred1, pred2)\n\nParametric coefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)  90.3697     0.8604     105   &lt;2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nApproximate significance of smooth terms:\n                  edf Ref.df      F p-value    \nti(pred1)       1.684  2.057 51.445 &lt; 2e-16 ***\nti(pred2)       1.000  1.000  6.817 0.00974 ** \nti(pred1,pred2) 5.706  7.263  5.105 2.3e-05 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nR-sq.(adj) =   0.44   Deviance explained = 46.4%\nGCV =  115.6  Scale est. = 110.17    n = 200\n\n\nThe wiggly regression surface of m1 has here been decomposed into the main effects of pred1 and pred2 and an interaction between them. Both main effects and the interaction are significant.\nThe edf values in the table above, incidentally, express how nonlinear the effects are estimated to be. Values close to 1 indicate that the effect is linear, whereas a value of 2 suggests that the effect can be modelled using a linear and a quadratic term etc. Here both main effects could sensibly be modelled linearly, but the interaction couldn’t. (A linear interaction means that you multiply both predictors together and use this product as an additional predictor.)\nThere’s much more to fitting nonlinearities and interactions using gam() than discussed here, but hopefully this was useful enough to dissuade you from carving up continuous variables when you’re interested in their interaction."
  },
  {
    "objectID": "posts/2017-06-26-continuous-interactions/index.html#software-versions",
    "href": "posts/2017-06-26-continuous-interactions/index.html#software-versions",
    "title": "Fitting interactions between continuous variables",
    "section": "Software versions",
    "text": "Software versions\n\ndevtools::session_info()\n\n─ Session info ───────────────────────────────────────────────────────────────\n setting  value\n version  R version 4.3.1 (2023-06-16)\n os       Ubuntu 22.04.2 LTS\n system   x86_64, linux-gnu\n ui       X11\n language en_US\n collate  en_US.UTF-8\n ctype    en_US.UTF-8\n tz       Europe/Zurich\n date     2023-08-07\n pandoc   3.1.1 @ /usr/lib/rstudio/resources/app/bin/quarto/bin/tools/ (via rmarkdown)\n\n─ Packages ───────────────────────────────────────────────────────────────────\n package     * version date (UTC) lib source\n cachem        1.0.6   2021-08-19 [2] CRAN (R 4.2.0)\n callr         3.7.3   2022-11-02 [1] CRAN (R 4.3.1)\n cli           3.6.1   2023-03-23 [1] CRAN (R 4.3.0)\n crayon        1.5.2   2022-09-29 [1] CRAN (R 4.3.1)\n devtools      2.4.5   2022-10-11 [1] CRAN (R 4.3.1)\n digest        0.6.29  2021-12-01 [2] CRAN (R 4.2.0)\n ellipsis      0.3.2   2021-04-29 [2] CRAN (R 4.2.0)\n evaluate      0.15    2022-02-18 [2] CRAN (R 4.2.0)\n fastmap       1.1.0   2021-01-25 [2] CRAN (R 4.2.0)\n fs            1.5.2   2021-12-08 [2] CRAN (R 4.2.0)\n glue          1.6.2   2022-02-24 [2] CRAN (R 4.2.0)\n htmltools     0.5.5   2023-03-23 [1] CRAN (R 4.3.0)\n htmlwidgets   1.6.2   2023-03-17 [1] CRAN (R 4.3.1)\n httpuv        1.6.11  2023-05-11 [1] CRAN (R 4.3.1)\n jsonlite      1.8.7   2023-06-29 [1] CRAN (R 4.3.1)\n knitr         1.39    2022-04-26 [2] CRAN (R 4.2.0)\n later         1.3.1   2023-05-02 [1] CRAN (R 4.3.1)\n lattice     * 0.21-8  2023-04-05 [4] CRAN (R 4.3.0)\n lifecycle     1.0.3   2022-10-07 [1] CRAN (R 4.3.0)\n magrittr      2.0.3   2022-03-30 [1] CRAN (R 4.3.0)\n Matrix        1.6-0   2023-07-08 [4] CRAN (R 4.3.1)\n memoise       2.0.1   2021-11-26 [2] CRAN (R 4.2.0)\n mgcv        * 1.9-0   2023-07-11 [4] CRAN (R 4.3.1)\n mime          0.10    2021-02-13 [2] CRAN (R 4.0.2)\n miniUI        0.1.1.1 2018-05-18 [1] CRAN (R 4.3.1)\n nlme        * 3.1-162 2023-01-31 [4] CRAN (R 4.2.2)\n pkgbuild      1.4.2   2023-06-26 [1] CRAN (R 4.3.1)\n pkgload       1.3.2.1 2023-07-08 [1] CRAN (R 4.3.1)\n prettyunits   1.1.1   2020-01-24 [2] CRAN (R 4.2.0)\n processx      3.8.2   2023-06-30 [1] CRAN (R 4.3.1)\n profvis       0.3.8   2023-05-02 [1] CRAN (R 4.3.1)\n promises      1.2.0.1 2021-02-11 [1] CRAN (R 4.3.1)\n ps            1.7.5   2023-04-18 [1] CRAN (R 4.3.1)\n purrr         1.0.1   2023-01-10 [1] CRAN (R 4.3.0)\n R6            2.5.1   2021-08-19 [2] CRAN (R 4.2.0)\n Rcpp          1.0.11  2023-07-06 [1] CRAN (R 4.3.1)\n remotes       2.4.2   2021-11-30 [2] CRAN (R 4.2.0)\n rlang         1.1.1   2023-04-28 [1] CRAN (R 4.3.0)\n rmarkdown     2.21    2023-03-26 [1] CRAN (R 4.3.0)\n rstudioapi    0.14    2022-08-22 [1] CRAN (R 4.3.0)\n sessioninfo   1.2.2   2021-12-06 [2] CRAN (R 4.2.0)\n shiny         1.7.4.1 2023-07-06 [1] CRAN (R 4.3.1)\n stringi       1.7.12  2023-01-11 [1] CRAN (R 4.3.1)\n stringr       1.5.0   2022-12-02 [1] CRAN (R 4.3.0)\n urlchecker    1.0.1   2021-11-30 [1] CRAN (R 4.3.1)\n usethis       2.2.2   2023-07-06 [1] CRAN (R 4.3.1)\n vctrs         0.6.3   2023-06-14 [1] CRAN (R 4.3.0)\n xfun          0.39    2023-04-20 [1] CRAN (R 4.3.0)\n xtable        1.8-4   2019-04-21 [1] CRAN (R 4.3.1)\n yaml          2.3.5   2022-02-21 [2] CRAN (R 4.2.0)\n\n [1] /home/jan/R/x86_64-pc-linux-gnu-library/4.3\n [2] /usr/local/lib/R/site-library\n [3] /usr/lib/R/site-library\n [4] /usr/lib/R/library\n\n──────────────────────────────────────────────────────────────────────────────"
  },
  {
    "objectID": "posts/2020-05-23-nonparametric/index.html",
    "href": "posts/2020-05-23-nonparametric/index.html",
    "title": "Nonparametric tests aren’t a silver bullet when parametric assumptions are violated",
    "section": "",
    "text": "Some researchers adhere to a simple strategy when comparing data from two or more groups: when they think that the data in the groups are normally distributed, they run a parametric test (\\(t\\)-test or ANOVA); when they suspect that the data are not normally distributed, they run a nonparametric test (e.g., Mann–Whitney or Kruskal–Wallis). Rather than follow such an automated approach to analysing data, I think researchers ought to consider the following points:\nIn this blog post, I’ll share the results of some simulations that demonstrate that the Mann–Whitney (a) picks up on differences in the variance between two distributions, even if they have the same mean and median; (b) picks up on differences in the median between two distributions, even if they have the same mean and variance; and (c) picks up on differences in the mean between two distributions, even if they have the same median and variance. These points aren’t new (see Zimmerman 1998), but since the automated strategy (‘parametric when normal, otherwise nonparemetric’) is pretty widespread, they bear repeating."
  },
  {
    "objectID": "posts/2020-05-23-nonparametric/index.html#same-mean-same-median-different-variance",
    "href": "posts/2020-05-23-nonparametric/index.html#same-mean-same-median-different-variance",
    "title": "Nonparametric tests aren’t a silver bullet when parametric assumptions are violated",
    "section": "Same mean, same median, different variance",
    "text": "Same mean, same median, different variance\nThe first simulation demonstrates the Mann–Whitney’s sensitivity to differences in the variance. I simulated samples from a uniform distribution going from \\(-\\sqrt{3}\\) to \\(\\sqrt{3}\\) as well as from a uniform distribution going from \\(-3\\sqrt{3}\\) to \\(3\\sqrt{3}\\). Both distributions have a mean and median of 0, but the standard deviation of the first is 1 and that of the second is 3. I compared these samples using a Mann–Whitney and recorded the \\(p\\)-value. I generated samples of both 50 and 500 observations and repeated this process 10,000 times. You can reproduce this simulation using the code below.\nFigure 1 shows the distribution of the \\(p\\)-values. Even though the distributions’ means and medians are the same, the Mann–Whitney returns significance (\\(p &lt; 0.05\\)) in about 7% of the comparisons for the smaller samples and 8% for the larger samples. If the test were sensitive only to differences in the mean or median, if should return significance in only 5% of the comparisons.\n\n# Load package for plotting\nlibrary(ggplot2)\n\n# Set number of simulation runs\nn_sim &lt;- 10000\n\n# Draw a sample of 50 observations from two uniform distributions with the same \n# mean and median but with different variances/standard deviations.\n# Run the Mann-Whitney on them (wilcox.test()).\n# Repeat this n_sim times.\npvals_50 &lt;- replicate(n_sim, {\n  x &lt;- runif(50, min = -3*sqrt(3), max = 3*sqrt(3))\n  y &lt;- runif(50, min = -sqrt(3), max = sqrt(3))\n  wilcox.test(x, y)$p.value\n})\n\n# Same but with samples of 500 observations.\npvals_500 &lt;- replicate(n_sim, {\n  x &lt;- runif(500, min = -3*sqrt(3), max = 3*sqrt(3))\n  y &lt;- runif(500, min = -sqrt(3), max = sqrt(3))\n  wilcox.test(x, y)$p.value\n})\n\n# Put in data frame\nd &lt;- data.frame(\n  p = c(pvals_50, pvals_500),\n  n = rep(c(50, 500), each = n_sim)\n)\n\n# Plot\nggplot(data = d,\n       aes(x = p,\n           fill = (p &lt; 0.05))) +\n  geom_histogram(\n    breaks = seq(0, 1, 0.05),\n    colour = \"grey20\") +\n  scale_fill_manual(values = c(\"grey70\", \"red\")) +\n  facet_wrap(~ n) +\n  geom_hline(yintercept = n_sim*0.05, linetype = 2) +\n  theme(legend.position = \"none\") +\n  labs(\n    title = element_blank(),\n    subtitle = \"Same mean, same median, different variance\",\n    caption = \"Comparison for two sample sizes (50 vs. 500 observations per group):\n    uniform distribution from -sqrt(3) to sqrt(3)\n    vs. uniform distribution from -3*sqrt(3) to 3*sqrt(3)\"\n  )\n\n\n\n\nFigure 1."
  },
  {
    "objectID": "posts/2020-05-23-nonparametric/index.html#same-mean-different-median-same-variance",
    "href": "posts/2020-05-23-nonparametric/index.html#same-mean-different-median-same-variance",
    "title": "Nonparametric tests aren’t a silver bullet when parametric assumptions are violated",
    "section": "Same mean, different median, same variance",
    "text": "Same mean, different median, same variance\nThe second simulation demonstrates that the Mann–Whitney does not compare means. The simulation set-up was the same as before, but the samples were drawn from different distributions. The first sample was drawn from a log-normal distribution with mean \\(\\exp{(\\ln{10} + \\frac{1}{2})} \\approx 16.5\\), median 10 and standard deviation \\(\\sqrt{(\\exp{(1)}-1)\\exp{(2\\ln{10}+1)}} \\approx 21.6\\). The second sample was drawn from a normal distribution with the same mean (i.e., about 16.5) and the same standard deviation (i.e., about 21.6), but with a different median (viz., 16.5 rather than 10).\nFigure 2 shows that the Mann–Whitney returned significance for 12% of the comparisons of the smaller samples and 92% of the comparisons for the larger samples. So the Mann–Whitney does not test for differences in the mean; otherwise only 5% of the comparisons should have been significant (since the means of the distributions are the same).\n\n\n\n\n\nFigure 2."
  },
  {
    "objectID": "posts/2020-05-23-nonparametric/index.html#different-mean-same-median-same-variance",
    "href": "posts/2020-05-23-nonparametric/index.html#different-mean-same-median-same-variance",
    "title": "Nonparametric tests aren’t a silver bullet when parametric assumptions are violated",
    "section": "Different mean, same median, same variance",
    "text": "Different mean, same median, same variance\nThe last simulation demonstrates that the Mann–Whitney does not compare medians, either. The first sample was again drawn from a log-normal distribution with mean \\(\\exp{(\\ln{10} + \\frac{1}{2})} \\approx 16.5\\), median 10 and standard deviation \\(\\sqrt{(\\exp{(1)}-1)\\exp{(2\\ln{10}+1)}} \\approx 21.6\\). The second sample was now drawn from a normal distribution with the same median (i.e., 10) and the same standard deviation (i.e., about 21.6), but with a different mean (viz., 10 rather than 16.5).\nFigure 3 shows that the Mann–Whitney returned significance for 20% of the comparisons of the smaller samples and 91% of the comparisons for the larger samples. So the Mann–Whitney does not test for differences in the median; otherwise only 5% of the comparisons should have been significant (since the medians of the distributions are the same).\n\n\n\n\n\nFigure 3."
  },
  {
    "objectID": "posts/2020-05-23-nonparametric/index.html#nonparametric-tests-make-assumptions-too",
    "href": "posts/2020-05-23-nonparametric/index.html#nonparametric-tests-make-assumptions-too",
    "title": "Nonparametric tests aren’t a silver bullet when parametric assumptions are violated",
    "section": "Nonparametric tests make assumptions, too",
    "text": "Nonparametric tests make assumptions, too\nMany researchers think that nonparametric tests don’t make any assumptions about the distributions from which the data were drawn. This belief is half-true (i.e., wrong): Nonparametric tests such as the Mann–Whitney don’t assume that the data were drawn from a specific distribution (e.g., from a normal distribution). However, they do assume that the data in the different groups being compared were drawn from the same distribution (but for a shift in the location of this distribution). If researchers run nonparametric tests because they are worried about violating the assumptions of parametric tests, I suggest they worry about the assumptions of their nonparametric tests, too.\nBut a better solution in my view is to them to consider more carefully what they actually want to compare. If it is really the means that are of interest, parametric tests are often okay, and their results can be double-checked using the bootstrap if needed. Permutation tests would be an alternative. If it is the medians that are of interest, quantile regression, bootstrapping, or permutation tests may be useful. If another measure of the data’s central tendency is of interest, robust regression may be useful. A discussion of these techniques is beyond the scope of this blog post, whose aims merely were to alert researchers to the fact that nonparametric tests aren’t a silver bullet when parametric assumptions are violated and that nonparametric tests aren’t just sensitive to differences in the mean or median."
  },
  {
    "objectID": "posts/2020-05-23-nonparametric/index.html#reference",
    "href": "posts/2020-05-23-nonparametric/index.html#reference",
    "title": "Nonparametric tests aren’t a silver bullet when parametric assumptions are violated",
    "section": "Reference",
    "text": "Reference\nZimmerman, Donald W. 1998. Invalidation of parametric and nonparametric statistical tests by concurrent violation of two assumptions. Journal of Experimental Education 67(1). 55-68."
  },
  {
    "objectID": "posts/2020-05-23-nonparametric/index.html#software-versions",
    "href": "posts/2020-05-23-nonparametric/index.html#software-versions",
    "title": "Nonparametric tests aren’t a silver bullet when parametric assumptions are violated",
    "section": "Software versions",
    "text": "Software versions\nPlease note that I reran the code on this page on August 6, 2023.\n\ndevtools::session_info()\n\n─ Session info ───────────────────────────────────────────────────────────────\n setting  value\n version  R version 4.3.1 (2023-06-16)\n os       Ubuntu 22.04.2 LTS\n system   x86_64, linux-gnu\n ui       X11\n language en_US\n collate  en_US.UTF-8\n ctype    en_US.UTF-8\n tz       Europe/Zurich\n date     2023-08-06\n pandoc   3.1.1 @ /usr/lib/rstudio/resources/app/bin/quarto/bin/tools/ (via rmarkdown)\n\n─ Packages ───────────────────────────────────────────────────────────────────\n package     * version date (UTC) lib source\n cachem        1.0.6   2021-08-19 [2] CRAN (R 4.2.0)\n callr         3.7.3   2022-11-02 [1] CRAN (R 4.3.1)\n cli           3.6.1   2023-03-23 [1] CRAN (R 4.3.0)\n colorspace    2.1-0   2023-01-23 [1] CRAN (R 4.3.0)\n crayon        1.5.2   2022-09-29 [1] CRAN (R 4.3.1)\n devtools      2.4.5   2022-10-11 [1] CRAN (R 4.3.1)\n digest        0.6.29  2021-12-01 [2] CRAN (R 4.2.0)\n dplyr       * 1.1.2   2023-04-20 [1] CRAN (R 4.3.0)\n ellipsis      0.3.2   2021-04-29 [2] CRAN (R 4.2.0)\n evaluate      0.15    2022-02-18 [2] CRAN (R 4.2.0)\n fansi         1.0.4   2023-01-22 [1] CRAN (R 4.3.1)\n farver        2.1.1   2022-07-06 [1] CRAN (R 4.3.0)\n fastmap       1.1.0   2021-01-25 [2] CRAN (R 4.2.0)\n forcats     * 1.0.0   2023-01-29 [1] CRAN (R 4.3.0)\n fs            1.5.2   2021-12-08 [2] CRAN (R 4.2.0)\n generics      0.1.3   2022-07-05 [1] CRAN (R 4.3.0)\n ggplot2     * 3.4.2   2023-04-03 [1] CRAN (R 4.3.0)\n glue          1.6.2   2022-02-24 [2] CRAN (R 4.2.0)\n gtable        0.3.3   2023-03-21 [1] CRAN (R 4.3.0)\n hms           1.1.3   2023-03-21 [1] CRAN (R 4.3.0)\n htmltools     0.5.5   2023-03-23 [1] CRAN (R 4.3.0)\n htmlwidgets   1.6.2   2023-03-17 [1] CRAN (R 4.3.1)\n httpuv        1.6.11  2023-05-11 [1] CRAN (R 4.3.1)\n jsonlite      1.8.7   2023-06-29 [1] CRAN (R 4.3.1)\n knitr         1.39    2022-04-26 [2] CRAN (R 4.2.0)\n labeling      0.4.2   2020-10-20 [1] CRAN (R 4.3.0)\n later         1.3.1   2023-05-02 [1] CRAN (R 4.3.1)\n lifecycle     1.0.3   2022-10-07 [1] CRAN (R 4.3.0)\n lubridate   * 1.9.2   2023-02-10 [1] CRAN (R 4.3.0)\n magrittr      2.0.3   2022-03-30 [1] CRAN (R 4.3.0)\n memoise       2.0.1   2021-11-26 [2] CRAN (R 4.2.0)\n mime          0.10    2021-02-13 [2] CRAN (R 4.0.2)\n miniUI        0.1.1.1 2018-05-18 [1] CRAN (R 4.3.1)\n munsell       0.5.0   2018-06-12 [1] CRAN (R 4.3.0)\n pillar        1.9.0   2023-03-22 [1] CRAN (R 4.3.0)\n pkgbuild      1.4.2   2023-06-26 [1] CRAN (R 4.3.1)\n pkgconfig     2.0.3   2019-09-22 [2] CRAN (R 4.2.0)\n pkgload       1.3.2.1 2023-07-08 [1] CRAN (R 4.3.1)\n prettyunits   1.1.1   2020-01-24 [2] CRAN (R 4.2.0)\n processx      3.8.2   2023-06-30 [1] CRAN (R 4.3.1)\n profvis       0.3.8   2023-05-02 [1] CRAN (R 4.3.1)\n promises      1.2.0.1 2021-02-11 [1] CRAN (R 4.3.1)\n ps            1.7.5   2023-04-18 [1] CRAN (R 4.3.1)\n purrr       * 1.0.1   2023-01-10 [1] CRAN (R 4.3.0)\n R6            2.5.1   2021-08-19 [2] CRAN (R 4.2.0)\n Rcpp          1.0.11  2023-07-06 [1] CRAN (R 4.3.1)\n readr       * 2.1.4   2023-02-10 [1] CRAN (R 4.3.0)\n remotes       2.4.2   2021-11-30 [2] CRAN (R 4.2.0)\n rlang         1.1.1   2023-04-28 [1] CRAN (R 4.3.0)\n rmarkdown     2.21    2023-03-26 [1] CRAN (R 4.3.0)\n rstudioapi    0.14    2022-08-22 [1] CRAN (R 4.3.0)\n scales        1.2.1   2022-08-20 [1] CRAN (R 4.3.0)\n sessioninfo   1.2.2   2021-12-06 [2] CRAN (R 4.2.0)\n shiny         1.7.4.1 2023-07-06 [1] CRAN (R 4.3.1)\n stringi       1.7.12  2023-01-11 [1] CRAN (R 4.3.1)\n stringr     * 1.5.0   2022-12-02 [1] CRAN (R 4.3.0)\n tibble      * 3.2.1   2023-03-20 [1] CRAN (R 4.3.0)\n tidyr       * 1.3.0   2023-01-24 [1] CRAN (R 4.3.0)\n tidyselect    1.2.0   2022-10-10 [1] CRAN (R 4.3.0)\n tidyverse   * 2.0.0   2023-02-22 [1] CRAN (R 4.3.1)\n timechange    0.2.0   2023-01-11 [1] CRAN (R 4.3.0)\n tzdb          0.4.0   2023-05-12 [1] CRAN (R 4.3.0)\n urlchecker    1.0.1   2021-11-30 [1] CRAN (R 4.3.1)\n usethis       2.2.2   2023-07-06 [1] CRAN (R 4.3.1)\n utf8          1.2.3   2023-01-31 [1] CRAN (R 4.3.1)\n vctrs         0.6.3   2023-06-14 [1] CRAN (R 4.3.0)\n withr         2.5.0   2022-03-03 [2] CRAN (R 4.2.0)\n xfun          0.39    2023-04-20 [1] CRAN (R 4.3.0)\n xtable        1.8-4   2019-04-21 [1] CRAN (R 4.3.1)\n yaml          2.3.5   2022-02-21 [2] CRAN (R 4.2.0)\n\n [1] /home/jan/R/x86_64-pc-linux-gnu-library/4.3\n [2] /usr/local/lib/R/site-library\n [3] /usr/lib/R/site-library\n [4] /usr/lib/R/library\n\n──────────────────────────────────────────────────────────────────────────────"
  },
  {
    "objectID": "posts/2014-08-14-pretest-posttest-ancova/index.html",
    "href": "posts/2014-08-14-pretest-posttest-ancova/index.html",
    "title": "Analysing pretest/posttest data",
    "section": "",
    "text": "Assigning participants randomly to the control and experimental programmes and testing them before and after the programme is the gold standard for determining the efficacy of pedagogical interventions. But the analyses reported in research articles are often needlessly complicated and may be suboptimal in terms of statistical power."
  },
  {
    "objectID": "posts/2014-08-14-pretest-posttest-ancova/index.html#a-randomised-pretestposttest-control-group-study",
    "href": "posts/2014-08-14-pretest-posttest-ancova/index.html#a-randomised-pretestposttest-control-group-study",
    "title": "Analysing pretest/posttest data",
    "section": "A randomised pretest/posttest control group study",
    "text": "A randomised pretest/posttest control group study\nSay you’ve developed a new method for autonomously learning to read a related foreign language and you want to establish if your method is more efficient than the one currently used. To address this question, you design an experiment along the following lines:\n\nYou recruite 40 motivated students and randomly assign half of them to the control group (current method) and half to the experimental group (new method).\nTo take pre-existing differences in foreign-language reading skills into account, you administer a pretest to all participants.\nSix weeks into the programme, the participants are tested again.\n\nThere you have it – a classic randomised pretest/posttest control group experiment! But how do you go about analysing the data?"
  },
  {
    "objectID": "posts/2014-08-14-pretest-posttest-ancova/index.html#four-analytical-options",
    "href": "posts/2014-08-14-pretest-posttest-ancova/index.html#four-analytical-options",
    "title": "Analysing pretest/posttest data",
    "section": "Four analytical options",
    "text": "Four analytical options\nBy and large, analyses of pretest/posttest experiments in the literature fall into four categories: ANOVAs on the posttest scores only, repeated-measures ANOVAs, ANOVAs on the pretest/posttest differences, and ANCOVAs. The first two are underpowered and overcomplicated, respectively, whereas the third is subject to an assumption that is likely to be violated in real data. The points I want to make aren’t new (see Hendrix et al. 1978; Huck & McLean, 1975), but it can’t hurt to reiterate them – especially since I wasn’t aware of them myself until a couple of days ago.\n\nANOVA on the posttest scores\nOne (thankfully infrequent) option is to compare the control and experimental groups by running an ANOVA or, equivalently, a t-test on the posttest scores whilst disregarding the pretest scores. This amounts to pretending you’ve ran a posttest-only experiment and forgoes the benefits afforded by the pretest/posttest design: Since the participants have been randomly assigned to the conditions, your estimate of the new method’s effect will be correct on average (as they would’ve been in a posttest-only experiment). But by not taking into account pre-existing individual differences, the uncertainty about this estimate (i.e. its standard error) is larger than it needs to be, resulting in a loss of statistical power, as the simulations below show.\nSometimes, the pretest scores are used in a complementary ANOVA or t-test that is intended to verify whether the two groups were comparable at the start of the programme. A discussion of such ‘randomisation checks’ or ‘balance tests’ could be the topic of another blog post; suffice it to say for now that such additional analyses are completely superfluous and uninformative in randomised experiments and that acting on them can invalidate the p-values of the main analysis.\n\n\nRepeated-measures ANOVA\nA far superior alternative is to take both the pretest and the posttest into account in the main analysis. This is often accomplished by fitting a 2 (control vs experimental group) × 2 (pretest vs posttest) repeated-measures ANOVA. This method is superior to merely using the posttest scores as every participant now serves as their own control, which reduced the error variance and hence the statistical power.\nAs Huck & McLean (1975) point out, however, it is also needlessly complicated: the RM-ANOVA table features 3 effects (main effect of condition, main effect of test as well as the interaction between condition and test), only one of which (the interaction) is relevant to the research question. The other two terms provide either irrelevant (main effect of condition) or trivial (main effect of test) information and are bound to lead to faulty interpretations. In short, RM-ANOVAs is likely to cause information overload for both researchers and readers.\n\n\nANOVA on the gain scores\nAn altogether more straightforward and more reader-friendly tack is to compute gain scores by subtracting the pretest scores from the posttest scores and running a one-way ANOVA (or t-test) on them. The p value associated with the effect of condition will be identical to the one associated with the interaction term in the RM-ANOVA. In a nutshell, RM-ANOVAs don’t offer anything relevant over and beyond an ordinary ANOVA or a simple t-test when analysing simple pretest/posttest data.\n\n\nWe’re not there yet: Pretest scores as a covariate (ANCOVA)\nRM-ANOVAs or, equivalently, one-way ANOVAs on gain scores come with an assumption that I don’t think is widely appreciated – viz. that the pretest and posttest scores are linearly related with a slope equal to 1 (see Hendrix et al. 1978; Huck & McLean, 1975). At least, I wasn’t aware of this assumption until a while ago! The ‘slope = 1’ assumption is clearly violated when the pretest and posttest scores are on different scales, e.g. a 7-point scale pretest and a 100-point scale posttest. Less obviously, the assumption can be violated by mere everyday measurement error that results in regression to the mean.\nWhen the construct of, say, foreign-language reading skills is operationalised by means of a necessarily imperfect test, the test result will overestimate some participants’ true skills and underestimate others’ due to extraneous factors such as form on the day, topic of the reading test etc. – in a word: luck. When the same participants are tested again at posttest, participants who over- or underperformed by a wide margin at pretest aren’t likely to be as lucky or unlucky at posttest. The result is that the slope of the linear relationship between pretest and posttest scores will tend to be less than 1, even if both tests are scored on the same scale.\nWith ANCOVA (analysis of covariance), we can bring the pretest scores into the model as a covariate. Unlike when using RM-ANOVAs or gain score ANOVAs, we wouldn’t have to assume that the slope linking the pretest and the posttest scores was 1: we can estimate the slope from the data. This, in principle, would make for more accurate inferences with regard to the effect of condition, but at the cost of one degree of freedom. So how do the two methods (ANOVA and ANCOVA) compare in terms of statistical power and Type-I error rate?"
  },
  {
    "objectID": "posts/2014-08-14-pretest-posttest-ancova/index.html#a-simulation",
    "href": "posts/2014-08-14-pretest-posttest-ancova/index.html#a-simulation",
    "title": "Analysing pretest/posttest data",
    "section": "A simulation",
    "text": "A simulation\nTo get an idea of the Type-I error rate and statistical power associated with posttest score ANOVAs, gain score ANOVAs and ANCOVAs, I programmed a simulation of the hypothetical study described above (R code below).\nThe participants pretest ability (the underlying construct) is programmed to be normally distributed with a to-be-specified standard deviation (sdPretestAbility). The average expected improvement due to the control method and the experimental method are specified as ControlEffect and ExperimentEffect, respectively. Additionally, participants are allowed to differ in their learning progress; their learning aptitude, if you will, is normally distributed with a standard deviation set in sdSensitivity. Lastly, the pre- and posttests have independent but identically distributed measurement errors, whose standard deviation is set in sdMeasurement. This means that the tests are equally accurate but that ‘being lucky’ on the pretest shouldn’t be associated with being lucky on the posttest. (If pretest ability is distributed with a standard deviation of 2 and the standard deviation of the measurement errors is 1, the pretest scores account for 80% of the variance in pretest ability (R² = 2² / (2² + 1²) = 80%). For sdMeasurement values of 0, 2 and 4, the R² values are 100%, 50% and 20%, respectively.)\n\n# Parameters\nn = 20 # number of participants in each condition\nsdPretestAbility = 2 # standard deviation of ABILITY at pretest\nControlEffect = 1 # average improvement in ABILITY for control group\nExperimentEffect = 1 # average improvement in ABILITY for experimental group\nsdSensitivity = 0.5 # standard deviation of the participants' sensitivity to the treatment\nsdMeasurement = 0 # standard deviation of measurement error at pre- and posttest\n\nThe function simulatePrePost.fnc() simulates a single experiment and conducts three analyses on it: a one-way ANOVA on the posttest scores, a one-way ANOVA on the gain scores (again, this is equivalent to running a RM-ANOVA) and an ANCOVA on the posttest scores with the pretest scores as a covariate. The p values associated with the effect of condition in the three analyses are then returned. replicatePrePost.fnc() runs simulatePrePost.fnc() a number of times (e.g. 1000 times) and returns the proportion of significant p values for each analysis type as well as some additional bits and pieces (e.g. the average slope linking pretest and posttest scores in the simulations).\nThe parameters for the simulation were set as specified above with the exception of ExperimentEffect and sdMeasurement, which varied between 1 and 2.6 (as effective to more than twice as effective as the control) and 0 and 4 (no measurement error to only a very rough approximation of reading skills), respectively. For every combination of ExperimentEffect and sdMeasurement I simulated 1000 datasets, which were analysed by means of posttest score ANOVA, gain score ANOVA and ANCOVA. The results of this simulation are available here.\n\nType-I error rate\n‘Type-I error rate’ is just stats speak for ‘How often do we find a significant effect when there isn’t any?’ By tradition, we typically accept a nominal Type-I error rate of 5%, meaning that even if the control and experimental treatments are equally effective, we expect to find a significant difference in our sample in about 50 out of 1000 runs.\nTo investigate the Type-I error rate, I just consider the simulation runs for which I set ExperimentEffect to the same value as ControlEffect (i.e. 1). The following graph plots the observed Type-I error rate by analysis method and measurement error. The solid horizontal line represents the nominal 5% Type-I error rate; the dashed lines give you an idea by how much the error rate can vary due to random sampling: if the true Type-I error rate is 0.05, the points will lie between the dashed lines in 95% of cases.\n\nAll methods perform on par in terms of Type-I error rate – any differences between them don’t seem to be systematic and can likely be accounted for by sampling error.\n\n\nStatistical power\n‘Statistical power’ refers to your chances of finding a significant effect when the treatments do differ in efficacy. Power increases with increasing effects and more precise measurement – a truism that is reflected in the graphs below. As is also obvious, posttest-only ANOVAs compare poorly to analyses that take the pretest scores into consideration. For datasets characterised by substantial measurement error, ANCOVAs outperform gain score ANOVAs fairly systematically, but for datasets with negligible measurement error, both methods are roughly equally as good."
  },
  {
    "objectID": "posts/2014-08-14-pretest-posttest-ancova/index.html#conclusions",
    "href": "posts/2014-08-14-pretest-posttest-ancova/index.html#conclusions",
    "title": "Analysing pretest/posttest data",
    "section": "Conclusions",
    "text": "Conclusions\nHere’s the tl;dr summary:\n\nUse pretest scores if available.\nRepeated-measures ANOVA is too fancy-shmancy for a pretest/posttest design.\nANCOVA is (a bit) more powerful.\n\nMy intuition is that gain score ANOVAs will outperform ANCOVAs in very small samples when the measurement errors are negligible (due to the loss of one degree of freedom that goes into estimating the slope parameter). That said, one advantage of ANCOVAs that we haven’t looked at is that they don’t require that the pre- and posttests be measured on the same scale. Additionally, they can account for non-linear relationships between pretest and posttest scores by adding higher-order terms. But that’ll be for another time."
  },
  {
    "objectID": "posts/2014-08-14-pretest-posttest-ancova/index.html#simulation-code",
    "href": "posts/2014-08-14-pretest-posttest-ancova/index.html#simulation-code",
    "title": "Analysing pretest/posttest data",
    "section": "Simulation code",
    "text": "Simulation code\nTo run these simulations yourself or extend them, you can use the following R code:\n\nsimulatePrePost.fnc &lt;- function(n = 20,\n                                sdPretestAbility = 3,\n                                ExperimentEffect = 2,\n                                ControlEffect = 2,\n                                sdSensitivity = 1,\n                                sdMeasurement = 1) {\n  # Simulate pretest ability\n  PretestAbility &lt;- rnorm(n*2, 10, sdPretestAbility)\n  # Control and experiment effects\n  InterventionEffect &lt;- c(rep(ControlEffect, n), # control group\n                          rep(ExperimentEffect, n)) # intervention group \n  # Individual sensitivity to the effects\n  InterventionSensitivity &lt;- rnorm(n*2, 1, sd = sdSensitivity)\n  # Add group labels\n  Group &lt;- c(rep(\"Control\", n),\n             rep(\"Intervention\", n))  \n  # Pretest scores (with measurement error)\n  Pretest &lt;- PretestAbility + rnorm(n*2, 0, sdMeasurement)\n  # Posttest scores: pretest ability + effect * sensitivity + measurement error\n  Posttest &lt;- PretestAbility + InterventionEffect * InterventionSensitivity + rnorm(n*2, 0, sdMeasurement)\n  \n  # p-value ANOVA on posttests\n  pANOVAPost = anova(lm(Posttest ~ Group))$'Pr(&gt;F)'[[1]]\n  # p-value ANOVA on gain scores\n  pANOVAGain = anova(lm(I(Posttest-Pretest) ~ Group))$'Pr(&gt;F)'[[1]]\n  # p-value ANCOVA\n  pANCOVA =  anova(lm(Posttest ~ Pretest + Group))$'Pr(&gt;F)'[[2]]\n  # slope between pretest and posttest\n  slope = coef(lm(Posttest ~ Pretest + Group))['Pretest']\n  \n  # spit it all out\n  return(list(pANOVAPost = pANOVAPost,\n              pANOVAGain = pANOVAGain,\n              pANCOVA = pANCOVA,\n              slope))\n}\n\nreplicatePrePost.fnc &lt;- function(runs = 1000,\n                                 n = 200,\n                                 sdPretestAbility = 3,\n                                 ExperimentEffect = 3,\n                                 ControlEffect = 2,\n                                 sdSensitivity = 1,\n                                 sdMeasurement = 1) {\n  \n  # run simulatePrePost.fnc() n times\n  sims &lt;- replicate(runs, simulatePrePost.fnc(n,\n                                              sdPretestAbility,\n                                              ExperimentEffect,\n                                              ControlEffect,\n                                              sdSensitivity,\n                                              sdMeasurement))\n  # Compute proportion of significant results and average slope\n  sigANOVAPost = mean(unlist(sims[1,])&lt;=0.05)\n  sigANOVAGain = mean(unlist(sims[2,])&lt;=0.05)\n  sigANCOVA = mean(unlist(sims[3,])&lt;=0.05)\n  meanSlope = mean(unlist(sims[4,]))\n  \n  # Spit it all out\n  return(list(sigANOVAPost = sigANOVAPost,\n              sigANOVAGain = sigANOVAGain,\n              sigANCOVA = sigANCOVA,\n              sdMeasurement = sdMeasurement,\n              Effect = ExperimentEffect - ControlEffect,\n              meanSlope = meanSlope))\n}\n\n# This tabulates all relevant combinations of sdMeasurement and ExperimentEffect\ngrid &lt;- expand.grid(sdMeasurement = seq(0, 4, 0.5),\n                    ExperimentEffect = seq(1, 2.6, 0.2))\n\n# Load parallel package to speed up computations\nlibrary(parallel)\n# Run replicatePrePost.fnc for every combination of sdMeasurement and ExperimentEffect contained in 'grid'\n# I'm not sure whether this works on Mac or Windows; perhaps use mapply instead of mcmapply.\nsimulatedResults &lt;- mcmapply(replicatePrePost.fnc,\n       sdMeasurement = grid$sdMeasurement,\n       ExperimentEffect = grid$ExperimentEffect,\n       # set fixed parameters\n       MoreArgs = list(runs = 1000,\n                       ControlEffect = 1,\n                       sdPretestAbility = 2,\n                       sdSensitivity = 0.5,\n                       n = 20),\n       # distribute work over CPU cores\n       mc.cores = detectCores())\n# Output results (transposed for clarity)\nsimulatedResults &lt;- data.frame(t(simulatedResults))"
  },
  {
    "objectID": "posts/2016-02-23-uninteresting-main-effects/index.html",
    "href": "posts/2016-02-23-uninteresting-main-effects/index.html",
    "title": "Silly significance tests: The main effects no one is interested in",
    "section": "",
    "text": "We are often more interested in interaction effects than in main effects. In a given study, we may not so much be interested in whether high-proficiency second-language (L2) learners react more quickly to target-language stimuli than low-proficiency L2 learners nor in whether L2 learners react more quickly to L1–L2 cognates than to non-cognates. Rather, what we may be interested in is whether the latency difference on cognates and non-cognates differs between high- and low-proficiency learners. When running an ANOVA of the two-way interaction, we should include the main effects, too, and our software package will dutifully report the F-tests for these main effects (i.e., for proficiency and cognacy).\nBut if it is the interaction that is of specific interest, I do not think that we have to actually care about the significance of the main effects – or that we have to clutter the text by reporting them. Similarly, if a three-way interaction is what is of actual interest, the significance tests for the three main effects and two two-way interactions are not directly relevant to the research question, and the five corresponding F-tests can be omitted."
  },
  {
    "objectID": "posts/2016-02-23-uninteresting-main-effects/index.html#an-example",
    "href": "posts/2016-02-23-uninteresting-main-effects/index.html#an-example",
    "title": "Silly significance tests: The main effects no one is interested in",
    "section": "An example",
    "text": "An example\nTo be clear, I’m not saying that reporting these main effects and lower-order interactions invalidates the inferences for the interaction of interest. Rather, my point is that they’re distracting. To illustrate this, I’ve reproduced below three consecutive paragraphs of a results section in a paper on cognitive advantages of bilingualism. Any of several papers would’ve done, but with the recent discussion about this topic, I thought most readers would be able to relate to this one.\nAccording to the authors, “[a]n effect of bilingualism would be indexed by either a main effect of ‘Group of participants’ in any of [the task versions] or by an interaction between ‘Group of participants’ and ‘Type of flanker’” (p. 140). Here is the extract from the results section:\n\nIn the RT analyses the main effect of “Task version” was not significant. The main effects of “Block” (F(2, 240) = 4.63, MSE = 921.93, p = .011) and “Flanker type” (F(1, 120) = 1482.84, MSE = 1667.65, p &lt; .0001) were significant. Crucially, the main effect of “Group of participants” was significant (F(1, 120) = 11.86, MSE = 16976.12, p &lt; .001), with bilinguals producing faster RTs than monolinguals [cross-reference to a table].\n\n\nThe main effects reported above need to be qualified by the multiple interactions observed in this experiment. The interaction between “Flanker type” and “Task version” (F(1, 120) = 49.60, MSE = 1667.65, p &lt; .0001) was significant, revealing that, although the conflict effect was present in both conditions (all ps &lt; .01), it was larger in the 75% congruent version. Also, the magnitude of the conflict effect varied across experimental blocks (“Block” and “Flanker type” (F(2, 240) = 9.20, MSE = 320.76, p = .0001).\n\n\nLet’s now turn to the most interesting interactions involving the factor “Group of participants”. First, the differences between monolinguals and bilinguals in overall RTs tended to be smaller in Block 2 and 3 than in Block 1 (“Group of Participants” X “Block” (F(2, 240) = 17.6, MSE = 921.93, p = .0001). Although no other two-way interactions were observed involving the variable “Group of Participants”, an informative three-way interaction between “Block”, “Flanker type” and “Group of Participants” was significant (F(2, 240) = 4.66, MSE = 320.76, p &lt; .01). A closer look at this interaction revealed that, bilinguals tended to suffer a smaller conflict effect than monolinguals but only in the Block 1. In fact, this reduction in the conflict effect was present mostly in the 75% congruent task version. To get a better understanding of this pattern and given that the four-way interaction between “Block”, “Flanker type”, “Task version” and “Group of Participants” approached significant values (F(2, 240) = 2.38, MSE = 320.76, p &lt; .095), we conducted separate analyses for the two task versions. (etc.)\n\nThat’s a lot of F-, MSE- and p-values, and I found it difficult to see the forest for the trees – and there’s a lot more p-values elsewhere in the paper, too. Of the plethora of inferential statistics in the paragraphs above, only some have any direct bearing on the researchers’ stated hypothesis, viz. the main effect of “Group of participants”, the unreported interaction between “Group and of participants” and “Type of flanker”, and perhaps the same main effect and interaction split up by task version (i.e., an interaction between “Group of participants”, “Type of flanker” and “Task version”). Given the large number of tests, I consider this analysis to be more exploratory than confirmatory (which is fine), but even if the goal is to tease apart subtle effects of bilingualism vs.  monolingualism in an exploratory analysis, I don’t really see why the main effects of “Task version”, “Block”, and “Flanker type” or the interaction effects not involving “Group of participants” should be of interest.\n(There are other aspects of these paragraphs that could be discussed, but I’ll leave it at that.)"
  },
  {
    "objectID": "posts/2016-02-23-uninteresting-main-effects/index.html#silly-tests",
    "href": "posts/2016-02-23-uninteresting-main-effects/index.html#silly-tests",
    "title": "Silly significance tests: The main effects no one is interested in",
    "section": "‘Silly’ tests",
    "text": "‘Silly’ tests\nI’ve written about ‘silly’ significance tests – tests that don’t add anything but make the report harder to read – before, and for me, significance tests for uninteresting main effects and lower-order interactions fall in that category. They clutter the text, and if a text contains too many of them, I’d expect my students (who take statistics on a voluntary basis if at all) to skip straight to the discussion.\nI think that research papers would be easier to read and ultimately have more impact if the analyses focused on what the authors were really interested in. Consequently, I suggest researchers think twice and consider the paper’s readability before reporting and discussing main effects and lower-order interactions if what they’re interested in is a specific interaction. And if these tests are included for the sake of transparency, I propose the following alternatives in ascending order of my preference: (a) putting the ANOVA results in a separate table and only discussing the relevant bits in the running text; (b) including more informative graphs which also show the individual data points and refraining from rubber-stamping them with F- and p-values and what not; and, obviously, (c) making your data and code available."
  },
  {
    "objectID": "posts/2016-06-02-drawing-a-scatterplot/index.html",
    "href": "posts/2016-06-02-drawing-a-scatterplot/index.html",
    "title": "Tutorial: Drawing a scatterplot",
    "section": "",
    "text": "Graphs are incredibly useful both for understanding your own data and for communicating your insights to your audience. This is why the next few blog posts will consist of tutorials on how to draw four kinds of graphs that I find most useful: scatterplots, linecharts, boxplots and some variations, and Cleveland dotplots. These tutorials are aimed primarily at the students in our MA programme. Today’s graph: the scatterplot."
  },
  {
    "objectID": "posts/2016-06-02-drawing-a-scatterplot/index.html#whats-a-scatterplot",
    "href": "posts/2016-06-02-drawing-a-scatterplot/index.html#whats-a-scatterplot",
    "title": "Tutorial: Drawing a scatterplot",
    "section": "What’s a scatterplot?",
    "text": "What’s a scatterplot?\nA scatterplot is one of the most useful kind of graphs in your toolbox. Any time your data consists of pairs of fairly fine-grained measurements such as people’s heights and weights, a scatterplot is one of the top alternatives. To draw one, you plot each pair of measurements in an x/y plane, like this:\n\n\n\n\n\nPlotting the data in this way often gives the reader – and yourself – a good idea of how the two measurements are related. We can immediately see what range both variables span and how differences in one variable are related to differences in the other one. In this case, the relationship between the two variable is distinctly non-linear, the direction of the relationship changing when the first variable is 0. Additionally, it’s immediately obvious that one pair of measurements stands out from the rest of the data and may need triple-checking.\nWhen researchers are interested in how two variables are correlated, they often overeagerly jump straight to computing correlation coefficients. But correlation coefficients can deceive: low correlation coefficients (r) can hide strong but non-linear relationships (left panel), whereas high correlation coefficients can be caused by a single outlying data point (middle) or may gloss over distinct groups in the datasets within each of which the direction of relationship is actually the reverse of the one indicated by the correlation coefficient (right). It’s only in a scatterplot that the meaning of a correlation coefficient – or lack thereof – becomes clear.\n\n\n\n\n\nBottom line: Any time you want to compute a correlation coefficient, draw a scatterplot first and show it to your audience."
  },
  {
    "objectID": "posts/2016-06-02-drawing-a-scatterplot/index.html#tutorial-drawing-a-scatterplot-in-ggplot2",
    "href": "posts/2016-06-02-drawing-a-scatterplot/index.html#tutorial-drawing-a-scatterplot-in-ggplot2",
    "title": "Tutorial: Drawing a scatterplot",
    "section": "Tutorial: Drawing a scatterplot in ggplot2",
    "text": "Tutorial: Drawing a scatterplot in ggplot2\nIn this tutorial, you’ll learn how to draw a basic scatterplot and how you can tweak it. For this, we’ll make use of the free statistical program R and the add-on package ggplot2. ggplot2 is an extremely powerful tool for plotting data, and gaining familiarity with it through fairly simple examples will pay dividends if you’re ever going to work with more complex data.\n\nWhat you’ll need\n\nThe free program R.\nThe graphical user interface RStudio – also free. Download and install R first and only then RStudio.\n\nI’m going to assume some familiarity with these programs. Specifically, I’ll assume that you know how to enter commands in RStudio and import datasets stored in the CSV file format. If you need help with this, see Chapter 3 of my introduction to statistics (in German) or Google importing data R.\n\nThe ggplot2 add-on package for R. To install it, simply enter the following command at the prompt in RStudio. (Update (2023-08-08): By now, the ggplot2 package has been integrated in the tidyverse suite of packages.)\n\n\ninstall.packages(\"ggplot2\")\n\n\nA dataset with pairs of fairly fine-grained measurements. For this tutorial, we’ll use a dataset on receptive multilingualism which was compiled by showing about 100 German speakers lists of Danish, Dutch, Frisian and Swedish words and asking them to translate these words into German. The dataset consists of four variables: the word shown; the language of the word; the degree of orthographic distance between this word and its translation equivalent in German, English or French (possible values between 0 and 1); and the proportion of participants that correctly translated the word (between 0 and 1). Download this dataset to a local drive.\n\n\n\nPreliminaries\nIn RStudio, read in the data. Setting encoding to \"UTF-8\" ensures that letters with diacritics (ä, ö, å etc.) are read in correctly.\n\ntranslations &lt;- read.csv(file.choose(), encoding = \"UTF-8\",\n                         stringsAsFactors = TRUE)\n\nIf the summary looks like this, you’re ready to go.\n\nsummary(translations)\n\n         Item        Language  OrthographicDistance ProportionCorrect\n aai       :  1   Danish :45   Min.   :0.0000       Min.   :0.0000   \n aarde     :  1   Dutch  :46   1st Qu.:0.2222       1st Qu.:0.2243   \n anslutning:  1   Frisian:45   Median :0.3333       Median :0.4860   \n antal     :  1   Swedish:45   Mean   :0.3563       Mean   :0.4906   \n applåd    :  1                3rd Qu.:0.4545       3rd Qu.:0.7757   \n armoede   :  1                Max.   :0.8571       Max.   :0.9907   \n (Other)   :175                                                      \n\n\n\n\nA first attempt\nLoad the ggplot2 package you installed earlier. You don’t need to reinstall the ggplot2 package every time you use it, but you do need to load it each time you fire up R/RStudio.\n\nlibrary(ggplot2)\n\nAbove, we read in the dataset as translations. The variables in this dataset that we want to plot are OrthographicDistance and ProportionCorrect. Since it stands to reason that the orthographic distance between a word and its translation equivalents affects how easily it can be understood, but not vice versa, we’ll put OrthographicDistance along the x-axis and ProportionCorrect along the y-axis.\nIn the ggplot function, we first specify which dataset contains the variables we want to display and then define the ‘aesthetics’ of the graph we want to draw, i.e., which variable we want to put on the x-axis and which one on the y-axis:\n\nggplot(data = translations,\n       aes(x = OrthographicDistance,\n           y = ProportionCorrect))\n\n\n\n\nThe plot above shows an empty coordinate system. The reason is that we haven’t told ggplot how we want to display the pairs of measurements. Usually, we plot them as dots or circles. To achieve this, we add another layer consisting of points to the coordinate system:\n\nggplot(data = translations,\n       aes(x = OrthographicDistance,\n           y = ProportionCorrect)) + \n  geom_point()\n\n\n\n\nIt’s possible to change the appearance of these dots, for instance by setting the shape parameter. For further examples, see the ggplot2 documentation.\n\nggplot(data = translations,\n       aes(x = OrthographicDistance,\n           y = ProportionCorrect)) + \n  geom_point(shape = 1)\n\n\n\n\n\n\nMake it reader-friendly\nThe main trend is clear from the graph above: words with a larger orthographic distance to their translation equivalents are translated correctly less often. (Nothing out of the ordinary, I’ll admit.) Graphs like these are great for learning about your own data, but they need some tweaking before you can use them in a presentation or term paper. Specifically, the default axis names aren’t usually very meaningful to other people, and even if they are, they look a bit slapdash. By specifying the xlab and ylab layers, you can give the axes more interpretable titles. The theme_bw layer get rid of the default grey background.\n\nggplot(data = translations,\n       aes(x = OrthographicDistance,\n           y = ProportionCorrect)) + \n  geom_point(pch = 1) +\n  xlab(\"orthographic Levenshtein distance\") +\n  ylab(\"proportion of correct translations\") +\n  theme_bw()\n\n\n\n\n\n\nLabels instead of points\nInstead of plotting points, we can plot the words that the points stand for. This makes it easier to identify data points that go against the grain – words with high orthographic distances that are easy to understand and vice versa. The words are stored in the Item column; to plot them, we need to add the label aesthetic to the ggplot call and specify that we want to plot text labels instead of points.\n\nggplot(data = translations,\n       aes(x = OrthographicDistance,\n           y = ProportionCorrect,\n           label = Item)) +                    # specify label aesthetic\n  geom_text() +                                # plot text labels instead of points\n  xlab(\"orthographic Levenshtein distance\") +\n  ylab(\"proportion of correct translations\") +\n  theme_bw()\n\n\n\n\nWords that are easy to understand despite their high orthographic distance (e.g., eftermiddag) or that are difficult to understand despite their low orthographic distance (e.g., ræsonnement) can now be identified. That said, with all the overlapping labels, the scatterplot looks crowded. We can make the font size a bit smaller, but with 181 labels, this plot is always going to look crowded. That said, this kind of plot would work well for smaller datasets.\n\nggplot(data = translations,\n       aes(x = OrthographicDistance,\n           y = ProportionCorrect,\n           label = Item)) + \n  geom_text(size = 3) + # change font size\n  xlab(\"orthographic Levenshtein distance\") +\n  ylab(\"proportion of correct translations\") +\n  theme_bw()\n\n\n\n\nInstead of plotting labels for all words, we could just plot the labels of a handful of words and use dots for the rest:\n\nggplot(data = translations,\n       aes(x = OrthographicDistance,\n           y = ProportionCorrect,\n           label = Item)) + \n  geom_point(shape = 1) + # change font size\n  xlab(\"orthographic Levenshtein distance\") +\n  ylab(\"proportion of correct translations\") +\n  theme_bw() +\n  annotate(\"text\", x = 0.72, y = 0.86, label = \"eftermiddag\", colour = \"red\")\n\n\n\n\n\n\nAdding a trend line\nTo highlight the trend in the data even more, you can add a trend line to the scatterplot. In ggplot2, this is a matter of adding another layer to the call (geom_smooth). If you put the geom_smooth after the geom_text layer, the trend line is plotted on top of the text labels; if you put it before geom_text, it’s plotted below the labels.\n\nggplot(data = translations,\n       aes(x = OrthographicDistance,\n           y = ProportionCorrect)) + \n  geom_text(size = 3,\n            aes(label = Item)) +\n  geom_smooth(method = \"loess\",\n              formula = \"y ~  x\") +\n  xlab(\"orthographic Levenshtein distance\") +\n  ylab(\"proportion of correct translations\") +\n  theme_bw()\n\n\n\n\nBy default, the trend line is a non-linear one, is plotted in blue and is accompanied by a grey confidence band. This can all be modified, however. Below, I turn off the confidence band (se = FALSE) because it extents to proportions below 0 – which doesn’t make sense here. I also change the colour to red, just because.\n\nggplot(data = translations,\n       aes(x = OrthographicDistance,\n           y = ProportionCorrect)) + \n  geom_text(size = 3,\n            aes(label = Item)) +\n  geom_smooth(method = \"loess\", \n              formula = \"y ~  x\",\n              se = FALSE, colour = \"red\") +\n  xlab(\"orthographic Levenshtein distance\") +\n  ylab(\"proportion of correct translations\") +\n  theme_bw()\n\n\n\n\n\n\nUsing panels\nIf the data consist of different groups, it can be a good idea to draw separate scatterplots for each group. In this example, 45 of the 181 words were Danish, 46 were Dutch, 45 Frisian and 45 Swedish. To draw separate scatterplots for each of these four languages, we add facet_wrap to the ggplot call:\n\nggplot(data = translations,\n       aes(x = OrthographicDistance,\n           y = ProportionCorrect)) + \n  geom_point(shape = 1) +\n  geom_smooth(method = \"loess\", \n              formula = \"y ~ x\",\n              se = FALSE, col = \"red\") +\n  xlab(\"orthographic Levenshtein distance\") +\n  ylab(\"proportion of correct translations\") +\n  facet_wrap(vars(Language), ncol = 4) + # split up by Language and plot in four columns\n  theme_bw()\n\n\n\n\nOr with labels instead of points, so that we can see which items cause the trend line for Danish to look so different from the other ones:\n\nggplot(data = translations,\n       aes(x = OrthographicDistance,\n           y = ProportionCorrect)) + \n  geom_text(size = 2.2,\n            aes(label = Item)) +\n  geom_smooth(method = \"loess\",\n              formula = \"y ~ x\",\n              se = FALSE, col = \"red\") +\n  xlab(\"orthographic Levenshtein distance\") +\n  ylab(\"proportion of correct translations\") +\n  facet_wrap(vars(Language), ncol = 2) + # 2 columns\n  theme_bw()"
  },
  {
    "objectID": "posts/2016-06-02-drawing-a-scatterplot/index.html#learning-more",
    "href": "posts/2016-06-02-drawing-a-scatterplot/index.html#learning-more",
    "title": "Tutorial: Drawing a scatterplot",
    "section": "Learning more",
    "text": "Learning more\nThe ggplot2 online documentation is great, and you’ll find a wealth of information by googling keywords such as ggplot2 label points scatterplot."
  },
  {
    "objectID": "posts/2016-06-02-drawing-a-scatterplot/index.html#software-versions",
    "href": "posts/2016-06-02-drawing-a-scatterplot/index.html#software-versions",
    "title": "Tutorial: Drawing a scatterplot",
    "section": "Software versions",
    "text": "Software versions\n\ndevtools::session_info(pkgs = \"attached\")\n\n─ Session info ───────────────────────────────────────────────────────────────\n setting  value\n version  R version 4.3.1 (2023-06-16 ucrt)\n os       Windows 10 x64 (build 18363)\n system   x86_64, mingw32\n ui       RTerm\n language (EN)\n collate  English_United Kingdom.utf8\n ctype    English_United Kingdom.utf8\n tz       Europe/Zurich\n date     2023-08-09\n pandoc   3.1.1 @ C:/Program Files/RStudio/resources/app/bin/quarto/bin/tools/ (via rmarkdown)\n\n─ Packages ───────────────────────────────────────────────────────────────────\n package * version date (UTC) lib source\n ggplot2 * 3.4.2   2023-04-03 [1] CRAN (R 4.3.1)\n\n [1] C:/Users/VanhoveJ/AppData/Local/R/win-library/4.3\n [2] C:/Program Files/R/R-4.3.1/library\n\n──────────────────────────────────────────────────────────────────────────────"
  },
  {
    "objectID": "posts/2019-12-05-simplifying-articles/index.html",
    "href": "posts/2019-12-05-simplifying-articles/index.html",
    "title": "Five suggestions for simplifying research reports",
    "section": "",
    "text": "Whenever I’m looking for empirical research articles to discuss in my classes on second language acquisition, I’m struck by how needlessly complicated and unnecessarily long most articles in the field are. Here are some suggestions for reducing the numerical fluff in quantitative research reports."
  },
  {
    "objectID": "posts/2019-12-05-simplifying-articles/index.html#round-more",
    "href": "posts/2019-12-05-simplifying-articles/index.html#round-more",
    "title": "Five suggestions for simplifying research reports",
    "section": "(1) Round more",
    "text": "(1) Round more\nOther than giving off an air of scientific exactitude, there is no reason for reporting the mean age of a sample of participants as “41.01 years” rather than as just “41 years”. Nothing hinges on the 88 hours implied by “.01 years”, and people systematically underreport their age anyway: I’d report my age as 33 years, not as 33.34 years. Overprecise reporting makes it harder to spot patterns in results (see Ehrenberg 1977, 1981). Moreover, it can give readers the impression that there is much more certainty about the findings than there really is if the overprecise numbers aren’t accompanied by a measure of uncertainty.\nSo round more. Average reaction times needn’t be reported to one-hundredth of a millisecond. Correlation coefficients rounded to two decimal places are probably overprecise enough already. Average responses on a 5-point scale are probably best reported to one decimal place rather than to two or—heaven forbid—three. Percentages rarely need to be reported to decimal places at all. There are exceptions to these suggestions. But they’re exceptions."
  },
  {
    "objectID": "posts/2019-12-05-simplifying-articles/index.html#show-the-main-results-graphically",
    "href": "posts/2019-12-05-simplifying-articles/index.html#show-the-main-results-graphically",
    "title": "Five suggestions for simplifying research reports",
    "section": "(2) Show the main results graphically",
    "text": "(2) Show the main results graphically\nSome sort of a visualisation goes a long way in making the results of a study more understandable. Instead of detailed numerical descriptions, try to come up with one or a couple of graphs on the basis of which readers can answer the research questions themselves even if they don’t understand the ins and outs of the analysis. Depending on the data you’re working with, you could plot the raw data that went into the analyses or some summary statistics (e.g., a plot of group means or proportions) or draw model-based effect plots.\nAdmittedly, drawing a good graph can be difficult and can cost a lot of time, but perhaps some of the tutorials I wrote are of some help:\n\nTutorial: Drawing a scatterplot\nTutorial: Drawing a line chart\nTutorial: Drawing a boxplot\nTutorial: Drawing a dot plot\nTutorial: Plotting regression models\nVisualising statistical uncertainty using model-based graphs"
  },
  {
    "objectID": "posts/2019-12-05-simplifying-articles/index.html#run-and-report-much-fewer-significance-tests",
    "href": "posts/2019-12-05-simplifying-articles/index.html#run-and-report-much-fewer-significance-tests",
    "title": "Five suggestions for simplifying research reports",
    "section": "(3) Run and report much fewer significance tests",
    "text": "(3) Run and report much fewer significance tests\nA sizable number of significance tests are run and reported as a matter of course, without there being any real scientific benefit to them. In fact, some of these tests are detrimental to the study’s main inferential results. But my main concern for the purposes of this blog post is that they make research reports all but impregnable to readers who haven’t yet learnt that, frankly, most numbers in an average research report are just fluff. Some tests are easy to do without. These include:\n\ntautological tests, where researchers group participants, items, etc. based on some variable and then confirm that the groups differ with respect to said variable;\nbalance tests, where researchers randomly assign participants to conditions and then check whether the different groups are ‘balanced’ with respect to some measured confound variables. Balance tests in studies without random assignment are less silly, but I’m yet to be convinced they’re useful.\n\nSome other tests are reported so routinely that may be more difficult to leave out, but they are merely clutter all the same. These tests include:\n\nsignificance tests for the main effects if it’s only the interaction that’s of interest. When the research question concerns an interaction, it’s almost always necessary to include the main effects in the analysis. The statistical software will dutifully output significance tests for these main results. But if they’re not relevant to the research question, I don’t see why these main effects should then be discussed in the main text.\nsignificance tests for control variables. There may be excellent reasons for including control variables in the analysis: These may make the estimate for the effect of interest more precise. But they’re almost always not of scientific relevance. So again, I don’t see why the significance tests for control variables should be discussed in the main text.\n\nMy own preference is to report this information in the supplementary materials or to make available the data and computer code so that readers who, for some reason, might be interested in it can run these significance tests themselves. A workable alternative would be to disregard APA guidelines and to report all of the significance tests in a table instead of in the main text and then to only discuss the relevant test in the main text."
  },
  {
    "objectID": "posts/2019-12-05-simplifying-articles/index.html#details-that-someone-somewhere-might-interested-in-dont-belong-in-the-main-text",
    "href": "posts/2019-12-05-simplifying-articles/index.html#details-that-someone-somewhere-might-interested-in-dont-belong-in-the-main-text",
    "title": "Five suggestions for simplifying research reports",
    "section": "(4) Details that someone, somewhere might interested in don’t belong in the main text",
    "text": "(4) Details that someone, somewhere might interested in don’t belong in the main text\nTry to resist the urge to include in the main text all information that isn’t directly relevant to your research question but that someone, somewhere might be interested in. Instead, report this information in the appendix or in the online supplementary materials. If you’re able to put your data and analysis script online, you can be skimpy on many details. Examples of this include:\n\nwhich optimiser you used when fitting models with mixed-effects models. The people that might care would have no problem identifying this piece of information from an R script that you put online.\nin a within-subjects design, what the correlation between the outcome in the different conditions is. This information could be useful for people who want to run a power analysis for their own study, but it’s unlikely to be relevant to your research question. If you put the data online, though, these people have access to this piece of information and you don’t have to befuddle your readers with it.\nfull-fledged alternative analyses. For instance, you may have run a couple of alternative analyses (e.g., in- or excluding outliers) that yielded essentially identical results. One sentence and a reference to the appendix or supplementary materials should suffice; there’s no need to report the alternative analyses in full.\nstandardised effect sizes. I’m not a fan of them, so I don’t report them. But meta-analysts should be able to compute them from the summary statistics or from the raw data provided in the supplementary materials."
  },
  {
    "objectID": "posts/2019-12-05-simplifying-articles/index.html#if-two-analyses-yield-essentially-the-same-results-report-the-simpler-one",
    "href": "posts/2019-12-05-simplifying-articles/index.html#if-two-analyses-yield-essentially-the-same-results-report-the-simpler-one",
    "title": "Five suggestions for simplifying research reports",
    "section": "(5) If two analyses yield essentially the same results, report the simpler one",
    "text": "(5) If two analyses yield essentially the same results, report the simpler one\nSome analyses look complicated, but they always boil down to analyses that are fairly easy. In these cases, the complicated analysis has no added value. Other complicated analyses are arguably a priori more suitable than their more commonly used counterparts, but they often yield similar results. In these cases, I think it’s sensible to carry out both analyses, but only report the more complicated one in the main text if it produces substantially different results from the simpler one.\n\n‘Mixed’ repeated-measures ANOVA vs. a t-test\nOne example of a complicated analysis that always yields the same result as a simpler analysis is repeated-measures ANOVA for analysing pretest/posttest experiments:\n\n“A repeated-measures ANOVA yielded a nonsignificant main effect of Condition (\\(F(1, 48) &lt; 1\\)) but a significant main effect of Time (\\(F(1, 48) = 154.6\\), \\(p &lt; 0.001\\)): In both groups, the posttest scores were higher than the pretest scores. In addition, the Condition × Time interaction was significant (\\(F(1, 48) = 6.2\\), \\(p = 0.016\\)): The increase in reading scores relative to baseline was higher in the treatment than in the control group.”\n\nAs I’ve written before, it’s only the interaction that’s of any interest here, and exactly the same p-value can be obtained using a simpler method:\n\n“We calculated the difference between the pretest and posttest performance for each participant. A two-sample t-test showed that the treatment group showed a higher increase in reading scores than the control group (\\(t(48) = 2.49\\), \\(p = 0.016\\)).”\n\nOther ‘mixed’ repeated-measures ANOVAs can similarly be simplified. For instance, research on cognitive advantages of bilingualism often compares bilinguals with monolinguals on tasks such as the Simon or flanker task. These tasks consist of both congruent and incongruent trials, and the idea is that cognitive advantages of bilingualism would be reflected in a smaller effect of congruency in the bilingual than in the monolingual participants. The results of such a study are then often reported as follows:\n\n“A repeated-measures ANOVA showed a significant main effect of Congruency, with longer reaction times for incongruent than for congruent items (\\(F(1, 58) = 14.3, p &lt; 0.001\\)). The main effect for Language Group did not reach significance, however (\\(F(1, 58) = 1.4, p = 0.24\\)). The crucial interaction between Congruency and Language Group was significant, with bilingual participants showing a smaller Congruency effect than monolinguals (\\(F(1, 58) = 5.8\\), \\(p = 0.02\\)).”\n\nIf the question of interest is merely whether the Congruency effect is smaller in bilinguals than in monolinguals, the following analysis will yield the same inferential results but is easier to navigate through:\n\n“For each participant, we compute the difference between their mean reaction time on congruent and on incongruent items. On average, these differences were smaller for the bilingual than for the monolingual participants (\\(t(58) = 2.4\\), \\(p = 0.02\\)).”\n\nIf three or more groups are compared, a one-way ANOVA could be substituted for the t-test, which is still easier to report and navigate than a two-way RM-ANOVA that produces two significance tests that don’t answer the research question.\nTo seasoned researchers, the difference the original write-ups and my suggestions may not seem like much. But novices—sensibly but incorrectly—assume that each reported significance test must have its role in a research paper. The two irrelevant significance tests detract them from the paper’s true objective. Additionally, novices are more likely to be familiar with t-tests than with repeated-measures ANOVA, so the simpler write-up may be considerably less daunting to them.\n\n\nMultilevel models vs. cluster-level analyses\nCluster-randomised experiments are experiments in which pre-existing groups of participants are assigned in their entirety to the experimental conditions. Importantly, the fact that the participants weren’t all assigned to the condition independently of one another needs to be taken into account in the analysis since the inferences can otherwise be spectacularly anti-conservative. A write-up of a cluster-randomised experiments could look as follows:\n\n“Fourteen classes with 18 pupils each participated in the experiment. Seven randomly picked classes were assigned in their entirety to the intervention condition, the others constituted the control condition. (…) To deal with the clusters in the data (pupils in classes), we fitted a multilevel model using the lme4 package for R (Bates et al. 2015) with class as a random effect. p-values were computed using Satterthwaite’s degrees of freedom method as implemented in the lmerTest package (Kuznetsova et al. 2017) and showed a significant intervention effect (\\(t(12) = 2.3\\), \\(p = 0.04\\)).”\n\nTechnically, this analysis is perfectly valid, but a novice may be too bamboozled by the specialised software and the sophisticated analyses to be motivated enough to follow along. Compare this to the following write-up:\n\n“Fourteen classes with 18 pupils each participated in the experiment. Seven randomly picked classes were assigned in their entirety to the intervention condition, the others constituted the control condition. (…) To deal with the clusters in the data (pupils in classes), we computed the mean outcome per class and submitted these means to a t-test comparing the intervention and the control classes. This revealed a significant intervention effect (\\(t(12) = 2.3\\), \\(p = 0.04\\)).”\n\nComputing means is easy enough, as is running a t-test: The entire analysis could easily be run in the reader’s spreadsheet program of choice. Moreover, the result is exactly the same in this case. In fact, if the cluster sizes are all the same, the multilevel approach and the cluster-mean approach will always yield the exact same result.\nIf the cluster sizes aren’t all the same, the results that both approaches yield won’t be exactly the same. As far as I know, there are no published comparisons of which approach is best in such cases, but my own simulations indicate that both are equally powerful statistically. If that doesn’t put you at ease, you could run both analyses. If they yield approximately the same results, you could report the easier one in the main text and the more complicated one in the appendix; if they yield different results, you’d report the more complicated one in the main text and the simpler one in the appendix. In terms of p-values, you could define them to be “approximately the same” if they both fall into the same predefined bracket, e.g., “both \\(p &lt; 0.01\\)” or “both \\(0.01 &lt; p &lt; 0.05\\)” or “both \\(p &gt; 0.05\\)”. Of course, what you shouldn’t do is to always just report the analysis that yielded the lowest p-value.\n\n\nParametric vs. non-parametric tests\nI would follow the same procedure as above: If, for some reason, you’re convinced you should run a non-parametric test but it yields approximately the same results, report the results for the parametric test, which more readers will be familiar with, in the main text.\n\n\nOrdinal models vs. linear models\nSome people argued for the need to analyse Likert-type data using ordinal regression models rather than using linear models (including t-tests). I myself find it difficult to wrap my head around the different types of ordinal models, I don’t find the model visualisations intuitively understandable, and for some reason, my ordinal models don’t converge in a reasonable time. But that’s for some other blog post. My point is that, even though I accept that ordinal models represent a better default for dealing with Likert-type data, I find them much more complicated than linear models. I suspect that the same would go for any readership.\nAt the same time, I also suspect that, in a great many cases, ordinal and linear models applied to the same data would point to the same answer to the research question. In those cases, I’d prefer for the linear model to be reported in the main text and the ordinal model in the appendix."
  },
  {
    "objectID": "posts/2020-12-16-quant-meth/index.html",
    "href": "posts/2020-12-16-quant-meth/index.html",
    "title": "Quantitative methodology: An introduction",
    "section": "",
    "text": "I’ve taught my last class for the semester and I thought I’d make available the booklet that I wrote for teaching my class on quantitative methodology. You can download it here.\nIt contains seven reading assignments (mostly empirical studies that serve as examples) and ten chapters with lectures:\n\nAssociation and causality.\nConstructing a control group.\nAlternative explanations.\nInferential statistics 101. (The course is not a statistics course, but there’s no avoiding talking about p-values given their omnipresence.)\nIncreasing precision.\nPedagogical interventions.\nWithin-subjects experiments.\nQuasi-experiments and correlational studies.\nConstructs and indicators.\nQuestionable research practices.\n\nI’ve also included two appendices:\n\nReading difficult results sections.\nReporting research transparently.\n\nHopefully some of you find it useful, and feel free to let me know what you think."
  },
  {
    "objectID": "posts/2016-04-22-r-squared/index.html",
    "href": "posts/2016-04-22-r-squared/index.html",
    "title": "Why reported R² values are often too high",
    "section": "",
    "text": "After reading a couple of papers whose conclusions were heavily based on R² (“variance explained”) values, I thought I’d summarise why I’m often skeptical of such conclusions. The reason, in a nutshell, is that reported R² values tend to overestimate how much of the variance in the outcome variable the model can actually “explain”. To dyed-in-the-wool quantitative researchers, none of this blog post will be new, but I hope that it will make some readers think twice before focusing heavily on R² values."
  },
  {
    "objectID": "posts/2016-04-22-r-squared/index.html#whats-r²",
    "href": "posts/2016-04-22-r-squared/index.html#whats-r²",
    "title": "Why reported R² values are often too high",
    "section": "What’s R²?",
    "text": "What’s R²?\nR², or the coefficient of determination, takes on values between 0 and 1 and represents the proportion of the variance in the outcome variable that the predictors in a regression model jointly “explain”. There are a couple of ways to calculate R², and for ordinary regression models, all of them produce the same result. However, for other regression models, such as logistic regression or mixed-effects models, the different definitions of R² can produce different results, so that it’s not clear which definition, if indeed any, is the right one in every case. Here, I’ll stick to discussing R² for ordinary regression models.\nIncidentally, I put “variance explained” between scare quotes, as I think “variance described” would be a better way of putting it. “Variance explained” could suggest that the regression model has the status of an explanatory theoretical model that truly attempts to explain why the data look the way they do. The regression model does no such thing."
  },
  {
    "objectID": "posts/2016-04-22-r-squared/index.html#problems-with-r²",
    "href": "posts/2016-04-22-r-squared/index.html#problems-with-r²",
    "title": "Why reported R² values are often too high",
    "section": "Problems with R²",
    "text": "Problems with R²\nI’ve written about my skepticism about standardised effect sizes before (here and here). R² inherits all of the problems I discussed there and adds some additional ones. To be clear, I don’t doubt that R² can be used sensibly; it’s just that it often isn’t.\nThe first problem with taking R² values at face value is that they tend to be too high: even if the predictor variables and the outcome are actually unrelated, you’ll find that the predictors “explain” some proportion of the variance in the outcome. This is easiest to see when you have one predictor variable and one outcome variable. Even if the predictor and the outcome are unrelated, sampling error will cause the correlation coefficient (r) to deviate from 0 in any one sample. These deviations from 0 can be both positive or negative (left panels in the figure below), and averaged over many samples, the correlation coefficient will be 0 (dashed lines). When you have one predictor and one outcome variable, R² can simply be calculated by squaring the correlation coefficient. But when you square negative values, you get positive numbers, so that sampling error will cause R² to be positive (right panels) – there will always be some “variance explained”! What’s more, since sampling error plays a bigger role in small samples, this overestimation of “variance explained” is larger in smaller samples (top vs. bottom panels).\n\n\n\n\n\nSecond, R² always increases when you add more predictors to the regression model. But by adding more and more predictors, you’re bound to model random fluctuations (‘noise’) in the data. As a result, if you were to apply the same model to a new dataset, you could find that the more complex model actually has a worse fit than a simpler model.\n\n‘R² hacking’\nYou may object that while the above may be true, it’s also irrelevant: your statistics package also outputs an ‘adjusted R²’ value that corrects the R² value based on the sample size and the number of predictors. Adjusted R² values can be negative so that they aren’t biased away from 0 (figure below).\n\n\n\n\n\nThis is true in principle. In practice, though, reported adjusted R² values are often too high, too. The reason is what I’ll call ‘R² hacking’: analytical decisions in variable selection, data transformation and outlier treatment with which the analyst deliberately or indeliberately enhances the model’s fit to the present data, but which cause the model to generalise poorly to new data. R² hacking is like p hacking (Update (2023-08-09): This link is broken.), but rather than selecting for statistical significance, the analyst (deliberately or indeliberately) selects for larger (adjusted or unadjusted) R² values.\nThe effects of R² hacking are most easily illustrated in the simple scenario where the analyst has several predictor variable at their disposal, for the sake of parsimony selects the predictor with the highest absolute sample correlation to the outcome as the sole predictor in a simple regression model, and then reports the model’s adjusted R². The code below simulates such a scenario and assumes that none of the predictors are actually related to the outcome.\n\n# Run 10,000 simulations\nadjrs &lt;- replicate(1e4, {\n  # Generate dataset with 25 observations for 1 outcome and 10 predictors.\n  # None of the predictors is actually related to the outcome here:\n  dat &lt;- data.frame(y = rnorm(25),\n                    x1 = rnorm(25), x2 = rnorm(25), x3 = rnorm(25),\n                    x4 = rnorm(25), x5 = rnorm(25), x6 = rnorm(25),\n                    x7 = rnorm(25), x8 = rnorm(25), x9 = rnorm(25),\n                    x10 = rnorm(25))\n  # Select predictor with highest absolute sample correlation to outcome\n  # as sole predictor in simple regression model\n  formula.lm &lt;- as.formula(paste(\"y ~ \",\n                                 noquote(names(which.max(abs(cor(dat)[1,-1]))))))\n  # Run simple regression model and calculate adjusted R²\n  adjr &lt;- summary(lm(formula.lm, dat))$adj.r.squared\n  # Return adjusted R²\n  return(adjr)\n})\n\n# Plot histogram with results\npar(mfrow = c(1,1), las = 1)\nhist(adjrs, col = \"#4DAF4A\",\n     xlim = c(-0.2, 1),\n     ylim = c(0, 4000),\n     main = \"adjusted R²\\n(n = 25)\",\n     xlab = \"adjusted R²\")\nabline(v = mean(adjrs), lty = 2)\n\n\n\n\nAs the histogram above shows, prior variable screening causes the average adjusted R² value to be higher than zero. This positive bias also occurs when the predictors actually are related to the outcome. For the figure below, I also simulated 10,000 datasets with one outcome variable and ten possible predictors. This time, all of the predictors were weakly related to the outcome. The left panel shows the distribution of the adjusted R² value when we always choose the same predictor for the simple regression model (i.e., no data-dependent variable selection). The average adjusted R² in this case is about 4%. The panel on the right shows what happens when we select the predictor with the highest sample correlation (data-dependent variable selection): the average adjusted R² is now about 21%. So, predictor selection inflates the “variance explained” by a factor of 5 in this case.\nUpdate (2023-08-09): I added and reran the R code.\n\n# Run 10,000 simulations\nadjrs &lt;- replicate(1e4, {\n  # Generate dataset with 25 observations for 1 outcome and 10 predictors.\n  # All of the predictors are weakly related to the outcome.\n  y = rnorm(25)\n  x1 = 0.2*y + rnorm(25)\n  x2 = 0.2*y + rnorm(25)\n  x3 = 0.2*y + rnorm(25)\n  x4 = 0.2*y + rnorm(25)\n  x5 = 0.2*y + rnorm(25) \n  x6 = 0.2*y + rnorm(25)\n  x7 = 0.2*y + rnorm(25) \n  x8 = 0.2*y + rnorm(25)\n  x9 = 0.2*y + rnorm(25)\n  x10 = 0.2*y + rnorm(25)\n  dat &lt;- data.frame(y, x1, x2, x3, x4, x5, x6, x7, x8, x9, x10)\n\n  # Select predictor with highest absolute sample correlation to outcome\n  # as sole predictor in simple regression model\n  formula.lm &lt;- as.formula(paste(\"y ~ \",\n                                 noquote(names(which.max(abs(cor(dat)[1,-1]))))))\n  # Run simple regression model and calculate adjusted R²\n  adjr_selected &lt;- summary(lm(formula.lm, dat))$adj.r.squared\n  # Always pick first variable\n  adjr_fixed    &lt;- summary(lm(y ~ x1, dat))$adj.r.squared\n  # Return adjusted R²\n  return(list(adjr_selected, adjr_fixed))\n})\n\n# Plot histogram with results\npar(mfrow = c(1, 2), oma = c(0, 0, 3, 0), las = 1)\nhist(unlist(adjrs[2, ]), col = \"#FF7F00\",\n     xlim = c(-0.2, 1),\n     ylim = c(0, 6000),\n     main = \"no variable selection\\n(n = 25)\",\n     xlab = \"adjusted R²\")\nabline(v = mean(unlist(adjrs[2, ])), lty = 2)\n\nhist(unlist(adjrs[1, ]), col = \"#984EA3\",\n     xlim = c(-0.2, 1),\n     ylim = c(0, 6000),\n     main = \"variable selection\\n(n = 25)\",\n     xlab = \"adjusted R²\")\nabline(v = mean(unlist(adjrs[1, ])), lty = 2)\n\ntitle(\"Effect of variable selection on adjusted R²\", outer = TRUE)\n\n\n\n\nThe reason for this positive bias is that the adjusted R² value was corrected for the number of predictors that occur in the model (1) – not for the number of predictors that were actually consider when building the model (10). But at least in this case, we know what the number of predictors considered was. Similar situations arise, however, when the predictors were selected by more informal procedures such as visually inspecting the data before deciding which predictors are worth a place in the model or trying out different data transformations and picking the one that yield the prettiest scatterplots."
  },
  {
    "objectID": "posts/2016-04-22-r-squared/index.html#conclusion",
    "href": "posts/2016-04-22-r-squared/index.html#conclusion",
    "title": "Why reported R² values are often too high",
    "section": "Conclusion",
    "text": "Conclusion\nThe reason I’m often skeptical about conclusions based on adjusted or unadjusted R² values is that these values are bound to be overestimates: the same model applied to a new dataset will in all likelihood produce appreciably poorer fits and may sometimes be worse than having no regression model at all. There are methods to minimise this danger, but those are for another time."
  },
  {
    "objectID": "posts/2016-04-22-r-squared/index.html#software-versions",
    "href": "posts/2016-04-22-r-squared/index.html#software-versions",
    "title": "Why reported R² values are often too high",
    "section": "Software versions",
    "text": "Software versions\n\ndevtools::session_info()\n\n─ Session info ───────────────────────────────────────────────────────────────\n setting  value\n version  R version 4.3.1 (2023-06-16)\n os       Ubuntu 22.04.3 LTS\n system   x86_64, linux-gnu\n ui       X11\n language en_US\n collate  en_US.UTF-8\n ctype    en_US.UTF-8\n tz       Europe/Zurich\n date     2023-08-09\n pandoc   3.1.1 @ /usr/lib/rstudio/resources/app/bin/quarto/bin/tools/ (via rmarkdown)\n\n─ Packages ───────────────────────────────────────────────────────────────────\n package     * version date (UTC) lib source\n cachem        1.0.6   2021-08-19 [2] CRAN (R 4.2.0)\n callr         3.7.3   2022-11-02 [1] CRAN (R 4.3.1)\n cli           3.6.1   2023-03-23 [1] CRAN (R 4.3.0)\n crayon        1.5.2   2022-09-29 [1] CRAN (R 4.3.1)\n devtools      2.4.5   2022-10-11 [1] CRAN (R 4.3.1)\n digest        0.6.29  2021-12-01 [2] CRAN (R 4.2.0)\n ellipsis      0.3.2   2021-04-29 [2] CRAN (R 4.2.0)\n evaluate      0.15    2022-02-18 [2] CRAN (R 4.2.0)\n fastmap       1.1.0   2021-01-25 [2] CRAN (R 4.2.0)\n fs            1.5.2   2021-12-08 [2] CRAN (R 4.2.0)\n glue          1.6.2   2022-02-24 [2] CRAN (R 4.2.0)\n htmltools     0.5.5   2023-03-23 [1] CRAN (R 4.3.0)\n htmlwidgets   1.6.2   2023-03-17 [1] CRAN (R 4.3.1)\n httpuv        1.6.11  2023-05-11 [1] CRAN (R 4.3.1)\n jsonlite      1.8.7   2023-06-29 [1] CRAN (R 4.3.1)\n knitr         1.39    2022-04-26 [2] CRAN (R 4.2.0)\n later         1.3.1   2023-05-02 [1] CRAN (R 4.3.1)\n lifecycle     1.0.3   2022-10-07 [1] CRAN (R 4.3.0)\n magrittr      2.0.3   2022-03-30 [1] CRAN (R 4.3.0)\n memoise       2.0.1   2021-11-26 [2] CRAN (R 4.2.0)\n mime          0.10    2021-02-13 [2] CRAN (R 4.0.2)\n miniUI        0.1.1.1 2018-05-18 [1] CRAN (R 4.3.1)\n pkgbuild      1.4.2   2023-06-26 [1] CRAN (R 4.3.1)\n pkgload       1.3.2.1 2023-07-08 [1] CRAN (R 4.3.1)\n prettyunits   1.1.1   2020-01-24 [2] CRAN (R 4.2.0)\n processx      3.8.2   2023-06-30 [1] CRAN (R 4.3.1)\n profvis       0.3.8   2023-05-02 [1] CRAN (R 4.3.1)\n promises      1.2.0.1 2021-02-11 [1] CRAN (R 4.3.1)\n ps            1.7.5   2023-04-18 [1] CRAN (R 4.3.1)\n purrr         1.0.1   2023-01-10 [1] CRAN (R 4.3.0)\n R6            2.5.1   2021-08-19 [2] CRAN (R 4.2.0)\n Rcpp          1.0.11  2023-07-06 [1] CRAN (R 4.3.1)\n remotes       2.4.2   2021-11-30 [2] CRAN (R 4.2.0)\n rlang         1.1.1   2023-04-28 [1] CRAN (R 4.3.0)\n rmarkdown     2.21    2023-03-26 [1] CRAN (R 4.3.0)\n rstudioapi    0.14    2022-08-22 [1] CRAN (R 4.3.0)\n sessioninfo   1.2.2   2021-12-06 [2] CRAN (R 4.2.0)\n shiny         1.7.4.1 2023-07-06 [1] CRAN (R 4.3.1)\n stringi       1.7.12  2023-01-11 [1] CRAN (R 4.3.1)\n stringr       1.5.0   2022-12-02 [1] CRAN (R 4.3.0)\n urlchecker    1.0.1   2021-11-30 [1] CRAN (R 4.3.1)\n usethis       2.2.2   2023-07-06 [1] CRAN (R 4.3.1)\n vctrs         0.6.3   2023-06-14 [1] CRAN (R 4.3.0)\n xfun          0.39    2023-04-20 [1] CRAN (R 4.3.0)\n xtable        1.8-4   2019-04-21 [1] CRAN (R 4.3.1)\n yaml          2.3.5   2022-02-21 [2] CRAN (R 4.2.0)\n\n [1] /home/jan/R/x86_64-pc-linux-gnu-library/4.3\n [2] /usr/local/lib/R/site-library\n [3] /usr/lib/R/site-library\n [4] /usr/lib/R/library\n\n──────────────────────────────────────────────────────────────────────────────"
  },
  {
    "objectID": "posts/2015-03-16-standardised-es-revisited/index.html",
    "href": "posts/2015-03-16-standardised-es-revisited/index.html",
    "title": "More on why I don’t like standardised effect sizes",
    "section": "",
    "text": "A couple of weeks ago, I outlined four reasons why I don’t like standardised effect sizes such as Cohen’s d or Pearson’s r and prefer raw effect sizes instead. In this post, I develop my argument a bit further and argue that reliance on standardised effect sizes can hinder comparisons between studies and that it may have a perverse effect on discussions related to statistical power."
  },
  {
    "objectID": "posts/2015-03-16-standardised-es-revisited/index.html#a-fictitious-example",
    "href": "posts/2015-03-16-standardised-es-revisited/index.html#a-fictitious-example",
    "title": "More on why I don’t like standardised effect sizes",
    "section": "A fictitious example",
    "text": "A fictitious example\nLet’s say two research teams want to investigate the effect of testosterone treatment in bats on the number of twists and turns the bats make per minute when they’re out hunting. Both teams take a random sample of 200 bats and randomly assign 100 of them to the testosterne treatment and 100 to the no-treatment group. The testosterone treatment is identical for both research teams. After treatment, the bats are observed hunting and the number of turns they make is tallied. Both teams adhere to the same definition of what a turn is.\nThe first team notes that the testosterone treatment seemed to have led to an increase in the number of turns. In standardised effect sizes, the Cohen’s d (i.e. difference between the group means divided by the pooled standard deviation) of their treatment is 0.60 (95% CI: [0.30; 0.89]). The second research team similarly notes an testosterone-induced increase, but their Cohen’s d is more than twice as high: 1.26 (95% CI: [0.93; 1.60]). Even before the results get published, chiropterologists (yes, I had to look that up) are already vigorously debating the results and some are drafting up grant proposals to uncover what may lie behind this difference.\nSuch enthusiasm would be premature, however. In actual fact, both research teams found the exact same effect. The raw testosterone-induced increase in number of twists and turns per minute was 4 in both the first experiment (95% CI: [2.1; 5.9]) and the second experiment (95% CI: [3.1; 4.9]). The second research team found a larger Cohen’s d, but as I’ll discuss below, this is an ‘artifact’ (for want of a better term) of their having collected more precise measures. When I recently wrote that standardised effect sizes can artificially inflate the estimated effect, this is what I meant. I’ll elaborate on this in what follows."
  },
  {
    "objectID": "posts/2015-03-16-standardised-es-revisited/index.html#standardised-effect-sizes-and-optimising-research-design",
    "href": "posts/2015-03-16-standardised-es-revisited/index.html#standardised-effect-sizes-and-optimising-research-design",
    "title": "More on why I don’t like standardised effect sizes",
    "section": "Standardised effect sizes and optimising research design",
    "text": "Standardised effect sizes and optimising research design\nCohen’s d and other standardised effect sizes express the effect in the sample (e.g. a difference between two group means) relative to the variability within the sample. Thus, standardised effect sizes are larger when the effect in the sample gets larger or when the variability in the sample decreases. The same effect (e.g. 4 turns per minute, as in the example above) can be associated with a different Cohen’s d if the variability in the sample differs (SD = 6.7 vs. 3.2 in this case).\nBut isn’t the difference in variability between the two samples that Cohen’s d indirectly points to noteworthy? Doesn’t this suggest that the first team used a sample of bats that was much more variable with regard to the number of turns they took per minute?\nWell, no and no. Apart from the fact that the standard deviations do a much better job at highlighting this difference than Cohen’s d does, such difference is purely due to the differences in exactitude between the two teams. Both carried out perfectly valid and unbiased experiments, but the second did a much better job at eliminating extraneous variability.\nTo elaborate: The population of bats at large differs with respect to their inclination to twist and turn when hunting. For the sake of this example, I’ve assumed that untreated bats take 78.1 turns per minute on average compared to 82.1 turns per minute for treated bats. Since everyone likes a bell-curve, we’ll say that the inclination to take twists and turns is normally distibuted with a standard deviation of 3.\nHowever, our estimates of an individual bat’s turning inclination will be tainted by measurement error: our instruments aren’t entirely accurate so there’s always at least some noise on our dependent measure. Variances add up, so measurement error contributes to variability in the measurements and hence affects Cohen’s d and other standardised effect sizes. But as long as measurement errors don’t exert a skew on the results, raw effect size measures are unaffected by them.\nImportantly, good experimental design seeks to minimise measurement error, e.g. by using more precise instruments or by taking repeated measures per unit and averaging them. This improves the study precision but shouldn’t affect the (expected value of the) point estimate of the effect in question.\nIn the example above, the first research team video-taped each bat for one minute and let one rater who was blind to bats’ experimental treatment tally the number of turns taken by the bat in that one minute. This produced an unbiased but imprecise estimate of each bat’s turning inclination. The second research team taped each bat for three minutes, yielding a more stable estimate of the bats’ turning inclination. They then let ten blind raters tally the number of turns per bat. The raters’ scores were then averaged, reducing measurement error even further. This procedure, too, produces an unbiased but much more precise estimate of each bat’s turning inclination.\nThe graph below shows the ‘true’ distribution of turns per minute in treated bats in black (mean = 82.1, sd = 3; no measurement error). The red graph shows how substantial but unbiased measurement error can affect the measurement: the distribution is more spread out. The blue graph shows how measurements with much less measurement error would be distributed.\n\n\n\n\n\nAs a result of higher measurement error, the data of control (dotted line) and treatment groups (solid lines) overlap to a much higher extent in the first experiment:\n\n\n\n\n\nthan in the second one:\n\n\n\n\n\nIt is this difference in conspicuousness that Cohen’s d picks up on. But the effect itself, i.e. 4 turns per minute, is the same in both cases.\n(I’ve focused on measurement error here, but standardised effect sizes are also affected by other design features.)"
  },
  {
    "objectID": "posts/2015-03-16-standardised-es-revisited/index.html#why-do-i-care",
    "href": "posts/2015-03-16-standardised-es-revisited/index.html#why-do-i-care",
    "title": "More on why I don’t like standardised effect sizes",
    "section": "Why do I care?",
    "text": "Why do I care?\n\nd taints comparisons between studies\nAs long as everyone in the field samples bats (or participants) from the same population, uses instruments with the same precision and uses them in the same way, results between studies can directly be compared using Cohen’s d as well as using raw effect sizes. (In which case standardisation is superfluous.) But presumably, different studies are conducted with different degrees of precision, and the research community hopefully adopts more precise instruments in the course of time. When the standard of precision isn’t associated with measurement bias, raw effect sizes are unbiased: the first team conducted a less sophisticated but nonetheless valid experiment. Cohen’s d, however, will tend to be larger for more sophisticated experiments.\nWe can derive some feeling of justice from this, but it isn’t desirable. Studies conducted to differing exacting standards should still be comparable if they’re unbiased. By relying on Cohen’s d, we run the risk of seeing differences where there aren’t any (as in the example above) or finding similarities where there might in fact be differences (similar Cohen’s d, radically different raw effect sizes). Of course, in drawing conclusions from multiple studies, the findings of more exacting researchers should be weighted more, but this is because they report more precise esimates, not because they tend to find larger standardised effect sizes.\n\n\nd’s (perverse?) effect on power computations\nOne genuine worry I have is that Cohen’s d or other standardised effect sizes have a perverse effect on our notion of statistical power. From where I’m standing, it seems as though just about every power computation either (a) takes a pre-specified Cohen’s d (say d = 0.4) as its starting point and calculates the number of participants needed to attain 80% power or (b) calculates how large an effect (again in Cohen’s d) is detectable with 80% power given the sample size.\nIn my view, such an approach to computing power puts too much emphasis on sample size and too little on optimising the experimental design and refining measurements. In the example above, a power computation based on raw effect sizes would reveal that both studies had 80% power to detect an effect of d = 0.4. My point isn’t that this computation is wrong, but that it misses the point: A Cohen’s d of 0.4 corresponds to a substantially smaller effect in the second study than in the first (i.e. the second can detect smaller effects more easily than the first).\nTo be clear: give me a large sample over a comparable small one any day. But not all large samples are created equal, and design features matter when assessing a study’s power."
  },
  {
    "objectID": "posts/2015-03-16-standardised-es-revisited/index.html#wrapping-up",
    "href": "posts/2015-03-16-standardised-es-revisited/index.html#wrapping-up",
    "title": "More on why I don’t like standardised effect sizes",
    "section": "Wrapping up",
    "text": "Wrapping up\nI have referred to standardised effect sizes myself when arguing that drawing conclusions from non-significant results on the basis of sample sizes of 7 participants is fundamentally unsound. But the more I think about standardised effect sizes, the less I’m convinced that they serve any useful role other than rhetoric that can’t be taken up with more aplomb by raw effect sizes. My view on this may be limited, however; if you have any examples where standardised effect sizes do play a meaningful role, do drop a line in the comments (which allows more than 140 characters)."
  },
  {
    "objectID": "posts/2015-06-08-unrelated-tests/index.html",
    "href": "posts/2015-06-08-unrelated-tests/index.html",
    "title": "Silly significance tests: Tests unrelated to the genuine research questions",
    "section": "",
    "text": "Quite a few significance tests in applied linguistics don’t seem to be relevant to the genuine aims of the study. Too many of these tests can make the text an intransparent jumble of t-, F- and p-values, and I suggest we leave them out.\nAn example of the kind of test I have in mind is this. Let’s say a student wants to investigate the efficiency of a new teaching method. After going to great lengths to collect data in several schools, he compares the mean test scores of classes taught with the new method to those of classes taught with the old method. But in addition to this key comparison, he also runs separate tests to see whether the test scores of the girls differ from those of the boys or whether the pupils’ socio-econonomic status (SES) is linked to their test performance.\nThese additional tests aren’t needed to test the efficiency of the new teaching method (the genuine aim of the study), yet tests like these are commonly run and reported. They constitute the third type of ‘silly test’ that we can do without (the first two being superfluous balance tests and tautological tests). In what follows, I respond to three possible arguments in favour of these tests."
  },
  {
    "objectID": "posts/2015-06-08-unrelated-tests/index.html#argument-1-but-test-scores-are-likely-to-be-correlated-with-sex-ses-age-etc.",
    "href": "posts/2015-06-08-unrelated-tests/index.html#argument-1-but-test-scores-are-likely-to-be-correlated-with-sex-ses-age-etc.",
    "title": "Silly significance tests: Tests unrelated to the genuine research questions",
    "section": "Argument 1: ‘But test scores are likely to be correlated with sex / SES / age etc.’",
    "text": "Argument 1: ‘But test scores are likely to be correlated with sex / SES / age etc.’\nWhen designing a study, it’s often possible to identify variables that could help to account for differences in the dependent variable (here: test scores). It sounds reasonable that we should collect these additional variables and take them into account in the analysis before jumping to conclusions about the experimental manipulation (here: teaching method). What needs to be appreciated, however, is that additional variables such as the learners’ sex, SES or age may be important predictors of test performance but nonetheless uninteresting in this study.\nTo take an example from another field, a nutrition researcher may be interested in the effect of protein-rich diets on children’s height. The parents’ height is obviously an important determinant of the children’s height, but this relationship isn’t the focus of the study. Nonetheless, it would be a good idea to include the parents’ height in the analysis – not because the researcher is interested in this link but in order to account for differences between children that are uninteresting for the present study. Accounting for sources of uninteresting differences in the dependent variable reduces the residual error, which in turn leads to a more precise estimate of what the researcher is interested in.\nCrucially, if the idea behind running analyses on sex, SES etc. is to get a more precise estimate of the efficiency of the new teaching method, all of these variables need to be included in the same statistical model. This means that instead of running a t-test with teaching method as the independent variable, another t-test with sex as the independent variable and an ANOVA with SES as the independent variable, you run one ANOVA with teaching method, sex and SES as the independent variables. If you have a continuous additional variable that shows a linear relationship to the dependent variable, you can include it alongside the other variables in an ANCOVA or multiple regression. The decision to include additional variables in the analysis, incidentally, is one to be taken in advance: fiddling about with such additional variables until you get the result you were hoping for is bad practice as it increases the study’s false positive rate (finding a significant effect when none exists).\nSometimes researchers try to contain the influence of additional variables by ensuring that the experimental groups are perfectly balanced with respect to them. For instance, they ensure that the number of boys and girls in the different conditions are equal. This is known as ‘blocking’. Once again, however, precision only increases if the blocking variable is included in the same statistical model as the variable of interest.\nLastly, as long as the reason for including additional variables in the analysis is to increase precision, we shouldn’t care about whether these additional variables have significant effects: whether you find a significant effect of sex or SES on test performance would be immaterial to the real aim of the study. Consequently, the effects of these additional variables don’t have to be discussed any further (see Oehlert 2010, p. 321), and I even think you can do without reporting their p-values – thereby focusing the report more strongly on what’s really of interest."
  },
  {
    "objectID": "posts/2015-06-08-unrelated-tests/index.html#argument-2-the-efficiency-of-the-new-teaching-method-might-be-different-according-to-sex-socio-econonomic-status-age-etc.",
    "href": "posts/2015-06-08-unrelated-tests/index.html#argument-2-the-efficiency-of-the-new-teaching-method-might-be-different-according-to-sex-socio-econonomic-status-age-etc.",
    "title": "Silly significance tests: Tests unrelated to the genuine research questions",
    "section": "Argument 2: ‘The efficiency of the new teaching method might be different according to sex / socio-econonomic status / age etc.’",
    "text": "Argument 2: ‘The efficiency of the new teaching method might be different according to sex / socio-econonomic status / age etc.’\nThe second objection that I would anticipate is that the variable of interest (here: teaching method) might have a different effect depending on some additional variable such as the learners’ sex, SES, age etc. This argument is different from the first one: According to the first argument, boys are expected to perform differently from girls in the experimental (new method) and control condition (old method) alike. According to the second argument, boys might profit more from the new teaching method than do girls (or vice versa). Such a finding could be an important nuance when comparing the new and the old method.\nRunning an additional t-test with sex as the independent variable doesn’t account for this nuance, however. Instead, it’s the interaction between teaching method and sex that’s of interest here.\nIn my experience, however, the rationale behind such interaction tests is rarely theoretically or empirically buttressed: the idea is merely that the effect of interest might somehow differ according to sex, SES etc. Clearly, then, the interactions aren’t the focus of the study. I therefore think it’s best to explicitly label any such interaction tests as exploratory, if you want to run them at all, and demarcate them from the study’s main aim for greater reader-friendliness. Any interesting patterns can then be left to a new study that explicitly targets these interactions."
  },
  {
    "objectID": "posts/2015-06-08-unrelated-tests/index.html#argument-3-after-painstakingly-collecting-this-much-data-running-just-one-test-seems-a-bit-meagre.",
    "href": "posts/2015-06-08-unrelated-tests/index.html#argument-3-after-painstakingly-collecting-this-much-data-running-just-one-test-seems-a-bit-meagre.",
    "title": "Silly significance tests: Tests unrelated to the genuine research questions",
    "section": "Argument 3: ‘After painstakingly collecting this much data, running just one test seems a bit meagre.’",
    "text": "Argument 3: ‘After painstakingly collecting this much data, running just one test seems a bit meagre.’\nThe third objection isn’t so much a rational argument as an emotional appeal – and one that I’m entirely sympathetic to: After travelling through the country to collect data, negotiating with school principals, sending out several reminders for getting parental consent, trying to make sense of illegible handwriting etc., running a single straightforward t-test seems pretty underwhelming.\nSaying that a straightforward analysis is the reward for a good design and that the scientific value of a study isn’t a (positive) function of the number of significance tests it features probably offers only scant consolation. Nothing speaks against conducting additional analyses on your painstakingly collected data, however, provided that these exploratory analyses are labelled as such and, ideally, clearly demarcated from the main analysis. Furthermore, keeping track of tens of test results when reading a research paper is a challenge, which is why I think it pays to be selective when conducting and reporting exploratory analyses. First, exploratory analyses are ideally still theoretically guided and pave the way towards a follow-up study. Second, I think exploratory analyses should only compare what can sensibly be compared in the study. For instance, learners’ comprehension of active and passive sentences might sensibly be compared inasmuch as these form each other’s counterpart (especially if the active and passive sentences express the same proposition). But it’d be more difficult to justify a comparison between the comprehension of object questions and that of unrelated relative clauses. Third, before drawing sweeping conclusions from exploratory analyses, researchers should remind themselves that their chances of finding some significant results increase with each comparison, even if no real differences exist.\nLastly, I think there’s an argument to be made to report exploratory analyses descriptively only, e.g. using graphs and descriptive statistics but without t-tests, ANOVAs, p-values and the like, but I fear that reviewers and editors would probably insist on some inferential measures."
  },
  {
    "objectID": "posts/2015-06-08-unrelated-tests/index.html#summary-and-conclusion",
    "href": "posts/2015-06-08-unrelated-tests/index.html#summary-and-conclusion",
    "title": "Silly significance tests: Tests unrelated to the genuine research questions",
    "section": "Summary and conclusion",
    "text": "Summary and conclusion\nA thread running through this blog is my convinction that typical quantitative research papers in applied linguistics and related fields contain too many significance tests, which can make for a challenging read even for dyed-in-the-wool quantitative researchers. In addition to doing away with balance tests and obviously tautological tests, I suggest that we get rid of tests that don’t contribute to the study’s primary aim. To that end, I propose three guidelines:\n\nIf you analyse variables that you aren’t genuinely interested in because they may nonetheless give rise to differences in the dependent variable, consider including them in the same analysis as the variables that you are interested in.\nIf you analyse such variables because they could reasonably interact with the effect you’re really interested in, it’s the interaction effect you want to take a look at.\nBy all means, conduct exploratory analyses on rich datasets, but show some restraint in choosing which comparisons to run and in interpreting them.\n\nTo wrap off, here’s a rule of thumb that could have some heuristic value: If a comparison isn’t worth the time and effort and for a decent-looking graph to show it, it probably isn’t worth testing it."
  },
  {
    "objectID": "posts/2017-05-12-visualising-models-2/index.html",
    "href": "posts/2017-05-12-visualising-models-2/index.html",
    "title": "Tutorial: Adding confidence bands to effect displays",
    "section": "",
    "text": "In the previous blog post, I demonstrated how you can draw effect displays to render regression models more intelligible to yourself and to your audience. These effect displays did not contain information about the uncertainty inherent to estimating regression models, however. To that end, this blog post demonstrates how you can add confidence bands to effect displays for multiple regression, logistic regression, and logistic mixed-effects models, and explains how these confidence bands are constructed."
  },
  {
    "objectID": "posts/2017-05-12-visualising-models-2/index.html#multiple-regression",
    "href": "posts/2017-05-12-visualising-models-2/index.html#multiple-regression",
    "title": "Tutorial: Adding confidence bands to effect displays",
    "section": "Multiple regression",
    "text": "Multiple regression\nThe Data, Model, and Effect display subsections are identical to those in the previous post, so I left out the accompanying text. In the following subsections, I first show how you can easily add confidence bands to such effect displays and then explain how this method works under the hood.\n\nData\n\n# Load tidyverse package\n# (incl. ggplot2 etc.)\nlibrary(tidyverse)\n\n# Read in data\nex1 &lt;- read.csv(\"http://homeweb.unifr.ch/VanhoveJ/Pub/Data/participants_163.csv\",\n                stringsAsFactors = TRUE)\n\n# Retain variables of interest\nex1 &lt;- ex1[, c(\"Subject\", \"Spoken\", \"Sex\", \"Raven.Right\", \"English.Cloze\")]\n\n# Retain complete cases\nex1 &lt;- ex1[complete.cases(ex1), ]\n\n\n\nModel\n\n# Define shorthand for centring at sample mean,\n# i.e., subtracting sample mean from each value\nc. &lt;- function(x) x - mean(x)\n\n# Centre numeric predictors at sample mean\nex1$c.Raven &lt;- c.(ex1$Raven.Right)\nex1$c.English &lt;- c.(ex1$English.Cloze)\n\n# Sum-code binary predictor (-0.5 vs. 0.5)\nex1$n.Sex &lt;- as.numeric(ex1$Sex) - 1.5\n\n# Fit multiple regression model with n.Sex, \n# c.Raven, their interaction, and c.English\nmod1 &lt;- lm(Spoken ~ n.Sex * c.Raven + c.English,\n           data = ex1)\n\n\n\nEffect display\n\n# Create grid with predictor value combinations\n# for which to plot the predictions.\nnd1_eng &lt;- expand.grid(\n  # Both women and men\n  n.Sex = c(-0.5, 0.5),\n  # Fix c.Raven at its sample mean (= 0)\n  c.Raven = 0,\n  # Unique c.English values in sample\n  c.English = unique(ex1$c.English)\n)\n\n# Use mod1 to 'predict' the Spoken values for the data in nd1_eng,\n# and add these 'predictions' to nd1_eng:\nnd1_eng$Prediction &lt;- predict(mod1, newdata = nd1_eng)\n\n# De-centre c.English\nnd1_eng$English.Cloze &lt;- nd1_eng$c.English + mean(ex1$English.Cloze)\n\n# Relabel n.Sex\nnd1_eng$Sex &lt;- ifelse(nd1_eng$n.Sex == -0.5, \"women\", \"men\")\n\n\n\nAdd confidence bands using predict()’s built-in method\nFor regression models fitted using R’s lm() function, confidence bands can easily be constructed using the predict() function we already used to calculate the outcome values for the predictor variables in the previous subsection. In addition to specifying the model and a data frame containing the combinations of predictor variables for which you want to generate predictions, you need to explicitly set the parameter interval to \"confidence\". If you don’t want to construct 95% confidence bands (the default), you can specify the desired confidence level using the level parameter. Below I set the desired confidence level to 90%, just to demonstrate the parameter’s use.\nWhen used in this fashion, predict() generates a matrix with 3 columns. The first column contains the predictions themselves, i.e., what you’d get if you didn’t specify interval or level, whereas the second and third column contain the lower (lwr) and upper (upr) boundaries of the confidence intervals about the predictions. The code below adds these two latter columns to nd1_eng.\n\nnd1_eng$LoCI &lt;- predict(mod1, newdata = nd1_eng, \n                        interval = \"confidence\", \n                        level = 0.9)[, 2]\nnd1_eng$HiCI &lt;- predict(mod1, newdata = nd1_eng, \n                        interval = \"confidence\", \n                        level = 0.9)[, 3]\n\nWe can now draw ribbons showing the 90% confidence band around the lines showing the predictions. Using ggplot, you can use either geom_ribbon() or geom_smooth() to this end, but I find geom_smooth() easier.\n\nggplot(nd1_eng,\n       aes(x = English.Cloze,\n           y = Prediction)) +\n  # Add a ribbon with the confidence band\n  geom_smooth(\n    aes(\n      # lower and upper bound of the ribbon\n      ymin = LoCI, ymax = HiCI,\n      # Different colour for men/women\n      fill = Sex, colour = Sex\n      ),\n    stat = \"identity\") +\n  xlab(\"English cloze score\") +\n  ylab(\"Modelled spoken score\")\n\n\n\n\n\n\nHow does predict() construct confidence bands?\nWhen you want to construct confidence bands for more complex models, such as the logistic mixed-effects model we’ll discuss below, the predict() function is of little help and you’ll need to construct them ‘by hand’. For this, it’s useful to know how predict() works under the hood so that we’ll be able to apply the same principles to more complex models.\nThe main idea behind this approach is that you can compute the standard errors of the predicted values on the basis of (a) the combinations of predictor variables for which the predictions were computed and (b) the variance–covariance matrix of the estimated parameters of the model you fitted.\nAs for (a), we need to construct the model matrix for the data frame with the combinations of predictor variables (nd1_eng). The model matrix is just an array of numbers which, when multiplied with the estimated model coefficients, yields the model’s predictions. This matrix is very similar to the data frame with the predictor combinations itself, except that it contains an additional column containing 1’s (representing the intercept in the model) and one for which the n.Sex and c.Raven values have been multiplied (representing the interaction between these two predictors in the model). Since all c.Raven values in nd1_eng are set to 0, this column, too, only contains 0’s.\n\n# Construct model matrix from nd1_eng.\n# Make sure the predictors are identical\n# to the ones in mod1 and they're in\n# the same order!\nmm &lt;- model.matrix(~ n.Sex * c.Raven + c.English,\n                   data = nd1_eng)\n# Show first 6 rows of this matrix\nhead(mm)\n\n  (Intercept) n.Sex c.Raven c.English n.Sex:c.Raven\n1           1  -0.5       0   -3.6375             0\n2           1   0.5       0   -3.6375             0\n3           1  -0.5       0    4.3625             0\n4           1   0.5       0    4.3625             0\n5           1  -0.5       0  -12.6375             0\n6           1   0.5       0  -12.6375             0\n\n\nIf you multiply this matrix by the estimated model coefficients, you get the predicted outcomes for these predictor combinations:\n\nhead(mm %*% coef(mod1))\n\n      [,1]\n1 15.92963\n2 14.69550\n3 18.56257\n4 17.32844\n5 12.96758\n6 11.73345\n\n\nIt so happens that the variances of the products of a matrix (such as mm) and a random vector (such as coef(mod1)) can be computed by multiplying the matrix by the variance–covariance matrix of the random vector and then by the transpose of the matrix. This is quite a mouthful, but what it means is that if we get the variance–covariance matrix of the model coefficients, which summarises the uncertainty about the model coefficients and their relationships, we can compute the variances of the predicted outcomes. Luckily, you can easily extract the variance–covariance matrix and calculate this three-way product:\n\n# Get the variance-covariance matrix of mod1\nvcov.m &lt;- vcov(mod1)\n\n# Multiply mm by vcov by transpose of mm\nvars &lt;- mm %*% vcov.m %*% t(mm)\n\nIn the present case, this yields a 50-by-50 matrix that has the variances of the 50 predicted outcomes on the main diagonal. The square roots of these variances are the standard errors of the predicted outcomes. Let’s extract these:\n\nsds &lt;- sqrt(diag(vars))\n\nThese standard errors, in turn, can be used to compute confidence intervals about the predicted outcomes by multiplying them by the appropriate t-value. For a symmetric 90% confidence interval, we need the t-value corresponding to the 95th percentile of the t-distribution with the model’s residual degrees of freedom (90% of the distribution is contained between the 5th and the 95th percentile; the t-value corresponding to the 5th percentile has the same absolute value as the one corresponding to the 95th but is negative, so we take the t-value corresponding to the 95th percentile):\n\n# Multiply this by the appropriate t-value\n# to get a confidence interval about the prediction\nt.val &lt;- qt(1 - (1 - 0.9)/2, mod1$df.residual)\nt.val\n\n[1] 1.654744\n\n\nMultiply the standard errors by this t-value (1.65) and add or subtract the product from the predicted outcomes to get the confidence intervals about the predictions:\n\n# Lower CI bound\nnd1_eng$LoCI.man &lt;- nd1_eng$Prediction - t.val * sds\n\n# Higher CI bound\nnd1_eng$HiCI.man &lt;- nd1_eng$Prediction + t.val * sds\n\nAs you can see, these manually computed boundaries correspond to the boundaries computed using predict():\n\nnd1_eng |&gt;\n  select(LoCI, HiCI,\n         LoCI.man, HiCI.man) |&gt;\n  head()\n\n      LoCI     HiCI LoCI.man HiCI.man\n1 15.10475 16.75452 15.10475 16.75452\n2 13.83692 15.55409 13.83692 15.55409\n3 17.76858 19.35655 17.76858 19.35655\n4 16.39751 18.25937 16.39751 18.25937\n5 11.57080 14.36436 11.57080 14.36436\n6 10.38965 13.07725 10.38965 13.07725\n\n\nBy stringing together these confidence intervals, you get a confidence band. Or if you want to be more precise, a pointwise confidence band. What this is means is that the coverage probability of the confidence band is (in this case) 90% for each point on the line—which makes sense, because that’s how the confidence band was constructed: by stringing together 90% confidence intervals.\n(There also exists another type of confidence band: simultaneous confidence bands. I’m not going to discuss these here.)\nThe confidence bands for the Raven-by-sex interaction is left as an exercise to the reader :)"
  },
  {
    "objectID": "posts/2017-05-12-visualising-models-2/index.html#some-miscellaneous-points-about-confidence-bands",
    "href": "posts/2017-05-12-visualising-models-2/index.html#some-miscellaneous-points-about-confidence-bands",
    "title": "Tutorial: Adding confidence bands to effect displays",
    "section": "Some miscellaneous points about confidence bands",
    "text": "Some miscellaneous points about confidence bands\n\nConfidence bands are narrowest for the mean predictor value.\n\nIf you fix non-focal predictors at a typical value for the purposes of drawing an effect display, fixing them at their sample mean yields narrower confidence bands than fixing them at any other value."
  },
  {
    "objectID": "posts/2017-05-12-visualising-models-2/index.html#logistic-model",
    "href": "posts/2017-05-12-visualising-models-2/index.html#logistic-model",
    "title": "Tutorial: Adding confidence bands to effect displays",
    "section": "Logistic model",
    "text": "Logistic model\nWe can similarly construct confidence bands for logistic regression models. See the previous post for details about the data and the model, which I’ll skip here.\n\nData, model and effect display\nNote that the predicted values are expressed in log-odds, not in probabilities (type = \"link\").\n\n# Read in data\nex2 &lt;- read.csv(\"http://homeweb.unifr.ch/VanhoveJ/Pub/Data/ExampleLogisticRegression.csv\",\n                stringsAsFactors = TRUE)\n\n# Retain only the observations for participant `DB3`:\nex2 &lt;- droplevels(subset(ex2, Subject == \"DB3\"))\n\n# Centre predictors\nex2$c.Lev &lt;- c.(ex2$MinLevGermanic)\nex2$clog.Freq &lt;- c.(ex2$log.FreqGermanic)\n\n# Fit model\nmod2 &lt;- glm(Correct ~ c.Lev + clog.Freq,\n            data = ex2,\n            family = \"binomial\")\n\n# Effect display for MinLevGermanic\nnd2_lev &lt;- expand.grid(\n  # A sequence of c.Lev values\n  # from the sample minimum\n  # through the sample maximum\n  # in steps of 0.05\n  c.Lev = seq(min(ex2$c.Lev),\n              max(ex2$c.Lev),\n              by = 0.05),\n  # Fix clog.Freq at its sample mean\n  clog.Freq = 0\n)\n\n# Fill in predicted values in LOG-ODDS\nnd2_lev$Prediction &lt;- predict(mod2, nd2_lev,\n                              type = \"link\")\n\n# Decentre c.Lev\nnd2_lev$Levenshtein &lt;- nd2_lev$c.Lev + mean(ex2$MinLevGermanic)\n\n\n\nConstructing confidence bands using predict()\nFor generalised linear models, such as logistic regression models, predict() doesn’t return confidence intervals for predicted values, but it can return standard errors. We can use these to construct the confidence bands ourselves.\n\n# Add standard errors to nd2_lev\nnd2_lev$SE &lt;- predict(mod2, nd2_lev,\n                      type = \"link\",\n                      se.fit = TRUE)$se.fit\n\nThese standard errors are expressed in log-odds. To construct confidence intervals, we need to find the appropriate multiplier. For ordinary least-squares regression (cf. above), there is an exact solution to this problem, namely by referring to a t-distribution with the appropriate degrees of freedom. For logistic regression, we have to make do with an approximation based on the normal distribution.\n\n# Find multiplier for 90% CI\nz.val &lt;- qnorm(1 - (1 - 0.90)/2)\nz.val\n\n[1] 1.644854\n\n\nNow multiply the standard errors by this multiplier and add/subtract them from the predicted outcomes:\n\nnd2_lev$LoCI &lt;- nd2_lev$Prediction - z.val * nd2_lev$SE\nnd2_lev$HiCI &lt;- nd2_lev$Prediction + z.val * nd2_lev$SE\n\nThis yields confidence intervals in log-odds. Probabilities are easier to interpret, though, so we run these through the logistic function to transform the log-odds to probabilities:\n\nnd2_lev$prob.Prediction &lt;- plogis(nd2_lev$Prediction)\nnd2_lev$prob.LoCI &lt;- plogis(nd2_lev$LoCI)\nnd2_lev$prob.HiCI &lt;- plogis(nd2_lev$HiCI)\n\nStringing the confidence intervals together, we get the 90% pointwise confidence band:\n\nggplot(nd2_lev,\n       aes(x = Levenshtein,\n           y = prob.Prediction)) +\n  geom_smooth(aes(ymin = prob.LoCI, \n                  ymax = prob.HiCI),\n              stat = \"identity\") +\n  xlab(\"Levenshtein value\") +\n  ylab(\"Probability of\\ncorrect translation\")"
  },
  {
    "objectID": "posts/2017-05-12-visualising-models-2/index.html#mixed-effects-logistic-regression",
    "href": "posts/2017-05-12-visualising-models-2/index.html#mixed-effects-logistic-regression",
    "title": "Tutorial: Adding confidence bands to effect displays",
    "section": "Mixed-effects logistic regression",
    "text": "Mixed-effects logistic regression\nOnce you get the hang of it, constructing confidence bands for the fixed effects in a mixed-effects model isn’t really that much more difficult than for the other models.\nFor details about the data, the model specification and the basic effect display, please refer to the previous blog post.\n\nData, model and effect display\nNote that this time, the predictions are generated in log-odds, not in probabilities (type = \"link\").\n\n# Read in data\nex3 &lt;- read.csv(\"http://homeweb.unifr.ch/VanhoveJ/Pub/Data/ExampleLogisticRegression.csv\",\n                stringsAsFactors = TRUE)\n\n# Centre numeric variables\nex3$c.Lev &lt;- c.(ex3$MinLevGermanic)\nex3$clog.Freq &lt;- c.(ex3$log.FreqGermanic)\nex3$c.Eng &lt;- c.(ex3$EngReading)\n\n# Express Sex as numeric variable (+/- 0.5)\n# where -0.5 = m(an) and 0.5 = w(oman)\nex3$n.Sex &lt;- as.numeric(ex3$Sex) - 1.5\n\n# Fit model\nlibrary(lme4)\n\nLoading required package: Matrix\n\n\n\nAttaching package: 'Matrix'\n\n\nThe following objects are masked from 'package:tidyr':\n\n    expand, pack, unpack\n\nmod3 &lt;- glmer(Correct ~ c.Lev + clog.Freq +\n                c.Eng + n.Sex +\n                (1 + c.Lev | Subject) +\n                (1 + c.Eng | Stimulus),\n              data = ex3,\n              family = \"binomial\")\n\n# Effect display for Levenshtein\nnd3_lev &lt;- expand.grid(\n  c.Lev = seq(min(ex3$c.Lev),\n              max(ex3$c.Lev),\n              by = 0.05),\n  clog.Freq = 0,\n  c.Eng = 0,\n  n.Sex = 0\n)\n\n# Add prediction IN LOG-ODDS\nnd3_lev$Prediction &lt;- predict(mod3, nd3_lev,\n                              type = \"link\",\n                              re.form = NA)\n\n# Decentre c.Lev\nnd3_lev$Levenshtein &lt;- nd3_lev$c.Lev + mean(ex3$MinLevGermanic)\n\n\n\nConstruct confidence band manually\npredict() doesn’t generate confidence intervals or standard errors for mixed-effects models, so we’ll have to compute them manually. The method is the same as for the ordinary least-squares model above; see http://glmm.wikidot.com/faq#predconf for a summary.\nFirst, construct the model matrix for the combinations of predictor variables we generated the outcomes for.\n\n# Model matrix of nd3_lev\n# Make sure the predictor variables\n# occur in the same order as in the model!\nmm &lt;- model.matrix(~ c.Lev + clog.Freq + c.Eng + n.Sex, \n                   nd3_lev)\n\nThen extract the variance–covariance matrix of the model and use it to compute the standard errors for the predictions (in log-odds):\n\n# Variance--covariance of mod3\nvcov.m &lt;- vcov(mod3)\n\n# Compute variances\nvars &lt;- mm %*% vcov.m %*% t(mm)\n\n# Extract square roots of main diagonal\n# (= standard errors)\nsds &lt;- sqrt(diag(vars))\n\nNow find the multiplier appropriate for 90% confidence intervals and carry out the same calculation as the previous two times:\n\nz.val &lt;- qnorm(1 - (1 - 0.9)/2)\n\nnd3_lev$LoCI &lt;- nd3_lev$Prediction - z.val * sds\nnd3_lev$HiCI &lt;- nd3_lev$Prediction + z.val * sds\n\nRun the predicted values and their confidence bands through the logistic function to transform them to probabilities:\n\nnd3_lev$prob.Prediction &lt;- plogis(nd3_lev$Prediction)\nnd3_lev$prob.LoCI &lt;- plogis(nd3_lev$LoCI)\nnd3_lev$prob.HiCI &lt;- plogis(nd3_lev$HiCI)\n\nAnd plot:\n\nggplot(nd3_lev,\n       aes(x = Levenshtein,\n           y = prob.Prediction)) +\n  geom_smooth(aes(ymin = prob.LoCI, \n                  ymax = prob.HiCI),\n              stat = \"identity\") +\n  xlab(\"Levenshtein value\") +\n  ylab(\"Probability of\\ncorrect translation\")\n\n\n\n\nNote that this confidence band is based solely on the fixed-effects. As such, it should be taken to reflect the uncertainty about the average participant’s regression curve; the regression curves of individual participants can differ wildly from this average.\nThis post hardly makes for captivating reading, but I hope it helps some of you who want to visualise the uncertainty in their regression models."
  },
  {
    "objectID": "posts/2017-05-12-visualising-models-2/index.html#software-versions",
    "href": "posts/2017-05-12-visualising-models-2/index.html#software-versions",
    "title": "Tutorial: Adding confidence bands to effect displays",
    "section": "Software versions",
    "text": "Software versions\n\ndevtools::session_info()\n\n─ Session info ───────────────────────────────────────────────────────────────\n setting  value\n version  R version 4.3.1 (2023-06-16)\n os       Ubuntu 22.04.2 LTS\n system   x86_64, linux-gnu\n ui       X11\n language en_US\n collate  en_US.UTF-8\n ctype    en_US.UTF-8\n tz       Europe/Zurich\n date     2023-08-07\n pandoc   3.1.1 @ /usr/lib/rstudio/resources/app/bin/quarto/bin/tools/ (via rmarkdown)\n\n─ Packages ───────────────────────────────────────────────────────────────────\n package     * version date (UTC) lib source\n boot          1.3-28  2021-05-03 [4] CRAN (R 4.2.0)\n cachem        1.0.6   2021-08-19 [2] CRAN (R 4.2.0)\n callr         3.7.3   2022-11-02 [1] CRAN (R 4.3.1)\n cli           3.6.1   2023-03-23 [1] CRAN (R 4.3.0)\n codetools     0.2-19  2023-02-01 [4] CRAN (R 4.2.2)\n colorspace    2.1-0   2023-01-23 [1] CRAN (R 4.3.0)\n crayon        1.5.2   2022-09-29 [1] CRAN (R 4.3.1)\n devtools      2.4.5   2022-10-11 [1] CRAN (R 4.3.1)\n digest        0.6.29  2021-12-01 [2] CRAN (R 4.2.0)\n dplyr       * 1.1.2   2023-04-20 [1] CRAN (R 4.3.0)\n ellipsis      0.3.2   2021-04-29 [2] CRAN (R 4.2.0)\n evaluate      0.15    2022-02-18 [2] CRAN (R 4.2.0)\n fansi         1.0.4   2023-01-22 [1] CRAN (R 4.3.1)\n farver        2.1.1   2022-07-06 [1] CRAN (R 4.3.0)\n fastmap       1.1.0   2021-01-25 [2] CRAN (R 4.2.0)\n forcats     * 1.0.0   2023-01-29 [1] CRAN (R 4.3.0)\n fs            1.5.2   2021-12-08 [2] CRAN (R 4.2.0)\n generics      0.1.3   2022-07-05 [1] CRAN (R 4.3.0)\n ggplot2     * 3.4.2   2023-04-03 [1] CRAN (R 4.3.0)\n glue          1.6.2   2022-02-24 [2] CRAN (R 4.2.0)\n gtable        0.3.3   2023-03-21 [1] CRAN (R 4.3.0)\n hms           1.1.3   2023-03-21 [1] CRAN (R 4.3.0)\n htmltools     0.5.5   2023-03-23 [1] CRAN (R 4.3.0)\n htmlwidgets   1.6.2   2023-03-17 [1] CRAN (R 4.3.1)\n httpuv        1.6.11  2023-05-11 [1] CRAN (R 4.3.1)\n jsonlite      1.8.7   2023-06-29 [1] CRAN (R 4.3.1)\n knitr         1.39    2022-04-26 [2] CRAN (R 4.2.0)\n labeling      0.4.2   2020-10-20 [1] CRAN (R 4.3.0)\n later         1.3.1   2023-05-02 [1] CRAN (R 4.3.1)\n lattice       0.21-8  2023-04-05 [4] CRAN (R 4.3.0)\n lifecycle     1.0.3   2022-10-07 [1] CRAN (R 4.3.0)\n lme4        * 1.1-34  2023-07-04 [1] CRAN (R 4.3.1)\n lubridate   * 1.9.2   2023-02-10 [1] CRAN (R 4.3.0)\n magrittr      2.0.3   2022-03-30 [1] CRAN (R 4.3.0)\n MASS          7.3-60  2023-05-04 [4] CRAN (R 4.3.1)\n Matrix      * 1.6-0   2023-07-08 [4] CRAN (R 4.3.1)\n memoise       2.0.1   2021-11-26 [2] CRAN (R 4.2.0)\n mime          0.10    2021-02-13 [2] CRAN (R 4.0.2)\n miniUI        0.1.1.1 2018-05-18 [1] CRAN (R 4.3.1)\n minqa         1.2.5   2022-10-19 [1] CRAN (R 4.3.1)\n munsell       0.5.0   2018-06-12 [1] CRAN (R 4.3.0)\n nlme          3.1-162 2023-01-31 [4] CRAN (R 4.2.2)\n nloptr        2.0.3   2022-05-26 [1] CRAN (R 4.3.1)\n pillar        1.9.0   2023-03-22 [1] CRAN (R 4.3.0)\n pkgbuild      1.4.2   2023-06-26 [1] CRAN (R 4.3.1)\n pkgconfig     2.0.3   2019-09-22 [2] CRAN (R 4.2.0)\n pkgload       1.3.2.1 2023-07-08 [1] CRAN (R 4.3.1)\n prettyunits   1.1.1   2020-01-24 [2] CRAN (R 4.2.0)\n processx      3.8.2   2023-06-30 [1] CRAN (R 4.3.1)\n profvis       0.3.8   2023-05-02 [1] CRAN (R 4.3.1)\n promises      1.2.0.1 2021-02-11 [1] CRAN (R 4.3.1)\n ps            1.7.5   2023-04-18 [1] CRAN (R 4.3.1)\n purrr       * 1.0.1   2023-01-10 [1] CRAN (R 4.3.0)\n R6            2.5.1   2021-08-19 [2] CRAN (R 4.2.0)\n Rcpp          1.0.11  2023-07-06 [1] CRAN (R 4.3.1)\n readr       * 2.1.4   2023-02-10 [1] CRAN (R 4.3.0)\n remotes       2.4.2   2021-11-30 [2] CRAN (R 4.2.0)\n rlang         1.1.1   2023-04-28 [1] CRAN (R 4.3.0)\n rmarkdown     2.21    2023-03-26 [1] CRAN (R 4.3.0)\n rstudioapi    0.14    2022-08-22 [1] CRAN (R 4.3.0)\n scales        1.2.1   2022-08-20 [1] CRAN (R 4.3.0)\n sessioninfo   1.2.2   2021-12-06 [2] CRAN (R 4.2.0)\n shiny         1.7.4.1 2023-07-06 [1] CRAN (R 4.3.1)\n stringi       1.7.12  2023-01-11 [1] CRAN (R 4.3.1)\n stringr     * 1.5.0   2022-12-02 [1] CRAN (R 4.3.0)\n tibble      * 3.2.1   2023-03-20 [1] CRAN (R 4.3.0)\n tidyr       * 1.3.0   2023-01-24 [1] CRAN (R 4.3.0)\n tidyselect    1.2.0   2022-10-10 [1] CRAN (R 4.3.0)\n tidyverse   * 2.0.0   2023-02-22 [1] CRAN (R 4.3.1)\n timechange    0.2.0   2023-01-11 [1] CRAN (R 4.3.0)\n tzdb          0.4.0   2023-05-12 [1] CRAN (R 4.3.0)\n urlchecker    1.0.1   2021-11-30 [1] CRAN (R 4.3.1)\n usethis       2.2.2   2023-07-06 [1] CRAN (R 4.3.1)\n utf8          1.2.3   2023-01-31 [1] CRAN (R 4.3.1)\n vctrs         0.6.3   2023-06-14 [1] CRAN (R 4.3.0)\n withr         2.5.0   2022-03-03 [2] CRAN (R 4.2.0)\n xfun          0.39    2023-04-20 [1] CRAN (R 4.3.0)\n xtable        1.8-4   2019-04-21 [1] CRAN (R 4.3.1)\n yaml          2.3.5   2022-02-21 [2] CRAN (R 4.2.0)\n\n [1] /home/jan/R/x86_64-pc-linux-gnu-library/4.3\n [2] /usr/local/lib/R/site-library\n [3] /usr/lib/R/site-library\n [4] /usr/lib/R/library\n\n──────────────────────────────────────────────────────────────────────────────"
  },
  {
    "objectID": "posts/2016-02-16-cluster-randomisation-correction/index.html",
    "href": "posts/2016-02-16-cluster-randomisation-correction/index.html",
    "title": "Experiments with intact groups: spurious significance with improperly weighted t-tests",
    "section": "",
    "text": "When analysing experiments in which intact groups (clusters) were assigned to the experimental conditions, t-tests on cluster means that weight these means for cluster size are occasionally used. In fact, I too endorsed this approach as a straightforward and easily implemented way to account for clustering. It seems, however, that these weighted t-test are still anti-conservative, i.e. they find too many significant differences when there is in fact no effect. In this post, I present simulation results to illustrate this and I also correct another published error of mine."
  },
  {
    "objectID": "posts/2016-02-16-cluster-randomisation-correction/index.html#what-are-cluster-randomised-experiments-again",
    "href": "posts/2016-02-16-cluster-randomisation-correction/index.html#what-are-cluster-randomised-experiments-again",
    "title": "Experiments with intact groups: spurious significance with improperly weighted t-tests",
    "section": "What are cluster-randomised experiments again?",
    "text": "What are cluster-randomised experiments again?\nCluster-randomised experiments are experiments in which whole groups, rather than individual participants, are assigned to the experimental conditions. In education, for instance, it is often impractical or undesirable to randomly assign individual students to a new pedagogical programme or a control programme. Instead, entire classes or even entire schools or school districts are assigned to one of the programmes, and all students within the class, school or school district (‘cluster’) participate in the same programme.\nThis seemingly minor difference between cluster-randomised experiments and typical randomised experiments has enormous consequences for the way in which the data should be analysed. See the blog post on Analysing experiments with intact groups for more details."
  },
  {
    "objectID": "posts/2016-02-16-cluster-randomisation-correction/index.html#weighting-clusters-for-their-size",
    "href": "posts/2016-02-16-cluster-randomisation-correction/index.html#weighting-clusters-for-their-size",
    "title": "Experiments with intact groups: spurious significance with improperly weighted t-tests",
    "section": "Weighting clusters for their size",
    "text": "Weighting clusters for their size\nIn a paper on Analyzing randomized controlled interventions, I wrote the following on analysing cluster-randomised experiments:\n\nA conceptually straightforward approach [for taking clustering into account] is to calculate the mean (or another summary measure) of each cluster and run a t test on them rather than on the original observations. When the number of observations differs from cluster to cluster, a t test in which the cluster means are weighted for cluster size is recommended (see, e.g., Campbell, Donner, & Klar, 2007).\n\nI regret that the recommendation to weight cluster means for cluster size does not stem from M. J. Campbell et al. (2007) but from M. K. Campbell et al. (2000): “When the size of the clusters varies widely, it is preferable to carry out a weighted t-test, using cluster sizes as weights” (pp. 193-194). From there, the recommendation can be traced back to Kerry & Bland (1998).\nMore imporantly, using cluster sizes as weights does not perfectly account for violations of the independence assumption, i.e. it does not guarantee that the Type-I error rate will be at its nominal level. I noticed this problem when running some simulations for a previous blog post. You can read about the details below or skip straight to the discussion."
  },
  {
    "objectID": "posts/2016-02-16-cluster-randomisation-correction/index.html#the-simulation",
    "href": "posts/2016-02-16-cluster-randomisation-correction/index.html#the-simulation",
    "title": "Experiments with intact groups: spurious significance with improperly weighted t-tests",
    "section": "The simulation",
    "text": "The simulation\nThe full simulation code is available from GitHub. Here, I’ll just give you the main points.\nThe code first creates a dataset in individual data points form a number of clusters. The size of the cluster varies between clusters, and half of the clusters is assigned to the control condition and half to the intervention condition. There is both within- and between-cluster variance in the outcome measure (i.e., there is statistical clustering), but the intervention did not have any effect whatsoever (i.e., the null hypothesis is true).\nThen, three analyses are carried out on the data. The first analysis ignores clustering altogether: a t-test on the participants’ outcomes. The second analysis is the weighted t-test introduced above: the data analysed are the cluster means, weighted for cluster size. The third analysis is an unweighted t-test on the cluster means. The p-values of each analysis are saved and the process is repeated a number of times.\nFor this simulation, I set the number of clusters to 10 (5 in the control, 5 in the intervention condition) with cluster sizes 8, 10, 13, 14, 41, 45, 50, 62, 80 and 86. (You’re welcome to change these numbers.) The intra-class correlation coefficient, which expresses the degree of clustering, was set to 0.1. The simulation was run 10,000 times."
  },
  {
    "objectID": "posts/2016-02-16-cluster-randomisation-correction/index.html#results",
    "href": "posts/2016-02-16-cluster-randomisation-correction/index.html#results",
    "title": "Experiments with intact groups: spurious significance with improperly weighted t-tests",
    "section": "Results",
    "text": "Results\nSeeing as the null hypothesis in this simulation was true, we should expect to find a significant difference between the control and intervention conditions in only 5% of cases. The naive analysis – the one that ignored clustering – had a hugely inflated Type-I error rate of 44%, which doesn’t come as a surprise. However, the weighted t-test also had an inflated Type-I error rate: it returned spurious significance in 9% of cases. The unweighted t-test, by contrast, was on par with a 5% Type-I error rate.\nThe numbers for the naive analysis and the weighted t-test vary depending on the ICC and the cluster sizes, but what’s important is that t-tests on cluster means weighted for cluster size find too many false positives."
  },
  {
    "objectID": "posts/2016-02-16-cluster-randomisation-correction/index.html#discussion-and-conclusion",
    "href": "posts/2016-02-16-cluster-randomisation-correction/index.html#discussion-and-conclusion",
    "title": "Experiments with intact groups: spurious significance with improperly weighted t-tests",
    "section": "Discussion and conclusion",
    "text": "Discussion and conclusion\nI’m not sure where the recommendation to use t-tests with cluster means weighted for cluster size for analysing cluster-randomised experiments ultimately originates. In their book-length treatment of clusted-randomised experiments, Hayes & Moulton (2009)\nlist the weighted t-test as a theoretical alternative to the unweighted t-test on cluster means. (Actually, cluster proportions, but they claim that “[t]he extension to means and rates is obvious” (p. 178)). But the weights they propose aren’t merely the cluster sizes, but \\(m / (1 + ICC * (m - 1))\\) (where m is the cluster size and ICC the intra-class correlation coefficient). Using these weights in the simulation does result in a 5% Type-I error rate. As Hayes & Moulton note, however, this weighting requires that the ICC be known with great precision, which isn’t usually the case. Hence, they do not “generally recommend use of the weighted t-test unless there are good prior estimates of [the ICC]” (p. 179).\nIn conclusion, weighting cluster means for cluster size is not generally recommended. Unweighted t-tests on cluster means are still available as a straightforward alternative with an on-par Type-I error rate, whereas multilevel models present the analyst with more flexibility as regards the inclusion of covariates, modelling further hierarchical dependencies etc."
  },
  {
    "objectID": "posts/2016-02-16-cluster-randomisation-correction/index.html#another-correction",
    "href": "posts/2016-02-16-cluster-randomisation-correction/index.html#another-correction",
    "title": "Experiments with intact groups: spurious significance with improperly weighted t-tests",
    "section": "Another correction",
    "text": "Another correction\nIn the Analyzing randomized controlled interventions paper, I included a graph to illustrate how Type-I error rates soar when clustering is ignored (Figure 1 on page 143). When running simulations, I noted that this graph slightly exaggerated the Type-I error inflation. The reason is that my analytical derivation of the Type-I error rate contained some errors.\nLuckily, Hedges (2007) provides an analytical derivation whose results do agree with the simulated Type-I error rates. For the record, here it is in R form, along with the corrected plot:\n\ntypeI.fnc &lt;- function(k = 10, m = 20, ICC = 0.1) {\n  # Computation from Hedges 2007,\n  # where ICC is the intra-class correlation coefficient,\n  # k the number of clusters and\n  # m the number of observations per cluster\n  h &lt;- (((k*m - 2) - 2*(m-1)*ICC)^2) / (((k*m-2)*(1-ICC)^2 + m*(k*m - 2*m)*ICC^2 + 2*(k*m - 2*m)*ICC*(1-ICC)))\n  c &lt;- sqrt((((k*m-2)-(2*(m-1)*ICC))/((k*m-2)*(1+(m-1)*ICC))))\n  critical.value.t &lt;- qt(0.975, k*m-2)\n  \n  return(2*(1-pt(c*critical.value.t, h)))\n}\n\n\n\n\n\n\nFigure 1 – corrected: Type-I error rates for cluster-randomized experiments when analyzed by means of a t-test on the participants’ scores as a function of the intraclass correlation coefficient (ICC) and the number of participants per cluster (m). For this graph, the number of clusters was fixed at 10, but the Type-I error rate differs only slightly for different numbers of clusters.\n\n\n\n\nThe take-home message from both graphs is the same (ignoring clustering drastically increases Type-I error rates, more so for larger clusters and larger ICC values), but the precise Type-I error rates are somewhat lower."
  },
  {
    "objectID": "posts/2017-07-14-OtherRoadsToPower/index.html",
    "href": "posts/2017-07-14-OtherRoadsToPower/index.html",
    "title": "Abandoning standardised effect sizes and opening up other roads to power",
    "section": "",
    "text": "Numerical summaries of research findings will typically feature an indication of the sizes of the effects that were studied. These indications are often standardised effect sizes, which means that they are expressed relative to the variance in the data rather than with respect to the units in which the variables were measured. Popular standardised effect sizes include Cohen’s d, which expresses the mean difference between two groups as a proportion of the pooled standard deviation, and Pearson’s r, which expresses the difference in one variable as a proportion of its standard deviation that is associated with a change of one standard deviation of another variable. There exists a rich literature that discusses which standardised effect sizes ought to be used depending on the study’s design, how this or that standardised effect size should be adjusted for this and that bias, and how confidence intervals should be constructed around standardised effect sizes (blog post Confidence intervals for standardised mean differences). But most of this literature should be little importance to the practising scientist for the simple reason that standardised effect sizes themselves ought to be of little importance.\nIn what follows, I will defend this point of view, which I’ve outlined in two previous blog posts (Why I don’t like standardised effect sizes and More on why I don’t like standardised effect sizes), by sharing some quotes by respected statisticians and methodologists. In particular, I hope to, first, make you think about the deterimental effect that the use of standardised effect sizes entails on the interpretability of research findings and the accumulation of knowledge and, second, convince you that the use of standardised effect sizes for planning studies overly stresses sample size as the main determinant of a study’s power and that by abandoning it other roads to power can be opened up."
  },
  {
    "objectID": "posts/2017-07-14-OtherRoadsToPower/index.html#interpretability",
    "href": "posts/2017-07-14-OtherRoadsToPower/index.html#interpretability",
    "title": "Abandoning standardised effect sizes and opening up other roads to power",
    "section": "Interpretability",
    "text": "Interpretability\nWhen justifying their use of standardised effect sizes, researchers usually cite the need to be able to compare results that were obtained on different scales or to render results on scales that are difficult to understand more meaningful. I understand this argument up to a point, but I think it’s overused. Firstly, to the extent that different outcome measures for similar constructs are commonly used, it should be possible to rescale them without relying on the variance of the sample at hand. This could be done by making reference to norming studies. Moreover, standardised effect sizes should not be an excuse to ignore one’s measurements:\n\n“To work constructively with ‘raw’ regression coefficients and confidence intervals, psychologists have to start respecting the units they work with, or develop measurement units they can respect enough so that researchers in a given field or subfield can agree to use them.” (Cohen, 1994)\n\nSecondly, when different instruments are used to measure different constructs, then I think it’s actually an advantage when the measurements cannot directly be compared. Thirdly, I agree with Tukey (1969) in that I think that the increase in interpretability from standardised effect sizes is largely deceptive:\n\n“Why then are correlation coefficients so attractive? Only bad reasons seem to come to mind. Worst of all, probably, is the absence of any need to think about units for either variable. (…) [W]e think we know what r = -.7 means. Do we? How often? Sweeping things under the rug is the enemy of of good data analysis. (…) Being so disinterested in our variables that we do not care about their units can hardly be desirable.” (Tukey, 1969)\n\nIndeed, correlation coefficients in particular are regularly misinterpreted as somehow representing the slope of the function characterising the relationship between two variables (see Vanhove, 2013). I think that the apparent enhanced interpretability of standardised effect sizes stems from most researchers’ “knowing” that r = 0.10 represents a ‘small’ effect size, r = 0.30 a ‘medium’ one, and r = 0.50 a ‘large’ one according to Cohen (1992), which enables them to map any correlation coefficient somewhere on this grid. (Plonsky & Oswald, 2014, propose different reference values for L2 research, but the idea is the same.) But apart from positioning their own standardised effect size relative to the distribution of standardised effect sizes in a biased literature, most of which isn’t related to their own study, I don’t see what this buys us in terms of interpretability.\nOne situation where I grant that standardised effect sizes are more useful and fairly easy to interpret is when you want to express how much information one variable contains about another. For instance, when you want to see how collinear two or more predictor variables are so that you know whether they can sensibly be used in the same regression model, or when you want to argue that two cognitive traits may or may not be isomorphic. But more often, we’re interested in characterising the functional relationship between variables, often in terms of a causal narrative. For such a purpose, standardised effect sizes are wholly unsuited:\n\n“The major problem with correlations applied to research data is that they can not provide useful information on causal strength because they change with the degree of variability of the variables they relate. Causality operates on single instances, not on populations whose members vary. The effect of A on B for me can hardly depend on whether I’m in a group that varies greatly in A or another that does not vary at all.” (Cohen, 1994)\n\nCohen also writes that\n\n“(…) I’ve found that when dealing with variables expressed in units whose magnitude we understand, the effect size in linear relationships is better comprehended with regression than with correlation coefficients.” (Cohen, 1990)\n\nI obviously agree, and I’d like to point out that I think that “variables expressed in units whose magnitude we understand” doesn’t merely include lengths in metres and response latencies in milliseconds, but also responses on 5- or 7-point scales."
  },
  {
    "objectID": "posts/2017-07-14-OtherRoadsToPower/index.html#cumulative-science",
    "href": "posts/2017-07-14-OtherRoadsToPower/index.html#cumulative-science",
    "title": "Abandoning standardised effect sizes and opening up other roads to power",
    "section": "Cumulative science",
    "text": "Cumulative science\nStandardised effect sizes are often used in the pinnacle of cumulative science, namely meta-analyses. That said, as I’ve written in a previous blog post, standardised effect sizes can make it seem as though two studies on the same phenomenon contradict each other when they in fact found the exact same result, and vice versa. As Tukey writes,\n\n“I find the use of a correlation coefficient a dangerous symptom. It is an enemy of generalization, a focuser on the”here and now” to the exclusion of the “there and then.” Any influence that exerts selection on one variable and not on the other will shift the correlation coefficient. What usually remains constant under such circumstances is one of the regression coefficients. If we wish to seek for constancies, then, regression coefficients are much more likely to serve us than correlation coefficients.” (Tukey, 1969)\n\nI know too little of meta-analyses to have any firm views on them. But if standardised effect sizes are indispensible to the meta-analyst, they’re easy to compute on the basis of the raw effect sizes and other summaries provided, so there’s little need to other researchers to focus on them."
  },
  {
    "objectID": "posts/2017-07-14-OtherRoadsToPower/index.html#standardised-effect-sizes-in-power-analyses",
    "href": "posts/2017-07-14-OtherRoadsToPower/index.html#standardised-effect-sizes-in-power-analyses",
    "title": "Abandoning standardised effect sizes and opening up other roads to power",
    "section": "Standardised effect sizes in power analyses",
    "text": "Standardised effect sizes in power analyses\nI know come to the main point I want to make. One major use of standardised effect sizes is in discussing the statistical power of studies. Indeed, it was Cohen’s (1977) treatise on power analyses that popularised standard effect sizes in the behavioural sciences. The reason Cohen (1977) used standardised effect sizes when discussing statistical power was a practical one (see p. 11): statistical power is a function of one’s sample size and the ratio of the population-wide effect to the within-population variability. Instead of providing one table with power values for a population-wide effect of 3 units and for a within-population standard deviation of 10 units and another table for an effect of 12 units and a standard deviation of 40 units (and so on), he could just provide a table for an effect-to-standard deviation ratio of 0.3.\nCurrent discussions about statistical power are almost invariably castin terms of standardised effect sizes. The problem is that in such discussions the standardised effect size of a phenomenon in a particular social context is typically treated as immutable. That is, you can’t change the standardised effect size of the phenomenon you’re investigating at the population level. However, standardised effect sizes in fact conflate the (raw, possibly causal) effect size of what you’re investigating—which indeed you can’t change as a researcher—with the variability of the data, which you can change through optimising the research design:\n\n“[T]he control of various sources of variation through the use of improved research designs serves to increase [standardised] effect sizes as they are defined here.” (…) “Thus, operative effect sizes may be increased not only by improvement in measurement and experimental technique, but also by improved experimental designs.” (Cohen, 1977)\n\nAs a result of the assumption of immutable standardised effect sizes, discussions about statistical power overly focus on the other determinant of power, i.e., sample size. Large sample sizes are obviously a good thing, but there are other roads to high-powered studies (or to studies with high precision) that don’t get as much attention. In the next section, I’ll discuss three other roads to power."
  },
  {
    "objectID": "posts/2017-07-14-OtherRoadsToPower/index.html#other-roads-to-power",
    "href": "posts/2017-07-14-OtherRoadsToPower/index.html#other-roads-to-power",
    "title": "Abandoning standardised effect sizes and opening up other roads to power",
    "section": "Other roads to power",
    "text": "Other roads to power\n\nReducing measurement error\nFor a given raw effect size and a given sample size, studies are more powerful when there is less residual variance in the outcome variable. Much of this residual variance will be related to differences at the construct level (e.g., people differ with respect to how introvert they are or to how well they can detect grammatical rules in a miniature language). But some part of it will be due to measurement error, i.e., variance unrelated to the construct you’re interested in. If we could reduce the measurement error in the outcome variable, we’d reduce the residual variance and we’d consequently improve the study’s statistical power.\nNow, it’s easy for me to say that everyone, myself included, ought to use instruments with less measurement error. Apart from taking time and money to develop and validate, highly reliable instruments can take forbiddingly long to administrate. But there are sometimes easier ways to reduce one’s measurement error. For instance, labelling some or all of the points on a rating scale enhances its reliability (Krosnick & Presser, 2010). As another example, when the outcome variable consists of human ratings of text quality, it may be difficult to get raters to agree on a score but it may be fairly easy to recruit additional raters. By averaging the judgements of multiple raters, the ratings’ measurement error can be much reduced.\nFor further discussion about measurement error and statistical power, see Sam Schwarzkopf’s blog post (Update (2023-08-07): This external link appears to be broken.).\n\n\nStatistical control using covariates\nThe residual variance can also be reduced by statistically accounting for known sources of variability in the data. This is usually done by means of covariates. Covariates seem to have a bad reputation nowadays since they can easily be abused to turn non-significant results into significant findings. But used properly, they can work wonders power- and precision-wise. I won’t discuss covariate control in more detail here and refer to these blog posts instead:\n\nAnalysing pretest/posttest data\nThe curious neglect of covariates in discussions of statistical power\nCovariate adjustment in logistic regression — and some counterintuitive findings\nCovariate adjustment in logistic mixed models: Is it worth the effort?\n\n\n\nPurposeful selective sampling\nThe last underappreciated road to power that I’ll discuss is purposeful selective sampling: instead of sampling collecting data on whichever participants you can convince to sign up for your study, you screen the pool of potential participants and target only a subset of them. Selective sampling is particularly attractive when the outcome variable is difficult or expensive to collect (e.g., because it’s based on a task battery that takes hours to complete), but when the predictor variable of interest (or a proxy of it) can easily be collected in advance (e.g., the participants’ age or their performance on a school test they took the year before). If you’re willing to assume that a linear relationship exists between the predictor and the outcome, you can achieve excellent power for a fraction of the resources that would be needed to attain the same power using random sampling. And if you’re not willing to assume a linear relationship, selective sampling can still be highly efficient.\nTo illustrate this, let’s say you want to investigate the relationship between an easy-to-collect predictor and a difficult-to-collect outcome. Unbeknownst to you, the population-level variance–covariance matrix for this relationship is\n\\[\n\\Sigma = \\begin{pmatrix} 1 & 0.3 \\\\ 0.3 & 1\\end{pmatrix}.\n\\]\nThat is, both variables have a standard deviation of one unit at the population level and a 1-unit increase along the predictor is associated with a 0.3-unit increase in the outcome. In other words, the population-wide regression coefficient for this relationship is 0.3. Since the variables have standard deviations of one unit, the correlation coefficient at the population level is 1 as well, which simplifies the comparisons below.\nIf we wanted to have a 90% chance to detect a significant correlation between these two predictors, we would have to sample 112 participants:\n\npwr::pwr.r.test(n = NULL, r = 0.3, power = 0.9)\n\n\n     approximate correlation power calculation (arctangh transformation) \n\n              n = 111.8068\n              r = 0.3\n      sig.level = 0.05\n          power = 0.9\n    alternative = two.sided\n\n\nBut this is assuming we sampled participants randomly from the population. Instead, we could sample participants from the extremes. The figure below illustrates a scenario where you collect the predictor data for 100 participants but only go on to collect the outcome data for the 10 participants with the highest score and for the 10 participants with the lowest score, for a total sample size of 20.\nUpdate (2023-08-07): I added the R code for these simulations.\n\nlibrary(tidyverse)\nSigma &lt;- rbind(\n  c(1, 0.3)\n  , c(0.3, 1)\n)\nd &lt;- MASS::mvrnorm(n = 100, mu = c(0, 0), Sigma = Sigma)\nranks &lt;- rank(d[, 1])\nselected &lt;- ifelse((ranks &lt;= 10) | (ranks &gt;= 91), \"yes\", \"no\")\nd_tbl &lt;- tibble(\n  predictor = d[, 1]\n  , outcome = d[, 2]\n  , selected = selected\n)\n\nggplot(d_tbl,\n       aes(x = predictor,\n           y = outcome,\n           fill = selected)) +\n  geom_point(shape = 21) +\n  scale_fill_manual(values = c(\"white\", \"red\"),\n                    name = \"Participants retained\") +\n  xlab(\"predictor (screened)\") +\n  ylab(\"outcome\") +\n  labs(title = \"Sampling at the extremes\") +\n  theme(legend.position = \"bottom\")\n\n\n\n\nIn this scenario, sampling at the extremes leads to a study with 65% power. For reference, randomly sampling 20 participants from the population only gives 26% power. Obviously, power can further be increased by sampling more participants at the extremes. But more crucially, casting a wider net during screening, e.g., by screening 200 participants rather than 100, leads to an ever bigger increase in power. As the figure below shows, screening 200 participants and retaining 30 or screening 500 and retaining only 20 yields 89-90% power—a considerable improvement over random sampling in terms of efficiency! Sampling at the extremes results in larger correlation coefficients, but crucially, it doesn’t affect regression coefficients and so still allows us to correctly characterise the relationship between the two variables (see my earlier blog post as well as Baguley, 2009).\n\n# For estimating power when using extreme sampling\npower_extreme_sampling &lt;- function(n_screened, n_retained, \n                                   beta = 0.3,\n                                   sims = 10000) {\n  Sigma &lt;- rbind(c(1, 0.3), \n                 c(0.3, 1))\n  \n  one_run &lt;- function(n_screened, n_retained, Sigma) {\n    d &lt;- MASS::mvrnorm(n = n_screened, mu = c(0, 0), Sigma = Sigma)\n    ranks &lt;- rank(d[, 1])\n    d &lt;- d[ranks &lt;= ceiling(n_retained/2) | ranks &gt; n_screened - floor(n_retained/2), ]\n    summary(lm(d[, 2] ~ d[, 1]))$coefficients[2, 4]\n  }\n  \n  results &lt;- replicate(sims, one_run(n_screened, n_retained, Sigma))\n  \n  mean(results &lt; 0.05)\n}\n\n# Compute power for different parameter combinations\nparams &lt;- expand.grid(n_screened = c(100, 200, 500),\n                      n_retained = c(20, 30))\nparams$power &lt;- mapply(\n  power_extreme_sampling\n  , n_screened = params$n_screened\n  , n_retained = params$n_retained\n)\n\n# Graph\nggplot(params,\n       aes(x = factor(n_retained),\n           y = power,\n           colour = factor(n_screened),\n           group = factor(n_screened))) +\n  geom_line() +\n  geom_point() +\n  xlab(\"Participants retained\") +\n  ylab(\"Power to detect β = 0.3\") +\n  labs(title = \"Power when sampling at the extremes\") +\n  scale_colour_brewer(name = \"Participants screened\",\n                      type = \"qual\") +\n  theme(legend.position = \"bottom\")\n\n\n\n\nOne major drawback to sampling at the extremes is that you have to be willing to assume a linear relationship between the two variables. If you’re not willing to assume such a relationship, you can instead sample both at the extremes and at a couple of midway points. This way, you can check whether the relationship is indeed linear. The figure below illustrates the scenario where you screen 200 participants and then go on to collect the outcome data for the four participants with the highest predictor score, the four with the lowest predictor score, and four participants each closest to the 25th, 50th and 75th screening sample percentile, for a total of 20 participants in the final sample.\n\nn_screened &lt;- 200\nn_retained &lt;- 20\nn_sampling_points &lt;- 5\n\nd &lt;- MASS::mvrnorm(n = n_screened, mu = c(0, 0), Sigma = Sigma)\nranks &lt;- rank(d[, 1])\n\ntarget_ranks &lt;- seq(from = 1, to = n_screened, length.out = n_sampling_points)\nn_per_point &lt;- floor(n_retained / n_sampling_points)\n\n# Helper function for identifying the data points closest to the midway points\nfind_closest_indices &lt;- function(x, target_value, n) {\n  sorted_indices &lt;- order(abs(x - target_value))\n  closest_indices &lt;- sorted_indices[1:n]\n  return(closest_indices)\n}\n\n# Obtain indices of data points closest to midway points\nindices &lt;- vector()\nfor (point in target_ranks) {\n  indices &lt;- c(indices,\n               find_closest_indices(ranks, point, n_per_point))\n}\nselected &lt;- rep(FALSE, n_screened)\nselected[indices] &lt;- TRUE\n\nd_tbl &lt;- tibble(\n  predictor = d[, 1]\n  , outcome = d[, 2]\n  , selected = ifelse(selected, \"yes\", \"no\")\n)\n\nggplot(d_tbl,\n       aes(x = predictor,\n           y = outcome,\n           fill = selected)) +\n  geom_point(shape = 21) +\n  scale_fill_manual(values = c(\"white\", \"red\"),\n                    name = \"Participants retained\") +\n  xlab(\"predictor (screened)\") +\n  ylab(\"outcome\") +\n  labs(title = \"Sampling at the extremes and 3 midway points\") +\n  theme(legend.position = \"bottom\")\n\n\n\n\nIn this scenario, sampling at both the extremes and 3 midway points leads to 57% power, which is still a respectable boost relative to the paltry 26% random sampling gives you. The figure below shows that with a wide enough net, excellent power can be achieved with as few as 40–50 participants while enabling you to assess whether the relationship between the two variables is approximately linear. Picking only 2 midway points gives increases power somewhat, and different strategies for determining the midway points may be more efficient still. But the main point is that abandoning standardised effect sizes as the basis for power computations enables you to explore ways to improve the statistical power or precision of your studies other than doubling your sample size.\n\n# For estimating power when sampling at midway points\npower_midway_sampling &lt;- function(n_screened, n_retained, \n                                  beta = 0.3, n_sampling_points = 5,\n                                  sims = 10000) {\n  Sigma &lt;- rbind(c(1, 0.3), \n                 c(0.3, 1))\n  \n  # Helper function for identifying the data points closest to the midway points\n  find_closest_indices &lt;- function(x, target_value, n) {\n    sorted_indices &lt;- order(abs(x - target_value))\n    closest_indices &lt;- sorted_indices[1:n]\n    return(closest_indices)\n  }\n  \n  one_run &lt;- function(n_screened, n_retained, Sigma, n_sampling_points) {\n    d &lt;- MASS::mvrnorm(n = n_screened, mu = c(0, 0), Sigma = Sigma)\n    ranks &lt;- rank(d[, 1])\n    target_ranks &lt;- seq(from = 1, to = n_screened, length.out = n_sampling_points)\n    n_per_point &lt;- floor(n_retained / n_sampling_points)\n    \n    indices &lt;- vector()\n    for (point in target_ranks) {\n      indices &lt;- c(indices, find_closest_indices(ranks, point, n_per_point))\n    }\n    \n    selected &lt;- rep(FALSE, n_screened)\n    selected[indices] &lt;- TRUE\n\n    d &lt;- d[selected, ]\n    summary(lm(d[, 2] ~ d[, 1]))$coefficients[2, 4]\n  }\n  \n  results &lt;- replicate(sims, one_run(n_screened, n_retained, \n                                     Sigma, n_sampling_points))\n  \n  mean(results &lt; 0.05)\n}\n\n# Compute power for different parameter combinations\nparams &lt;- expand.grid(n_screened = c(100, 200, 500),\n                      n_retained = c(20, 30, 40, 50))\nparams$power &lt;- mapply(\n  power_midway_sampling\n  , n_screened = params$n_screened\n  , n_retained = params$n_retained\n  , sims = 10000\n)\n\n# Graph\nggplot(params,\n       aes(x = factor(n_retained),\n           y = power,\n           colour = factor(n_screened),\n           group = factor(n_screened))) +\n  geom_line() +\n  geom_point() +\n  xlab(\"Participants retained\") +\n  ylab(\"Power to detect β = 0.3\") +\n  labs(title = \"Power when sampling at the extremes and 3 midway points\") +\n  scale_colour_brewer(name = \"Participants screened\",\n                      type = \"qual\") +\n  theme(legend.position = \"bottom\")"
  },
  {
    "objectID": "posts/2017-07-14-OtherRoadsToPower/index.html#tldr",
    "href": "posts/2017-07-14-OtherRoadsToPower/index.html#tldr",
    "title": "Abandoning standardised effect sizes and opening up other roads to power",
    "section": "tl;dr",
    "text": "tl;dr\nStandardised effect sizes conflate raw, possibly causal effect sizes with data variability, and aren’t as readily interpretable as often believed. Discussions about statistical power based on standard effect sizes correctly stress the importance of sample size, but often gloss over other, complementary ways to design high-powered studies. Untangling raw effect size from data variability when discussing power can lead to practical recommendations to increase statistical power more efficiency."
  },
  {
    "objectID": "posts/2017-07-14-OtherRoadsToPower/index.html#references",
    "href": "posts/2017-07-14-OtherRoadsToPower/index.html#references",
    "title": "Abandoning standardised effect sizes and opening up other roads to power",
    "section": "References",
    "text": "References\nBaguley, Thom. 2009. Standardized or simple effect size: What should be reported. British Journal of Psychology 100. 603-617.\nCohen, Jacob. 1977. Statistical power analysis for the behavioral sciences (rev. edn.). New York: Academic Press.\nCohen, Jacob. 1990. Things I have learned (so far). American Psychologist 45(12). 1304-1312.\nCohen, Jacob. 1992. A power primer. Psychological Bulletin 112(1). 115-159.\nCohen, Jacob. 1994. The Earth is round (p &lt; .05). American Psychologist 49(12). 997-1003.\nKrosnick, Jon A. & Stanley Presser. 2010. Question and questionnaire design. In Peter V. Marsden & James D. Wright (eds.), Handbook of survey research (2nd edn.), 263-313. Bingley, UK: Emerald.\nPlonsky, Luke & Frederick L. Oswald. 2014. How big is “big”? Interpreting effect sizes in L2 research. Language Learning 64(4). 878-912.\nTukey, John W. 1969. Analyzing data: Sanctification or detective work. American Psychologist 24. 83-91.\nVanhove, Jan. 2013. The critical period hypothesis in second language acquisition: A statistical critique and a reanalysis. PLOS ONE 8(7). e69172."
  },
  {
    "objectID": "posts/2017-07-14-OtherRoadsToPower/index.html#software-versions",
    "href": "posts/2017-07-14-OtherRoadsToPower/index.html#software-versions",
    "title": "Abandoning standardised effect sizes and opening up other roads to power",
    "section": "Software versions",
    "text": "Software versions\n\ndevtools::session_info()\n\n─ Session info ───────────────────────────────────────────────────────────────\n setting  value\n version  R version 4.3.1 (2023-06-16 ucrt)\n os       Windows 10 x64 (build 18363)\n system   x86_64, mingw32\n ui       RTerm\n language (EN)\n collate  English_United Kingdom.utf8\n ctype    English_United Kingdom.utf8\n tz       Europe/Zurich\n date     2023-08-07\n pandoc   3.1.1 @ C:/Program Files/RStudio/resources/app/bin/quarto/bin/tools/ (via rmarkdown)\n\n─ Packages ───────────────────────────────────────────────────────────────────\n package      * version date (UTC) lib source\n cachem         1.0.8   2023-05-01 [1] CRAN (R 4.3.1)\n callr          3.7.3   2022-11-02 [1] CRAN (R 4.3.1)\n cli            3.6.1   2023-03-23 [1] CRAN (R 4.3.1)\n codetools      0.2-19  2023-02-01 [2] CRAN (R 4.3.1)\n colorspace     2.1-0   2023-01-23 [1] CRAN (R 4.3.1)\n crayon         1.5.2   2022-09-29 [1] CRAN (R 4.3.1)\n devtools       2.4.5   2022-10-11 [1] CRAN (R 4.3.1)\n digest         0.6.33  2023-07-07 [1] CRAN (R 4.3.1)\n dplyr        * 1.1.2   2023-04-20 [1] CRAN (R 4.3.1)\n ellipsis       0.3.2   2021-04-29 [1] CRAN (R 4.3.1)\n evaluate       0.21    2023-05-05 [1] CRAN (R 4.3.1)\n fansi          1.0.4   2023-01-22 [1] CRAN (R 4.3.1)\n farver         2.1.1   2022-07-06 [1] CRAN (R 4.3.1)\n fastmap        1.1.1   2023-02-24 [1] CRAN (R 4.3.1)\n forcats      * 1.0.0   2023-01-29 [1] CRAN (R 4.3.1)\n fs             1.6.3   2023-07-20 [1] CRAN (R 4.3.1)\n generics       0.1.3   2022-07-05 [1] CRAN (R 4.3.1)\n ggplot2      * 3.4.2   2023-04-03 [1] CRAN (R 4.3.1)\n glue           1.6.2   2022-02-24 [1] CRAN (R 4.3.1)\n gtable         0.3.3   2023-03-21 [1] CRAN (R 4.3.1)\n hms            1.1.3   2023-03-21 [1] CRAN (R 4.3.1)\n htmltools      0.5.5   2023-03-23 [1] CRAN (R 4.3.1)\n htmlwidgets    1.6.2   2023-03-17 [1] CRAN (R 4.3.1)\n httpuv         1.6.11  2023-05-11 [1] CRAN (R 4.3.1)\n jsonlite       1.8.7   2023-06-29 [1] CRAN (R 4.3.1)\n knitr          1.43    2023-05-25 [1] CRAN (R 4.3.1)\n labeling       0.4.2   2020-10-20 [1] CRAN (R 4.3.0)\n later          1.3.1   2023-05-02 [1] CRAN (R 4.3.1)\n lifecycle      1.0.3   2022-10-07 [1] CRAN (R 4.3.1)\n lubridate    * 1.9.2   2023-02-10 [1] CRAN (R 4.3.1)\n magrittr       2.0.3   2022-03-30 [1] CRAN (R 4.3.1)\n MASS           7.3-60  2023-05-04 [2] CRAN (R 4.3.1)\n memoise        2.0.1   2021-11-26 [1] CRAN (R 4.3.1)\n mime           0.12    2021-09-28 [1] CRAN (R 4.3.0)\n miniUI         0.1.1.1 2018-05-18 [1] CRAN (R 4.3.1)\n munsell        0.5.0   2018-06-12 [1] CRAN (R 4.3.1)\n pillar         1.9.0   2023-03-22 [1] CRAN (R 4.3.1)\n pkgbuild       1.4.2   2023-06-26 [1] CRAN (R 4.3.1)\n pkgconfig      2.0.3   2019-09-22 [1] CRAN (R 4.3.1)\n pkgload        1.3.2.1 2023-07-08 [1] CRAN (R 4.3.1)\n prettyunits    1.1.1   2020-01-24 [1] CRAN (R 4.3.1)\n processx       3.8.2   2023-06-30 [1] CRAN (R 4.3.1)\n profvis        0.3.8   2023-05-02 [1] CRAN (R 4.3.1)\n promises       1.2.0.1 2021-02-11 [1] CRAN (R 4.3.1)\n ps             1.7.5   2023-04-18 [1] CRAN (R 4.3.1)\n purrr        * 1.0.1   2023-01-10 [1] CRAN (R 4.3.1)\n R6             2.5.1   2021-08-19 [1] CRAN (R 4.3.1)\n RColorBrewer   1.1-3   2022-04-03 [1] CRAN (R 4.3.0)\n Rcpp           1.0.11  2023-07-06 [1] CRAN (R 4.3.1)\n readr        * 2.1.4   2023-02-10 [1] CRAN (R 4.3.1)\n remotes        2.4.2.1 2023-07-18 [1] CRAN (R 4.3.1)\n rlang          1.1.1   2023-04-28 [1] CRAN (R 4.3.1)\n rmarkdown      2.23    2023-07-01 [1] CRAN (R 4.3.1)\n rstudioapi     0.15.0  2023-07-07 [1] CRAN (R 4.3.1)\n scales         1.2.1   2022-08-20 [1] CRAN (R 4.3.1)\n sessioninfo    1.2.2   2021-12-06 [1] CRAN (R 4.3.1)\n shiny          1.7.4.1 2023-07-06 [1] CRAN (R 4.3.1)\n stringi        1.7.12  2023-01-11 [1] CRAN (R 4.3.0)\n stringr      * 1.5.0   2022-12-02 [1] CRAN (R 4.3.1)\n tibble       * 3.2.1   2023-03-20 [1] CRAN (R 4.3.1)\n tidyr        * 1.3.0   2023-01-24 [1] CRAN (R 4.3.1)\n tidyselect     1.2.0   2022-10-10 [1] CRAN (R 4.3.1)\n tidyverse    * 2.0.0   2023-02-22 [1] CRAN (R 4.3.1)\n timechange     0.2.0   2023-01-11 [1] CRAN (R 4.3.1)\n tzdb           0.4.0   2023-05-12 [1] CRAN (R 4.3.1)\n urlchecker     1.0.1   2021-11-30 [1] CRAN (R 4.3.1)\n usethis        2.2.2   2023-07-06 [1] CRAN (R 4.3.1)\n utf8           1.2.3   2023-01-31 [1] CRAN (R 4.3.1)\n vctrs          0.6.3   2023-06-14 [1] CRAN (R 4.3.1)\n withr          2.5.0   2022-03-03 [1] CRAN (R 4.3.1)\n xfun           0.39    2023-04-20 [1] CRAN (R 4.3.1)\n xtable         1.8-4   2019-04-21 [1] CRAN (R 4.3.1)\n yaml           2.3.7   2023-01-23 [1] CRAN (R 4.3.0)\n\n [1] C:/Users/VanhoveJ/AppData/Local/R/win-library/4.3\n [2] C:/Program Files/R/R-4.3.1/library\n\n──────────────────────────────────────────────────────────────────────────────"
  },
  {
    "objectID": "posts/2016-05-18-surviving-anova-onslaught/index.html",
    "href": "posts/2016-05-18-surviving-anova-onslaught/index.html",
    "title": "Surviving the ANOVA onslaught",
    "section": "",
    "text": "At a workshop last week, we were asked to bring along examples of good and bad academic writing. Several of the bad examples were papers where the number of significance tests was so large that the workshop participants felt that they couldn’t make sense of the Results section. It’s not that they didn’t understand each test separately but rather that they couldn’t see the forest for the trees. I, too, wish researchers would stop inundating their readers with t, F and p-values (especially in the running text), but until such time readers need to learn how to survive the ANOVA onslaught. Below I present a list of guidelines to help them with that.\n\n\nIdentify the central, genuine research questions and the corresponding hypotheses.\nResearch papers surprisingly often contain ‘padding’ research questions (RQs) that are largely unrelated to the core goal of the study. When scanning the Results section, you can usually leave aside the paragraphs about these uninteresting RQs. For example, in a report on a pretest/posttest experiment where participants were randomly assigned to conditions, you may find RQs such as Do participants in the treatment condition have different pretest scores from those in the control condition? or Do participants have higher scores on the posttest than on the pretest? Both questions are uninteresting as they don’t tell you whether the treatment actually worked.\nDraw a graph of the predictions.\nHaving identified the key RQs and hypotheses, I often find it useful to sketch how the data would look like if the researchers’ predictions panned out and what kind of data would, within reason, falsify their hypotheses. These graphs are usually simple hand-drawn line charts that illustrate the expected group differences, but I find that they help me to better understand the logic behind the study and to focus on the important analyses in the Results section.\nLook for a graph in the paper.\nIdeally, the paper will contain a graph of the main results that you can then compare with the graphs you drew yourself. Do the results seem to confirm or disconfirm the researchers’ predictions? Sometimes, a good graph will allow you to carry out the easiest of significance tests yourself: the ‘inter-ocular trauma test’–if the conclusion hits you between the eyes, it’s significant. If the results are less clear cut, you’ll need to scan the Results section for the more details, but by now, you should have a clearer idea of what you’re looking for–and what you can ignore for now. If the paper doesn’t contain a graph, you can often draw one yourself on the basis of the data provided in the tables.\nIgnore tests unrelated to the central research questions.\nResults sections sometimes contain significance tests that are unrelated to the research questions the authors formulated up front. Such tests include include balance tests in randomised experiments (e.g. The control and intervention group did not differ in terms of SES (t(36) = 0.8, p = 0.43).), tautological tests (e.g. A one-way ANOVA confirmed that participants categorised as young, middle-aged and old differed in age (F(2, 57) = 168.2, p &lt; 0.001).) as well as some less obvious cases. By and large, these tests tend to add little to nothing to the study and can be ignored the first time round. In non-randomised experiments, systematic differences on background variables between the groups may represent confounds, but these can be assessed based on the descriptive statistics and don’t need to be rubber-stamped with a significance test.\nMain effect or interaction?\nComplex designs often target the interaction between two independent variables rather than each independent variable separately. If researchers write that they’re expecting an interaction between variable A and variable B, then what they mean is that they expect A to have a different effect on the outcome variable depending on what value B has. When running the test for the interaction between A and B, statistical software also returns the significance tests for A and B separately, and researchers often report these, too. But if you’re not really interested in the effects of A and B separately, then you can ignore the tests for the main effects.\nTwo further references: interpreting main effects in the presence of an interaction; interpreting ‘removable’ interactions.\nDon’t get distracted by F1 and F2 analyses.\nUntil fairly recently, articles in psycholinguistics journals would often contain two F-tests, labelled F1 and F2, for each comparison. The goal of these double analyses was to find out whether the results would generalise not only to different participants but also to different stimuli from the ones used in the experiment. To this end, researchers calculated average scores, reaction times etc., for each participant within each ‘cell’ (combination of independent variables) and analysed these averages in a ‘subject ANOVA’ (F1). Then, again within each cell, the data were aggregated for each stimulus, and the by-stimulus averages were analysed in an ‘item ANOVA’ (F2). Findings were deemed to be generalisable when both analyses returned significance.\nF1 and F2 analyses were stopgap solutions and have now been eclipsed by more suitable methods of analysis.\nSo in case you were wondering what these F1 and F2 numbers mean in that pre-2012 paper you were reading: it’s to see whether the finding would generalise to both new participants and new stimuli.\nAsk yourself some critical questions.\n\nIs it the means you’re really interested in?\nANOVA compares group means, but when the data are strongly skewed or bimodal (= double-humped), means may be poor indicators of the tendencies in the data. Ideally, you’d be able to tell from the graphs how well the group means capture the tendencies in the data, but often all you get is a barplot with the group means.\nAre there dependencies in the data that the analysis doesn’t account for?\nIf the paper reports on a cluster-randomised experiment (e.g. an intervention study in which whole classes were assigned to the same condition), then these clusters (e.g. classes) need to be somehow taken into consideration in the analysis. Not doing so can dramatically affect the results. Clustering could also occur when the intervention involves students interacting with each other, for instance when learning vocabulary in a communicative task.\nAre the groups real groups or are they the result of arbitrarily cutting up a continuous variable?\nCutting up some continuous variable such as age so that participants can be categorised into ANOVA-friendly groups has its problems on the statistical side. Furthermore, when interpreting the results, discretising continuous variables may sometimes tempt the researchers or readers to infer threshold effects where none exist.\nIs the outcome variable really continuous or was is it binary variable converted to percentages?\nANOVAs and t-tests work for continuous outcome variables; binary outcome variables (e.g. accuracy) call for different statistical approaches.\nHow generous are the researchers when interpreting their results?\nRunning a test is one thing, interpreting its result is another. Common shortcuts include interpreting ‘no significant difference’ to mean ‘equal’, interpreting differences in significance and reading a lot into post-hoc analyses (see ‘researcher degrees of freedom’ and ‘the garden of forking paths’).\n\n\nThis may not cover all the bases, but I reckon it covers enough of them to get started."
  },
  {
    "objectID": "posts/2019-04-16-walkthrough-p-value/index.html",
    "href": "posts/2019-04-16-walkthrough-p-value/index.html",
    "title": "Walkthrough: A significance test for a two-group comparison",
    "section": "",
    "text": "I wrote an R function that’s hopefully useful to teach students what significance tests do and how they can and can’t be interpreted.\nUpdate (2023-08-06): By now, this function has long been integrated in the cannonball package. I’ve edited this blog post correspondingly. The installation instructions for the cannonball package are available from its GitHub page.\nThe basic use is simple. Just run walkthrough_p() and follow the on-screen instructions.\ncannonball::walkthrough_p()"
  },
  {
    "objectID": "posts/2019-04-16-walkthrough-p-value/index.html#what-does-the-function-do",
    "href": "posts/2019-04-16-walkthrough-p-value/index.html#what-does-the-function-do",
    "title": "Walkthrough: A significance test for a two-group comparison",
    "section": "What does the function do?",
    "text": "What does the function do?\nData are generated from a normal distribution with the requested standard deviation. Then, the data points are randomly assigned to two equal-sized groups. Data points in the intervention group receive a boost as specified by diff. Finally, a significance test is run on the data.\nBy default, the significance test is a two-sample Student’s t-test. Technically, the p-value from this test is the probability that a t-statistic larger than the one observed would’ve been observed if only chance were at play, but the walkthrough text says that is the probability that a mean difference larger than the one observed would’ve been observed if only chance were at play. That is, I use the t-test as an approximation to a permutation test. Switch on pedant mode if you want to run a permutation test."
  },
  {
    "objectID": "posts/2019-04-16-walkthrough-p-value/index.html#parameters",
    "href": "posts/2019-04-16-walkthrough-p-value/index.html#parameters",
    "title": "Walkthrough: A significance test for a two-group comparison",
    "section": "Parameters",
    "text": "Parameters\n\nn: the number of data points per group\ndiff: The boost that participants in the intervention group receive.\nsd: The standard deviation of the normal distribution from which the data were generated.\nshowdata: Do you want to output a dataframe containing the plotted data (TRUE) or not (FALSE, default)?\npedant: Do you want to run the significance test in pedant mode (TRUE; performs a permutation test) or not (FALSE, default; performs Student’s t-test)?"
  },
  {
    "objectID": "posts/2019-04-16-walkthrough-p-value/index.html#examples",
    "href": "posts/2019-04-16-walkthrough-p-value/index.html#examples",
    "title": "Walkthrough: A significance test for a two-group comparison",
    "section": "Examples",
    "text": "Examples\n\ncannonball::walkthrough_p(n = 12, diff = 0.2, sd = 1.3)\n\n# Save data and double check results\ndat &lt;- cannonball::walkthrough_p(n = 10, diff = 0.2, \n                                 sd = 2, showdata = TRUE)\nt.test(score ~ group, data = dat, var.equal = TRUE)\n\n# Run in pedant mode (= permutation test)\ndat &lt;- cannonball::walkthrough_p(n = 13, diff = 1, sd = 4, \n                                 pedant = TRUE, showdata = TRUE)\nt.test(score ~ group, data = dat, var.equal = TRUE)"
  },
  {
    "objectID": "posts/2020-06-12-stats-course/index.html",
    "href": "posts/2020-06-12-stats-course/index.html",
    "title": "Interpreting regression models: a reading list",
    "section": "",
    "text": "Last semester I taught a class for PhD students and collaborators that focused on how the output of regression models is to be interpreted. Most participants had at least some experience with fitting regression models, but I had noticed that they were often unsure about the precise statistical interpretation of the output of these models (e.g., What does this parameter estimate of 1.2 correspond to in the data?). Moreover, they were usually a bit too eager to move from the model output to a subject-matter interpretation (e.g., What does this parameter estimate of 1.2 tell me about language learning?). I suspect that the same goes for many applied linguists, and social scientists more generally, so below I provide an overview of the course contents as well as the reading list."
  },
  {
    "objectID": "posts/2020-06-12-stats-course/index.html#course-content",
    "href": "posts/2020-06-12-stats-course/index.html#course-content",
    "title": "Interpreting regression models: a reading list",
    "section": "Course content",
    "text": "Course content\n\nWeek 1: Uses of statistical models: Description vs. prediction vs. inference\nRegression models have three main uses. The first is to describe the data at hand. The difficulty here mostly consists in figuring out what aspects of the data the parameter estimates reflect. Weeks 6, 7, 8 and 11 are devoted to statistical interpretation of model parameters and how variables can be recoded so that the model output aligns more closely with the research questions.\nHowever, the main use of regression models in the social sciences is to draw inferences, usually causal ones. Moving from a descriptive to causal interpretation of a statistical model requires making additional assumptions. Weeks 2 through 5 are devoted to a tool (directed acyclic graphs) that allows you to make explicit the assumptions you’re willing to make about the causal relationships between your variables and that allows you to derive from these assumptions any further permissible causal claims. Another type of inference is the move from observable quantities (e.g., test scores) to unobervables (e.g., language skills). Weeks 10 and 11 are devoted to this topic.\nThe third use is to use the model to make predictions about new data. This week’s text (Shmueli 2010) explains why a model that has been optimised for making predictions about new data may be all but worthless for inference, and why a model that has been optimised for inference may not yield the best possible predictions. The take-home points are that when planning a research project, you need to be crystal-clear what its main goal is (e.g., causal inference or prediction) and that you should be careful not to assume that a model selected for its predictive power is best-suited for drawing causal conclusions.\n\nText: Shmueli (2010).\nFurther reading: Breiman (2001); Yarkoni and Westfall (2017).\n\n\n\nWeek 2: Causal inferences from observational data (I)\nThe texts for weeks 2 through 5 introduce directed acyclic graphs (DAGs) and go through numerous examples for them. DAGs are useful for identifying the variables that you should control for and the ones you should not control for if you want to estimate some causal relationship in your data. (Some researchers seem to assume that the more variables you control for, the better, but controlling for the wrong variables can mess up your inferences entirely.) This, of course, is most useful when you’re still planning your research project, because otherwise you may find that you need to control for a variable that you didn’t collect, or that you controlled (on purpose or by accident) for a variable you shouldn’t have controlled for.\n\nText: McElreath (2020), Chapter 5.\n\n\n\nWeek 3: Causal inferences from observational data (II)\n\nText: McElreath (2020), Chapter 6.\n\n\n\nWeek 4: Causal inferences from observational data (III)\n\nText: Rohrer (2018).\n\n\n\nWeek 5: Causal inferences from observational data (IV)\n\nNo obligatory reading.\nFurther reading: Elwert (2013).\n\n\n\nWeek 6: Understanding parameter estimates (I)\nLeaving causal interpretations aside, what do all those numbers in the output of a regression model actually express? DeBruine and Barr (2019) explain how you can analyse simulated datasets to learn which parameter estimates in the simulation correspond to which parameter settings in the simulation set-up.\nA related point that I highlighted in class was that the random effect estimates as well as the BLUPs in mixed-effects models should always be interpreted conditionally on the fixed effects in the model. This is true of all estimates in regression models, but people tend to have more difficulties in interpreting random effects and BLUPS. Another point was that you can also gain a better understanding of what the model parameters express by first fitting the model on your data and then having this model predict new data. By figuring out how the model came up with these predictions, you learn what each parameter estimate literally means.\n\nText: DeBruine and Barr (2019).\n\n\n\nWeek 7: Understanding parameter estimates (II)\nWeeks 7 and 8 were devoted to contrast coding, i.e., how you can recode non-numeric predictors such that the model’s output aligns more closely with what you want to know. I’ve recently blogged about contrast coding, and I was surprised I didn’t learn about this useful technique until 2020 (of all years).\n\nText: Schad et al. (2020), up to and including the section What makes a good set of contrasts?\n\n\n\nWeek 8: Understanding parameter estimates (III)\n\nNo obligatory reading.\nFurther reading: Schad et al. (2020), from the section A closer look at hypothesis and contrast matrices.\n\n\n\nWeek 9: Consequences of measurement error (I)\nThe measured variables included in a model are often but approximations of what is actually of interest. For instance, you may be interested in the learners’ L2 skills, but what you’ve measured is their performance on an L2 test. The test results will only approximately reflect the learners’ true skills. Interpreting the output of a model, which may be valid at the level of the observed variables, in terms of such unobserved but inferred constructs is fraught with difficulties that researchers and consumers of research need to be aware of.\nThe reading for week 9 deals with some consequences of measurement error on a predictor variable. The reading of week 10 doesn’t strictly deal with measurement error but with the mapping of the observed outcome variable on the unobserved construct of interest and how it affects the interpretation of interactions.\n\nText: Westfall and Yarkoni (2016).\nFurther reading: Berthele and Vanhove (2020).\n\n\n\nWeek 10: Consequences of measurement error (II)\n\nText: Wagenmakers et al. (2012).\nFurther reading: Wagenmakers (2015).\n\n\n\nWeek 11: Understanding parameter estimates (IV)\nLogistic regression models can be difficult to understand, and the linear probability model (i.e., ordinary linear regression) isn’t to be dismissed out of hand when working with binary data. A related blog post is Interactions in logistic regression models.\n\nText: Huang (2019).\n\n\n\nWeek 12: Translating verbal research questions into quantitative hypotheses\nIn week 12 I went through some examples of verbal research questions or hypotheses that at first blush seem pretty well delineated. On closer inspection, however, it becomes clear that radically different patterns in the data would yield the same answer to these questions, and that the research questions or hypotheses were, in fact, underspecified. Drawing several possible data patterns and interpreting them in light of your literal research question or hypothesis can help you rephrase that question or hypothesis less ambiguously.\nNo texts.\n\n\nWeek 13: Ascension (no class)\n\n\nWeek 14: Take-home points + working reproducibly\nFor the last week, I stressed the following take-home points from this course:\n\nBe crystal-clear about the main aim of your statistical model: Describing the data, predicting new data, or drawing inferences about causality or unobserved phenomena? Plan accordingly by identifying the factors that must be controlled for and those that mustn’t be controlled for.\nAnticipate the consequences of measurement error. If measurement error could mess up the interpretation of the results, try to collect several indictators of the constructs of interest and adopt a latent variable approach.\nOutline precisely how you’d interpret the possible patterns in the data in terms of your research question.\nIf a regression model is necessary, recode your predictors so that you can interpret the parameter estimates directly in terms of your research question.\nAnalyse simulated data if you’re unsure what the model’s parameter estimates correspond to.\nKeep in mind that parameter estimates are always to be interpreted conditionally on the other predictors in the model. I suspect that lots of counterintuitive findings stem from researchers interpreting their parameter estimates unconditionally.\n\nI also showed how you can make your analyses reproducible by working with RStudio projects, the here package, and R Markdown.\nNo texts."
  },
  {
    "objectID": "posts/2020-06-12-stats-course/index.html#references",
    "href": "posts/2020-06-12-stats-course/index.html#references",
    "title": "Interpreting regression models: a reading list",
    "section": "References",
    "text": "References\nBerthele, Raphael and Jan Vanhove. 2020. What would disprove interdependence? Lessons learned from a study on biliteracy in Portuguese heritage language speakers in Switzerland. International Journal of Bilingual Education and Bilingualism 23(5). 550-566.\nBreiman, Leo. 2001. Statistical modeling: The two cultures. Statistical Science 16. 199-215.\nDeBruine, Lisa M. & Dale J. Barr. 2019. Understanding mixed effects models through data simulation. PsyArXiv.\nElwert, Felix. 2013. Graphical causal models. In S. L. Morgan (ed.), Handbook of Causal Analysis for Social Research, pp. 245-273. Dordrecht, The Netherlands: Springer.\nHuang, Francis L. 2019. Alternatives to logistic regression models in experimental studies. Journal of Experimental Education.\nMcElreath, Richard. 2020. Statistical rethinking: A Bayesian course with examples in R and Stan, 2nd edn. Boca Raton, FL: CRC Press.\nRohrer, Julia. 2018. Thinking clearly about correlations and causation: Graphical causal models for observational data. Advances in Methods and Practices in Psychological Science 1(1). 27-42.\nSchad, Daniel J., Shravan Vasishth, Sven Hohenstein and Reinhold Kliegl. 2020. How to capitalize on a priori contrasts in linear (mixed) models: A tutorial. Journal of Memory and Language 110.\nShmueli, Galit. 2010. To explain or to predict? Statistical Science 25. 289-310.\nWagenmakers, Eric-Jan. 2015. A quartet of interactions. Cortex 73. 334-335.\nWagenmakers, Eric-Jan, Angelos-Miltiadis Krypotos, Amy H. Criss and Geoff Iverson. 2012. On the interpretation of removable interactions: A survey of the field 33 years after Lotus. Memory & Cognition 40. 145-160.\nWestfall, Jacob and Tal Yarkoni. 2016. Statistically controlling for confounding constructs is harder than you think. PLoS ONE 11(3). e0152719.\nYarkoni, Tal and Jacob Westfall. 2017. Choosing prediction over explanation in psychology: Lessons from machine learning. Perspectives on Psychological Science 12. 1100-1122."
  },
  {
    "objectID": "posts/2018-12-20-contrasts-interaction/index.html",
    "href": "posts/2018-12-20-contrasts-interaction/index.html",
    "title": "Baby steps in Bayes: Recoding predictors and homing in on specific comparisons",
    "section": "",
    "text": "Interpreting models that take into account a host of possible interactions between predictor variables can be a pain, especially when some of the predictors contain more than two levels. In this post, I show how I went about fitting and then making sense of a multilevel model containing a three-way interaction between its categorical fixed-effect predictors. To this end, I used the brms package, which makes it relatively easy to fit Bayesian models using a notation that hardly differs from the one used in the popular lme4 package. I won’t discuss the Bayesian bit much here (I don’t think it’s too important), and I will instead cover the following points:"
  },
  {
    "objectID": "posts/2018-12-20-contrasts-interaction/index.html#the-data",
    "href": "posts/2018-12-20-contrasts-interaction/index.html#the-data",
    "title": "Baby steps in Bayes: Recoding predictors and homing in on specific comparisons",
    "section": "The data",
    "text": "The data\nFor a longitudinal project, 328 children wrote narrative and argumentative texts in Portuguese at three points in time. About a third of the children hailed from Portugal, about a third were children of Portuguese heritage living in the French-speaking part of Switzerland, and about a third were children of Portuguese heritage living in the German-speaking part of Switzerland. Not all children wrote both kinds of texts at all three points in time, and 1,040 texts were retained for the analysis. For each text, we computed the Guiraud index, which is a function of the number of words (tokens) and the number of different words (types) in the texts. Higher values are assumed to reflect greater diversity in vocabulary use.\nIf you want to know more about this project, check out Bonvin et al. (2018), Lambelet et al. (2017a,b) and Vanhove et al. (2019); you’ll find the references at the bottom of this page.\nUpdate (2023-08-06): I ran all of the R code again with newer software versions when converting the format of this blog.\nRead in the data:\n\n# Load tidyverse suite\nlibrary(tidyverse)\n\n# Read in data from my webspace\nd &lt;- read_csv(\"http://homeweb.unifr.ch/VanhoveJ/Pub/Data/portuguese_guiraud.csv\")\n\n# Need to code factors explicitly\nd$Group    &lt;- factor(d$Group)\nd$TextType &lt;- factor(d$TextType)\nd$Time     &lt;- factor(d$Time)\n\nstr(d)\n\nspc_tbl_ [1,040 × 6] (S3: spec_tbl_df/tbl_df/tbl/data.frame)\n $ Group   : Factor w/ 3 levels \"monolingual Portuguese\",..: 1 1 1 1 1 1 1 1 1 1 ...\n $ Class   : chr [1:1040] \"monolingual Portuguese_AI\" \"monolingual Portuguese_AI\" \"monolingual Portuguese_AI\" \"monolingual Portuguese_AI\" ...\n $ Child   : chr [1:1040] \"monolingual Portuguese_AI_1\" \"monolingual Portuguese_AI_1\" \"monolingual Portuguese_AI_1\" \"monolingual Portuguese_AI_10\" ...\n $ Time    : Factor w/ 3 levels \"T1\",\"T2\",\"T3\": 1 1 2 1 1 3 3 1 3 1 ...\n $ TextType: Factor w/ 2 levels \"argumentative\",..: 1 2 2 1 2 1 2 2 2 1 ...\n $ Guiraud : num [1:1040] 4.73 5.83 3.9 4.22 4.57 ...\n - attr(*, \"spec\")=\n  .. cols(\n  ..   Group = col_character(),\n  ..   Class = col_character(),\n  ..   Child = col_character(),\n  ..   Time = col_character(),\n  ..   TextType = col_character(),\n  ..   Guiraud = col_double()\n  .. )\n - attr(*, \"problems\")=&lt;externalptr&gt; \n\nsummary(d)\n\n                    Group        Class              Child           Time    \n monolingual Portuguese:360   Length:1040        Length:1040        T1:320  \n Portuguese-French     :360   Class :character   Class :character   T2:340  \n Portuguese-German     :320   Mode  :character   Mode  :character   T3:380  \n                                                                            \n                                                                            \n                                                                            \n          TextType      Guiraud    \n argumentative:560   Min.   :2.32  \n narrative    :480   1st Qu.:3.93  \n                     Median :4.64  \n                     Mean   :4.75  \n                     3rd Qu.:5.48  \n                     Max.   :8.43  \n\n\nLet’s also plot the data. Incidentally, and contrary to popular belief, I don’t write ggplot code such as this from scratch. What you see is the result of drawing and redrawing (see comments).\n\n# Plot Guiraud scores\nggplot(d,\n       aes(x = Time,\n           y = Guiraud,\n           # reorder: sort the Groups by their median Guiraud value\n           fill = reorder(Group, Guiraud, median))) +\n  # I prefer empty (shape = 1) to filled circles (shape = 16).\n  geom_boxplot(outlier.shape = 1) +\n  facet_grid(. ~ TextType) +\n  # The legend name (\"Group\") seems superfluous, so suppress it;\n  # the default colours contain red and green, which can be hard to\n  #  distinguish for some people.\n  scale_fill_brewer(name = element_blank(), type = \"qual\") +\n  # I prefer the black and white look to the default grey one.\n  theme_bw() +\n  # Put the legend at the bottom rather than on the right\n  theme(legend.position = \"bottom\")\n\n\n\n\nFigure 1. The texts’ Guiraud values by time of data collection, text type, and language background."
  },
  {
    "objectID": "posts/2018-12-20-contrasts-interaction/index.html#a-multilevel-model-with-treatment-coding",
    "href": "posts/2018-12-20-contrasts-interaction/index.html#a-multilevel-model-with-treatment-coding",
    "title": "Baby steps in Bayes: Recoding predictors and homing in on specific comparisons",
    "section": "A multilevel model with treatment coding",
    "text": "A multilevel model with treatment coding\nOur data are nested: Each child wrote up to 6 texts, and the data were collected in classes, with each child belong to one class. It’s advisable to take such nesting into account since you may end up overestimating your degree of certainty about the results otherwise. I mostly use lme4’s lmer() and glmer() functions to handle such data, but as will become clearer in a minute, brms’s brm() function offers some distinct advantages. So let’s load that package:\n\nlibrary(brms)\n\n\nFitting the model\nWe’ll fit a model with a three-way fixed-effect interaction between Time, TextType and Group as well as with by-Child and by-Class random intercepts. In order to take into account the possibility that children vary in the development of their lexical diversity, we add a random slope of Time by Child, and in order to take into account the possibility that their lexical diversity varies by text type, we do the same for TextType. Similarly, we add by-Class random slopes for Time and TextType.\n\nm_default &lt;- brm(Guiraud ~ Time*TextType*Group +\n                   (1 + TextType + Time|Class) +\n                   (1 + TextType + Time|Child),\n                 cores = 4, iter = 4000,\n                 silent = 2,\n                 control = list(adapt_delta = 0.95),\n                 data = d)\n\nRunning /usr/lib/R/bin/R CMD SHLIB foo.c\nusing C compiler: ‘gcc (Ubuntu 11.4.0-1ubuntu1~22.04) 11.4.0’\ngcc -I\"/usr/share/R/include\" -DNDEBUG   -I\"/home/jan/R/x86_64-pc-linux-gnu-library/4.3/Rcpp/include/\"  -I\"/home/jan/R/x86_64-pc-linux-gnu-library/4.3/RcppEigen/include/\"  -I\"/home/jan/R/x86_64-pc-linux-gnu-library/4.3/RcppEigen/include/unsupported\"  -I\"/home/jan/R/x86_64-pc-linux-gnu-library/4.3/BH/include\" -I\"/home/jan/R/x86_64-pc-linux-gnu-library/4.3/StanHeaders/include/src/\"  -I\"/home/jan/R/x86_64-pc-linux-gnu-library/4.3/StanHeaders/include/\"  -I\"/home/jan/R/x86_64-pc-linux-gnu-library/4.3/RcppParallel/include/\"  -I\"/home/jan/R/x86_64-pc-linux-gnu-library/4.3/rstan/include\" -DEIGEN_NO_DEBUG  -DBOOST_DISABLE_ASSERTS  -DBOOST_PENDING_INTEGER_LOG2_HPP  -DSTAN_THREADS  -DUSE_STANC3 -DSTRICT_R_HEADERS  -DBOOST_PHOENIX_NO_VARIADIC_EXPRESSION  -DBOOST_NO_AUTO_PTR  -include '/home/jan/R/x86_64-pc-linux-gnu-library/4.3/StanHeaders/include/stan/math/prim/fun/Eigen.hpp'  -D_REENTRANT -DRCPP_PARALLEL_USE_TBB=1       -fpic  -g -O2 -ffile-prefix-map=/build/r-base-1upgAf/r-base-4.3.1=. -fstack-protector-strong -Wformat -Werror=format-security -Wdate-time -D_FORTIFY_SOURCE=2  -c foo.c -o foo.o\nIn file included from /home/jan/R/x86_64-pc-linux-gnu-library/4.3/RcppEigen/include/Eigen/Core:88,\n                 from /home/jan/R/x86_64-pc-linux-gnu-library/4.3/RcppEigen/include/Eigen/Dense:1,\n                 from /home/jan/R/x86_64-pc-linux-gnu-library/4.3/StanHeaders/include/stan/math/prim/fun/Eigen.hpp:22,\n                 from &lt;command-line&gt;:\n/home/jan/R/x86_64-pc-linux-gnu-library/4.3/RcppEigen/include/Eigen/src/Core/util/Macros.h:628:1: error: unknown type name ‘namespace’\n  628 | namespace Eigen {\n      | ^~~~~~~~~\n/home/jan/R/x86_64-pc-linux-gnu-library/4.3/RcppEigen/include/Eigen/src/Core/util/Macros.h:628:17: error: expected ‘=’, ‘,’, ‘;’, ‘asm’ or ‘__attribute__’ before ‘{’ token\n  628 | namespace Eigen {\n      |                 ^\nIn file included from /home/jan/R/x86_64-pc-linux-gnu-library/4.3/RcppEigen/include/Eigen/Dense:1,\n                 from /home/jan/R/x86_64-pc-linux-gnu-library/4.3/StanHeaders/include/stan/math/prim/fun/Eigen.hpp:22,\n                 from &lt;command-line&gt;:\n/home/jan/R/x86_64-pc-linux-gnu-library/4.3/RcppEigen/include/Eigen/Core:96:10: fatal error: complex: No such file or directory\n   96 | #include &lt;complex&gt;\n      |          ^~~~~~~~~\ncompilation terminated.\nmake: *** [/usr/lib/R/etc/Makeconf:191: foo.o] Error 1\n\n\n\n\nInterpreting the parameter estimates\n\nsummary(m_default)\n\n Family: gaussian \n  Links: mu = identity; sigma = identity \nFormula: Guiraud ~ Time * TextType * Group + (1 + TextType + Time | Class) + (1 + TextType + Time | Child) \n   Data: d (Number of observations: 1040) \n  Draws: 4 chains, each with iter = 4000; warmup = 2000; thin = 1;\n         total post-warmup draws = 8000\n\nGroup-Level Effects: \n~Child (Number of levels: 328) \n                                 Estimate Est.Error l-95% CI u-95% CI Rhat\nsd(Intercept)                        0.42      0.05     0.33     0.52 1.00\nsd(TextTypenarrative)                0.26      0.09     0.07     0.43 1.01\nsd(TimeT2)                           0.09      0.07     0.00     0.25 1.01\nsd(TimeT3)                           0.39      0.09     0.19     0.55 1.01\ncor(Intercept,TextTypenarrative)     0.23      0.28    -0.26     0.80 1.00\ncor(Intercept,TimeT2)               -0.10      0.41    -0.80     0.74 1.00\ncor(TextTypenarrative,TimeT2)        0.06      0.43    -0.77     0.82 1.00\ncor(Intercept,TimeT3)                0.16      0.24    -0.25     0.67 1.01\ncor(TextTypenarrative,TimeT3)       -0.06      0.30    -0.60     0.60 1.00\ncor(TimeT2,TimeT3)                   0.05      0.45    -0.79     0.83 1.04\n                                 Bulk_ESS Tail_ESS\nsd(Intercept)                        2748     4437\nsd(TextTypenarrative)                 462     1266\nsd(TimeT2)                           1314     2095\nsd(TimeT3)                            523      969\ncor(Intercept,TextTypenarrative)      697     1957\ncor(Intercept,TimeT2)                5842     5625\ncor(TextTypenarrative,TimeT2)        3522     4685\ncor(Intercept,TimeT3)                 474     1509\ncor(TextTypenarrative,TimeT3)         546      696\ncor(TimeT2,TimeT3)                    152      478\n\n~Class (Number of levels: 25) \n                                 Estimate Est.Error l-95% CI u-95% CI Rhat\nsd(Intercept)                        0.16      0.08     0.01     0.33 1.00\nsd(TextTypenarrative)                0.24      0.08     0.09     0.42 1.00\nsd(TimeT2)                           0.11      0.07     0.00     0.28 1.00\nsd(TimeT3)                           0.10      0.07     0.00     0.28 1.00\ncor(Intercept,TextTypenarrative)    -0.13      0.37    -0.76     0.66 1.00\ncor(Intercept,TimeT2)               -0.09      0.42    -0.83     0.74 1.00\ncor(TextTypenarrative,TimeT2)        0.07      0.40    -0.72     0.78 1.00\ncor(Intercept,TimeT3)                0.11      0.43    -0.72     0.84 1.00\ncor(TextTypenarrative,TimeT3)       -0.14      0.42    -0.84     0.71 1.00\ncor(TimeT2,TimeT3)                   0.11      0.44    -0.76     0.85 1.00\n                                 Bulk_ESS Tail_ESS\nsd(Intercept)                        1094     1326\nsd(TextTypenarrative)                1827     1824\nsd(TimeT2)                           1983     2669\nsd(TimeT3)                           2187     2828\ncor(Intercept,TextTypenarrative)     1479     2428\ncor(Intercept,TimeT2)                4581     5429\ncor(TextTypenarrative,TimeT2)        5598     5821\ncor(Intercept,TimeT3)                5042     4693\ncor(TextTypenarrative,TimeT3)        5856     5884\ncor(TimeT2,TimeT3)                   4193     5326\n\nPopulation-Level Effects: \n                                                Estimate Est.Error l-95% CI\nIntercept                                           5.25      0.13     4.99\nTimeT2                                              0.57      0.13     0.31\nTimeT3                                              0.91      0.14     0.64\nTextTypenarrative                                  -0.26      0.17    -0.60\nGroupPortugueseMFrench                             -1.06      0.17    -1.40\nGroupPortugueseMGerman                             -1.30      0.17    -1.63\nTimeT2:TextTypenarrative                           -0.24      0.16    -0.56\nTimeT3:TextTypenarrative                           -0.06      0.17    -0.39\nTimeT2:GroupPortugueseMFrench                      -0.55      0.18    -0.91\nTimeT3:GroupPortugueseMFrench                      -0.38      0.19    -0.74\nTimeT2:GroupPortugueseMGerman                      -0.64      0.19    -1.00\nTimeT3:GroupPortugueseMGerman                      -0.23      0.19    -0.60\nTextTypenarrative:GroupPortugueseMFrench            0.14      0.24    -0.32\nTextTypenarrative:GroupPortugueseMGerman            0.17      0.23    -0.29\nTimeT2:TextTypenarrative:GroupPortugueseMFrench     0.37      0.24    -0.11\nTimeT3:TextTypenarrative:GroupPortugueseMFrench     0.25      0.24    -0.23\nTimeT2:TextTypenarrative:GroupPortugueseMGerman     0.55      0.25     0.05\nTimeT3:TextTypenarrative:GroupPortugueseMGerman     0.27      0.25    -0.23\n                                                u-95% CI Rhat Bulk_ESS Tail_ESS\nIntercept                                           5.50 1.00     2481     3890\nTimeT2                                              0.83 1.00     2430     4080\nTimeT3                                              1.18 1.00     2750     4480\nTextTypenarrative                                   0.07 1.00     2323     3570\nGroupPortugueseMFrench                             -0.72 1.00     2655     3588\nGroupPortugueseMGerman                             -0.96 1.00     2574     4200\nTimeT2:TextTypenarrative                            0.08 1.00     2586     4743\nTimeT3:TextTypenarrative                            0.27 1.00     2454     4467\nTimeT2:GroupPortugueseMFrench                      -0.18 1.00     2706     3864\nTimeT3:GroupPortugueseMFrench                       0.00 1.00     3087     5057\nTimeT2:GroupPortugueseMGerman                      -0.28 1.00     2642     4234\nTimeT3:GroupPortugueseMGerman                       0.15 1.00     3000     4337\nTextTypenarrative:GroupPortugueseMFrench            0.62 1.00     2668     4283\nTextTypenarrative:GroupPortugueseMGerman            0.62 1.00     2537     4079\nTimeT2:TextTypenarrative:GroupPortugueseMFrench     0.84 1.00     2969     5155\nTimeT3:TextTypenarrative:GroupPortugueseMFrench     0.72 1.00     2858     4537\nTimeT2:TextTypenarrative:GroupPortugueseMGerman     1.03 1.00     2896     4577\nTimeT3:TextTypenarrative:GroupPortugueseMGerman     0.76 1.00     2666     4880\n\nFamily Specific Parameters: \n      Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS\nsigma     0.60      0.02     0.55     0.65 1.01      730     1692\n\nDraws were sampled using sampling(NUTS). For each parameter, Bulk_ESS\nand Tail_ESS are effective sample size measures, and Rhat is the potential\nscale reduction factor on split chains (at convergence, Rhat = 1).\n\n\nThe output looks pretty similar to what we’d obtain when using lmer(), but let’s review what these estimates actually refer to. By default, R uses treatment coding. This entails that the Intercept refers to a specific combination of factors: the combination of all reference levels. Again by default, the reference levels are chosen alphabetically:\n\nTime consists of three levels (T1, T2, T3); for alphabetical reasons, T1 is chosen as the default reference level.\nGroup also consists of three levels (monolingual Portuguese, Portuguese-French, Portuguese-German); monolingual Portuguese is chosen as the default level.\nTextType consists of two levels (argumentative, narrative); argumentative is the default reference level.\n\nThe Intercept, then, shows the modelled mean Guiraud value of argumentative texts written by monolingual Portuguese children at T1: 5.25.\nIf you’re unsure which factor level was used as the reference level, you can use the contrasts() function. The reference level is the one in whose rows only zeroes occur.\n\ncontrasts(d$Group)\n\n                       Portuguese-French Portuguese-German\nmonolingual Portuguese                 0                 0\nPortuguese-French                      1                 0\nPortuguese-German                      0                 1\n\n\nCrucially, all other estimated effects are computed with respect to this intercept. That is, TimeT2 (0.57) shows the difference between T1 and T2 for monolingual Portuguese children writing argumentative texts. Similarly, TimeT3 (0.91) shows the difference between T1 and T3 for monolingual Portuguese children writing argumentative texts, and TextTypenarrative (-0.27) shows the difference between the mean Guiraud values of argumentative and narrative texts written by monolingual Portuguese children writing at T1. The texts written by the Portuguese-German and Portuguese-French bilinguals don’t enter into these estimates.\nNow, it’s possible to piece together the mean values associated with each combination of predictor values, but questions such as the following remain difficult to answer with just these estimates at hand:\n\nWhat’s the overall difference between T2 and T3 and its uncertainty?\nWhat’s the overall difference between the Guiraud values of texts written by Portuguese-French and Portuguese-German children and its uncertainty?\n…\n\nWe’ll tackle these questions in a minute; for now, the point is merely that the estimated parameters above all refer to highly specific comparisons that may not be the most relevant.\n\n\nPlotting the fitted values and the uncertainty about them\nWhen working with brms, it’s relatively easy to obtain the modelled average outcome value for each combination of the predictor variables as well as a measure of the uncertainty associated with them.\nFirst construct a small data frame containing the unique combinations of predictor variables in our dataset:\n\nd_pred &lt;- d |&gt; \n  select(Group, Time, TextType) |&gt; \n  distinct() |&gt; \n  arrange(Group, Time, TextType)\nd_pred\n\n# A tibble: 18 × 3\n   Group                  Time  TextType     \n   &lt;fct&gt;                  &lt;fct&gt; &lt;fct&gt;        \n 1 monolingual Portuguese T1    argumentative\n 2 monolingual Portuguese T1    narrative    \n 3 monolingual Portuguese T2    argumentative\n 4 monolingual Portuguese T2    narrative    \n 5 monolingual Portuguese T3    argumentative\n 6 monolingual Portuguese T3    narrative    \n 7 Portuguese-French      T1    argumentative\n 8 Portuguese-French      T1    narrative    \n 9 Portuguese-French      T2    argumentative\n10 Portuguese-French      T2    narrative    \n11 Portuguese-French      T3    argumentative\n12 Portuguese-French      T3    narrative    \n13 Portuguese-German      T1    argumentative\n14 Portuguese-German      T1    narrative    \n15 Portuguese-German      T2    argumentative\n16 Portuguese-German      T2    narrative    \n17 Portuguese-German      T3    argumentative\n18 Portuguese-German      T3    narrative    \n\n\nIf you feed the model (here: m_default) and the data frame we’ve just created (d_pred) to the fitted() function, it outputs the modelled mean estimate for each combination of predictor values (Estimate), the estimated error of this mean estimate (Est.Error), and a 95% uncertainty interval about the estimate (Q2.5 and Q97.5). One more thing: The re_formula = NA line specifies that we do not want the variability associated with the by-Class and by-Child random effects to affect the estimates and their uncertainty. This is what I typically want.\n\ncbind(\n  d_pred, \n  fitted(m_default, \n         newdata = d_pred, \n         re_formula = NA)\n  )\n\n                    Group Time      TextType Estimate Est.Error Q2.5 Q97.5\n1  monolingual Portuguese   T1 argumentative     5.25     0.128 4.99  5.50\n2  monolingual Portuguese   T1     narrative     4.99     0.172 4.65  5.33\n3  monolingual Portuguese   T2 argumentative     5.82     0.143 5.54  6.10\n4  monolingual Portuguese   T2     narrative     5.32     0.186 4.95  5.69\n5  monolingual Portuguese   T3 argumentative     6.16     0.157 5.85  6.47\n6  monolingual Portuguese   T3     narrative     5.84     0.189 5.47  6.23\n7       Portuguese-French   T1 argumentative     4.19     0.115 3.97  4.42\n8       Portuguese-French   T1     narrative     4.07     0.163 3.76  4.39\n9       Portuguese-French   T2 argumentative     4.21     0.123 3.97  4.46\n10      Portuguese-French   T2     narrative     4.21     0.156 3.91  4.53\n11      Portuguese-French   T3 argumentative     4.73     0.130 4.48  4.99\n12      Portuguese-French   T3     narrative     4.80     0.157 4.49  5.11\n13      Portuguese-German   T1 argumentative     3.95     0.109 3.73  4.17\n14      Portuguese-German   T1     narrative     3.86     0.148 3.56  4.15\n15      Portuguese-German   T2 argumentative     3.88     0.116 3.65  4.11\n16      Portuguese-German   T2     narrative     4.10     0.157 3.78  4.41\n17      Portuguese-German   T3 argumentative     4.64     0.127 4.38  4.89\n18      Portuguese-German   T3     narrative     4.76     0.147 4.47  5.04\n\n\nSo where do these estimates and uncertainty intervals come from? In the Bayesian approach, every model parameter hasn’t got just one estimate but an entire distribution of estimates. Moreover, everything that depends on model parameters also has an entire distribution of estimates associated with it. The mean modelled outcome values per cell depend on the model parameters, so they, too, have entire distributions associated with them. The fitted() function summarises these distributions for us: it returns their means as Estimate, their standard deviations as Est.Error and their 2.5th and 97.5 percentiles as Q2.5 and Q97.5. If so inclined, you can generate these distributions yourself using the posterior_linpred() function:\n\nposterior_fit &lt;- posterior_linpred(m_default, newdata = d_pred, re_formula = NA)\ndim(posterior_fit)\n\n[1] 8000   18\n\n\nThis returns matrix of 4000 rows and 18 columns. 4000 is the number of ‘post-warmup samples’ (see the output of summary(m_default); 18 is the number of combinations of predictor values in d_pred.\nThe first column of posterior_fit contains the distribution associated with the first row in d_pred. If you compute its mean, standard deviation and 2.5th and 97.5th percentiles, you end up with the same numbers as above:\n\nmean(posterior_fit[, 1])\n\n[1] 5.25\n\nsd(posterior_fit[, 1])\n\n[1] 0.128\n\nquantile(posterior_fit[, 1], probs = c(0.025, 0.975))\n\n 2.5% 97.5% \n 4.99  5.50 \n\n\nOr similarly for the 10th row of d_pred (Portuguese-French, T2, narrative):\n\nmean(posterior_fit[, 10])\n\n[1] 4.21\n\nsd(posterior_fit[, 10])\n\n[1] 0.156\n\nquantile(posterior_fit[, 10], probs = c(0.025, 0.975))\n\n 2.5% 97.5% \n 3.91  4.53 \n\n\nAt the moment, using posterior_linpred() has no added value, but it’s good to know where these numbers come from.\nLet’s draw a graph showing these modelled averages and the uncertainty about them. 95% uncertainty intervals are typically used, but they may instill dichotomous thinking. To highlight that such an interval highlights but two points on a continuum, I’m tempted to add 80% intervals as well:\n\n# Obtain fitted values + uncertainty\nfitted_values &lt;- fitted(m_default, newdata = d_pred, re_formula = NA, \n                        # 95% interval: between 2.5th and 97.5th percentile\n                        # 80% interval: between 10th and 90th percentile\n                        probs = c(0.025, 0.10, 0.90, 0.975))\n# Combine fitted values with predictor values\nfitted_values &lt;- cbind(d_pred, fitted_values)\nfitted_values\n\n                    Group Time      TextType Estimate Est.Error Q2.5  Q10  Q90\n1  monolingual Portuguese   T1 argumentative     5.25     0.128 4.99 5.09 5.41\n2  monolingual Portuguese   T1     narrative     4.99     0.172 4.65 4.77 5.20\n3  monolingual Portuguese   T2 argumentative     5.82     0.143 5.54 5.64 6.00\n4  monolingual Portuguese   T2     narrative     5.32     0.186 4.95 5.09 5.55\n5  monolingual Portuguese   T3 argumentative     6.16     0.157 5.85 5.97 6.36\n6  monolingual Portuguese   T3     narrative     5.84     0.189 5.47 5.60 6.08\n7       Portuguese-French   T1 argumentative     4.19     0.115 3.97 4.04 4.33\n8       Portuguese-French   T1     narrative     4.07     0.163 3.76 3.86 4.28\n9       Portuguese-French   T2 argumentative     4.21     0.123 3.97 4.06 4.36\n10      Portuguese-French   T2     narrative     4.21     0.156 3.91 4.02 4.41\n11      Portuguese-French   T3 argumentative     4.73     0.130 4.48 4.56 4.89\n12      Portuguese-French   T3     narrative     4.80     0.157 4.49 4.60 5.00\n13      Portuguese-German   T1 argumentative     3.95     0.109 3.73 3.81 4.09\n14      Portuguese-German   T1     narrative     3.86     0.148 3.56 3.68 4.05\n15      Portuguese-German   T2 argumentative     3.88     0.116 3.65 3.73 4.03\n16      Portuguese-German   T2     narrative     4.10     0.157 3.78 3.90 4.30\n17      Portuguese-German   T3 argumentative     4.64     0.127 4.38 4.48 4.80\n18      Portuguese-German   T3     narrative     4.76     0.147 4.47 4.57 4.94\n   Q97.5\n1   5.50\n2   5.33\n3   6.10\n4   5.69\n5   6.47\n6   6.23\n7   4.42\n8   4.39\n9   4.46\n10  4.53\n11  4.99\n12  5.11\n13  4.17\n14  4.15\n15  4.11\n16  4.41\n17  4.89\n18  5.04\n\n\nAnd now for the graph:\n\n# Move all points apart horizontally to reduce overlap\nposition_adjustment &lt;- position_dodge(width = 0.3)\n\nggplot(fitted_values,\n       aes(x = Time,\n           y = Estimate,\n           # Sort Groups from low to high\n           colour = reorder(Group, Estimate),\n           group = Group)) +\n  # Move point apart:\n  geom_point(position = position_adjustment) +\n  # Move lines apart:\n  geom_path(position = position_adjustment) +\n  # Add 95% intervals; move them apart, too\n  geom_linerange(aes(ymin = Q2.5, ymax = Q97.5), linewidth = 0.4,\n                 position = position_adjustment) +\n  # Add 80% intervals; move them apart, too\n  geom_linerange(aes(ymin = Q10, ymax = Q90), linewidth = 0.9,\n                 position = position_adjustment) +\n  facet_wrap(~ TextType) +\n  # Override default colour\n  scale_colour_brewer(name = element_blank(), type = \"qual\") +\n  ylab(\"Modelled mean Guiraud\") +\n  theme_bw() +\n  theme(legend.position = \"bottom\")\n\n\n\n\nFigure 2. The modelled mean Guiraud values and their uncertainty (thick vertical lines: 80% interval; thin vertical lines: 95% interval)."
  },
  {
    "objectID": "posts/2018-12-20-contrasts-interaction/index.html#a-model-with-more-sensible-coding",
    "href": "posts/2018-12-20-contrasts-interaction/index.html#a-model-with-more-sensible-coding",
    "title": "Baby steps in Bayes: Recoding predictors and homing in on specific comparisons",
    "section": "A model with more sensible coding",
    "text": "A model with more sensible coding\n\nTailoring the coding of categorical predictors to the research questions\nThe summary() output for m_default was difficult to interpret because treatment coding was used. However, we can override this default behaviour to end up with estimates that are more readily and more usefully interpretable.\nThe first thing we can do is to override the default refence level. Figure 1 showed that the Guiraud values at T2 tend to be somewhere midway between those at T1 and T3, so we can make the intercept estimate more representative of the dataset as a whole by making T2 the reference level of Time rather than T1. A benefit of doing so is that we will now have two parameters, TimeT1 and TimeT3 that specify the difference between T1-T2 and T2-T3, respectively. In other words, the estimated parameters will directly reflect the progression from data collection to data collection. (Before, the parameter estimates specified the differences between T1-T2 and T1-T3, so a direct estimate for T2-T3 was lacking.)\n\n# Set T2 as default time; retain treatment coding\nd$Time &lt;- relevel(d$Time, \"T2\")\n\nSecond, there’s no reason for preferring argumentative or narrative texts as the reference level. If we sum-code this predictor, the intercept reflects the grand mean of the argumentative and narrative texts (at T2), and the estimated parameter then specifies how far the mean Guiraud value of each text type is removed from this mean:\n\n# Sum (or deviation) coding for TextType (2 levels)\ncontrasts(d$TextType) &lt;- contr.sum(2)\n\nSimilarly, there are a couple of reasonable ways to choose the reference level for Group when using treatment coding. But you can also sum-code this predictor so that the intercept reflects the grand mean of the Guiraud values of texts written by monolingual Portuguese and bilingual Portuguese-French and Portuguese-German kids (at T2).\n\n# Sum (or deviation) coding for Group (3 levels)\ncontrasts(d$Group) &lt;- contr.sum(3)\n\n\n\nRefitting the model\nNo difference here.\n\nm_recoded &lt;- brm(Guiraud ~ Time*TextType*Group +\n                   (1 + TextType + Time|Class) +\n                   (1 + TextType + Time|Child),\n                 cores = 4, iter = 4000,\n                 silent = 2,\n                 control = list(adapt_delta = 0.95),\n                 data = d)\n\nRunning /usr/lib/R/bin/R CMD SHLIB foo.c\nusing C compiler: ‘gcc (Ubuntu 11.4.0-1ubuntu1~22.04) 11.4.0’\ngcc -I\"/usr/share/R/include\" -DNDEBUG   -I\"/home/jan/R/x86_64-pc-linux-gnu-library/4.3/Rcpp/include/\"  -I\"/home/jan/R/x86_64-pc-linux-gnu-library/4.3/RcppEigen/include/\"  -I\"/home/jan/R/x86_64-pc-linux-gnu-library/4.3/RcppEigen/include/unsupported\"  -I\"/home/jan/R/x86_64-pc-linux-gnu-library/4.3/BH/include\" -I\"/home/jan/R/x86_64-pc-linux-gnu-library/4.3/StanHeaders/include/src/\"  -I\"/home/jan/R/x86_64-pc-linux-gnu-library/4.3/StanHeaders/include/\"  -I\"/home/jan/R/x86_64-pc-linux-gnu-library/4.3/RcppParallel/include/\"  -I\"/home/jan/R/x86_64-pc-linux-gnu-library/4.3/rstan/include\" -DEIGEN_NO_DEBUG  -DBOOST_DISABLE_ASSERTS  -DBOOST_PENDING_INTEGER_LOG2_HPP  -DSTAN_THREADS  -DUSE_STANC3 -DSTRICT_R_HEADERS  -DBOOST_PHOENIX_NO_VARIADIC_EXPRESSION  -DBOOST_NO_AUTO_PTR  -include '/home/jan/R/x86_64-pc-linux-gnu-library/4.3/StanHeaders/include/stan/math/prim/fun/Eigen.hpp'  -D_REENTRANT -DRCPP_PARALLEL_USE_TBB=1       -fpic  -g -O2 -ffile-prefix-map=/build/r-base-1upgAf/r-base-4.3.1=. -fstack-protector-strong -Wformat -Werror=format-security -Wdate-time -D_FORTIFY_SOURCE=2  -c foo.c -o foo.o\nIn file included from /home/jan/R/x86_64-pc-linux-gnu-library/4.3/RcppEigen/include/Eigen/Core:88,\n                 from /home/jan/R/x86_64-pc-linux-gnu-library/4.3/RcppEigen/include/Eigen/Dense:1,\n                 from /home/jan/R/x86_64-pc-linux-gnu-library/4.3/StanHeaders/include/stan/math/prim/fun/Eigen.hpp:22,\n                 from &lt;command-line&gt;:\n/home/jan/R/x86_64-pc-linux-gnu-library/4.3/RcppEigen/include/Eigen/src/Core/util/Macros.h:628:1: error: unknown type name ‘namespace’\n  628 | namespace Eigen {\n      | ^~~~~~~~~\n/home/jan/R/x86_64-pc-linux-gnu-library/4.3/RcppEigen/include/Eigen/src/Core/util/Macros.h:628:17: error: expected ‘=’, ‘,’, ‘;’, ‘asm’ or ‘__attribute__’ before ‘{’ token\n  628 | namespace Eigen {\n      |                 ^\nIn file included from /home/jan/R/x86_64-pc-linux-gnu-library/4.3/RcppEigen/include/Eigen/Dense:1,\n                 from /home/jan/R/x86_64-pc-linux-gnu-library/4.3/StanHeaders/include/stan/math/prim/fun/Eigen.hpp:22,\n                 from &lt;command-line&gt;:\n/home/jan/R/x86_64-pc-linux-gnu-library/4.3/RcppEigen/include/Eigen/Core:96:10: fatal error: complex: No such file or directory\n   96 | #include &lt;complex&gt;\n      |          ^~~~~~~~~\ncompilation terminated.\nmake: *** [/usr/lib/R/etc/Makeconf:191: foo.o] Error 1\n\n\n\n\nInterpreting the parameter estimates\n\nsummary(m_recoded)\n\n Family: gaussian \n  Links: mu = identity; sigma = identity \nFormula: Guiraud ~ Time * TextType * Group + (1 + TextType + Time | Class) + (1 + TextType + Time | Child) \n   Data: d (Number of observations: 1040) \n  Draws: 4 chains, each with iter = 4000; warmup = 2000; thin = 1;\n         total post-warmup draws = 8000\n\nGroup-Level Effects: \n~Child (Number of levels: 328) \n                         Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS\nsd(Intercept)                0.47      0.04     0.38     0.55 1.00     2449\nsd(TextType1)                0.13      0.05     0.02     0.22 1.01      597\nsd(TimeT1)                   0.10      0.07     0.00     0.27 1.00     1114\nsd(TimeT3)                   0.38      0.09     0.19     0.55 1.01      616\ncor(Intercept,TextType1)    -0.47      0.22    -0.86    -0.00 1.00     2195\ncor(Intercept,TimeT1)       -0.03      0.39    -0.74     0.74 1.00     5843\ncor(TextType1,TimeT1)        0.03      0.43    -0.79     0.81 1.00     3610\ncor(Intercept,TimeT3)        0.14      0.22    -0.24     0.64 1.01      945\ncor(TextType1,TimeT3)        0.05      0.32    -0.65     0.64 1.01      412\ncor(TimeT1,TimeT3)          -0.07      0.42    -0.82     0.74 1.01      280\n                         Tail_ESS\nsd(Intercept)                4309\nsd(TextType1)                1011\nsd(TimeT1)                   1881\nsd(TimeT3)                   1077\ncor(Intercept,TextType1)     2927\ncor(Intercept,TimeT1)        5186\ncor(TextType1,TimeT1)        4759\ncor(Intercept,TimeT3)        1621\ncor(TextType1,TimeT3)         672\ncor(TimeT1,TimeT3)            823\n\n~Class (Number of levels: 25) \n                         Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS\nsd(Intercept)                0.18      0.08     0.03     0.34 1.00     1193\nsd(TextType1)                0.12      0.04     0.05     0.21 1.00     2767\nsd(TimeT1)                   0.10      0.07     0.00     0.27 1.00     2164\nsd(TimeT3)                   0.10      0.07     0.00     0.27 1.00     2111\ncor(Intercept,TextType1)    -0.19      0.35    -0.79     0.53 1.00     2478\ncor(Intercept,TimeT1)       -0.12      0.43    -0.85     0.74 1.00     5476\ncor(TextType1,TimeT1)       -0.02      0.42    -0.78     0.76 1.00     5773\ncor(Intercept,TimeT3)        0.04      0.43    -0.77     0.81 1.00     6506\ncor(TextType1,TimeT3)        0.15      0.41    -0.69     0.85 1.00     5899\ncor(TimeT1,TimeT3)           0.13      0.44    -0.76     0.86 1.00     4553\n                         Tail_ESS\nsd(Intercept)                1273\nsd(TextType1)                2274\nsd(TimeT1)                   3196\nsd(TimeT3)                   3346\ncor(Intercept,TextType1)     3033\ncor(Intercept,TimeT1)        5363\ncor(TextType1,TimeT1)        5650\ncor(Intercept,TimeT3)        5504\ncor(TextType1,TimeT3)        6160\ncor(TimeT1,TimeT3)           5721\n\nPopulation-Level Effects: \n                        Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS\nIntercept                   4.59      0.06     4.46     4.72 1.00     4216\nTimeT1                     -0.20      0.06    -0.32    -0.08 1.00     6096\nTimeT3                      0.56      0.06     0.44     0.69 1.00     5736\nTextType1                   0.05      0.05    -0.05     0.14 1.00     4230\nGroup1                      0.98      0.09     0.78     1.16 1.00     3129\nGroup2                     -0.38      0.09    -0.55    -0.20 1.00     2923\nTimeT1:TextType1            0.03      0.05    -0.07     0.13 1.00     6671\nTimeT3:TextType1           -0.02      0.05    -0.12     0.07 1.00     5945\nTimeT1:Group1              -0.24      0.08    -0.40    -0.07 1.00     5325\nTimeT3:Group1              -0.13      0.09    -0.31     0.05 1.00     5069\nTimeT1:Group2               0.12      0.09    -0.05     0.29 1.00     5178\nTimeT3:Group2              -0.01      0.09    -0.18     0.16 1.00     5133\nTextType1:Group1            0.21      0.07     0.07     0.34 1.00     3122\nTextType1:Group2           -0.04      0.07    -0.18     0.09 1.00     3293\nTimeT1:TextType1:Group1    -0.15      0.07    -0.29    -0.01 1.00     5085\nTimeT3:TextType1:Group1    -0.07      0.07    -0.20     0.07 1.00     5624\nTimeT1:TextType1:Group2     0.03      0.07    -0.12     0.17 1.00     5303\nTimeT3:TextType1:Group2    -0.01      0.07    -0.15     0.12 1.00     5218\n                        Tail_ESS\nIntercept                   4839\nTimeT1                      5507\nTimeT3                      5002\nTextType1                   5381\nGroup1                      3741\nGroup2                      4022\nTimeT1:TextType1            6505\nTimeT3:TextType1            5426\nTimeT1:Group1               5184\nTimeT3:Group1               5097\nTimeT1:Group2               5426\nTimeT3:Group2               5284\nTextType1:Group1            4089\nTextType1:Group2            4598\nTimeT1:TextType1:Group1     5711\nTimeT3:TextType1:Group1     6277\nTimeT1:TextType1:Group2     5777\nTimeT3:TextType1:Group2     5951\n\nFamily Specific Parameters: \n      Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS\nsigma     0.60      0.02     0.55     0.65 1.00      867     2169\n\nDraws were sampled using sampling(NUTS). For each parameter, Bulk_ESS\nand Tail_ESS are effective sample size measures, and Rhat is the potential\nscale reduction factor on split chains (at convergence, Rhat = 1).\n\n\nNow the Intercept reflects the grand mean of the Guiraud values for both argumentative and narrative texts for all three groups written at T2. The TimeT1 estimate (-0.20) shows the difference between T1 and T2 averaged over all text types and all groups (0.20 points worse at T1); the TimeT3 estimate (0.56) shows the difference between T2 and T3 averaged over all text types and all groups (0.56 points better at T3).\nTextType1 (0.05) shows that the mean Guiraud value of one text type (still written at T2!) averaged over all groups is 0.05 points higher than the grand mean; and by implication that the mean Guiraud value of the other text type is 0.05 lower than the grand mean. To find out which text type is which, use contrasts():\n\ncontrasts(d$TextType)\n\n              [,1]\nargumentative    1\nnarrative       -1\n\n\nSince argumentative is coded as 1, it’s the argumentative texts that have the higher Guiraud values at T2.\nSimilarly, Group1 (0.98) shows that one group has higher-than-average Guiraud values averaged across text types at T2, whereas Group2 (-0.38) shows that another group has a mean Guiraud value that lies 0.38 points below the average at T2. By implication, the third group’s mean Guiraud value lies 0.60 points below average ((0.98-0.38-0.60)/3 = 0). To see which group is which, use contrasts():\n\ncontrasts(d$Group)\n\n                       [,1] [,2]\nmonolingual Portuguese    1    0\nPortuguese-French         0    1\nPortuguese-German        -1   -1\n\n\nmonolingual Portuguese is ‘1’ for the purposes of Group1, Portuguese-French is 1 for the purposes of Group2, and Portuguese-German is the third group.\nWe can double-check these numbers by generating the modelled mean values for each predictor value combination:\n\ndouble_check &lt;- cbind(\n  d_pred, \n  fitted(m_recoded, \n         newdata = d_pred, \n         re_formula = NA)\n  )\ndouble_check\n\n                    Group Time      TextType Estimate Est.Error Q2.5 Q97.5\n1  monolingual Portuguese   T1 argumentative     5.25     0.147 4.96  5.54\n2  monolingual Portuguese   T1     narrative     4.98     0.166 4.65  5.32\n3  monolingual Portuguese   T2 argumentative     5.82     0.142 5.54  6.10\n4  monolingual Portuguese   T2     narrative     5.31     0.159 4.99  5.62\n5  monolingual Portuguese   T3 argumentative     6.16     0.169 5.82  6.49\n6  monolingual Portuguese   T3     narrative     5.84     0.176 5.48  6.18\n7       Portuguese-French   T1 argumentative     4.19     0.128 3.94  4.45\n8       Portuguese-French   T1     narrative     4.06     0.163 3.75  4.38\n9       Portuguese-French   T2 argumentative     4.21     0.125 3.97  4.47\n10      Portuguese-French   T2     narrative     4.21     0.138 3.94  4.48\n11      Portuguese-French   T3 argumentative     4.73     0.135 4.47  5.00\n12      Portuguese-French   T3     narrative     4.80     0.152 4.49  5.10\n13      Portuguese-German   T1 argumentative     3.95     0.116 3.71  4.18\n14      Portuguese-German   T1     narrative     3.87     0.147 3.58  4.16\n15      Portuguese-German   T2 argumentative     3.88     0.115 3.65  4.10\n16      Portuguese-German   T2     narrative     4.11     0.148 3.81  4.40\n17      Portuguese-German   T3 argumentative     4.63     0.137 4.36  4.90\n18      Portuguese-German   T3     narrative     4.75     0.141 4.47  5.03\n\n\nSome sanity checks:\n\nIntercept = 4.59 = grand mean at T2:\n\n\ndouble_check |&gt; \n  filter(Time == \"T2\") |&gt; \n  summarise(mean_est = mean(Estimate))\n\n  mean_est\n1     4.59\n\n\n\nTimeT3 = 0.56 = T2/T3 difference across texts and groups:\n\n\ndouble_check |&gt; \n  group_by(Time) |&gt; \n  summarise(mean_est = mean(Estimate)) |&gt; \n  spread(Time, mean_est) |&gt; \n  summarise(diff_T2T3 = T3 - T2)\n\n# A tibble: 1 × 1\n  diff_T2T3\n      &lt;dbl&gt;\n1     0.562\n\n\n\nPortuguese-German lies 0.60 below average at T2 across texts:\n\n\ndouble_check |&gt; \n  filter(Time == \"T2\") |&gt; \n  group_by(Group) |&gt; \n  summarise(mean_est = mean(Estimate)) |&gt; \n  mutate(diff_mean = mean_est - mean(mean_est))\n\n# A tibble: 3 × 3\n  Group                  mean_est diff_mean\n  &lt;fct&gt;                     &lt;dbl&gt;     &lt;dbl&gt;\n1 monolingual Portuguese     5.56     0.975\n2 Portuguese-French          4.21    -0.378\n3 Portuguese-German          3.99    -0.597\n\n\nI won’t plot the modelled averages and their uncertainty, because the result will be the same as before: Recoding the predictors in this way doesn’t affect the modelled averages per cell; it just makes the summary output easier to parse.\n\n\nHoming in on specific comparisons\nFinally, let’s see how we can target some specific comparisons without having to refit the model several times. A specific comparison you might be interested in could be “How large is the difference in Guiraud scores for narrative texts written by Portuguese-French bilinguals between T1 and T2?” Or a more complicated one: “How large is the difference in the progression from T1 to T3 for argumentative texts between Portuguese-French and Portuguese-German children?”\nTo answer such questions, we need to generate the distribution of the modelled averages per predictor value combination:\n\nposterior_fit &lt;- posterior_linpred(m_recoded, newdata = d_pred, re_formula = NA)\n\n\nQuestion 1: Progression T1-T2 for narrative texts, Portuguese-French bilinguals?\nThis question requires us to compare the modelled average for narrative texts written by Portuguese-French bilinguals at T2 to that of the narrative texts written by Portuguese-French bilinguals at T1. The first combination of predictor values can be found in row 10 in d_pred, so the corresponding estimates are in column 10 in posterior_fit. The second combination of predictor values can be found in row 8 in d_pred, so the corresponding estimates are in column 8 in posterior_fit.\n\nt2 &lt;- posterior_fit[, 10]\nt1 &lt;- posterior_fit[, 8]\ndf &lt;- data.frame(t2, t1)\n\nNow compute and plot the pairwise differences:\n\ndf &lt;- df |&gt; \n  mutate(progression = t2 - t1)\nggplot(df,\n       aes(x = progression)) +\n  geom_histogram(bins = 50, fill = \"lightgrey\", colour = \"black\") +\n  theme_bw()\n\n\n\n\nFigure 3. Estimate of the progression in Guiraud values for narrative texts by Portuguese-French bilinguals from T1 to T2.\n\n\n\n\nThe mean progression is easily calculated:\n\nmean(df$progression)\n\n[1] 0.148\n\n\nThe estimated error for this estimate is:\n\nsd(df$progression)\n\n[1] 0.149\n\n\nAnd its 95% uncertainty interval is:\n\nquantile(df$progression, probs = c(0.025, 0.975))\n\n  2.5%  97.5% \n-0.152  0.436 \n\n\nAccording to the model, there’s about a 84% chance that there’s indeed some progression going from T1 to T2.\n\nmean(df$progression &gt; 0)\n\n[1] 0.845\n\n\n\n\nQuestion 2: T1-T3 progression for argumentative texts, Portuguese-French vs. Portuguese-German?\nThis question requires us to take into consideration the modelled average for argumentative texts written by Portuguese-French bilinguals at T1, that for argumentative texts written by Portuguese-French bilinguals at T3, and the same for the texts written by Portuguese-German bilinguals. We need the following columns in posterior_fit:\n\n7 (Portuguese-French, T1, argumentative)\n11 (Portuguese-French, T3, argumentative)\n13 (Portuguese-German, T1, argumentative)\n17 (Portuguese-German, T3, argumentative)\n\n\nfr_t1 &lt;- posterior_fit[, 7]\nfr_t3 &lt;- posterior_fit[, 11]\ngm_t1 &lt;- posterior_fit[, 13]\ngm_t3 &lt;- posterior_fit[, 17]\ndf &lt;- data.frame(fr_t1, fr_t3, gm_t1, gm_t3)\n\nWe compute the progression for the Portuguese-French bilinguals and that for the Portuguese-German bilinguals. Then we compute the difference between these progressions:\n\ndf &lt;- df |&gt; \n  mutate(prog_fr = fr_t3 - fr_t1,\n         prog_gm = gm_t3 - gm_t1,\n         diff_prog = prog_gm - prog_fr)\n\nThe mean progression for the Portuguese-French bilinguals was 0.54 compared to 0.68 for the Portuguese-German bilinguals:\n\nmean(df$prog_fr)\n\n[1] 0.542\n\nmean(df$prog_gm)\n\n[1] 0.684\n\n\nThe mean difference between these progressions, then, is 0.14 in favour of the Portuguese-German bilinguals:\n\nmean(df$diff_prog)\n\n[1] 0.142\n\n\nHowever, there is considerable uncertainty about this difference:\n\nggplot(df,\n       aes(x = diff_prog)) +\n  geom_histogram(bins = 50, fill = \"lightgrey\", colour = \"black\") +\n  theme_bw()\n\n\n\n\nThe probability that the Portuguese-German bilinguals make more progress than the Portuguese-French bilinguals is 77%, and according to the model, there’s a 95% chance its size is somewhere between -0.25 and 0.52 points.\n\nmean(df$diff_prog &gt; 0)\n\n[1] 0.768\n\nquantile(df$diff_prog, probs = c(0.025, 0.975))\n\n  2.5%  97.5% \n-0.251  0.530"
  },
  {
    "objectID": "posts/2018-12-20-contrasts-interaction/index.html#summary",
    "href": "posts/2018-12-20-contrasts-interaction/index.html#summary",
    "title": "Baby steps in Bayes: Recoding predictors and homing in on specific comparisons",
    "section": "Summary",
    "text": "Summary\nBy investing some time in recoding your predictors, you can make the parameter estimates more relevant to your questions. Any specific comparisons you may be interested in can additionally be addressed by making use of the entire distribution of estimates. You can also use these estimate distributions to draw effect plots."
  },
  {
    "objectID": "posts/2018-12-20-contrasts-interaction/index.html#further-resources",
    "href": "posts/2018-12-20-contrasts-interaction/index.html#further-resources",
    "title": "Baby steps in Bayes: Recoding predictors and homing in on specific comparisons",
    "section": "Further resources",
    "text": "Further resources\n\nR Library Contrast Coding Systems for categorical variables\nMatthew Kay. 2018. Extracting and visualizing tidy draws from brms models\nDaniel J. Schad, Sven Hohenstein, Shravan Vasishth and Reinhold Kliegl. 2018. How to capitalize on a priori contrasts in linear (mixed) models: A tutorial.\nIn simpler models, you can use bootstrapping to generate distributions of estimates. This would be possible for these data, too, but it would take a considerable amount of time."
  },
  {
    "objectID": "posts/2018-12-20-contrasts-interaction/index.html#references",
    "href": "posts/2018-12-20-contrasts-interaction/index.html#references",
    "title": "Baby steps in Bayes: Recoding predictors and homing in on specific comparisons",
    "section": "References",
    "text": "References\nAudrey Bonvin, Jan Vanhove, Raphael Berthele and Amelia Lambelet. 2018. Die Entwicklung von produktiven lexikalischen Kompetenzen bei Schüler(innen) mit portugiesischem Migrationshintergrund in der Schweiz. Zeitschrift für Interkulturellen Fremdsprachenunterricht 23(1). 135-148. Data and R code available from figshare.\nAmelia Lambelet, Raphael Berthele, Magalie Desgrippes, Carlos Pestana and Jan Vanhove. 2017a. Chapter 2: Testing interdependence in Portuguese Heritage speakers in Switzerland: the HELASCOT project. In Raphael Berthele and Amelia Lambelet (eds.), Heritage and school language literacy development in migrant children: Interdependence or independence?, pp. 26-33. Multilingual Matters.\nAmelia Lambelet, Magalie Desgrippes and Jan Vanhove. 2017b. Chapter 5: The development of argumentative and narrative writing skills in Portuguese heritage speakers in Switzerland (HELASCOT project). In Raphael Berthele and Amelia Lambelet (eds.), Heritage and school language literacy development in migrant children: Interdependence or independence?, pp. 83-96. Multilingual Matters.\nJan Vanhove, Audrey Bonvin, Amelia Lambelet and Raphael Berthele. 2019. Predicting perceptions of the lexical richness of short French, German, and Portuguese texts. Journal of Writing Research. Technical report, data (including texts), elicitation materials, and R code available from the Open Science Framework."
  },
  {
    "objectID": "posts/2018-12-20-contrasts-interaction/index.html#session-info",
    "href": "posts/2018-12-20-contrasts-interaction/index.html#session-info",
    "title": "Baby steps in Bayes: Recoding predictors and homing in on specific comparisons",
    "section": "Session info",
    "text": "Session info\n\ndevtools::session_info()\n\n─ Session info ───────────────────────────────────────────────────────────────\n setting  value\n version  R version 4.3.1 (2023-06-16)\n os       Ubuntu 22.04.2 LTS\n system   x86_64, linux-gnu\n ui       X11\n language en_US\n collate  en_US.UTF-8\n ctype    en_US.UTF-8\n tz       Europe/Zurich\n date     2023-08-06\n pandoc   3.1.1 @ /usr/lib/rstudio/resources/app/bin/quarto/bin/tools/ (via rmarkdown)\n\n─ Packages ───────────────────────────────────────────────────────────────────\n package        * version date (UTC) lib source\n abind            1.4-5   2016-07-21 [1] CRAN (R 4.3.1)\n backports        1.4.1   2021-12-13 [1] CRAN (R 4.3.0)\n base64enc        0.1-3   2015-07-28 [1] CRAN (R 4.3.0)\n bayesplot        1.10.0  2022-11-16 [1] CRAN (R 4.3.1)\n bit              4.0.5   2022-11-15 [1] CRAN (R 4.3.0)\n bit64            4.0.5   2020-08-30 [1] CRAN (R 4.3.0)\n bridgesampling   1.1-2   2021-04-16 [1] CRAN (R 4.3.1)\n brms           * 2.19.0  2023-03-14 [1] CRAN (R 4.3.1)\n Brobdingnag      1.2-9   2022-10-19 [1] CRAN (R 4.3.1)\n cachem           1.0.6   2021-08-19 [2] CRAN (R 4.2.0)\n callr            3.7.3   2022-11-02 [1] CRAN (R 4.3.1)\n checkmate        2.2.0   2023-04-27 [1] CRAN (R 4.3.1)\n cli              3.6.1   2023-03-23 [1] CRAN (R 4.3.0)\n coda             0.19-4  2020-09-30 [1] CRAN (R 4.3.1)\n codetools        0.2-19  2023-02-01 [4] CRAN (R 4.2.2)\n colorspace       2.1-0   2023-01-23 [1] CRAN (R 4.3.0)\n colourpicker     1.2.0   2022-10-28 [1] CRAN (R 4.3.1)\n crayon           1.5.2   2022-09-29 [1] CRAN (R 4.3.1)\n crosstalk        1.2.0   2021-11-04 [1] CRAN (R 4.3.1)\n curl             4.3.2   2021-06-23 [2] CRAN (R 4.2.0)\n devtools         2.4.5   2022-10-11 [1] CRAN (R 4.3.1)\n digest           0.6.29  2021-12-01 [2] CRAN (R 4.2.0)\n distributional   0.3.2   2023-03-22 [1] CRAN (R 4.3.1)\n dplyr          * 1.1.2   2023-04-20 [1] CRAN (R 4.3.0)\n DT               0.28    2023-05-18 [1] CRAN (R 4.3.1)\n dygraphs         1.1.1.6 2018-07-11 [1] CRAN (R 4.3.1)\n ellipsis         0.3.2   2021-04-29 [2] CRAN (R 4.2.0)\n evaluate         0.15    2022-02-18 [2] CRAN (R 4.2.0)\n fansi            1.0.4   2023-01-22 [1] CRAN (R 4.3.1)\n farver           2.1.1   2022-07-06 [1] CRAN (R 4.3.0)\n fastmap          1.1.0   2021-01-25 [2] CRAN (R 4.2.0)\n forcats        * 1.0.0   2023-01-29 [1] CRAN (R 4.3.0)\n fs               1.5.2   2021-12-08 [2] CRAN (R 4.2.0)\n generics         0.1.3   2022-07-05 [1] CRAN (R 4.3.0)\n ggplot2        * 3.4.2   2023-04-03 [1] CRAN (R 4.3.0)\n glue             1.6.2   2022-02-24 [2] CRAN (R 4.2.0)\n gridExtra        2.3     2017-09-09 [1] CRAN (R 4.3.0)\n gtable           0.3.3   2023-03-21 [1] CRAN (R 4.3.0)\n gtools           3.9.4   2022-11-27 [1] CRAN (R 4.3.1)\n hms              1.1.3   2023-03-21 [1] CRAN (R 4.3.0)\n htmltools        0.5.5   2023-03-23 [1] CRAN (R 4.3.0)\n htmlwidgets      1.6.2   2023-03-17 [1] CRAN (R 4.3.1)\n httpuv           1.6.11  2023-05-11 [1] CRAN (R 4.3.1)\n igraph           1.5.0.1 2023-07-23 [1] CRAN (R 4.3.1)\n inline           0.3.19  2021-05-31 [1] CRAN (R 4.3.1)\n jsonlite         1.8.5   2023-06-05 [1] CRAN (R 4.3.0)\n knitr            1.39    2022-04-26 [2] CRAN (R 4.2.0)\n labeling         0.4.2   2020-10-20 [1] CRAN (R 4.3.0)\n later            1.3.1   2023-05-02 [1] CRAN (R 4.3.1)\n lattice          0.21-8  2023-04-05 [4] CRAN (R 4.3.0)\n lifecycle        1.0.3   2022-10-07 [1] CRAN (R 4.3.0)\n loo              2.6.0   2023-03-31 [1] CRAN (R 4.3.1)\n lubridate      * 1.9.2   2023-02-10 [1] CRAN (R 4.3.0)\n magrittr         2.0.3   2022-03-30 [1] CRAN (R 4.3.0)\n markdown         1.7     2023-05-16 [1] CRAN (R 4.3.0)\n Matrix           1.6-0   2023-07-08 [4] CRAN (R 4.3.1)\n matrixStats      1.0.0   2023-06-02 [1] CRAN (R 4.3.1)\n memoise          2.0.1   2021-11-26 [2] CRAN (R 4.2.0)\n mime             0.10    2021-02-13 [2] CRAN (R 4.0.2)\n miniUI           0.1.1.1 2018-05-18 [1] CRAN (R 4.3.1)\n munsell          0.5.0   2018-06-12 [1] CRAN (R 4.3.0)\n mvtnorm          1.2-2   2023-06-08 [1] CRAN (R 4.3.1)\n nlme             3.1-162 2023-01-31 [4] CRAN (R 4.2.2)\n pillar           1.9.0   2023-03-22 [1] CRAN (R 4.3.0)\n pkgbuild         1.4.2   2023-06-26 [1] CRAN (R 4.3.1)\n pkgconfig        2.0.3   2019-09-22 [2] CRAN (R 4.2.0)\n pkgload          1.3.2.1 2023-07-08 [1] CRAN (R 4.3.1)\n plyr             1.8.8   2022-11-11 [1] CRAN (R 4.3.1)\n posterior        1.4.1   2023-03-14 [1] CRAN (R 4.3.1)\n prettyunits      1.1.1   2020-01-24 [2] CRAN (R 4.2.0)\n processx         3.8.2   2023-06-30 [1] CRAN (R 4.3.1)\n profvis          0.3.8   2023-05-02 [1] CRAN (R 4.3.1)\n promises         1.2.0.1 2021-02-11 [1] CRAN (R 4.3.1)\n ps               1.7.5   2023-04-18 [1] CRAN (R 4.3.1)\n purrr          * 1.0.1   2023-01-10 [1] CRAN (R 4.3.0)\n R6               2.5.1   2021-08-19 [2] CRAN (R 4.2.0)\n RColorBrewer     1.1-3   2022-04-03 [1] CRAN (R 4.3.0)\n Rcpp           * 1.0.11  2023-07-06 [1] CRAN (R 4.3.1)\n RcppParallel     5.1.7   2023-02-27 [1] CRAN (R 4.3.1)\n readr          * 2.1.4   2023-02-10 [1] CRAN (R 4.3.0)\n remotes          2.4.2   2021-11-30 [2] CRAN (R 4.2.0)\n reshape2         1.4.4   2020-04-09 [1] CRAN (R 4.3.1)\n rlang            1.1.1   2023-04-28 [1] CRAN (R 4.3.0)\n rmarkdown        2.21    2023-03-26 [1] CRAN (R 4.3.0)\n rstan            2.26.22 2023-08-01 [1] local\n rstantools       2.3.1.1 2023-07-18 [1] CRAN (R 4.3.1)\n rstudioapi       0.14    2022-08-22 [1] CRAN (R 4.3.0)\n scales           1.2.1   2022-08-20 [1] CRAN (R 4.3.0)\n sessioninfo      1.2.2   2021-12-06 [2] CRAN (R 4.2.0)\n shiny            1.7.4.1 2023-07-06 [1] CRAN (R 4.3.1)\n shinyjs          2.1.0   2021-12-23 [1] CRAN (R 4.3.1)\n shinystan        2.6.0   2022-03-03 [1] CRAN (R 4.3.1)\n shinythemes      1.2.0   2021-01-25 [1] CRAN (R 4.3.1)\n StanHeaders      2.26.27 2023-06-14 [1] CRAN (R 4.3.1)\n stringi          1.7.6   2021-11-29 [2] CRAN (R 4.2.0)\n stringr        * 1.5.0   2022-12-02 [1] CRAN (R 4.3.0)\n tensorA          0.36.2  2020-11-19 [1] CRAN (R 4.3.1)\n threejs          0.3.3   2020-01-21 [1] CRAN (R 4.3.1)\n tibble         * 3.2.1   2023-03-20 [1] CRAN (R 4.3.0)\n tidyr          * 1.3.0   2023-01-24 [1] CRAN (R 4.3.0)\n tidyselect       1.2.0   2022-10-10 [1] CRAN (R 4.3.0)\n tidyverse      * 2.0.0   2023-02-22 [1] CRAN (R 4.3.1)\n timechange       0.2.0   2023-01-11 [1] CRAN (R 4.3.0)\n tzdb             0.4.0   2023-05-12 [1] CRAN (R 4.3.0)\n urlchecker       1.0.1   2021-11-30 [1] CRAN (R 4.3.1)\n usethis          2.2.2   2023-07-06 [1] CRAN (R 4.3.1)\n utf8             1.2.3   2023-01-31 [1] CRAN (R 4.3.1)\n V8               4.3.0   2023-04-08 [1] CRAN (R 4.3.0)\n vctrs            0.6.3   2023-06-14 [1] CRAN (R 4.3.0)\n vroom            1.6.3   2023-04-28 [1] CRAN (R 4.3.0)\n withr            2.5.0   2022-03-03 [2] CRAN (R 4.2.0)\n xfun             0.39    2023-04-20 [1] CRAN (R 4.3.0)\n xtable           1.8-4   2019-04-21 [1] CRAN (R 4.3.1)\n xts              0.13.1  2023-04-16 [1] CRAN (R 4.3.1)\n yaml             2.3.5   2022-02-21 [2] CRAN (R 4.2.0)\n zoo              1.8-12  2023-04-13 [1] CRAN (R 4.3.1)\n\n [1] /home/jan/R/x86_64-pc-linux-gnu-library/4.3\n [2] /usr/local/lib/R/site-library\n [3] /usr/lib/R/site-library\n [4] /usr/lib/R/library\n\n──────────────────────────────────────────────────────────────────────────────"
  },
  {
    "objectID": "posts/2020-06-29-visualise-uncertainty/index.html",
    "href": "posts/2020-06-29-visualise-uncertainty/index.html",
    "title": "Tutorial: Visualising statistical uncertainty using model-based graphs",
    "section": "",
    "text": "I wrote a tutorial about visualising the statistical uncertainty in statistical models for a conference that took place a couple of months ago, and I’ve just realised that I’ve never advertised this tutorial in this blog. You can find the tutorial here: Visualising statistical uncertainty using model-based graphs.\nContents:\n\nWhy plot models, and why visualise uncertainty?\nThe principle: An example with simple linear regression\n\nStep 1: Fit the model\nStep 2: Compute the conditional means and confidence intervals\nStep 3: Plot!\n\nPredictions about individual cases vs. conditional means\nMore examples\n\nSeveral continuous predictors\nDealing with categorical predictors\nt-tests are models, too\nDealing with interactions\nOrdinary logistic regression\nMixed-effects models\nLogistic mixed effects models\n\nCaveats\n\nOther things may not be equal\nYour model may be misspecified\nOther models may yield different pictures"
  },
  {
    "objectID": "posts/2018-09-26-cannonball/index.html",
    "href": "posts/2018-09-26-cannonball/index.html",
    "title": "Introducing cannonball - Tools for teaching statistics",
    "section": "",
    "text": "I’ve put my first R package on GitHub! It’s called cannonball and contains a couple of functions that I use for teaching; perhaps others will follow. # Installation Make sure you have the devtools package:\ninstall.packages(\"devtools\")\nThen load it and install cannonball:\nlibrary(devtools)\ninstall_github(\"janhove/cannonball\")\nTo use it, load the package as per usual:\nlibrary(cannonball)"
  },
  {
    "objectID": "posts/2018-09-26-cannonball/index.html#overview-of-the-functions",
    "href": "posts/2018-09-26-cannonball/index.html#overview-of-the-functions",
    "title": "Introducing cannonball - Tools for teaching statistics",
    "section": "Overview of the functions",
    "text": "Overview of the functions\n\nplot_r(): Draw scatterplots with the same correlation coefficient\nPeople seem to like this function from my blog post What data patterns can lie behind a correlation coefficient?. Specify the number of observations and a desired sample Pearson correlation coefficient, and out come 16 rather different looking scatterplots conforming to these criteria:\n\nplot_r(r = -0.6, n = 42)\n\n\n\n\nFor more details, type in ?plot_r at the R prompt.\n\n\nclustered_data(): Simulate data from a cluster-randomised experiment\nCluster-randomised experiments are experiments in which whole groups of participants (e.g., entire classes) are necessarily assigned to the same condition. If the data from such experiments are analysed as though the participants were assigned to the conditions individually (e.g., by running a t-test on the individual data points), the false positive rate can go through the roof. This function simulates data from such an experiment and allows you to specify unequal cluster sizes (via the parts_per_class parameter):\n\n# Generate data\nd &lt;- clustered_data(ICC = 0.15, # intra-class correlation coefficient\n                    n_per_class = c(8, 13, 28,   # sizes of the control clusters\n                                    22, 18, 16), # sizes of the intervention clusters\n                    effect = 0) # population effect size\n\n# Plot\nlibrary(ggplot2)\nggplot(data = d,\n       aes(x = class,\n           y = outcome,\n           fill = condition)) +\n  geom_boxplot(outlier.shape = NA) +\n  geom_point(shape = 1,\n             position = position_jitter(width = 0.1, height = 0))\n\n\n\n\nI mostly use this function in a simulation to illustrate the effects of clustering on p-values. With a null effect, you’d expect only 5% of the p-values to be lower than 0.05. Let’s see what happens when you analyse the individual data from a cluster-randomised experiment using a t-test:\n\np_vals &lt;- replicate(5000, {\n  d &lt;- clustered_data(ICC = 0.15,\n                      n_per_class = c(8, 13, 28, 22, 18, 16),\n                      effect = 0)\n  p &lt;- t.test(outcome ~ condition, data = d)$p.value\n  return(p)\n})\nhist(p_vals)\n\n\n\n\n\nmean(p_vals &lt; 0.05)\n\n[1] 0.3208\n\n\nThe false positive rate is now through the roof (32%).\n\n\nGraphically checking model assumptions\nSee the full-fledged tutorial for these functions."
  },
  {
    "objectID": "posts/2018-09-26-cannonball/index.html#why-cannonball",
    "href": "posts/2018-09-26-cannonball/index.html#why-cannonball",
    "title": "Introducing cannonball - Tools for teaching statistics",
    "section": "Why cannonball?",
    "text": "Why cannonball?\nGlad you asked! Julian ‘Cannonball’ Adderley is one of my favourite alto saxophone players (check out his solos on Autumn Leaves (from around 2’03”; Somethin’ Else) or Freddie Freeloader (6’22”; Kind of Blue)!) and he was a consummate jazz educator to boot."
  },
  {
    "objectID": "posts/2018-09-26-cannonball/index.html#software-versions",
    "href": "posts/2018-09-26-cannonball/index.html#software-versions",
    "title": "Introducing cannonball - Tools for teaching statistics",
    "section": "Software versions",
    "text": "Software versions\nPlease note that I reran the code on this page on August 6, 2023.\n\ndevtools::session_info()\n\n─ Session info ───────────────────────────────────────────────────────────────\n setting  value\n version  R version 4.3.1 (2023-06-16)\n os       Ubuntu 22.04.2 LTS\n system   x86_64, linux-gnu\n ui       X11\n language en_US\n collate  en_US.UTF-8\n ctype    en_US.UTF-8\n tz       Europe/Zurich\n date     2023-08-06\n pandoc   3.1.1 @ /usr/lib/rstudio/resources/app/bin/quarto/bin/tools/ (via rmarkdown)\n\n─ Packages ───────────────────────────────────────────────────────────────────\n package     * version date (UTC) lib source\n cachem        1.0.6   2021-08-19 [2] CRAN (R 4.2.0)\n callr         3.7.3   2022-11-02 [1] CRAN (R 4.3.1)\n cannonball  * 0.1.1   2023-06-21 [1] Github (janhove/cannonball@fe70eff)\n cli           3.6.1   2023-03-23 [1] CRAN (R 4.3.0)\n colorspace    2.1-0   2023-01-23 [1] CRAN (R 4.3.0)\n crayon        1.5.2   2022-09-29 [1] CRAN (R 4.3.1)\n devtools      2.4.5   2022-10-11 [1] CRAN (R 4.3.1)\n digest        0.6.29  2021-12-01 [2] CRAN (R 4.2.0)\n dplyr         1.1.2   2023-04-20 [1] CRAN (R 4.3.0)\n ellipsis      0.3.2   2021-04-29 [2] CRAN (R 4.2.0)\n evaluate      0.15    2022-02-18 [2] CRAN (R 4.2.0)\n fansi         1.0.4   2023-01-22 [1] CRAN (R 4.3.1)\n farver        2.1.1   2022-07-06 [1] CRAN (R 4.3.0)\n fastmap       1.1.0   2021-01-25 [2] CRAN (R 4.2.0)\n fs            1.5.2   2021-12-08 [2] CRAN (R 4.2.0)\n generics      0.1.3   2022-07-05 [1] CRAN (R 4.3.0)\n ggplot2     * 3.4.2   2023-04-03 [1] CRAN (R 4.3.0)\n glue          1.6.2   2022-02-24 [2] CRAN (R 4.2.0)\n gtable        0.3.3   2023-03-21 [1] CRAN (R 4.3.0)\n htmltools     0.5.5   2023-03-23 [1] CRAN (R 4.3.0)\n htmlwidgets   1.6.2   2023-03-17 [1] CRAN (R 4.3.1)\n httpuv        1.6.11  2023-05-11 [1] CRAN (R 4.3.1)\n jsonlite      1.8.7   2023-06-29 [1] CRAN (R 4.3.1)\n knitr         1.39    2022-04-26 [2] CRAN (R 4.2.0)\n labeling      0.4.2   2020-10-20 [1] CRAN (R 4.3.0)\n later         1.3.1   2023-05-02 [1] CRAN (R 4.3.1)\n lifecycle     1.0.3   2022-10-07 [1] CRAN (R 4.3.0)\n magrittr      2.0.3   2022-03-30 [1] CRAN (R 4.3.0)\n memoise       2.0.1   2021-11-26 [2] CRAN (R 4.2.0)\n mime          0.10    2021-02-13 [2] CRAN (R 4.0.2)\n miniUI        0.1.1.1 2018-05-18 [1] CRAN (R 4.3.1)\n munsell       0.5.0   2018-06-12 [1] CRAN (R 4.3.0)\n pillar        1.9.0   2023-03-22 [1] CRAN (R 4.3.0)\n pkgbuild      1.4.2   2023-06-26 [1] CRAN (R 4.3.1)\n pkgconfig     2.0.3   2019-09-22 [2] CRAN (R 4.2.0)\n pkgload       1.3.2.1 2023-07-08 [1] CRAN (R 4.3.1)\n prettyunits   1.1.1   2020-01-24 [2] CRAN (R 4.2.0)\n processx      3.8.2   2023-06-30 [1] CRAN (R 4.3.1)\n profvis       0.3.8   2023-05-02 [1] CRAN (R 4.3.1)\n promises      1.2.0.1 2021-02-11 [1] CRAN (R 4.3.1)\n ps            1.7.5   2023-04-18 [1] CRAN (R 4.3.1)\n purrr         1.0.1   2023-01-10 [1] CRAN (R 4.3.0)\n R6            2.5.1   2021-08-19 [2] CRAN (R 4.2.0)\n Rcpp          1.0.11  2023-07-06 [1] CRAN (R 4.3.1)\n remotes       2.4.2   2021-11-30 [2] CRAN (R 4.2.0)\n rlang         1.1.1   2023-04-28 [1] CRAN (R 4.3.0)\n rmarkdown     2.21    2023-03-26 [1] CRAN (R 4.3.0)\n rstudioapi    0.14    2022-08-22 [1] CRAN (R 4.3.0)\n scales        1.2.1   2022-08-20 [1] CRAN (R 4.3.0)\n sessioninfo   1.2.2   2021-12-06 [2] CRAN (R 4.2.0)\n shiny         1.7.4.1 2023-07-06 [1] CRAN (R 4.3.1)\n stringi       1.7.12  2023-01-11 [1] CRAN (R 4.3.1)\n stringr       1.5.0   2022-12-02 [1] CRAN (R 4.3.0)\n tibble        3.2.1   2023-03-20 [1] CRAN (R 4.3.0)\n tidyselect    1.2.0   2022-10-10 [1] CRAN (R 4.3.0)\n urlchecker    1.0.1   2021-11-30 [1] CRAN (R 4.3.1)\n usethis       2.2.2   2023-07-06 [1] CRAN (R 4.3.1)\n utf8          1.2.3   2023-01-31 [1] CRAN (R 4.3.1)\n vctrs         0.6.3   2023-06-14 [1] CRAN (R 4.3.0)\n withr         2.5.0   2022-03-03 [2] CRAN (R 4.2.0)\n xfun          0.39    2023-04-20 [1] CRAN (R 4.3.0)\n xtable        1.8-4   2019-04-21 [1] CRAN (R 4.3.1)\n yaml          2.3.5   2022-02-21 [2] CRAN (R 4.2.0)\n\n [1] /home/jan/R/x86_64-pc-linux-gnu-library/4.3\n [2] /usr/local/lib/R/site-library\n [3] /usr/lib/R/site-library\n [4] /usr/lib/R/library\n\n──────────────────────────────────────────────────────────────────────────────"
  },
  {
    "objectID": "posts/2022-12-20-julia-fibonacci/index.html",
    "href": "posts/2022-12-20-julia-fibonacci/index.html",
    "title": "Adjusting to Julia: Generating the Fibonacci sequence",
    "section": "",
    "text": "I’m currently learning a bit of Julia and I thought I’d share with you a couple of my attempts at writing Julia code. I’ll spare you the sales pitch, and I’ll skip straight to the goal of this blog post: writing three different Julia functions that can generate the Fibonacci sequence."
  },
  {
    "objectID": "posts/2022-12-20-julia-fibonacci/index.html#the-fibonacci-sequence",
    "href": "posts/2022-12-20-julia-fibonacci/index.html#the-fibonacci-sequence",
    "title": "Adjusting to Julia: Generating the Fibonacci sequence",
    "section": "The Fibonacci sequence",
    "text": "The Fibonacci sequence\nThe famous Fibonacci sequence is an infinite sequence of natural numbers, the first of which are 1, 1, 2, 3, 5, 8, 13, …. The sequence is defined as follows:\n\\[\n\\textrm{Fibonacci}(n) =\n  \\begin{cases}\n    1, & \\textrm{for $n = 1$ or $n = 2$}, \\\\\n    \\textrm{Fibonacci}(n - 1) + \\textrm{Fibonacci}(n - 2), & \\textrm{otherwise}.  \n  \\end{cases}\n\\]\nLet’s write some Julia functions that can generate this sequence."
  },
  {
    "objectID": "posts/2022-12-20-julia-fibonacci/index.html#julia",
    "href": "posts/2022-12-20-julia-fibonacci/index.html#julia",
    "title": "Adjusting to Julia: Generating the Fibonacci sequence",
    "section": "Julia",
    "text": "Julia\nYou can download Julia from julialang.org. I’m currently using the Pluto.jl package that allows you to write Julia code in a reactive notebook. Check out the Pluto.jl page for more information."
  },
  {
    "objectID": "posts/2022-12-20-julia-fibonacci/index.html#first-alternative-a-purely-recursive-function",
    "href": "posts/2022-12-20-julia-fibonacci/index.html#first-alternative-a-purely-recursive-function",
    "title": "Adjusting to Julia: Generating the Fibonacci sequence",
    "section": "First alternative: A purely recursive function",
    "text": "First alternative: A purely recursive function\nThe Fibonacci sequence is defined recursively: To obtain the nth Fibonacci number, you first need to compute the n-1th and n-2th Fibonacci number and then add them. We can write a Julia function that exactly reflects the definition of the Fibonacci sequence like so:\n\nfunction fibonacci(n)\n  if n &lt;= 2\n    return 1\n  end\n  return fibonacci(n - 1) + fibonacci(n - 2)\nend\n\nfibonacci (generic function with 1 method)\n\n\nThis function tacitly assumes that n is a non-zero natural number. If n is equal to or lower than 2, i.e., if n is 1 or 2, it immediately returns 1, as per the definition of the sequence. If this condition isn’t met, the output is computed recursively. The function can be run as follows:\n\nfibonacci(10)\n\n55\n\n\nChecks out! But from a computational point of view, the fibonacci() function is quite wasteful. In order to obtain fibonacci(10), we need to compute fibonacci(9) and fibonacci(8). But in order to compute fibonacci(9), we also need to compute fibonacci(8). For both fibonacci(9) and fibonacci(8), we need to compute fibonacci(7), etc. In fact, we need to compute the value of fibonacci(8) two times, that of fibonacci(7) three times, that of fibonacci(6) five times, and that of fibonacci(5) seven times. So we’d be doing lots of computations over and over again. For this reason, the fibonacci() function is hopelessly inefficient: While you can compute fibonacci(10) in a fraction of a second, it may take minutes to compute, say, fibonacci(60). Luckily, we can speed up our function considerably."
  },
  {
    "objectID": "posts/2022-12-20-julia-fibonacci/index.html#second-alternative-recursion-with-memoisation",
    "href": "posts/2022-12-20-julia-fibonacci/index.html#second-alternative-recursion-with-memoisation",
    "title": "Adjusting to Julia: Generating the Fibonacci sequence",
    "section": "Second alternative: Recursion with memoisation",
    "text": "Second alternative: Recursion with memoisation\nMemoisation is a programming technique where any intermediate result that you computed is stored in an array. Before computing any further intermediate results, you then first look up in the array if you haven’t in fact already computed it, saving you a lot of unnecessary computations. The following Julia function is a bit more involved that the previous one, but it’s much more efficient.\n\nfunction fib_memo(n)\n  known = zeros(Int64, n)\n  function memoize(k)\n    if known[k] != 0\n      # do nothing\n    elseif k == 1 || k == 2\n      known[k] = 1\n    else\n      known[k] = memoize(k-1) + memoize(k-2)\n    end\n    return known[k]\n  end\n  return memoize(n)\nend\n\nfib_memo (generic function with 1 method)\n\n\nThe overall function that we’ll actually call is fib_memo(). It creates an array called known with n zeroes. Then it defines an inner function memoize(). This latter function obtains an integer k that in practice will range from 0 to n and does the following. First, it checks if the kth value in the array known is still 0. If it got changed, the function just returns the kth value in known. Otherwise, if k is equal to either 1 or 2, it sets the first or second value of known to 1. If k is greater than 2, the kth value of known is computed recursively. In all cases, the memoize() function returns the k value of the known array. The outer fib_memo() function then just returns the result of memoize(n).\nPerhaps by now, your computer has finished running fibonacci(60) and you can try out the alternative implementation:\n\nfib_memo(60)\n\n1548008755920\n\n\nNotice how much faster this new function is! Even the 200th Fibonacci number can be computed in a fraction of a second:\n\nfib_memo(200)\n\n-1123705814761610347\n\n\nUnfortunately, we’ve ran into a different problem now: integer overflow. The result of the computations has become so large that it exceeded the range of 64-bit integers. To fix this problem, we can work with BigIntegers instead:\n\nfunction fib_memo(n)\n  known = zeros(BigInt, n)\n  function memoize(k)\n    if known[k] != 0\n      # do nothing\n    elseif k == 1 || k == 2\n      known[k] = 1\n    else\n      known[k] = memoize(k-1) + memoize(k-2)\n    end\n    return known[k]\n  end\n  return memoize(n)\nend\n\nfib_memo (generic function with 1 method)\n\n\n\nfib_memo(200)\n\n280571172992510140037611932413038677189525\n\n\nNice!"
  },
  {
    "objectID": "posts/2022-12-20-julia-fibonacci/index.html#third-alternative-using-binets-formula",
    "href": "posts/2022-12-20-julia-fibonacci/index.html#third-alternative-using-binets-formula",
    "title": "Adjusting to Julia: Generating the Fibonacci sequence",
    "section": "Third alternative: Using Binet’s formula",
    "text": "Third alternative: Using Binet’s formula\nThe third alternative is more of a mathematical solution rather than a programming solution. According to Binet’s formula, the nth Fibonacci number can be computed as \\[\n\\textrm{Fibonacci}(n) = \\frac{\\varphi^n - \\psi^n}{\\sqrt{5}},\n\\] where \\(\\varphi = \\frac{1 + \\sqrt{5}}{2}\\), the Golden Ratio, and \\(\\psi = \\frac{1 - \\sqrt{5}}{2}\\), its conjugate. In Julia:\n\nfunction fib_binet(n)\n  φ = (1 + sqrt(5))/2\n  ψ = (1 - sqrt(5))/2\n  fib_n = 1/sqrt(5) * (φ^n - ψ^n)\n  return BigInt(round(fib_n))\nend\n\nfib_binet (generic function with 1 method)\n\n\nNote that you can use mathematical symbols like \\(\\varphi\\) and \\(\\psi\\) in Julia. This function runs very fast, too:\n\nfib_binet(60)\n\n1548008755920\n\n\n\nfib_binet(200)\n\n280571172992512015699912586503521287798784\n\n\nNotice, however, that the result for the 200th Fibonacci number differs by 27 orders of magnitude from the one obtained using fib_memo():\n\nfib_binet(200) - fib_memo(200)\n\n1875662300654090482610609259\n\n\nBy using Binet’s formula, we’ve left the fairly neat world of integer arithmetic and entered the realm of floating point arithmetic that is rife with approximation errors. While we’re at it, we might as well compute and plot the size of these approximation errors. In the snippet below, I first use list comprehension in order to compute the first 200 Fibonacci numbers using both fib_memo() and fib_binet(). Note that I added a dot (.) to both function names. This is Julia notation for running vectorised computations. Further note that I end all lines with a semi-colon so that the results don’t get printed to the prompt. Then, I compute the absolute values of the differences between the numbers obtained by both computation methods. Note again the use of a dot in both abs.() and .- that is required to have both of these functions work on vectors. Finally, I convert these absolute differences to differences relative to the correct answers;\n\nfib_integer = fib_memo.(1:200);\nfib_math    = fib_binet.(1:200);\nabs_diff = abs.(fib_math .- fib_integer);\nrel_diff = abs_diff ./ fib_integer;\n\nTo wrap off this blog post, let’s now plot these absolute and relative differences using the Plots.jl package. While Figure 1 shows that the absolute error becomes huge, Figure 2 shows that these discrepancies only amount to a negligble fraction of the correct answers.\n\nusing Plots\nplot(1:200, abs_diff, seriestype=:scatter,\n     xlabel = \"n\",\n     ylabel = \"absolute difference\",\n     label = \"\")\n\n\n\n\n  \n    \n  \n\n\n\n  \n    \n  \n\n\n\n  \n    \n  \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFigure 1. Absolute difference between the Fibonacci numbers obtained using fib_binet() and those obtained using fib_memo().\n\n\nplot(1:200, rel_diff, seriestype=:scatter, \n     xlabel = \"n\",\n     ylabel = \"relative difference\",\n     label = \"\")\n\n\n\n\n  \n    \n  \n\n\n\n  \n    \n  \n\n\n\n  \n    \n  \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFigure 2. Relative difference between the Fibonacci numbers obtained using fib_binet() and those obtained using fib_memo()."
  },
  {
    "objectID": "posts/2017-04-23-visualising-models-1/index.html",
    "href": "posts/2017-04-23-visualising-models-1/index.html",
    "title": "Tutorial: Plotting regression models",
    "section": "",
    "text": "The results of regression models, particularly fairly complex ones, can be difficult to appreciate and hard to communicate to an audience. One useful technique is to plot the effect of each predictor variable on the outcome while holding constant any other predictor variables. Fox (2003) discusses how such effect displays are constructed and provides an implementation in the effects package for R.\nSince I think it’s both instructive to see how effect displays are constructed from the ground up and useful to be able to tweak them yourself in R, this blog post illustrates how to draw such plots for three increasingly complex statistical models: ordinary multiple regression, logistic regression, and mixed-effects logistic regression. The goal in each of these three examples is to visualise the effects of the predictor variables without factoring in the uncertainty about these effects; visualising such uncertainty will be the topic of a future blog post."
  },
  {
    "objectID": "posts/2017-04-23-visualising-models-1/index.html#multiple-regression",
    "href": "posts/2017-04-23-visualising-models-1/index.html#multiple-regression",
    "title": "Tutorial: Plotting regression models",
    "section": "Multiple regression",
    "text": "Multiple regression\n\nData\nFor the first example, we’ll use a dataset from my PhD thesis. 163 German-speaking participants translated 45 spoken Swedish words into German and took a couple of cognitive and linguistic tests. For the sake of this example, we will model the number of correctly translated spoken words (Spoken) in terms of the participants’ performance on an intelligence test (Raven.Right) and an English-language test (English.Cloze) as well as their Sex. We’ll also fit an interaction between the intelligence test score and the participants’ sex, but this is mainly to illustrate how interactions can be visualised in effect displays.\nThe lines below read in the data, retain the variables of interest, filter out any rows with incomplete data, and summarise the structure of the dataset.\n\n# Read in data\nex1 &lt;- read.csv(\"http://homeweb.unifr.ch/VanhoveJ/Pub/Data/participants_163.csv\",\n                stringsAsFactors = TRUE)\n# Retain variables of interest\nex1 &lt;- ex1[, c(\"Subject\", \"Spoken\", \"Sex\", \"Raven.Right\", \"English.Cloze\")]\n# Retain complete cases\nex1 &lt;- ex1[complete.cases(ex1), ]\n# Summarise structure\nstr(ex1)\n\n'data.frame':   160 obs. of  5 variables:\n $ Subject      : int  1034 1076 1232 1264 1312 134 1350 1401 1404 1644 ...\n $ Spoken       : int  19 20 20 9 2 24 18 20 20 21 ...\n $ Sex          : Factor w/ 2 levels \"female\",\"male\": 1 2 2 2 2 2 2 2 1 1 ...\n $ Raven.Right  : int  23 32 8 19 2 24 31 21 27 19 ...\n $ English.Cloze: int  11 11 19 2 2 8 21 20 15 17 ...\n\n\n\n\nModel\nSince this isn’t a tutorial on modelling data, we won’t bother with whether this dataset can be analysed in a better way and we’ll just fit the data in a multiple regression model. Even though it isn’t really necessary in this example, it’s generally good practice to first centre the numeric predictors at their sample means and to convert the binary predictor Sex to a numeric predictor with the values -0.5 and 0.5 for female and male, respectively (= sum-coding). This makes the model’s intercept readily interpretable as the modelled Spoken score for someone of unknown sex with average Raven and English scores. In this example, it adds a couple of extra steps to the process, but centring and numerically recoding variables is a good habit to get into.\n\n# Define shorthand for centring at sample mean,\n# i.e., subtracting sample mean from each value\nc. &lt;- function(x) x - mean(x)\n\n# Centre numeric predictors at sample mean\nex1$c.Raven &lt;- c.(ex1$Raven.Right)\nex1$c.English &lt;- c.(ex1$English.Cloze)\n\n# Sum-code binary predictor (-0.5 vs. 0.5)\nex1$n.Sex &lt;- as.numeric(ex1$Sex) - 1.5\n# Confirm that female = -0.5, and male = 0.5\nxtabs(~ n.Sex + Sex, ex1)\n\n      Sex\nn.Sex  female male\n  -0.5     89    0\n  0.5       0   71\n\n# Fit multiple regression model with n.Sex, \n# c.Raven, their interaction, and c.English\nmod1 &lt;- lm(Spoken ~ n.Sex * c.Raven + c.English,\n           data = ex1)\n\n# Show model coefficients\ncoef(mod1)\n\n  (Intercept)         n.Sex       c.Raven     c.English n.Sex:c.Raven \n       16.510        -1.234         0.209         0.329         0.180 \n\n\n\n\nEffect displays\nFor the first effect display, we want to visualise the modelled relationship between the participants’ English score and their sex on the one hand and their Spoken score on the other hand while holding constant their performance on the Raven intelligence test. To this end, we create a new dataframe (I’ll prefix these with nd) that contains each unique c.English value that occurs in the dataset (there are 25 such values) in combination with each possible value of n.Sex (i.e., -0.5 and 0.5), yielding a dataframe with 50 entries. To this dataframe, we add the c.Raven value at which we want to hold this variable constant. A natural choice is the sample mean, which is obviously 0 for the centred variable. But you could also pick the sample median (median(ex1$c.Raven)) instead, or indeed any other value that you think is sensible. The expand.grid() call accomplishes all of this:\n\nnd1_eng &lt;- expand.grid(\n  # Two possible values for n.Sex\n  n.Sex = c(-0.5, 0.5),\n  # Fix c.Raven at its sample mean (= 0)\n  c.Raven = 0,\n  # Unique c.English values in sample\n  c.English = unique(ex1$c.English)\n)\n# Show first 6 rows\nhead(nd1_eng)\n\n  n.Sex c.Raven c.English\n1  -0.5       0     -3.64\n2   0.5       0     -3.64\n3  -0.5       0      4.36\n4   0.5       0      4.36\n5  -0.5       0    -12.64\n6   0.5       0    -12.64\n\n\nWe can now add to this dataframe the average Spoken values that you would expect to find according to the regression model for each combination of the n.Sex, c.Raven and c.English values listed. This is simply a matter of taking these values, multiplying them by the corresponding model efficients (shown under ‘Model’ above), and adding them up (don’t forget to include the intercept term!).\nLet’s walk through this for the first row in nd1_eng.\n\nTake the model’s estimated intercept: 16.51.\nMultiply -0.5 (n.Sex value) with the model’s n.Sex coefficient: -0.5 × -1.234 = 0.617.\nMultiply 0 (c.Raven value) with the model’s c.Raven coefficient, which obviously yields 0.\nMultiply -3.64 (c.English value) with the model’s c.English coefficient: -3.64 × 0.329 = -1.2.\nFor the interaction term between n.Sex and c.Raven, multiply the two values and the interaction coefficient: -0.5 × 0 × 0.180 = 0.\nAdd the results: 16.51 + 0.617 + 0 - 1.2 + 0 = 15.9.\n\n15.9, then, is the predicted average Spoken value for women whose Raven intelligence score is at the sample mean and whose English-language test result is 3.64 below the sample mean. (If you ever come across the expression Mβ in a statistics text, the above computation is what is meant: take the predictor values, multiply them with the respective model coefficients, and add them up together with the intercept.) The predict() does this calculation for all rows in nd1_eng:\n\n# Use mod1 to 'predict' the Spoken values for the data in nd1_eng,\n# and add these predictions to nd1_eng:\nnd1_eng$Prediction &lt;- predict(mod1, newdata = nd1_eng)\n\n# Show first 6 rows\nhead(nd1_eng)\n\n  n.Sex c.Raven c.English Prediction\n1  -0.5       0     -3.64       15.9\n2   0.5       0     -3.64       14.7\n3  -0.5       0      4.36       18.6\n4   0.5       0      4.36       17.3\n5  -0.5       0    -12.64       13.0\n6   0.5       0    -12.64       11.7\n\n\nNow, finally, we can use these predicted values to visualise the modelled relationship between c.English and n.Sex on the one hand and Spoken on the other hand. (Check out the tutorial on drawing line charts if the following commands are new to you.)\n\nlibrary(ggplot2)\nggplot(nd1_eng,\n       aes(x = c.English,\n           y = Prediction,\n           colour = factor(n.Sex))) +\n  geom_line()\n\n\n\n\nThe predictor variables c.English and n.Sex aren’t readily interpretable because they are centred and sum-coded, respectively. To make the plot above more informative, we can de-centre the c.English variable by simply adding the sample mean of English.Cloze in the original dataset (= ex1) to the c.English values in nd1_eng. Similarly, we can relabel the values in n.Sex:\n\n# De-centre c.English\nnd1_eng$English.Cloze &lt;- nd1_eng$c.English + mean(ex1$English.Cloze)\n# Relabel n.Sex\nnd1_eng$Sex &lt;- ifelse(nd1_eng$n.Sex == -0.5, \"woman\", \"man\")\n\n# Plot using decentred/relabel variables\nggplot(nd1_eng,\n       aes(x = English.Cloze,\n           y = Prediction,\n           colour = Sex)) +\n  geom_line() +\n  xlab(\"English cloze test score\") +\n  ylab(\"Modelled 'Spoken' score\")\n\n\n\n\nThe plot shows the average Spoken values predicted by the regression model for men and women with a Raven test score equal to the current sample mean depending on their performance on the English cloze test. Usually, though, the precise values matter less than the general pattern of the results.\nThe female advantage in the plot above is constant across the English.Cloze range, which isn’t surprising: we didn’t model an interaction between n.Sex and c.English, so we won’t find one if we plot the modelled effects. We did, however, model an interaction between n.Sex and c.Raven, so let’s see what an effect display of an interaction looks like:\n\n# Construct grid with relevant predictor combinations\nnd1_rav &lt;- expand.grid(\n  n.Sex = c(-0.5, 0.5),\n  # Vary the Raven scores\n  c.Raven = unique(ex1$c.Raven),\n  # Fix c.English at sample mean\n  c.English = 0\n)\n# Add predictions\nnd1_rav$Prediction &lt;- predict(mod1, newdata = nd1_rav)\n\n# Decentre/relabel predictors\nnd1_rav$Raven &lt;- nd1_rav$c.Raven + mean(ex1$Raven.Right)\nnd1_rav$Sex &lt;- ifelse(nd1_rav$n.Sex == -0.5, \"woman\", \"man\")\n\n# Plot\nggplot(nd1_rav,\n       aes(x = Raven,\n           y = Prediction,\n           colour = Sex)) +\n  geom_line()\n\n\n\n\nIncidentally, I wouldn’t read too much into this interaction since there are considerably better ways to analyse this dataset. The aim here was to illustrate how to draw such plots, not to compare different methods of analysis."
  },
  {
    "objectID": "posts/2017-04-23-visualising-models-1/index.html#logistic-regression",
    "href": "posts/2017-04-23-visualising-models-1/index.html#logistic-regression",
    "title": "Tutorial: Plotting regression models",
    "section": "Logistic regression",
    "text": "Logistic regression\n\nData\nIn the second and third example we will apply the same procedure as in the first example to logistic regression model. The dataset is from Vanhove & Berthele (2015), and the goal in both cases is to model and visualise the relationship between a cognate pair’s orthographic Levenshtein distance (the larger the distance, the less orthographic overlap the cognates have) and their corpus frequency (log.FreqGermanic; a logarithmically transformed frequency measure) on the one hand and the probability that readers will spot the cognate relationship on the other hand.\nFor the present example, we will model the data from a single, randomly chosen participant, viz. DB3, so that the data can be analysed in a straightforward logistic regression model; for the next example, we’ll analyse the data for all participants simultaneously in a mixed-effects model.\n\n# Read in the data\nex2 &lt;- read.csv(\"http://homeweb.unifr.ch/VanhoveJ/Pub/Data/ExampleLogisticRegression.csv\",\n                stringsAsFactors = TRUE)\n# Retain only the observations for participant `DB3`:\nex2 &lt;- droplevels(subset(ex2, Subject == \"DB3\"))\nstr(ex2)\n\n'data.frame':   180 obs. of  8 variables:\n $ Subject         : Factor w/ 1 level \"DB3\": 1 1 1 1 1 1 1 1 1 1 ...\n $ Stimulus        : Factor w/ 180 levels \"aai\",\"aarde\",..: 94 134 35 15 1 5 41 148 55 174 ...\n $ Lx              : Factor w/ 4 levels \"DK\",\"FR\",\"NL\",..: 2 1 2 4 2 4 4 1 3 3 ...\n $ Correct         : int  1 0 0 0 0 0 0 0 1 1 ...\n $ MinLevGermanic  : num  0.2 0.5 0.58 0.4 0.67 0.29 0.58 0.4 0.43 0 ...\n $ log.FreqGermanic: num  6.11 1.18 2.33 1.96 3.26 ...\n $ Sex             : Factor w/ 1 level \"m\": 1 1 1 1 1 1 1 1 1 1 ...\n $ EngReading      : int  2 2 2 2 2 2 2 2 2 2 ...\n\n\n\n\nModel\nAs in the first example, centre the predictor variables, just to get into the habit:\n\nex2$c.Lev &lt;- c.(ex2$MinLevGermanic)\nex2$clog.Freq &lt;- c.(ex2$log.FreqGermanic)\n\nFit the logistic regression model:\n\nmod2 &lt;- glm(Correct ~ c.Lev + clog.Freq,\n            data = ex2,\n            family = \"binomial\")\n\n# Estimated model coefficients\ncoef(mod2)\n\n(Intercept)       c.Lev   clog.Freq \n     -0.331      -3.599       0.212 \n\n\n\n\nEffects display\nTo draw an effects display for the Levenshtein variable, we do the exact same thing as in the first example: construct a dataframe that contains the predictors whose effects we want to plot as well as the predictors whose effects we want to keep constant. Here, we want to create a dataframe in which c.Lev varies along its range and where clog.Freq is kept constant (e.g., at its sample mean of 0). Here I use the seq() function rather than unique() to specify the c.Lev values, but either will work.\n\nnd2_lev &lt;- expand.grid(\n  # A sequence of c.Lev values\n  # from the sample minimum\n  # through the sample maximum\n  # in steps of 0.05\n  c.Lev = seq(min(ex2$c.Lev),\n              max(ex2$c.Lev),\n              by = 0.05),\n  # Fix clog.Freq at its sample mean\n  clog.Freq = 0\n)\nhead(nd2_lev)\n\n   c.Lev clog.Freq\n1 -0.361         0\n2 -0.311         0\n3 -0.261         0\n4 -0.211         0\n5 -0.161         0\n6 -0.111         0\n\n\nUsing predict(), we can again compute the expected values – according to the model – for each combination of predictor values in nd2_lev. Since this is a logistic model, the predictions are produced in log-odds, which we can plot:\n\n# Fill in predicted values\nnd2_lev$Prediction &lt;- predict(mod2, nd2_lev)\n\n# Plot Levenshtein effect\nggplot(nd2_lev,\n       aes(x = c.Lev,\n           y = Prediction)) +\n  geom_line() +\n  xlab(\"Centred Levenshtein distance\") +\n  ylab(\"Modelled outcome (in log-odds)\")\n\n\n\n\nThere is one problem: No-one really thinks in log-odds, and any audience will more readily understand probabilities than log-odds, so it’s better to express these predictions as probabilities. To do so, specify the parameter type in the predict() function as \"response\". (Alternatively, apply the logistic function to the values in log-odds using plogis().) If we then de-centre the c.Lev variable, we can draw an interpretable effect display:\n\n# Fill in predicted values as probabilities\nnd2_lev$Probability &lt;- predict(mod2, nd2_lev,\n                               type = \"response\")\n\n# De-centre c.Lev\nnd2_lev$Levenshtein &lt;- nd2_lev$c.Lev + mean(ex2$MinLevGermanic)\n\n# Plot\nggplot(nd2_lev,\n       aes(x = Levenshtein,\n           y = Probability)) +\n  geom_line() +\n  ylab(\"Modelled probability\\nof correct response\")\n\n\n\n\nImportantly, we modelled the effect linearly in log-odds, but when the effect is expressed in probabilities, this yields a nonlinear curve. This is not surprising nor does it tell us anything about what we’re investigating: if you model something linearly in log-odds, the effect will be nonlinear if expressed in probabilities.\nNote, furthermore, that we kept the frequency predictor clog.Freq constant at 0 for this plot. If you fix this predictor at different values, you’ll end up with curves that don’t run perfectly parallel to each other, as the plot below illustrates. This is different from effects displays for ordinary regression models, where fixing the non-focus predictors at different values produces parallel lines, and it is due to the log-odds–to–probability transformation.\n\n\n\n\n\nTo avoid making this blog post much too long, I’ll leave the effect display of the frequency variable as an exercise to the reader."
  },
  {
    "objectID": "posts/2017-04-23-visualising-models-1/index.html#mixed-effects-logistic-regression",
    "href": "posts/2017-04-23-visualising-models-1/index.html#mixed-effects-logistic-regression",
    "title": "Tutorial: Plotting regression models",
    "section": "Mixed-effects logistic regression",
    "text": "Mixed-effects logistic regression\n\nData\nWe’ll use the same dataset as in the second example, but this time we don’t restrict the dataset to one participant.\n\nex3 &lt;- read.csv(\"http://homeweb.unifr.ch/VanhoveJ/Pub/Data/ExampleLogisticRegression.csv\",\n                stringsAsFactors = TRUE)\n\n\n\nModel\nNot that it’s really necessary, but since it’s a good habit to develop, centre the numeric variables at their sample mean, and express the binary predictor Sex as a numeric variable with the values -0.5 and 0.5.\n\n# Centre numeric variables\nex3$c.Lev &lt;- c.(ex3$MinLevGermanic)\nex3$clog.Freq &lt;- c.(ex3$log.FreqGermanic)\nex3$c.Eng &lt;- c.(ex3$EngReading)\n\n# Express Sex as numeric variable (+/- 0.5)\n# where -0.5 = m(an) and 0.5 = w(oman)\nex3$n.Sex &lt;- as.numeric(ex3$Sex) - 1.5\nxtabs(~ n.Sex + Sex, ex3)\n\n      Sex\nn.Sex      m     w\n  -0.5  6120     0\n  0.5      0 12060\n\n\nFit a logistic mixed-effects regression model using the glmer() function from the lme4 package. All effects are modelled linearly (in log-odds) without interaction: Levenshtein distance (c.Lev), corpus frequency (clog.Freq), the participants’ English reading skills (c.Eng) and their sex (n.Sex). The model also includes by-item and by-subject random intercepts as well as by-item adjustments to the English reading skills slope and by-subject adjustments to the Levenshtein effect.\n\nlibrary(lme4)\n\nLoading required package: Matrix\n\nmod3 &lt;- glmer(Correct ~ c.Lev + clog.Freq +\n                c.Eng + n.Sex +\n                (1 + c.Lev | Subject) +\n                (1 + c.Eng | Stimulus),\n              data = ex3,\n              family = \"binomial\")\n\n# Uncomment the next line to show model\n# summary(mod3)\n\n\n\nEffect display\nFor the mixed-effects model, I’ll assume that you want to plot the effect of a predictor variable for an otherwise average item and an otherwise average participant, i.e., a plot that is based on the fixed-effect estimates alone. An alternative would be that you’d plot the effects associated with a specific item (taking into account the random-effect adjustments for that specific item), with a specific participant (taking into account the random-effect adjustments for that particular participant), or both. For this fixed-effects-only scenario, which I suspect is usually what is of interest, the logic is the same as in the previous examples.\nFirst construct a data frame where the focus predictor varies along its range (for instance, using unique() or seq()) and where the other predictors are fixed at sensible values. Here I fix the frequency and the English reading predictor at their sample means (0). The n.Sex variable, however, doesn’t have a sample mean of 0 but of 0.163. (There are 67 women and only 34 men in the sample.) I fixed n.Sex at 0 nonetheless because I want to plot the modelled probabilities for an unknown participant of unspecified sex. If instead I wanted to plot the modelled probabilities for an unknown participant who is a 2-to-1 favourite to be a woman, I would use 0.163 instead; if I wanted to plot the modelled probabilities for an unknown male participant, I’d use -0.5.\n\nnd3_lev &lt;- expand.grid(\n  c.Lev = seq(min(ex3$c.Lev),\n              max(ex3$c.Lev),\n              by = 0.05),\n  clog.Freq = 0,\n  c.Eng = 0,\n  n.Sex = 0\n)\n\nThen add the predicted probabilities to this dataframe (type = \"response\"). The re.form = NA bit specifies that we don’t want to take into account any random effects.\n\nnd3_lev$Prediction &lt;- predict(mod3, nd3_lev,\n                              type = \"response\",\n                              re.form = NA)\n\nThen plot the results:\n\nnd3_lev$Levenshtein &lt;- nd3_lev$c.Lev + mean(ex3$MinLevGermanic)\nggplot(nd3_lev,\n       aes(x = Levenshtein,\n           y = Prediction)) +\n  geom_line()\n\n\n\n\nAs for the interpretation of this plot, these are the probabilities of a correct response that the model predicts\n\nfor a participant of unknown sex (n.Sex was set to 0, i.e., the participant is equally likely to be a man or a woman);\nwhose English reading skills equal the current sample average (c.Eng was set to 0)\nand whose cognate guessing skills are otherwise average compared to the current sample (we didn’t take into account any positive or negative by-participant adjustments to the overall intercept and c.Lev effects)\nwhen responding to stimuli with varying Levenshtein values\nwhose logarithmically transformed corpus frequencies equal the current sample mean (clog.Freq was set to 0)\nand which are otherwise of average difficult compared to the current sample (we didn’t take into account any positive or negative adjustments to by-stimulus adjustments to the overall intercept and the c.Eng effects).\n\nA quicker way of putting this is that these are the probabilities with which a participant with average English reading skills of whom nothing else is known would respond correctly to stimuli with varying Levenshtein values and average corpus frequencies of which nothing else is known either. Again, though, the idea is usually not so much to interpret these probabilities themselves but rather to get or give a broad sense of how variation in one predictor is associated with variation in the outcome."
  },
  {
    "objectID": "posts/2017-04-23-visualising-models-1/index.html#caveats-about-effect-displays",
    "href": "posts/2017-04-23-visualising-models-1/index.html#caveats-about-effect-displays",
    "title": "Tutorial: Plotting regression models",
    "section": "Caveats about effect displays",
    "text": "Caveats about effect displays\nI find effect displays useful to convey regression results, but you can’t expect too much from them.\nFirst of all, in terms of effect displays, you can only get out of a model what you put into it. For instance, if you don’t model an interaction between two variables, you won’t find one in the effect display. Similarly, if you model the effect of a predictor linearly, you won’t find a nonlinear effect in the plot – unless, of course, you apply some nonlinear transformation in the process, such as expressing the effects modelle in a logistic regression as probabilities. In the latter case, you’re guaranteed to find a nonlinear effect.\nI labour the latter point because I have seen one or two cases where a nonlinear effect emerging from a logistic regression was interpreted in subject-matter terms as something quite surprising. It isn’t: it’s a direct consequence of how the model works.\nThe second caveat is that “showing the effect of one variable while keeping constant the other variables” is often little more than a convenient fiction. While you can use a statistical model to estimate how a person’s wage changes when varying people’s French reading skills and holding constant their French oral skills, years of education, and intelligence, there needn’t be anything in the real world that corresponds to this ‘effect’ nor would any results be actionable. (How would you go about improving someone’s French reading skills while keeping their years of education and their French oral skills constant, anyway?) This caveat is especially relevant in correlational studies where sets of predictors are highly correlated with each other, rendering difficult or even impossible the interpretation the effect of any single ‘independent’ variable."
  },
  {
    "objectID": "posts/2017-04-23-visualising-models-1/index.html#software-versions",
    "href": "posts/2017-04-23-visualising-models-1/index.html#software-versions",
    "title": "Tutorial: Plotting regression models",
    "section": "Software versions",
    "text": "Software versions\n\ndevtools::session_info()\n\n─ Session info ───────────────────────────────────────────────────────────────\n setting  value\n version  R version 4.3.1 (2023-06-16)\n os       Ubuntu 22.04.2 LTS\n system   x86_64, linux-gnu\n ui       X11\n language en_US\n collate  en_US.UTF-8\n ctype    en_US.UTF-8\n tz       Europe/Zurich\n date     2023-08-07\n pandoc   3.1.1 @ /usr/lib/rstudio/resources/app/bin/quarto/bin/tools/ (via rmarkdown)\n\n─ Packages ───────────────────────────────────────────────────────────────────\n package     * version date (UTC) lib source\n boot          1.3-28  2021-05-03 [4] CRAN (R 4.2.0)\n cachem        1.0.6   2021-08-19 [2] CRAN (R 4.2.0)\n callr         3.7.3   2022-11-02 [1] CRAN (R 4.3.1)\n cli           3.6.1   2023-03-23 [1] CRAN (R 4.3.0)\n codetools     0.2-19  2023-02-01 [4] CRAN (R 4.2.2)\n colorspace    2.1-0   2023-01-23 [1] CRAN (R 4.3.0)\n crayon        1.5.2   2022-09-29 [1] CRAN (R 4.3.1)\n devtools      2.4.5   2022-10-11 [1] CRAN (R 4.3.1)\n digest        0.6.29  2021-12-01 [2] CRAN (R 4.2.0)\n dplyr         1.1.2   2023-04-20 [1] CRAN (R 4.3.0)\n ellipsis      0.3.2   2021-04-29 [2] CRAN (R 4.2.0)\n evaluate      0.15    2022-02-18 [2] CRAN (R 4.2.0)\n fansi         1.0.4   2023-01-22 [1] CRAN (R 4.3.1)\n farver        2.1.1   2022-07-06 [1] CRAN (R 4.3.0)\n fastmap       1.1.0   2021-01-25 [2] CRAN (R 4.2.0)\n fs            1.5.2   2021-12-08 [2] CRAN (R 4.2.0)\n generics      0.1.3   2022-07-05 [1] CRAN (R 4.3.0)\n ggplot2     * 3.4.2   2023-04-03 [1] CRAN (R 4.3.0)\n glue          1.6.2   2022-02-24 [2] CRAN (R 4.2.0)\n gtable        0.3.3   2023-03-21 [1] CRAN (R 4.3.0)\n htmltools     0.5.5   2023-03-23 [1] CRAN (R 4.3.0)\n htmlwidgets   1.6.2   2023-03-17 [1] CRAN (R 4.3.1)\n httpuv        1.6.11  2023-05-11 [1] CRAN (R 4.3.1)\n jsonlite      1.8.7   2023-06-29 [1] CRAN (R 4.3.1)\n knitr         1.39    2022-04-26 [2] CRAN (R 4.2.0)\n labeling      0.4.2   2020-10-20 [1] CRAN (R 4.3.0)\n later         1.3.1   2023-05-02 [1] CRAN (R 4.3.1)\n lattice       0.21-8  2023-04-05 [4] CRAN (R 4.3.0)\n lifecycle     1.0.3   2022-10-07 [1] CRAN (R 4.3.0)\n lme4        * 1.1-34  2023-07-04 [1] CRAN (R 4.3.1)\n magrittr      2.0.3   2022-03-30 [1] CRAN (R 4.3.0)\n MASS          7.3-60  2023-05-04 [4] CRAN (R 4.3.1)\n Matrix      * 1.6-0   2023-07-08 [4] CRAN (R 4.3.1)\n memoise       2.0.1   2021-11-26 [2] CRAN (R 4.2.0)\n mime          0.10    2021-02-13 [2] CRAN (R 4.0.2)\n miniUI        0.1.1.1 2018-05-18 [1] CRAN (R 4.3.1)\n minqa         1.2.5   2022-10-19 [1] CRAN (R 4.3.1)\n munsell       0.5.0   2018-06-12 [1] CRAN (R 4.3.0)\n nlme          3.1-162 2023-01-31 [4] CRAN (R 4.2.2)\n nloptr        2.0.3   2022-05-26 [1] CRAN (R 4.3.1)\n pillar        1.9.0   2023-03-22 [1] CRAN (R 4.3.0)\n pkgbuild      1.4.2   2023-06-26 [1] CRAN (R 4.3.1)\n pkgconfig     2.0.3   2019-09-22 [2] CRAN (R 4.2.0)\n pkgload       1.3.2.1 2023-07-08 [1] CRAN (R 4.3.1)\n prettyunits   1.1.1   2020-01-24 [2] CRAN (R 4.2.0)\n processx      3.8.2   2023-06-30 [1] CRAN (R 4.3.1)\n profvis       0.3.8   2023-05-02 [1] CRAN (R 4.3.1)\n promises      1.2.0.1 2021-02-11 [1] CRAN (R 4.3.1)\n ps            1.7.5   2023-04-18 [1] CRAN (R 4.3.1)\n purrr         1.0.1   2023-01-10 [1] CRAN (R 4.3.0)\n R6            2.5.1   2021-08-19 [2] CRAN (R 4.2.0)\n Rcpp          1.0.11  2023-07-06 [1] CRAN (R 4.3.1)\n remotes       2.4.2   2021-11-30 [2] CRAN (R 4.2.0)\n rlang         1.1.1   2023-04-28 [1] CRAN (R 4.3.0)\n rmarkdown     2.21    2023-03-26 [1] CRAN (R 4.3.0)\n rstudioapi    0.14    2022-08-22 [1] CRAN (R 4.3.0)\n scales        1.2.1   2022-08-20 [1] CRAN (R 4.3.0)\n sessioninfo   1.2.2   2021-12-06 [2] CRAN (R 4.2.0)\n shiny         1.7.4.1 2023-07-06 [1] CRAN (R 4.3.1)\n stringi       1.7.12  2023-01-11 [1] CRAN (R 4.3.1)\n stringr       1.5.0   2022-12-02 [1] CRAN (R 4.3.0)\n tibble        3.2.1   2023-03-20 [1] CRAN (R 4.3.0)\n tidyselect    1.2.0   2022-10-10 [1] CRAN (R 4.3.0)\n urlchecker    1.0.1   2021-11-30 [1] CRAN (R 4.3.1)\n usethis       2.2.2   2023-07-06 [1] CRAN (R 4.3.1)\n utf8          1.2.3   2023-01-31 [1] CRAN (R 4.3.1)\n vctrs         0.6.3   2023-06-14 [1] CRAN (R 4.3.0)\n withr         2.5.0   2022-03-03 [2] CRAN (R 4.2.0)\n xfun          0.39    2023-04-20 [1] CRAN (R 4.3.0)\n xtable        1.8-4   2019-04-21 [1] CRAN (R 4.3.1)\n yaml          2.3.5   2022-02-21 [2] CRAN (R 4.2.0)\n\n [1] /home/jan/R/x86_64-pc-linux-gnu-library/4.3\n [2] /usr/local/lib/R/site-library\n [3] /usr/lib/R/site-library\n [4] /usr/lib/R/library\n\n──────────────────────────────────────────────────────────────────────────────"
  },
  {
    "objectID": "posts/2015-01-14-overaccuracy/index.html",
    "href": "posts/2015-01-14-overaccuracy/index.html",
    "title": "Overaccuracy and false precision",
    "section": "",
    "text": "I once attended a talk at a conference where the speaker underscored his claim by showing us a Pearson correlation coefficient – reported with 9-digit (give or take) precision…. Five years later, I can neither remember what the central claim was nor could I ball-park the correlation coefficient – I only recall that it consisted of nine or so digits. Hence the take-home message of this post: ask yourself how many digits are actually meaningful."
  },
  {
    "objectID": "posts/2015-01-14-overaccuracy/index.html#overaccuracy",
    "href": "posts/2015-01-14-overaccuracy/index.html#overaccuracy",
    "title": "Overaccuracy and false precision",
    "section": "Overaccuracy",
    "text": "Overaccuracy\nI’m currently reading Jeff Siegel’s Second Dialect Acquisition. So far I find it to be a well written summary of an interesting topic, combining as it does the subject matter of the class I’ll be teaching next semester (Second Language Acquisition) with a field I was rather fond of as an undergrad (dialectology) and the general theme of my thesis (closely related languages). But I also stumbled upon a couple of sentences like these:\n\nFor example, when his [= Donn Bayard’s] son was 8 years old, he used the NZE [New Zealand English] R-lessness variant 68.3 per cent of the time when talking to NZE-speaking friends, but only 34.2 per cent of the time when speaking to his NAE [North American English]-speaking parents. In other words, he maintained his D1 [native dialect] feature of non-prevocalic /r/ to some extent, using it 65.8 per cent of the time with his parents but repressing it with his peers, using it only 31.7 per cent of the time. (pp. 68-69)\n\nI don’t doubt that this is an accurate description of Bayard’s findings, which appeared in the 1995 volume of the New Zealand English Newsletter (Update (2023-08-26): Link broken.). Quite the contrary: I think it’s too accurate a description of his research findings. Overaccuracy is only a minor pet peeve of mine, but it ties in with my feeling that we can do with fewer tests when reporting research findings, so here goes.\nWithout having read Bayard’s paper (it’s unavailable), it’s necessarily true that its results were based on a sample of linguistic behaviour – if you were to run the same study again in a parallel universe, you’d be unlikely to come up with the exact same numbers (sampling error). Reporting these frequencies with one-in-thousand precision, however, projects more certainty about the child’s preferences than you really have. Of course, readers with some research experience under their belt will be aware that you can’t really know this sort of thing with per-mille precision. These readers will probably mentally transform the numbers reported to mean “roughly two in three” (68.3 and 65.8%) and “about one in three” (34.2 and 31.7%) and consider that to be the paper’s take-home message. But if that’s the (quite elegant, I think) take-home message, why not just say it like that? Admittedly, “about two thirds of the time” doesn’t sound very scientific, though I do think it’s an accurate summary of the trends in the data that also gives a fairer if rough estimate of the uncertainty surrounding that summary.\nAnother way to look at this involves computing a confidence interval around these frequencies. Assuming that Bayard’s son produced 1,000 relevant tokens when talking to his NZE-speaking friends (I don’t know as I don’t have access to the original study), out of which 683 were R-less, we can compute a 95% confidence interval around this proportion as follows:\n\nprop.test(683, 1000)$conf.int\n\n[1] 0.6529967 0.7115785\nattr(,\"conf.level\")\n[1] 0.95\n\n\nConfidence intervals are trickier to interpret than you’d think (I’m not sure whether I’ve entirely figured them out myself), but at the very least this interval (65-71%) helps to put the third digit in perspective: it’s scientifically – as well as rhetorically – meaningless at best and mentally cluttering at worst. There’s an argument to be made for reporting such intervals to counter overaccuracy, and you could question whether the second digit conveys any useful information, but I’d already be happy with the in-between solution of reporting such relative frequencies by rounding them up to the nearest percentage (i.e. 68, 66, 34 and 32%). Incidentally, I don’t want to single out Siegel’s rendition of Bayard’s findings: examples of over-accurate reporting abound in the scientific literature (Andrew Gelman occassionally discusses some cases, too)."
  },
  {
    "objectID": "posts/2015-01-14-overaccuracy/index.html#false-precision",
    "href": "posts/2015-01-14-overaccuracy/index.html#false-precision",
    "title": "Overaccuracy and false precision",
    "section": "False precision",
    "text": "False precision\nAn issue more basic than overaccuracy is false precision. An example of false precision is the following state of affairs. Researchers working with response latencies (measured in milliseconds) often tabulate the mean response latency as well as its standard deviation for each experimental group – and do so with one ten-thousandth of a second’s percision. For instance, when the latencies measured are 378, 512 and 483 ms, the mean and standard deviation will often be reported as 457.7 and 70.5 ms.\nThe problem with this isn’t that it’s an inaccurate summary of the data. Rather, the problem is that reporting a mean as being 457.7 milliseconds suggests that your instruments allow you to make measurements precise to one ten-thousandth of a second rather than to one thousandth of a second – an order-of-magnitude difference. This false precision is a slightly different problem from overaccuracy, e.g. reporting relative frequencies with per-mille precision. My issue with the overaccurate reporting of relative frequencies is that it portrays a greater degree of certainty about the ‘true’ proportion (i.e. the population parameter) than the data allow; the problem with false precision is that it conveys a level of confidence in the descriptive statistics (i.e. the sample estimate) and the instruments used that is off by one or more orders of magnitude.\nThe rule of thumb (instilled into my mind by my physics teachers) is to avoid reporting more significant figures in numerical summaries than the measurements themselves had. In other words, since the measurements were made with millisecond precision, descriptive statistics should be reported with – at most – millisecond precision, too. While this is a useful principle, I doubt that its wholesale adoption in applied linguistics is around the corner. But it’s something to keep in mind."
  },
  {
    "objectID": "posts/2015-01-14-overaccuracy/index.html#conclusion",
    "href": "posts/2015-01-14-overaccuracy/index.html#conclusion",
    "title": "Overaccuracy and false precision",
    "section": "Conclusion",
    "text": "Conclusion\nThe introductory example featured a 9-digit correlation coefficient. I don’t remember anything about the study itself, but let’s say the correlation coefficient reported was 0.228526687. Even with 10,000 observations, the 95% confidence interval around this coefficient is [0.21, 0.25]. (With 100 observations, it’s [0.0 - 0.4]. Confidence intervals around correlation coefficients are huge!) Thus, in terms of telling us something about the patterns in the population, the last 7 digits don’t buy us anything. If we additionally consider the coarseness of the measurements in social science research and in the humanities (e.g. questionnaire data), it’s doubtful whether even the second digit contains any meaningful information about the patterns in the sample.\nI don’t have any clear-cut guidelines to offer, but asking yourself how many digits actually contribute meaningful information seems like a good place to start. There’s nothing unscientific or sloppy about reporting ‘r = 0.23’ or even ‘r = 0.2’ if that better reflects the precision of the measurements and the uncertainty of the inferences."
  },
  {
    "objectID": "posts/2020-05-05-contrast-coding/index.html",
    "href": "posts/2020-05-05-contrast-coding/index.html",
    "title": "Tutorial: Obtaining directly interpretable regression coefficients by recoding categorical predictors",
    "section": "",
    "text": "The output of regression models is often difficult to parse, especially when categorical predictors and interactions between them are being modelled. The goal of this tutorial is to show you how you can obtain estimated coefficients that you can interpret directly in terms of your research question. I’ve learnt about this technique thanks to Schad et al. (2020), and I refer to them for a more detailed discussion. What I will do is go through three examples of increasing complexity that should enable you to apply the technique in your own analyses."
  },
  {
    "objectID": "posts/2020-05-05-contrast-coding/index.html#example-1-no-interactions-no-random-effects",
    "href": "posts/2020-05-05-contrast-coding/index.html#example-1-no-interactions-no-random-effects",
    "title": "Tutorial: Obtaining directly interpretable regression coefficients by recoding categorical predictors",
    "section": "Example 1: No interactions, no random effects",
    "text": "Example 1: No interactions, no random effects\nLet’s start off nice but not too easy by analysing an experiment with three conditions and only one observation per participant.\nThis dataset we’ll work with comes from a study by Vanhove (2019) and is available here. The details hardly matter, but there were three experimental conditions: information, no information and strategy. The no information condition serves as the baseline control condition, and the information and strategy conditions serve as the treatment conditions. The expectation was that the treatment conditions would outperform the control condition on the outcome variable (here: ProportionCongruent), and I was also interested in seeing if the strategy condition outperformed the information condition.\nThe condition means already show that the participants in the information condition did not in fact outperform those in the no information condition, but neither that nor the small sample size should keep us from using these data for our example.\n\nd &lt;- read.csv(\"http://homeweb.unifr.ch/VanhoveJ/Pub/Data/Vanhove2018.csv\")\ntable(d$Condition)\n\n\n   information no information       strategy \n            15             14             16 \n\ntapply(d$ProportionCongruent, d$Condition, mean)\n\n   information no information       strategy \n         0.517          0.554          0.627 \n\n\nIf we fit the model directly, R will apply the default coding scheme to the categorical predictor (viz., treatment coding):\n\n# The newest version of R doesn't recode strings as factors automatically,\n# so code Condition as a factor for good measure.\nd$Condition &lt;- factor(d$Condition)\nm_default &lt;- lm(ProportionCongruent ~ Condition, data = d)\nsummary(m_default)$coefficients\n\n                        Estimate Std. Error t value Pr(&gt;|t|)\n(Intercept)               0.5172     0.0512  10.101 8.31e-13\nConditionno information   0.0369     0.0737   0.501 6.19e-01\nConditionstrategy         0.1099     0.0713   1.542 1.31e-01\n\n\nBy default, the information condition is chosen as the reference level because it’s first in the alphabet. That is, the 0.52 is the estimated mean for the information condition. The second estimate (0.04) is the difference between the mean for the no information condition and that of the reference level (information). Similarly, the third estimate (0.11) is the difference between the mean for the strategy condition and that of the reference level (information). These estimates are all correct, and they’re fairly easy to interpret once you’ve figured out what the reference level is. But if we want to, we can obtain estimated coefficients that map more directly onto the research questions by recoding the Condition variable manually.\nCondition has three levels, and this means that we can obtain at most three estimated coefficients for it. It’s also possible to obtain fewer than the maximum, but this is not something I will go into here.\nThe first step is to write out what you want the model’s intercept to represent as a null hypothesis. In this example, it makes sense that the intercept should the mean performance in the no information condition. Written as a null hypothesis, this becomes \\(\\mu_{\\textrm{no info}} = 0\\). This null hypothesis is a bit silly, but that’s not important here, just go with it; the equation is easy enough. Then, rearrange the equation such that the right-hand side reads 0. This is already the case here. Finally, add the factor’s remaining levels to the left-hand side of the equation, but multiplied by 0. You’re just adding 0s to the left-hand side of the equation, which doesn’t affect it. For clarity, I’ve made it clear that \\(\\mu_{\\textrm{no info}} = 1\\mu_{\\textrm{no info}}\\). The result looks like this:\n\\[\n\\mu_{\\textrm{no info}} = 0 \\\\ \\Leftrightarrow 0\\mu_{\\textrm{info}} + 1\\mu_{\\textrm{no info}} + 0\\mu_{\\textrm{strategy}} = 0\n\\]\nMake sure that in the rearranged equation, the levels appear in the same order as they do in R. You can check the order of the levels using levels(). By default, the order is alphabetical. You can change the order of the factor levels, but then you’ll also need to change the order in which the coefficients appear in the rearranged equation:\n\nlevels(d$Condition)\n\n[1] \"information\"    \"no information\" \"strategy\"      \n\n\nThe second step is to write out null hypotheses for the comparisons that you want the remaining coefficients to estimate. For the sake of the exercise, let’s say that I want the first remaining coefficient to estimate the difference between the mean of the control group (\\(\\mu_{\\textrm{no info}}\\)) and the mean of the means of the two other groups (i.e., \\(\\frac{1}{2}(\\mu_{\\textrm{info}} + \\mu_{\\textrm{strategy}})\\)). First write this as a null hypothesis:\n\\[\n\\frac{1}{2}(\\mu_{\\textrm{info}} + \\mu_{\\textrm{strategy}}) = \\mu_{\\textrm{no info}}\n\\]\nNote that I write the ‘focus’ of the comparison on the left-hand side and what it’s being compared to on the right-hand side. This will make the signs of the coefficients we later get easier to interpret. Then, bring all \\(\\mu\\) terms to the left-hand side:\n\\[\n\\frac{1}{2}(\\mu_{\\textrm{info}} + \\mu_{\\textrm{strategy}}) = \\mu_{\\textrm{no info}} \\\\ \\Leftrightarrow \\frac{1}{2}\\mu_{\\textrm{info}} + -1\\mu_{\\textrm{no info}} + \\frac{1}{2}\\mu_{\\textrm{strategy}} = 0\n\\]\nDo not multiply any terms in the equation, i.e., do not write \\(1\\mu_{\\rm{info}} + -2\\mu_{\\rm{no\\ info}} + 1\\mu_{\\rm{strategy}} = 0\\) so that you don’t have to work with fractions. The hypotheses you’ll test will be the same, but the output will be more confusing than if you just rearrange the coefficients but keep the fractions.\nFor the final coefficient, let’s say that I want to estimate the difference in means between the info and strategy condition. Again, start from the corresponding null hypothesis (i.e., that these means are the same), and then bring all \\(\\mu\\)s to the left-hand side while adding the missing factor levels:\n\\[\n\\mu_{\\textrm{strategy}} = \\mu_{\\textrm{info}} \\\\\\Leftrightarrow -1\\mu_{\\textrm{info}} + 0\\mu_{\\textrm{no info}} + 1\\mu_{\\textrm{strategy}} = 0\n\\]\nThe third step is to put the coefficients of the rearranged equations into a hypothesis matrix. As you can see, each line in this matrix contains the coefficients belonging to the \\(\\mu\\) terms in the three equations above:\n\nHm &lt;- rbind(\n  H00 = c(info = 0, no_info = 1, strategy = 0),\n  H01 = c(info = 1/2, no_info = -1, strategy = 1/2),\n  H02 = c(info = -1, no_info = 0, strategy = 1)\n)\n\nFourth, convert this hypothesis matrix into a contrast matrix using the ginv2() function that Schad et al. (2020) provide:\n\nginv2 &lt;- function(x) {\n    MASS::fractions(provideDimnames(MASS::ginv(x), base = dimnames(x)[2:1]))\n}\n\nCm &lt;- ginv2(Hm)\n\nFifth, apply this contrast matrix, minus the first column (hence -1), as the coding scheme for Condition:\n\ncontrasts(d$Condition) &lt;- Cm[, -1]\n\nAnd finally, fit the model:\n\nm_manual &lt;- lm(ProportionCongruent ~ Condition, data = d)\nsummary(m_manual)$coefficients\n\n             Estimate Std. Error t value Pr(&gt;|t|)\n(Intercept)     0.554     0.0530  10.455 2.92e-13\nConditionH01    0.018     0.0639   0.282 7.79e-01\nConditionH02    0.110     0.0713   1.542 1.31e-01\n\n\nYou can check this yourselves, but the intercept now shows the mean of the no information condition, the first term (ConditionH01) estimates the difference between the no information mean and the mean of the means of the other two condidition, and the second term (ConditionH02) estimates the difference between the strategy mean and the information mean."
  },
  {
    "objectID": "posts/2020-05-05-contrast-coding/index.html#example-2-no-interactions-but-random-effects",
    "href": "posts/2020-05-05-contrast-coding/index.html#example-2-no-interactions-but-random-effects",
    "title": "Tutorial: Obtaining directly interpretable regression coefficients by recoding categorical predictors",
    "section": "Example 2: No interactions, but random effects",
    "text": "Example 2: No interactions, but random effects\nFor the second and third example, I’ll use data from Pestana et al. (2018), who measured the Portuguese reading skills of Portuguese children in Portugal, French-speaking Switzerland, and German-speaking Switzerland at three points in time. The data are available as part of the helascot package.\n\nlibrary(helascot)\nlibrary(tidyverse)\nlibrary(lme4)\n\n# Combine data and only retain Portuguese test data\nd &lt;- skills %&gt;% \n  left_join(background, by = \"Subject\") %&gt;% \n  filter(LanguageTested == \"Portuguese\") %&gt;% \n  filter(!is.na(Reading))\n\n# Code Time and LanguageGroup as factors\nd$Time &lt;- factor(d$Time)\nd$LanguageGroup &lt;- factor(d$LanguageGroup)\n\n# Draw graph\nggplot(data = d,\n       aes(x = Time, \n           y = Reading,\n           fill = LanguageGroup)) +\n  geom_boxplot()\n\n\n\n\nThere are up to three observations per child (Time 1, 2 and 3), and the children are clustered in classes. We will take this into account during the analysis using random effects by child and by class.\nFor the sake of this example, let’s say we’re interested in estimating the development of reading skills through time. The following model estimates the effect of Time and allows for this effect to vary between classes. Since there is only one data point per Subject per Time, no by-subject random slope for Time was estimated.\n\nm_default &lt;- lmer(Reading ~ Time + (1+Time|Class) + (1|Subject), data = d)\nsummary(m_default)$coefficients\n\n            Estimate Std. Error t value\n(Intercept)    0.532     0.0239   22.25\nTime2          0.104     0.0111    9.38\nTime3          0.194     0.0161   12.08\n\n\nWhen using R’s default coding, the (Intercept) represents the average reading skill score at Time 1, the next coefficient estimates the different in reading skill scores between Time 2 and Time 1, and the third coefficient estimates the difference between Time 3 and Time 1. This is fine, but let’s say we wanted to estimate the difference between Time 3 and Time 2 directly. We can obtain this estimate by coding the predictors ourselves.\nIn the equations below, the \\(\\mu\\)’s are in the same order as R knows them:\n\nlevels(d$Time)\n\n[1] \"1\" \"2\" \"3\"\n\n\nThe average performance at Time 1 is a reasonable choice for the intercept, so let’s stick with that. The silly null hypothesis is that \\(\\mu_{T1} = 0\\), which we can elaborate with \\(\\mu_{T2}\\) and \\(\\mu_{T3}\\) as follows:\n\\[\n\\mu_{T1} = 0 \\\\ \\Leftrightarrow 1\\mu_{T1} + 0\\mu_{T2} + 0\\mu_{T3} = 0\n\\]\nIf we want the next coefficient to estimate the difference between the average reading skill scores at Time 2 and Time 1, we need the null hypothesis that these average reading skill scores are the same, i.e., \\(\\mu_{T2} = \\mu_{T1}\\). (Remember to put the ‘focus’ of the comparison on the left.) From there:\n\\[\n\\mu_{T2} = \\mu_{T1} \\\\ \\Leftrightarrow -1\\mu_{T1} + 1\\mu_{T2} + 0\\mu_{T3} = 0\n\\]\nSimilarly, if we want the third coefficient to estimate the difference between the average reading skill scores at Time 3 and Time 2, we need the null hypothesis that these average reading skill scores are the same, i.e., \\(\\mu_{T3} = \\mu_{T2}\\):\n\\[\n\\mu_{T3} = \\mu_{T2} \\\\ \\Leftrightarrow 0\\mu_{T1} + -1\\mu_{T2} + 1\\mu_{T3} = 0\n\\]\nPut the coefficients in the hypothesis matrix, convert this hypothesis matrix to a contrast matrix, apply this contrast matrix to the factor Time, and refit the model.\n\n# Put coefficients in hypothesis matrix\nHm &lt;- rbind(H00 = c(T1 = 1,  T2 = 0,  T3 = 0),\n            H01 = c(T1 = -1, T2 = 1,  T3 = 0),\n            H02 = c(T1 = 0,  T2 = -1, T3 = 1))\n\n# Convert to contrast matrix\nCm &lt;- ginv2(Hm)\n\n# I'm going to copy Time so we can reuse it in example 3:\nd$Time2 &lt;- d$Time\n\n# Apply contrast matrix to factor\ncontrasts(d$Time2) &lt;- Cm[, -1]\n\n# Refit model\nm_manual &lt;- lmer(Reading ~ Time2 + (1+Time2|Class) + (1|Subject), data = d)\nsummary(m_manual)$coefficients\n\n            Estimate Std. Error t value\n(Intercept)   0.5317     0.0239   22.25\nTime2H01      0.1040     0.0111    9.38\nTime2H02      0.0904     0.0137    6.59\n\n\nAs you can see, the third coefficient now estimates the difference between the average reading skill score at T3 and at T2. Compared to manually computing this difference from the first model’s output, the main advantage of coding the predictors yourself is that you also obtain a measure of the uncertainty about the estimate of interest (e.g., the standard error, or a confidence interval)."
  },
  {
    "objectID": "posts/2020-05-05-contrast-coding/index.html#example-3-interactions-and-random-effects",
    "href": "posts/2020-05-05-contrast-coding/index.html#example-3-interactions-and-random-effects",
    "title": "Tutorial: Obtaining directly interpretable regression coefficients by recoding categorical predictors",
    "section": "Example 3: Interactions and random effects",
    "text": "Example 3: Interactions and random effects\nFinally, let’s take a look at interactions. Still working with the dataset from the second example, we can fit a model that contains an interaction between Time and LanguageGroup, i.e., that allows the effect of Time to differ between the three language groups. Since Time varies within Class, but LanguageGroup doesn’t, we can’t estimate a by-class random slope for Language Group. I’m going to ignore the warning about the singular fit here, because it isn’t related to the topic of the tutorial and I don’t have too many other datasets where interactions need to be modelled.\n\nm_default &lt;- lmer(Reading ~ Time*LanguageGroup + (1+Time|Class) + (1|Subject), data = d)\n\nboundary (singular) fit: see help('isSingular')\n\nsummary(m_default)$coefficients\n\n                                            Estimate Std. Error t value\n(Intercept)                                   0.5422     0.0233  23.281\nTime2                                         0.1159     0.0198   5.850\nTime3                                         0.1915     0.0279   6.859\nLanguageGroupBilingual group German          -0.0893     0.0316  -2.822\nLanguageGroupControl group Portuguese         0.1316     0.0373   3.532\nTime2:LanguageGroupBilingual group German    -0.0196     0.0272  -0.720\nTime3:LanguageGroupBilingual group German     0.0164     0.0373   0.440\nTime2:LanguageGroupControl group Portuguese  -0.0121     0.0309  -0.390\nTime3:LanguageGroupControl group Portuguese  -0.0287     0.0451  -0.636\n\n\nI’m not going to go over the interpretation of all of these coefficients; the point is that they’re not too informative, but that we can obtain more useful estimates by recoding the predictors. To do that, I prefer to combine the combinations of the factors involved in the interaction into a single variable, which I’ll call Cell:\n\n# Combine combinations of Time and Language group into 1 factor\nd$Cell &lt;- factor(paste(d$Time, d$LanguageGroup))\ntable(d$Cell)\n\n\n  1 Bilingual group French   1 Bilingual group German \n                       104                        104 \n1 Control group Portuguese   2 Bilingual group French \n                        74                        105 \n  2 Bilingual group German 2 Control group Portuguese \n                        97                         75 \n  3 Bilingual group French   3 Bilingual group German \n                       105                         93 \n3 Control group Portuguese \n                        69 \n\n\nWe will eventually need to refer to these cells in the same order as they’re known in R:\n\n# Order of the factor levels\nlevels(d$Cell)\n\n[1] \"1 Bilingual group French\"   \"1 Bilingual group German\"  \n[3] \"1 Control group Portuguese\" \"2 Bilingual group French\"  \n[5] \"2 Bilingual group German\"   \"2 Control group Portuguese\"\n[7] \"3 Bilingual group French\"   \"3 Bilingual group German\"  \n[9] \"3 Control group Portuguese\"\n\n\nLet’s think about what we want our estimates to mean. I think it would make sense for the intercept to represent the mean reading skill score at Time 1 across the three language groups. Then, I’d like for the next coefficients to express the average progress (across language groups) from Time 1 to Time 2 and from Time 2 to Time 3. Next, I’d like to know, at each time point, what the average difference between the Portuguese and the bilingual (Swiss) children is, and what the average difference between the Portuguese-French and the Portuguese-German bilinguals is.\nNow, in what follows, you’re going to see some fairly long equations. They may look daunting, but they’re really easy: like before, we’re going to express what we want the coefficients to mean as null hypotheses. It’s just that this time we have to include nine \\(\\mu\\)’s per equation.\n\nThe intercept represents the grand mean of the Time 1 cells. The silly corresponding null hypothesis is that this grand mean is 0: \\(\\frac{1}{3}(\\mu_{1,F} + \\mu_{1,G} + \\mu_{1,P}) = 0\\).\n\n\\[\n\\frac{1}{3}(\\mu_{1,F} + \\mu_{1,G} + \\mu_{1,P}) = 0 \\\\\\Leftrightarrow \\frac{1}{3}\\mu_{1,F} + \\frac{1}{3}\\mu_{1,G} + \\frac{1}{3}\\mu_{1,P} + 0\\mu_{2,F} + 0\\mu_{2,G} + 0\\mu_{2,P} + 0\\mu_{3,F} + 0\\mu_{3,G} + 0\\mu_{3,P} = 0\n\\]\n\nThe next term represents the difference between the grand mean of the Time 2 cells and that of the Time 1 cells:\n\n\\[\n\\frac{1}{3}(\\mu_{2,F} + \\mu_{2,G} + \\mu_{2,P}) = \\frac{1}{3}(\\mu_{1,F} + \\mu_{1,G} + \\mu_{1,P}) \\\\\\Leftrightarrow -\\frac{1}{3}\\mu_{1,F} + -\\frac{1}{3}\\mu_{1,G} + -\\frac{1}{3}\\mu_{1,P} + \\frac{1}{3}\\mu_{2,F} + \\frac{1}{3}\\mu_{2,G} + \\frac{1}{3}\\mu_{2,P} + 0\\mu_{3,F} + 0\\mu_{3,G} + 0\\mu_{3,P} = 0\n\\]\n\nThe third term represents the difference between the grand mean of the Time 3 cells and that of the Time 2 cells:\n\n\\[\n\\frac{1}{3}(\\mu_{3,F} + \\mu_{3,G} + \\mu_{3,P}) = \\frac{1}{3}(\\mu_{2,F} + \\mu_{2,G} + \\mu_{2,P}) \\\\\\Leftrightarrow 0\\mu_{1,F} + 0\\mu_{1,G} + 0\\mu_{1,P} + -\\frac{1}{3}\\mu_{2,F} + -\\frac{1}{3}\\mu_{2,G} + -\\frac{1}{3}\\mu_{2,P} + \\frac{1}{3}\\mu_{3,F} + \\frac{1}{3}\\mu_{3,G} + \\frac{1}{3}\\mu_{3,P} = 0\n\\]\nNow for comparisons between the language groups at each point in time. For each time, I want a term testing if the Portuguese on the one hand and the French- and German-speaking pupils on the other hand perform the same as well as one if the French- and German-speaking pupils differ amongst themselves.\n\nThe fourth term represents the difference between the mean of the Portuguese scores at Time 1 and the grand mean of the two bilingual groups’ performance at Time 1:\n\n\\[\n\\mu_{1,P} = \\frac{1}{2}(\\mu_{1,F} + \\mu_{1,G}) \\\\\\Leftrightarrow -\\frac{1}{2}\\mu_{1,F} + -\\frac{1}{2}\\mu_{1,G} + 1\\mu_{1,P} + 0\\mu_{2,F} + 0\\mu_{2,G} + 0\\mu_{2,P} + 0\\mu_{3,F} + 0\\mu_{3,G} + 0\\mu_{3,P} = 0\n\\]\n\nThe fifth term represents the difference between the two bilingual groups at Time 1:\n\n\\[\n\\mu_{1,F} =\\mu_{1,G} \\\\\\Leftrightarrow 1\\mu_{1,F} + -1\\mu_{1,G} + 0\\mu_{1,P} + 0\\mu_{2,F} + 0\\mu_{2,G} + 0\\mu_{2,P} + 0\\mu_{3,F} + 0\\mu_{3,G} + 0\\mu_{3,P} = 0\n\\]\n\nSame as the fourth term, but for Time 2.\n\n\\[\n\\mu_{2,P} = \\frac{1}{2}(\\mu_{2,F} + \\mu_{2,G}) \\\\\\Leftrightarrow 0\\mu_{1,F} + 0\\mu_{1,G} + 0\\mu_{1,P} + -\\frac{1}{2}\\mu_{2,F} + -\\frac{1}{2}\\mu_{2,G} + 1\\mu_{2,P} + 0\\mu_{3,F} + 0\\mu_{3,G} + 0\\mu_{3,P} = 0\n\\]\n\nSame as the fifth term, but for Time 2.\n\n\\[\n\\mu_{2,F} =\\mu_{2,G} \\\\\\Leftrightarrow 0\\mu_{1,F} + 0\\mu_{1,G} + 0\\mu_{1,P} + 1\\mu_{2,F} + -1\\mu_{2,G} + 0\\mu_{2,P} + 0\\mu_{3,F} + 0\\mu_{3,G} + 0\\mu_{3,P} = 0\n\\]\n\nSame as the fourth and sixth terms, but for Time 3.\n\n\\[\n\\mu_{3,P} = \\frac{1}{2}(\\mu_{3,F} + \\mu_{3,G}) \\\\\\Leftrightarrow 0\\mu_{1,F} + 0\\mu_{1,G} + 0\\mu_{1,P} + 0\\mu_{2,F} + 0\\mu_{2,G} + 0\\mu_{2,P} + -\\frac{1}{2}\\mu_{3,F} + -\\frac{1}{2}\\mu_{3,G} + 1\\mu_{3,P} = 0\n\\]\n\nSame as the fifth and seventh terms, but for Time 3.\n\n\\[\n\\mu_{3,F} =\\mu_{3,G} \\\\\\Leftrightarrow 0\\mu_{1,F} + 0\\mu_{1,G} + 0\\mu_{1,P} + 0\\mu_{2,F} + 0\\mu_{2,G} + 0\\mu_{2,P} + 1\\mu_{3,F} + -1\\mu_{3,G} + 0\\mu_{3,P} = 0\n\\]\nPut all of these coefficients into a large hypothesis matrix and convert it to a contrast matrix:\n\nHm &lt;- rbind(GM_T1     = c(F1 = 1/3,  G1 = 1/3,  P1 = 1/3,  \n                          F2 = 0,    G2 = 0,    P2 = 0,    \n                          F3 = 0,    G3 = 0,    P3 = 0),\n            T2vT1     = c(F1 = -1/3, G1 = -1/3, P1 = -1/3, \n                          F2 = 1/3,  G2 = 1/3,  P2 = 1/3,  \n                          F3 = 0,    G3 = 0,    P3 = 0),\n            T3vT2     = c(F1 = 0,    G1 = 0,    P1 = 0,    \n                          F2 = -1/3, G2 = -1/3, P2 = -1/3, \n                          F3 = 1/3,  G3 = 1/3,  P3 = 1/3),\n            T1_PtvsBi = c(F1 = -1/2, G1 = -1/2, P1 = 1,    \n                          F2 = 0,    G2 = 0,    P2 = 0,    \n                          F3 = 0,    G3 = 0,    P3 = 0),\n            T1_FrvsGe = c(F1 = 1,    G1 = -1,   P1 = 0,    \n                          F2 = 0,    G2 = 0,    P2 = 0,    \n                          F3 = 0,    G3 = 0,    P3 = 0),\n            T2_PtvsBi = c(F1 = 0,    G1 = 0,    P1 = 0,    \n                          F2 = -1/2, G2 = -1/2, P2 = 1,    \n                          F3 = 0,    G3 = 0,    P3 = 0),\n            T2_FrvsGe = c(F1 = 0,    G1 = 0,    P1 = 0,    \n                          F2 = 1,    G2 = -1,   P2 = 0,    \n                          F3 = 0,    G3 = 0,    P3 = 0),\n            T3_PtvsBi = c(F1 = 0,    G1 = 0,    P1 = 0,    \n                          F2 = 0,    G2 = 0,    P2 = 0,    \n                          F3 = -1/2, G3 = -1/2, P3 = 1),\n            T3_FrvsGe = c(F1 = 0,    G1 = 0,    P1 = 0,    \n                          F2 = 0,    G2 = 0,    P2 = 0,    \n                          F3 = 1,    G3 = -1,   P3 = 0))\nCm &lt;- ginv2(Hm)\n\nApply the contrasts to the Cell variable and fit the model:\n\ncontrasts(d$Cell) &lt;- Cm[, -1]\nm_manual &lt;- lmer(Reading ~ Cell + (1|Class) + (1|Subject), data = d)\nsummary(m_manual)$coefficients\n\n              Estimate Std. Error t value\n(Intercept)     0.5586    0.01200   46.56\nCellT2vT1       0.1032    0.00975   10.58\nCellT3vT2       0.0812    0.00992    8.19\nCellT1_PtvsBi   0.1758    0.02725    6.45\nCellT1_FrvsGe   0.0818    0.02715    3.01\nCellT2_PtvsBi   0.1763    0.02723    6.48\nCellT2_FrvsGe   0.1052    0.02733    3.85\nCellT3_PtvsBi   0.1412    0.02765    5.11\nCellT3_FrvsGe   0.0794    0.02748    2.89\n\n\nThe coefficients mean exactly what it says on the tin. There is just one problem: I didn’t include a random slope that capture the varying effect of Time by Class yet. Adding a by-class random slope for Cell wouldn’t work: you’d end up estimating an enormous matrix of random effects since Cell has nine levels. Instead, we’ll have to first refit the model using the dummy variables in the contrast matrix of Cell as separate variables:\n\n# Add the dummy variables in the contrast matrix of Cell\n# to the dataset as separate variables\ncontrast_matrix &lt;- data.frame(Cm[, -1],\n                              Cell = levels(d$Cell))\nd &lt;- merge(d, contrast_matrix, by = \"Cell\")\n\n# Refit the model using these separate dummy variables\nm_manual &lt;- lmer(Reading ~ T2vT1 + T3vT2 +\n                   T1_PtvsBi + T1_FrvsGe +\n                   T2_PtvsBi + T2_FrvsGe +\n                   T3_PtvsBi + T3_FrvsGe +\n                   (1|Class) + (1|Subject), data = d)\nsummary(m_manual)$coefficients\n\n            Estimate Std. Error t value\n(Intercept)   0.5586    0.01200   46.56\nT2vT1         0.1032    0.00975   10.58\nT3vT2         0.0812    0.00992    8.19\nT1_PtvsBi     0.1758    0.02725    6.45\nT1_FrvsGe     0.0818    0.02715    3.01\nT2_PtvsBi     0.1763    0.02723    6.48\nT2_FrvsGe     0.1052    0.02733    3.85\nT3_PtvsBi     0.1412    0.02765    5.11\nT3_FrvsGe     0.0794    0.02748    2.89\n\n\nThe output is exactly the same as above. Now we need to think about which of these estimates can actually vary by Class. If you think about the way we coded these predictors, T2vsT1 and T3vT2 capture the effect of Time, whereas the other predictors capture the effects of LanguageGroup at different times. The effect of Time can vary according to Class, but the effects of LanguageGroup can’t (each Class belonged to only one LanguageGroup). So if we want random slopes of Time by Class, we need to let the effects of T2vT1 and T3vT2 vary by class:\n\nm_manual &lt;- lmer(Reading ~ T2vT1 + T3vT2 +\n                   T1_PtvsBi + T1_FrvsGe +\n                   T2_PtvsBi + T2_FrvsGe +\n                   T3_PtvsBi + T3_FrvsGe +\n                   (1+T2vT1+T3vT2|Class) + (1|Subject), data = d)\n\nboundary (singular) fit: see help('isSingular')\n\nsummary(m_manual)$coefficients\n\n            Estimate Std. Error t value\n(Intercept)   0.5563     0.0143   38.84\nT2vT1         0.1054     0.0120    8.77\nT3vT2         0.0820     0.0142    5.79\nT1_PtvsBi     0.1762     0.0331    5.32\nT1_FrvsGe     0.0893     0.0316    2.82\nT2_PtvsBi     0.1740     0.0315    5.52\nT2_FrvsGe     0.1088     0.0306    3.55\nT3_PtvsBi     0.1393     0.0275    5.07\nT3_FrvsGe     0.0729     0.0272    2.68\n\n\nThe warning isn’t relevant to the purposes of this tutorial. As a sanity check, we can compare the predictions of m_manual and m_default to confirm that m_manual is the same model as m_default, just with parameter estimates that are easier to interpret:\n\n# (I don't know why I need to specify 'newdata'...)\nd$predict_default &lt;- predict(m_default, newdata = d)\nd$predict_manual &lt;- predict(m_manual, newdata = d)\nplot(predict_manual ~ predict_default, d)\n\n\n\n\nBoth models make the same predictions, and the predictions align reasonably well with the data observed:\n\nggplot(data = d,\n       aes(x = Time, \n           y = predict_manual,\n           fill = LanguageGroup)) +\n  geom_boxplot() +\n  ylab(\"Model predictions\")"
  },
  {
    "objectID": "posts/2020-05-05-contrast-coding/index.html#what-if-it-doesnt-work",
    "href": "posts/2020-05-05-contrast-coding/index.html#what-if-it-doesnt-work",
    "title": "Tutorial: Obtaining directly interpretable regression coefficients by recoding categorical predictors",
    "section": "What if it doesn’t work?",
    "text": "What if it doesn’t work?\nIf, having specified your own hypothesis matrix, some lines in the regression output contain NA, the reason is probably that some of the rows in your hypothesis matrix are combinations of some of the other rows. In essence, you’re asking the model to answer the same question twice, so it only answers it once. Reformulating the hypotheses will usually work."
  },
  {
    "objectID": "posts/2020-05-05-contrast-coding/index.html#references",
    "href": "posts/2020-05-05-contrast-coding/index.html#references",
    "title": "Tutorial: Obtaining directly interpretable regression coefficients by recoding categorical predictors",
    "section": "References",
    "text": "References\nBerthele, Raphael and Amelia Lambelet (eds.). 2018. Heritage and school language literacy development in migrant children: Interdependence or independence? Multilingual Matters.\nPestana, Carlos, Amelia Lambelet and Jan Vanhove. 2018. Reading comprehension development in Portuguese heritage speakers in Switzerland (HELASCOT project). In Raphael Berthele and Amelia Lambelet (Eds.), Heritage language and school language literacy development in migrant children: Interdependence or independence? (pp. 58-82). Bristol, UK: Multilingual Matters. http://doi.org/10.21832/BERTHE9047\nSchad, Daniel J., Shravan Vasishth, Sven Hohenstein and Reinhold Kliegl. 2020. How to capitalize on a priori contrasts in linear (mixed) models: A tutorial. Journal of Memory and Language 110. https://doi.org/10.1016/j.jml.2019.104038\nVanhove, Jan. 2019. Metalinguistic knowledge about the native language and language transfer in gender assignment. Studies in Second Language Learning and Teaching 9(2). 397-419. https://doi.org/10.14746/ssllt.2019.9.2.7"
  },
  {
    "objectID": "posts/2020-05-05-contrast-coding/index.html#software-versions",
    "href": "posts/2020-05-05-contrast-coding/index.html#software-versions",
    "title": "Tutorial: Obtaining directly interpretable regression coefficients by recoding categorical predictors",
    "section": "Software versions",
    "text": "Software versions\nPlease note that I reran the code on this page on August 6, 2023.\n\ndevtools::session_info()\n\n─ Session info ───────────────────────────────────────────────────────────────\n setting  value\n version  R version 4.3.1 (2023-06-16)\n os       Ubuntu 22.04.2 LTS\n system   x86_64, linux-gnu\n ui       X11\n language en_US\n collate  en_US.UTF-8\n ctype    en_US.UTF-8\n tz       Europe/Zurich\n date     2023-08-06\n pandoc   3.1.1 @ /usr/lib/rstudio/resources/app/bin/quarto/bin/tools/ (via rmarkdown)\n\n─ Packages ───────────────────────────────────────────────────────────────────\n package     * version date (UTC) lib source\n boot          1.3-28  2021-05-03 [4] CRAN (R 4.2.0)\n cachem        1.0.6   2021-08-19 [2] CRAN (R 4.2.0)\n callr         3.7.3   2022-11-02 [1] CRAN (R 4.3.1)\n cli           3.6.1   2023-03-23 [1] CRAN (R 4.3.0)\n colorspace    2.1-0   2023-01-23 [1] CRAN (R 4.3.0)\n crayon        1.5.2   2022-09-29 [1] CRAN (R 4.3.1)\n devtools      2.4.5   2022-10-11 [1] CRAN (R 4.3.1)\n digest        0.6.29  2021-12-01 [2] CRAN (R 4.2.0)\n dplyr       * 1.1.2   2023-04-20 [1] CRAN (R 4.3.0)\n ellipsis      0.3.2   2021-04-29 [2] CRAN (R 4.2.0)\n evaluate      0.15    2022-02-18 [2] CRAN (R 4.2.0)\n fansi         1.0.4   2023-01-22 [1] CRAN (R 4.3.1)\n farver        2.1.1   2022-07-06 [1] CRAN (R 4.3.0)\n fastmap       1.1.0   2021-01-25 [2] CRAN (R 4.2.0)\n forcats     * 1.0.0   2023-01-29 [1] CRAN (R 4.3.0)\n fs            1.5.2   2021-12-08 [2] CRAN (R 4.2.0)\n generics      0.1.3   2022-07-05 [1] CRAN (R 4.3.0)\n ggplot2     * 3.4.2   2023-04-03 [1] CRAN (R 4.3.0)\n glue          1.6.2   2022-02-24 [2] CRAN (R 4.2.0)\n gtable        0.3.3   2023-03-21 [1] CRAN (R 4.3.0)\n helascot    * 1.0.0   2023-08-02 [1] Github (janhove/helascot@4cf3c1b)\n hms           1.1.3   2023-03-21 [1] CRAN (R 4.3.0)\n htmltools     0.5.5   2023-03-23 [1] CRAN (R 4.3.0)\n htmlwidgets   1.6.2   2023-03-17 [1] CRAN (R 4.3.1)\n httpuv        1.6.11  2023-05-11 [1] CRAN (R 4.3.1)\n jsonlite      1.8.7   2023-06-29 [1] CRAN (R 4.3.1)\n knitr         1.39    2022-04-26 [2] CRAN (R 4.2.0)\n labeling      0.4.2   2020-10-20 [1] CRAN (R 4.3.0)\n later         1.3.1   2023-05-02 [1] CRAN (R 4.3.1)\n lattice       0.21-8  2023-04-05 [4] CRAN (R 4.3.0)\n lifecycle     1.0.3   2022-10-07 [1] CRAN (R 4.3.0)\n lme4        * 1.1-34  2023-07-04 [1] CRAN (R 4.3.1)\n lubridate   * 1.9.2   2023-02-10 [1] CRAN (R 4.3.0)\n magrittr      2.0.3   2022-03-30 [1] CRAN (R 4.3.0)\n MASS          7.3-60  2023-05-04 [4] CRAN (R 4.3.1)\n Matrix      * 1.6-0   2023-07-08 [4] CRAN (R 4.3.1)\n memoise       2.0.1   2021-11-26 [2] CRAN (R 4.2.0)\n mime          0.10    2021-02-13 [2] CRAN (R 4.0.2)\n miniUI        0.1.1.1 2018-05-18 [1] CRAN (R 4.3.1)\n minqa         1.2.5   2022-10-19 [1] CRAN (R 4.3.1)\n munsell       0.5.0   2018-06-12 [1] CRAN (R 4.3.0)\n nlme          3.1-162 2023-01-31 [4] CRAN (R 4.2.2)\n nloptr        2.0.3   2022-05-26 [1] CRAN (R 4.3.1)\n pillar        1.9.0   2023-03-22 [1] CRAN (R 4.3.0)\n pkgbuild      1.4.2   2023-06-26 [1] CRAN (R 4.3.1)\n pkgconfig     2.0.3   2019-09-22 [2] CRAN (R 4.2.0)\n pkgload       1.3.2.1 2023-07-08 [1] CRAN (R 4.3.1)\n prettyunits   1.1.1   2020-01-24 [2] CRAN (R 4.2.0)\n processx      3.8.2   2023-06-30 [1] CRAN (R 4.3.1)\n profvis       0.3.8   2023-05-02 [1] CRAN (R 4.3.1)\n promises      1.2.0.1 2021-02-11 [1] CRAN (R 4.3.1)\n ps            1.7.5   2023-04-18 [1] CRAN (R 4.3.1)\n purrr       * 1.0.1   2023-01-10 [1] CRAN (R 4.3.0)\n R6            2.5.1   2021-08-19 [2] CRAN (R 4.2.0)\n Rcpp          1.0.11  2023-07-06 [1] CRAN (R 4.3.1)\n readr       * 2.1.4   2023-02-10 [1] CRAN (R 4.3.0)\n remotes       2.4.2   2021-11-30 [2] CRAN (R 4.2.0)\n rlang         1.1.1   2023-04-28 [1] CRAN (R 4.3.0)\n rmarkdown     2.21    2023-03-26 [1] CRAN (R 4.3.0)\n rstudioapi    0.14    2022-08-22 [1] CRAN (R 4.3.0)\n scales        1.2.1   2022-08-20 [1] CRAN (R 4.3.0)\n sessioninfo   1.2.2   2021-12-06 [2] CRAN (R 4.2.0)\n shiny         1.7.4.1 2023-07-06 [1] CRAN (R 4.3.1)\n stringi       1.7.12  2023-01-11 [1] CRAN (R 4.3.1)\n stringr     * 1.5.0   2022-12-02 [1] CRAN (R 4.3.0)\n tibble      * 3.2.1   2023-03-20 [1] CRAN (R 4.3.0)\n tidyr       * 1.3.0   2023-01-24 [1] CRAN (R 4.3.0)\n tidyselect    1.2.0   2022-10-10 [1] CRAN (R 4.3.0)\n tidyverse   * 2.0.0   2023-02-22 [1] CRAN (R 4.3.1)\n timechange    0.2.0   2023-01-11 [1] CRAN (R 4.3.0)\n tzdb          0.4.0   2023-05-12 [1] CRAN (R 4.3.0)\n urlchecker    1.0.1   2021-11-30 [1] CRAN (R 4.3.1)\n usethis       2.2.2   2023-07-06 [1] CRAN (R 4.3.1)\n utf8          1.2.3   2023-01-31 [1] CRAN (R 4.3.1)\n vctrs         0.6.3   2023-06-14 [1] CRAN (R 4.3.0)\n withr         2.5.0   2022-03-03 [2] CRAN (R 4.2.0)\n xfun          0.39    2023-04-20 [1] CRAN (R 4.3.0)\n xtable        1.8-4   2019-04-21 [1] CRAN (R 4.3.1)\n yaml          2.3.5   2022-02-21 [2] CRAN (R 4.2.0)\n\n [1] /home/jan/R/x86_64-pc-linux-gnu-library/4.3\n [2] /usr/local/lib/R/site-library\n [3] /usr/lib/R/site-library\n [4] /usr/lib/R/library\n\n──────────────────────────────────────────────────────────────────────────────"
  },
  {
    "objectID": "posts/2022-02-18-dont-do-things-you-dont-see-the-point-of/index.html",
    "href": "posts/2022-02-18-dont-do-things-you-dont-see-the-point-of/index.html",
    "title": "In research, don’t do things you don’t see the point of",
    "section": "",
    "text": "When I started reading quantitative research reports, I hadn’t taken any methods or statistics classes, so small wonder that I didn’t understand why certain background variables on the participants were collected, why it was reported how many of them were women and how many of them were men, and what all those numbers in the results sections meant. However, I was willing to assume that these reports had been written by some fairly intelligent people and that, by the Gricean maxim of relevance, these bits and bobs must be relevant — why else report them?\nIt’s now fifteen years later, and I still haven’t taken any methods or statistics classes. But, as you can tell from a quick glance at the blog archive, I’ve come round to the view that researchers often take actions that don’t actually help them to address their research questions and that much information that is almost routinely reported in research papers is irrelevant to the nominal goal of that research paper (i.e., answering its research questions). Part of the reason that researchers do things that don’t make much sense is that they have misunderstood what some statistical tool does. But I suspect that another part of the reason is that beginning researchers don’t quite see the point of some procedures they run and of some snippets of information they provide but nonetheless assume that other researchers do understand why these are important. From my own experience and discussions with former students, I think that there’s a vicious circle at play:\n\nStudents read articles with lots of numbers and procedures they don’t really understand or see the point of.\n\nThey reasonably but often incorrectly assume that these ubiquitous numbers and procedures must be integral to the research report.\n\nAs students become researchers, they still haven’t quite understood whether or why all those numbers and procedures are relevant. But they assume that they are relevant. So they’d better also include them in their own reports, or they’d be betraying their own ignorance. Luckily, even if you don’t know what p-values, correlation coefficients and reliability coefficients actually express, computing them is a piece of cake.\nDuring peer review, you’re more likely to be chastised for not including some piece of information than for including a couple of irrelevant numbers. So beginning researchers may rarely be forced to consider the added value of their go-to procedures and of the information they routinely provide.\nA new cohort of students reads the published research, see 1).\n\nIt’s not that the beginning researchers in this scenario have misunderstood the tools they use — they have no conception of what these tools do, let alone a false one. All that is required for them to run superfluous procedures and include irrelevant information in their reports is that they think that other people see the relevance of what they’re doing — even if they themselves do not.\nNow, it’s hard to stop using tools you’ve misunderstood the purpose of since you won’t know that you’ve misunderstood that purpose. But if you’re a young scholar and you want to run some analysis or report some numbers that are commonly run or reported in your line of work, first ask yourself and your colleagues how running this analysis or reporting these numbers would help you or readers of your work help answer your study’s research questions or make the answers easier to understand. Risk appearing ignorant and don’t cram your research reports with analyses and numbers you don’t see the added value of.\nBy the same token, if a young scholar asks you which statistical test they should use, first ask them why they think they need a test at all and what exactly it is they want to test. Similarly, if a novice asks you how they can run this or that analysis, ask them how they think running such an analysis would help them address their research question. Even if the added value of such an analysis is clear to you, it may not be clear to them.\nEdit (February 21, 2022): Also see Daniël Lakens’ blog post The New Heuristics, where he proposes researchers should adhere to the adage justify everything."
  },
  {
    "objectID": "posts/2015-02-05-standardised-vs-unstandardised-es/index.html",
    "href": "posts/2015-02-05-standardised-vs-unstandardised-es/index.html",
    "title": "Why I don’t like standardised effect sizes",
    "section": "",
    "text": "With researchers (thankfully) increasingly realising that p-values are only poorly related to the strength of the effects that they study, standardised effect sizes such as Cohen’s d or Pearson’s r are reported more and more frequently. In this blog post, I give four reasons why I don’t like such standardised effect size measures and why I prefer unstandardised effect sizes instead.\nAs a brief refresher, unstandardised effect sizes express the patterns found in the data in the units in which they were measured. For instance, if you say that there is a 7-centimetre mean difference in body length between two groups in your sample, that’s an unstandardised effect size. Similarly, if you say that lexical decision times slow down by 4 milliseconds a year, that’s an unstandardised effect size, too.\nStandardised effect sizes, by contrast, express these patterns by scaling the differences or trends by a measure of the variability within the sample. For instance, a 7-centimetre difference can be expressed as a difference of 0.6 standard deviations if the within-group standard deviation of body length is 12 centimetres (7/12 = 0.6). This standardised effect size measure is known as Cohen’s d. Similarly, 4-ms per year decrease in lexical decision times can correspond to a Pearson’s r of 0.27, of 0.05 and of 0.81 (among many other possibilities), depending on the variability in the participants’ ages and reaction times; Pearson’s r is a standardised effect size as well.\nWith that under our belts, here are the four reasons why I prefer unstandardised over standardised effect sizes."
  },
  {
    "objectID": "posts/2015-02-05-standardised-vs-unstandardised-es/index.html#reason-no.-1-unstandardised-effect-sizes-are-often-interpretable",
    "href": "posts/2015-02-05-standardised-vs-unstandardised-es/index.html#reason-no.-1-unstandardised-effect-sizes-are-often-interpretable",
    "title": "Why I don’t like standardised effect sizes",
    "section": "Reason No. 1: Unstandardised effect sizes are often interpretable",
    "text": "Reason No. 1: Unstandardised effect sizes are often interpretable\nAdvocates of standardised effect sizes often point out that much research in the social sciences makes use of variables that were measured on arbitrary scales such as responses to questionnaire items. Say, for instance, that two researchers want to investigate attitude differences between two populations. One researcher makes use of a 5-point Likert scale and finds a 1-point difference between her two samples, whereas the other researcher makes use of a 13-point Likert scale and finds a 2-point difference. These results can be hard to compare: the 2-point difference isn’t necessarily larger than the 1-point difference in any real terms. By scaling these differences relative to the variability in their respective samples, the two researchers can express their findings in a common metric (e.g. between-group difference divided by within-group variability) and more directly compare their results. (When dividing between-group differences by within-group differences, the measurement units cancel out; standardised effect sizes are therefore also called ‘dimensionless’.)\nI’ll admit that, in this situation, standardised effect sizes might serve a useful purpose (though see Reason No. 2 for a caveat). But, at least in the articles I read, it’s usually relatively straightforward to compare the raw effect sizes with one another. For instance, I can readily appreciate what a reaction time difference of 70 ms means in the context of a lexical decision task. In fact, it’s often easy enough to appreciate what a one-point difference on a 5-point Likert scale means (e.g. a shift from ‘no opinion’ to ‘tend to agree’), and what a two-point difference on a 7-point Likert scale means (e.g. a shift from ‘disagree slightly’ to ‘agree slightly’) if the response options have been appropriately labelled.\nSimilarly, when assessing the bivariate relationship between two variables, raw effect sizes (e.g. regression coefficients) are often straigthforward to interpret. Personally, I find an effect size statement like ‘response latencies slowed down by 4 milliseconds a year’ more meaningful than ‘response latencies were positively correlated with age (Pearson’s r = 0.17)’. Sure, the Pearson correlation coefficient provides information as to how much of the response time variance is ‘accounted for’ by age (in this example: 0.17² = 3%), but this is as much an indication about the research design than about the effect being studied—which brings us seamlessly to Reason No. 2."
  },
  {
    "objectID": "posts/2015-02-05-standardised-vs-unstandardised-es/index.html#reason-no.-2-standardised-effect-sizes-are-more-sensitive-to-the-research-design-and-thats-a-bad-thing",
    "href": "posts/2015-02-05-standardised-vs-unstandardised-es/index.html#reason-no.-2-standardised-effect-sizes-are-more-sensitive-to-the-research-design-and-thats-a-bad-thing",
    "title": "Why I don’t like standardised effect sizes",
    "section": "Reason No. 2: Standardised effect sizes are more sensitive to the research design (and that’s a bad thing)",
    "text": "Reason No. 2: Standardised effect sizes are more sensitive to the research design (and that’s a bad thing)\nCompared to unstandardised effect sizes, standardised effect sizes are too sensitive to the way in which the data were collected. When computing standardised effect sizes such as Cohen’s d, we need to divide the between-group mean difference by the within-group standard deviation. So, if we reduce the within-group standard deviation, we’ll get a higher Cohen’s d value, even if the between-group mean difference itself is unaffected.\nReducing within-group differences is one of the primary goals of good experimental design. For instance, better measurement instruments will yield a smaller within-group standard deviation since the amount of measurement error is reduced. Similarly, taking multiple measurements of each participants and averaging them will produce a smaller within-group standard deviation and hence a higher Cohen’s d. But the between-group difference itself, which is what we’re really interested in, won’t be (shouldn’t be) affected by such improvements in the experimental design. (The same holds for Pearson’s r.) In this case, higher standardised effect sizes reflect better design, not larger effects. Unstandardised effect sizes, by contrast, aren’t affected by such improvements in the experimental design, though their standard errors are, appropriately reflecting the higher degree of accuracy.\nStandardised effect sizes aren’t only sensitive to differences in measurement error, but also to differences in the sampling strategy. By way of example, let’s say that we want to characterise the (linear) relationship between two variables. To do this, we have the choice between two strategies:\n\nthe costly route of sampling 100 participants randomly from the population and measuring Variables 1 and 2;\nthe cheaper route of screening 100 participants drawn randomly from the population, measure them on Variable 1 and invite the 20 participants with the highest and the 20 participants with the lowest Variable 1 scores to the lab in order to run the costly test that yields Variable 2 (‘extreme group’ approach).\n\nDoes the difference in sampling strategy affect our estimate about the relationship between the two variables? I wrote a simulation with 10,000 runs to find out (R code).\nIn the panel on the left, I plotted the effect of random sampling vs. an extreme group approach on Pearson’s r. Most dots lie above the bisector, indicating that the extreme group approach yields standardised effect sizes that are higher than the random sampling approach. (In this case, the average Pearson’s r for the random sampling approach is 0.38, whereas it’s 0.52 for the extreme group approach.) The panel on the right shows the effect of different sampling strategies on the regression (‘beta’) coefficient, or better, the lack of such an effect. (The average beta coefficient is 0.22 for both approaches.)\n\n\n\n\n\nNon-random sampling doesn’t necessarily boost the standardised effect size: restricting the analysis to the participants who are in the top-30% with respect to Variable 1 decreases Pearson’s r (left panel) on average from 0.37 to 0.21. (This might sound pretty similar to restricting the analysis to university-educated participants…) But it doesn’t affect the regression coefficients (right panel; 0.22 for both approaches). (The dots don’t lie along the bisector due to the differences in sample size that aren’t compensated for by extreme group sampling, but the point is that there are about as many dots above the bisector as there are below.)\n\n\n\n\n\nThe upshot is that research design affects standardised effect sizes even if the effect itself doesn’t change: the relationships we sought to characterise were the same in all cases. But if we had based our conclusions on the relative effect sizes, we could’ve been tempted to try to explain ‘differences’ between results in terms of hidden interactions and the like. By looking at the unstandardised effect sizes, it becomes clear that there is no conflict between the findings in the first place.\n(Note that I’m not arguing in favour of extreme-group approaches, though it’s a given that as psychologists or linguists, we rarely if ever can make use of random sampling.)"
  },
  {
    "objectID": "posts/2015-02-05-standardised-vs-unstandardised-es/index.html#reason-no.-3-apples-and-oranges",
    "href": "posts/2015-02-05-standardised-vs-unstandardised-es/index.html#reason-no.-3-apples-and-oranges",
    "title": "Why I don’t like standardised effect sizes",
    "section": "Reason No. 3: Apples and oranges",
    "text": "Reason No. 3: Apples and oranges\nIn his 1992 Power Primer, Jacob Cohen tentatively suggested that correlation coefficients of 0.10, 0.30 and 0.50 could be considered small, medium and large effect sizes, respectively. Cohen derived this yardstick from the distribution of effect sizes in the psychological literature up till then.\nA useful complement to this article is Plonsky and Oswald’s recent review in Language Learning. In it, they analyse the distribution of effect sizes in second language research and recommend that correlation coefficients of 0.25, 0.40 and 0.60 be considered small, medium and large in L2 research. But what I found more interesting than the results they report is their speculation about ‘Domain maturity and changes over time’:\n\nThere is no reason to assume that effects in any particular domain would remain static over time. … A first scenario in which effect sizes might change over time involves a trajectory of research outcomes for a particular domain in which methodological adjustments or improvements lead to larger [standardised, JV] effect sizes over time … an alternate and somewhat opposite pattern of effects may also play out in a body of empirical literature characterized by theoretical developments and maturation taking place over time. Early research in a given area is often characterized by strong manipulations that set out to determine whether an effect exists and thereby determine whether the claims of a particular and usually novel hypothesis merit further attention. Such experiments would tend to yield larger effect sizes (…) Subsequently, after an effect is found, research efforts may shift to the generalizability of an effect across samples, settings, tasks, and so forth …\n\nSo when comparing a Pearson’s r of 0.5 with one of 0.2 and labelling the first ‘medium-strong’ and the latter ‘small’, what are we really comparing? Real effect size, methodological rigour, theoretical sophistication or the fact that there’s less low-hanging fruit as time goes on?\nA related point is that these yardsticks have become (or tend to become) gospel, much like the sacred 0.05 significance threshold. I hope I’m mistaken in this, but my impression is that most researchers doing power analyses out there just plug in a ‘medium’ or ‘large’ Cohen’s d or Pearson’s r and jot down the minimum sample size that the software returns. Perhaps it’s more useful to think about how sources of extraneous variance can be brought under control in order to increase power (tighter design, more accurate measures, including covariates), rather than assuming that whatever yardstick out there also applies to one’s own study."
  },
  {
    "objectID": "posts/2015-02-05-standardised-vs-unstandardised-es/index.html#reason-no.-4-what-about-more-complex-models",
    "href": "posts/2015-02-05-standardised-vs-unstandardised-es/index.html#reason-no.-4-what-about-more-complex-models",
    "title": "Why I don’t like standardised effect sizes",
    "section": "Reason No. 4: What about more complex models?",
    "text": "Reason No. 4: What about more complex models?\nAll discussions and tutorials on standardised effect sizes that I’ve come across assume that you have a comparatively easy statistical model: if you run a t-test, compute Cohen’s d; report η² if you run an ANOVA; report Pearson’s r if you have two continuous variables; report R² if you run a multiple regression model etc.\nBut these concepts translate poorly to the statistical tools that I need to make use of. For instance, there is no unambiguous counterpart of R² in logistic regression models nor in mixed-effect regressions. When I report on such models in research papers, I usually include a footnote to this effect in order to fend off the inevitable (and understandable) remark that effect sizes need to be reported. Up till now, this has done the trick for me, but Dale Barr (of mixed-effect model fame) seems to have had more difficulties in this department:\n\n\nTell them how to compute a dimensionless effect size measure for cross classified multilevel data, oh editor, or don’t make it a requirement\n\n— Dale Barr (@dalejbarr) February 4, 2015"
  },
  {
    "objectID": "posts/2015-02-05-standardised-vs-unstandardised-es/index.html#tldr",
    "href": "posts/2015-02-05-standardised-vs-unstandardised-es/index.html#tldr",
    "title": "Why I don’t like standardised effect sizes",
    "section": "tl;dr",
    "text": "tl;dr\nUnstandardised effect sizes do their job just fine."
  },
  {
    "objectID": "posts/2017-09-19-peeking-confidence-intervals/index.html",
    "href": "posts/2017-09-19-peeking-confidence-intervals/index.html",
    "title": "Confidence interval-based optional stopping",
    "section": "",
    "text": "Stopping to collect data early when the provisional results are significant (“optional stopping”) inflates your chances of finding a pattern in the data when nothing is going on. This can be countered using a technique known as sequential testing, but that’s not what this post is about. Instead, I’d like to illustrate that optional stopping isn’t necessarily a problem if your stopping rule doesn’t involve p-values. If instead of on p-values, you base your decision to collect more data or not on how wide your current confidence interval (or Bayesian credible interval) is, peeking at your data can be a reasonable strategy. Additionally, in this post, I want share some code for R functions that you can adapt in order to simulate the effects of different stopping rules."
  },
  {
    "objectID": "posts/2017-09-19-peeking-confidence-intervals/index.html#when-data-peeking-is-bad",
    "href": "posts/2017-09-19-peeking-confidence-intervals/index.html#when-data-peeking-is-bad",
    "title": "Confidence interval-based optional stopping",
    "section": "When data peeking is bad",
    "text": "When data peeking is bad\nOptional stopping—especially if undisclosed or unchecked by sequential testing—is what’s known as a “questionable research practice”. Almost universally in optional stopping, the decision about whether to collect more data or not is based on whether the current p-value is significant: if it is, you stop collecting data; if it isn’t, you continue to collect data until the p-value drops below 0.05 or until you run out of money or time. The problem with this practice is that it increases your chances of finding a significant result if nothing is going on. To find out by how much, you can use the function below.\nHere’s how it works. You have funds to run a within-subjects experiment with 300 participants, but you prefer not to waste money. So after you 20 participants, you run a t-test and check if it’s significant. If it is, you stop collecting more data right there. If it isn’t, you run another five participants, check the results again and so on. If you hit 300 participants without getting a significant result, you call it quits. Unbeknownst to you, there is nothing going on in the data. The function, creatively named bad_peeking.fnc() returns the final sample size, the width of the 95% confidence interval, the p-value, the estimated average difference between the two conditions, and whether the 95% confidence interval contains 0 (the true difference).\n\n# You can set initial_n, max_n and increment to different\n# values to see what happens.\nbad_peeking.fnc &lt;- function(initial_n = 20,\n                            max_n = 300,\n                            increment = 5) {\n  \n  # Generate full data from normal distribution\n  # with standard deviation as defined above\n  x &lt;- rnorm(n = max_n)\n  \n  # Set initial sample size\n  n &lt;- initial_n\n  \n  repeat {\n    # Check if p &lt; 0.05 for the first n observations\n    test &lt;- t.test(x[1:n])\n    p &lt;- test$p.value\n    \n    # Stop collecting more data when p &lt; 0.05, \n    # or when no more funds are available.\n    if(p &lt; 0.05 || n &gt;= max_n) {\n      break\n    }\n    \n    # If not, collect more data (increase n)\n    n &lt;- n + increment\n  }\n  \n  # Extract width of confidence interval, ES estimate,\n  # and check whether the true ES lies in the CI\n  ci_width &lt;- diff(test$conf.int)\n  es &lt;- test$estimate\n  in_interval &lt;- (0 &gt; test$conf.int[1] && 0 &lt; test$conf.int[2])\n  \n  # Also compute standard deviation of data\n  sd_x &lt;- sd(x[1:n])\n  \n  # Return results\n  return(list(final_n = n, \n              ci_width = ci_width, \n              p = p, \n              estimate = es, \n              in_ci = in_interval,\n              sd = sd_x))\n}\n\nBy running the bad_peeking.fnc() 10,000 times, we can gauge the influence of optional stopping on the sample size, confidence interval width, the p-value distribution, the estimated parameter distribution, and the confidence interval coverage properties.\nUpdate (2023-08-07): I reran the code and added the code for drawing the graphs.\n\n# Run this function 10,000 times\nresults_bad_peeking &lt;- replicate(10000, bad_peeking.fnc())\n\n\npar(mfrow = c(2, 2), las = 1)\n\nhist(unlist(results_bad_peeking[1, ])\n     , xlab = \"final sample size\"\n     , col = \"steelblue\"\n     , main = \"\")\n\nhist(unlist(results_bad_peeking[2, ])\n     , xlab = \"CI width\"\n     , col = \"steel blue\"\n     , main = \"\")\n\nhist(unlist(results_bad_peeking[3, ])\n     , xlab = \"p value\"\n     , col = \"red\"\n     , main = \"Too many low p-values\")\n\nhist(unlist(results_bad_peeking[4, ])\n     , xlab = \"estimated ES\"\n     , col = \"red\"\n     , main = \"Too many overestimated\\n effect sizes\")\n\n\n\n\nIn the top left histogram, you see that most of the time (some 70%), you keep on collecting data till the very end. Correspondingly, most of the time, you’ll end up with quite narrow confidence intervals (~ 4 times 1/sqrt(500)) (top right).\nHowever, even though you’re more likely to keep collecting more data, the minority of cases in which you’d stop collecting data before the 300th participant are enough to dramatically skew the distribution of p-values. This is visible in the bottom left graph. Since the null hypothesis is actually true in this simulation, the p-values ought to be uniformly distributed between 0 and 1. Here, however about 30% of the p-values is significant rather than the advertised 5%. By stopping collecting data conditional on a p-value, moreover, you’ll end up with too many overestimated effect sizes (either negatively or positively; bottom right): while the ES distribution is centred at zero, the distribution has “heavy tails”. These are caused by the times the preliminary result happened to be significant because, by sheer luck, it lay far enough from 0. Accordingly, the 95% confidence interval only covers the true parameter (0) in some 70% of cases rather than in 95% of them.\n(The standard deviation of the observations, which typically isn’t of much interest I think, is slightly underestimated in this approach: on average, it’s estimated to be 0.99 instead of 1.)\n\n# proportion p-values &lt;= 0.05\nmean(unlist(results_bad_peeking[3, ]) &lt;= 0.05)\n\n[1] 0.288\n\n# proportion true parameter in CI\nmean(unlist(results_bad_peeking[5, ]))\n\n[1] 0.712\n\n# estimated standard deviation\nmean(unlist(results_bad_peeking[6, ]))\n\n[1] 0.9865701"
  },
  {
    "objectID": "posts/2017-09-19-peeking-confidence-intervals/index.html#when-data-peeking-isnt-so-bad",
    "href": "posts/2017-09-19-peeking-confidence-intervals/index.html#when-data-peeking-isnt-so-bad",
    "title": "Confidence interval-based optional stopping",
    "section": "When data peeking isn’t so bad",
    "text": "When data peeking isn’t so bad\nOkay, so undisclosed optional stopping is pretty bad when the decision to stop or to continue is based on a p-value. But what if it’s based on the precision with which the effect has been estimated? What if, for instance, you decide beforehand to stop collecting more data when the 95% confidence interval spans fewer than 0.3 units?\n(The width of the desired confidence interval is chosen fairly arbitrarily here, which I grant is unfortunate. The main point, though, will be that optional stopping needn’t be quite as detrimental as in the previous case as long as the stopping rule isn’t p-value-based.)\n(Also, if you prefer Bayesian credible intervals to confidence intervals, just substitute “95% CrIs from a model with uninformative/weakly informative priors” for confidence interval.)\nI’ve adapted the function from above so that, instead of stopping the data collection when p &lt; 0.05, we stop when the width of the 95% CI is under 0.3 units.\n\n# This function also takes a stdev and a \n# ci_threshold parameter specifying the\n# standard deviation of the population\n# and the width of the desired 95% confidence\n# interval, respectively.\n# Note that a 95% CI spans 4 standard errors.\nreasonable_peeking.fnc &lt;- function(initial_n = 20,\n                            max_n = 300,\n                            increment = 5,\n                            stdev = 1,\n                            ci_threshold = 0.3) {\n  \n  # Generate full data\n  x &lt;- rnorm(n = max_n, sd = stdev)\n  \n  # Set initial sample size\n  n &lt;- initial_n\n  \n  repeat {\n    # Check if the CI is narrow enough\n    test &lt;- t.test(x[1:n])\n    ci_width &lt;- diff(test$conf.int)\n    \n    # Stop collecting more data when p &lt; 0.05, \n    # or when no more funds are available.\n    if(ci_width &lt; ci_threshold || n &gt;= max_n) {\n      break\n    }\n    \n    # If not, collect more data\n    n &lt;- n + increment\n  }\n  \n  # Extract p-value, ES estimate,\n  # and check whether the true ES lies in the CI\n  p &lt;- test$p.value\n  es &lt;- test$estimate\n  in_interval &lt;- (0 &gt; test$conf.int[1] && 0 &lt; test$conf.int[2])\n  \n  # Also compute standard deviation of data\n  sd_x &lt;- sd(x[1:n])\n  \n  # Return results\n  return(list(final_n = n, \n              ci_width = ci_width, \n              p = p, \n              estimate = es, \n              in_ci = in_interval,\n              sd = sd_x))\n}\n\n\nresults_reasonable_peeking &lt;- replicate(10000, reasonable_peeking.fnc())\n\npar(mfrow = c(2, 2), las = 1)\n\nhist(unlist(results_reasonable_peeking[1, ])\n     , xlab = \"final sample size\"\n     , col = \"steelblue\"\n     , main = \"\")\n\nhist(unlist(results_reasonable_peeking[2, ])\n     , xlab = \"CI width\"\n     , col = \"steel blue\"\n     , main = \"\")\n\nhist(unlist(results_reasonable_peeking[3, ])\n     , xlab = \"p value\"\n     , col = \"green\"\n     , main = \"\")\n\nhist(unlist(results_reasonable_peeking[4, ])\n     , xlab = \"estimated ES\"\n     , col = \"green\"\n     , main = \"\")\n\ntitle(\"All fine\", outer = TRUE, line = -1)\n\n\n\n\nRunning this function 10,000 and graphing the results, we see that we always stop collecting before the maximum sample size of 300 (top left). That would, of course, be different if we set ci_threshold to a lower value.\nBut what’s more relevant is the bottom left graph: when the null hypothesis is actually true (as it is here), CI-based optional stopping doesn’t inflate the Type-I error rate: only about 5% of the simulated samples return a significant results. Moreover, as illustrated in the bottom right graph, the distribution of the estimated effect sizes doesn’t have the heavy tails like it did before. Lastly, about 95% of the 95% confidence intervals do indeed cover the true parameter (0).\n(Again, the standard deviation of the observations is slightly underestimated in this approach: on average, it’s estimated to be 0.99 instead of 1.)\n\n# proportion p-values &lt;= 0.05\nmean(unlist(results_reasonable_peeking[3, ]) &lt;= 0.05)\n\n[1] 0.0513\n\n# proportion true parameter in CI\nmean(unlist(results_reasonable_peeking[5, ]))\n\n[1] 0.9487\n\n# estimated standard deviation\nmean(unlist(results_reasonable_peeking[6, ]))\n\n[1] 0.993268"
  },
  {
    "objectID": "posts/2017-09-19-peeking-confidence-intervals/index.html#precision-based-stopping-for-a-correlation",
    "href": "posts/2017-09-19-peeking-confidence-intervals/index.html#precision-based-stopping-for-a-correlation",
    "title": "Confidence interval-based optional stopping",
    "section": "Precision-based stopping for a correlation",
    "text": "Precision-based stopping for a correlation\nFor the last example, I wanted to explore what happens when (for some reason) you want to measure a correlation with enough precision. Whereas the width of a confidence interval around a mean depends on the number of observations and the variability in the data, the width of a confidence interval around a correlation coefficients depends on the number of observations and the correlation coefficient itself. As a result, by basing the stopping rule on the confidence interval, you’re also indirectly basing it on the parameter estimates themselves. This, in turn, could mean that the parameter estimates you get via optional stopping are biased for correlation coefficients.\n(As I’ve written a couple of times before, I’m not a big fan of using correlations for statistical inference inasmuch as I think that regression coefficients are typically better suited for this purpose. But regardless.)\nFor the sake of the argument, let’s say that “enough precision” means that the width of the 95% confidence interval spans less than 0.25. Again, this is an arbitrary decision here. I’ll explore two cases below: one where the true correlation between the two variables is 0, and one where it’s 0.5.\n\nNull effect\nIn the simulation below, I assume that the true correlation between the two variables is 0.\n\ncorrelation_peeking.fnc &lt;- function(initial_n = 20,\n                     final_n = 300,\n                     increment = 5,\n                     r = 0,\n                     ci_threshold = 0.25) {\n  \n  # Generate full data\n  x &lt;- MASS::mvrnorm(mu = c(0, 0), n = final_n, Sigma = rbind(c(1, r),\n                                                              c(r, 1)))\n  \n  # Set initial sample size\n  n &lt;- initial_n\n  \n  repeat {\n    # Test width of confidence interval\n    test &lt;- cor.test(x[1:n, 1], x[1:n, 2])\n    ci_width &lt;- diff(test$conf.int)\n    \n    # Stop collecting more data when CI is\n    # smaller than threshold, or when\n    # no more funds are available.\n    if(ci_width &lt; ci_threshold || n &gt;= final_n) {\n      break\n    }\n    # Collect more data\n    n &lt;- n + increment\n  }\n  \n  # Extract p-value, ES estimate,\n  # and check whether the true ES lies in the CI\n  p &lt;- test$p.value\n  es &lt;- test$estimate\n  in_interval &lt;- (r &gt; test$conf.int[1] && r &lt; test$conf.int[2])\n  \n  # Return results\n  return(list(final_n = n, \n              ci_width = ci_width, \n              p = p, \n              estimate = es, \n              in_ci = in_interval))\n}\n\n\nresults_nullcorrelation_peeking &lt;- replicate(10000, correlation_peeking.fnc(r = 0))\n\npar(mfrow = c(2, 2), las = 1)\n\nhist(unlist(results_nullcorrelation_peeking[1, ])\n     , xlab = \"final sample size\"\n     , col = \"steelblue\"\n     , main = \"\")\n\nhist(unlist(results_nullcorrelation_peeking[2, ])\n     , xlab = \"CI width\"\n     , col = \"steel blue\"\n     , main = \"\")\n\nhist(unlist(results_nullcorrelation_peeking[3, ])\n     , xlab = \"p value\"\n     , col = \"green\"\n     , main = \"\")\n\nhist(unlist(results_nullcorrelation_peeking[4, ])\n     , xlab = \"estimated ES\"\n     , col = \"green\"\n     , main = \"\")\n\ntitle(\"All fine\", outer = TRUE, line = -1)\n\n\n\n\nYou need pretty large samples to get a confidence interval narrow enough if the true correlation is 0, but the p-values are distributed as they should be (5% of them is below 5%), and the confidence intervals cover 0 in about 95% of the cases. There don’t seem to be too many overestimated effect sizes, neither positive or negative.\n\n# proportion p-values &lt;= 0.05\nmean(unlist(results_nullcorrelation_peeking[3, ]) &lt;= 0.05)\n\n[1] 0.0513\n\n# proportion parameter in CI\nmean(unlist(results_nullcorrelation_peeking[5, ]))\n\n[1] 0.9487\n\n\n\n\nNon-null effect\nThis time I’ll assume a true correlation of 0.5.\n\nresults_correlation_peeking &lt;- replicate(10000, correlation_peeking.fnc(r = 0.5))\n\npar(mfrow = c(2, 2), las = 1)\n\nhist(unlist(results_correlation_peeking[1, ])\n     , xlab = \"final sample size\"\n     , col = \"steelblue\"\n     , main = \"\")\n\nhist(unlist(results_correlation_peeking[2, ])\n     , xlab = \"CI width\"\n     , col = \"steel blue\"\n     , main = \"\")\n\nhist(unlist(results_correlation_peeking[3, ])\n     , xlab = \"p value\"\n     , col = \"steel blue\"\n     , main = \"Power\")\n\nhist(unlist(results_nullcorrelation_peeking[4, ])\n     , xlab = \"estimated ES\"\n     , col = \"purple\"\n     , main = \"Slight overestimate on average\")\n\n\n\n\nThis time, it’s not a problem that the p-value distribution is skewed since the null hypothesis is actually false (r = 0.5, not 0), so a skewed distribution is what you’d expect.\nAs for the estimated effect sizes, the distribution looks fine, but the average correlation coefficient seems to be a slight overestimate of the true correlation coefficient (0.51 instead of 0.50). This isn’t a fluke: you get it for all 10 subsets of 1,000 simulations, and you get it if you set r to another non-zero value, too. This confirms my suspicions about CI-based optional stopping for correlation coefficients (it biases the parameter estimates), but also goes to show that the biasing effect is tiny. As for the coverage rate of the confidence intervals, it is slightly lower than advertised (0.94 instead of 0.95).\n\n# proportion parameter in CI\nmean(unlist(results_correlation_peeking[5, ]))\n\n[1] 0.9415\n\n# average estimated correlation\nmean(unlist(results_correlation_peeking[4, ]))\n\n[1] 0.5108407"
  },
  {
    "objectID": "posts/2017-09-19-peeking-confidence-intervals/index.html#conclusion",
    "href": "posts/2017-09-19-peeking-confidence-intervals/index.html#conclusion",
    "title": "Confidence interval-based optional stopping",
    "section": "Conclusion",
    "text": "Conclusion\nSince I’ve never used optional stopping, be it p-value or CI-based, myself, I hesitate to offer recommendations concerning its use. Moreover, the simulations presented here hardly exhaust the possible scenarios (e.g., regression, ANCOVA, mixed modelling etc.), and any recommendation would rightly raise the question “How do you decide on the desired width of the confidence interval?”, which I have no answer to.\nThat said, I think it’s interesting that even in a frequentist framework, optional stopping doesn’t have to be detrimental to statistical inference, as long as the stopping rule isn’t p-value or parameter based: t-tests have their nominal Type-I error rate, confidence intervals their nominal coverage properties, and the parameter of interest isn’t biased. (The standard deviation estimate is biased a bit more than usual.) For correlation coefficients, stopping conditional on achieving a certain CI width only affects the parameter estimates and CI coverage properties minimally, at least in the situation simulated. Feel free to adapt the code above to check how other situations are affected by p- and CI-based optional stopping!"
  },
  {
    "objectID": "posts/2017-09-19-peeking-confidence-intervals/index.html#some-links",
    "href": "posts/2017-09-19-peeking-confidence-intervals/index.html#some-links",
    "title": "Confidence interval-based optional stopping",
    "section": "Some links",
    "text": "Some links\nJohn Kruschke discusses precision-based optional stopping on his blog. He defines precision in terms of Bayesian credible intervals, which I have no qualm with, but I don’t think the difference between confidence intervals and credible intervals will matter much in practice for unbounded data and when using weakly informative priors in the Bayesian approach.\nWhile they aren’t directly related to optional stopping, a couple of articles by Ken Kelley, Scott Maxwell and Joseph Rausch (e.g.) discuss a confidence interval counterpart to power analysis called “accuracy in parameter estimation” (AIPE). The idea in AIPE is to plan your study such that you’ll end up with a confidence interval narrower than a certain width a certain proportion of the time. The advantage of AIPE over power analysis, as I understand it, is that it doesn’t (usually?) require the analyst to specify a minimum effect size since it outcome depends on the variability of the data only. This variability still needs to be specified, but that’s no different from power analysis."
  },
  {
    "objectID": "posts/2017-09-19-peeking-confidence-intervals/index.html#software-version",
    "href": "posts/2017-09-19-peeking-confidence-intervals/index.html#software-version",
    "title": "Confidence interval-based optional stopping",
    "section": "Software version",
    "text": "Software version\n\ndevtools::session_info()\n\n─ Session info ───────────────────────────────────────────────────────────────\n setting  value\n version  R version 4.3.1 (2023-06-16)\n os       Ubuntu 22.04.2 LTS\n system   x86_64, linux-gnu\n ui       X11\n language en_US\n collate  en_US.UTF-8\n ctype    en_US.UTF-8\n tz       Europe/Zurich\n date     2023-08-07\n pandoc   3.1.1 @ /usr/lib/rstudio/resources/app/bin/quarto/bin/tools/ (via rmarkdown)\n\n─ Packages ───────────────────────────────────────────────────────────────────\n package     * version date (UTC) lib source\n cachem        1.0.6   2021-08-19 [2] CRAN (R 4.2.0)\n callr         3.7.3   2022-11-02 [1] CRAN (R 4.3.1)\n cli           3.6.1   2023-03-23 [1] CRAN (R 4.3.0)\n codetools     0.2-19  2023-02-01 [4] CRAN (R 4.2.2)\n crayon        1.5.2   2022-09-29 [1] CRAN (R 4.3.1)\n devtools      2.4.5   2022-10-11 [1] CRAN (R 4.3.1)\n digest        0.6.29  2021-12-01 [2] CRAN (R 4.2.0)\n ellipsis      0.3.2   2021-04-29 [2] CRAN (R 4.2.0)\n evaluate      0.15    2022-02-18 [2] CRAN (R 4.2.0)\n fastmap       1.1.0   2021-01-25 [2] CRAN (R 4.2.0)\n fs            1.5.2   2021-12-08 [2] CRAN (R 4.2.0)\n glue          1.6.2   2022-02-24 [2] CRAN (R 4.2.0)\n htmltools     0.5.5   2023-03-23 [1] CRAN (R 4.3.0)\n htmlwidgets   1.6.2   2023-03-17 [1] CRAN (R 4.3.1)\n httpuv        1.6.11  2023-05-11 [1] CRAN (R 4.3.1)\n jsonlite      1.8.7   2023-06-29 [1] CRAN (R 4.3.1)\n knitr         1.39    2022-04-26 [2] CRAN (R 4.2.0)\n later         1.3.1   2023-05-02 [1] CRAN (R 4.3.1)\n lifecycle     1.0.3   2022-10-07 [1] CRAN (R 4.3.0)\n magrittr      2.0.3   2022-03-30 [1] CRAN (R 4.3.0)\n MASS          7.3-60  2023-05-04 [4] CRAN (R 4.3.1)\n memoise       2.0.1   2021-11-26 [2] CRAN (R 4.2.0)\n mime          0.10    2021-02-13 [2] CRAN (R 4.0.2)\n miniUI        0.1.1.1 2018-05-18 [1] CRAN (R 4.3.1)\n pkgbuild      1.4.2   2023-06-26 [1] CRAN (R 4.3.1)\n pkgload       1.3.2.1 2023-07-08 [1] CRAN (R 4.3.1)\n prettyunits   1.1.1   2020-01-24 [2] CRAN (R 4.2.0)\n processx      3.8.2   2023-06-30 [1] CRAN (R 4.3.1)\n profvis       0.3.8   2023-05-02 [1] CRAN (R 4.3.1)\n promises      1.2.0.1 2021-02-11 [1] CRAN (R 4.3.1)\n ps            1.7.5   2023-04-18 [1] CRAN (R 4.3.1)\n purrr         1.0.1   2023-01-10 [1] CRAN (R 4.3.0)\n R6            2.5.1   2021-08-19 [2] CRAN (R 4.2.0)\n Rcpp          1.0.11  2023-07-06 [1] CRAN (R 4.3.1)\n remotes       2.4.2   2021-11-30 [2] CRAN (R 4.2.0)\n rlang         1.1.1   2023-04-28 [1] CRAN (R 4.3.0)\n rmarkdown     2.21    2023-03-26 [1] CRAN (R 4.3.0)\n rstudioapi    0.14    2022-08-22 [1] CRAN (R 4.3.0)\n sessioninfo   1.2.2   2021-12-06 [2] CRAN (R 4.2.0)\n shiny         1.7.4.1 2023-07-06 [1] CRAN (R 4.3.1)\n stringi       1.7.12  2023-01-11 [1] CRAN (R 4.3.1)\n stringr       1.5.0   2022-12-02 [1] CRAN (R 4.3.0)\n urlchecker    1.0.1   2021-11-30 [1] CRAN (R 4.3.1)\n usethis       2.2.2   2023-07-06 [1] CRAN (R 4.3.1)\n vctrs         0.6.3   2023-06-14 [1] CRAN (R 4.3.0)\n xfun          0.39    2023-04-20 [1] CRAN (R 4.3.0)\n xtable        1.8-4   2019-04-21 [1] CRAN (R 4.3.1)\n yaml          2.3.5   2022-02-21 [2] CRAN (R 4.2.0)\n\n [1] /home/jan/R/x86_64-pc-linux-gnu-library/4.3\n [2] /usr/local/lib/R/site-library\n [3] /usr/lib/R/site-library\n [4] /usr/lib/R/library\n\n──────────────────────────────────────────────────────────────────────────────"
  },
  {
    "objectID": "posts/2015-08-24-caveats-confounds-correlational-designs/index.html",
    "href": "posts/2015-08-24-caveats-confounds-correlational-designs/index.html",
    "title": "Controlling for confounding variables in correlational research: Four caveats",
    "section": "",
    "text": "In correlational studies, it is often claimed that a predictor variable is related to the outcome, even after statistically controlling for a likely confound. This is then usually taken to suggest that the relationship between the predictor and the outcome is truly causal rather than a by-product of some more important effect. This post discusses four caveats that I think should be considered when interpreting such causal claims. These caveats are well-known in the literature on ‘statistical control’, but I think it’s useful to discuss them here nonetheless."
  },
  {
    "objectID": "posts/2015-08-24-caveats-confounds-correlational-designs/index.html#whats-statistical-control",
    "href": "posts/2015-08-24-caveats-confounds-correlational-designs/index.html#whats-statistical-control",
    "title": "Controlling for confounding variables in correlational research: Four caveats",
    "section": "What’s statistical control?",
    "text": "What’s statistical control?\nBut first, let’s discuss what is meant by statistical control. When a true experimental design – including random assignment of participants to experimental conditions – is infeasible, researchers often resort to a correlational design. For instance, we may be interested in whether second-language (L2) learners’ musical ability affects their L2 skills (e.g. Slevc & Miyake 2006). Since we can’t manipulate the learners’ musical ability ourselves, we’ll have to make do with measuring the learners’ musical ability and their L2 skills in a correlational study.\nHowever, as Slevc and Miyake (2006) were also aware, a correlation between musical ability and L2 skills may come about due to a confound. For instance, phonological short-term memory may contribute to both musical ability and L2 skills. Participants with high musical ability would then tend to perform better in the L2, too, but there would not be a causal relationship between musical ability and L2 skills: the correlation would be spurious.\n\n\n\nSpurious correlation\n\n\nTo address such possibilities, researchers tend to collect a measure of the confound (here: phonological short-term memory) and include it as a control variable in the statistical model (e.g. multiple regression, ANCOVA or partial correlations). The confound is then said to have been statistically controlled for. The logic is that, if the correlation between musical ability and L2 skills is due to a shared reliance on phonological short-term memory, accounting for differences in phonological STM in the analysis should eliminate this correlation. Conversely, if a correlation between musical ability and L2 skills exists even after accounting for phonological STM, this indicates that musical ability can uniquely explain some differences in L2 skills (unique variance, see the diagram below), suggesting a direct causal link between musical ability and L2 skills.\n\n\n\nShared and unique variance\n\n\nUnfortunately, there are a couple of problems with this logic that researchers aren’t always aware of. Four of them are discussed here. Incidentally, while it was Slevc and Miyake’s (2006) study that prompted a discussion on statistical control in a class I taught last semester, these comments aren’t directed at their study specifically."
  },
  {
    "objectID": "posts/2015-08-24-caveats-confounds-correlational-designs/index.html#caveat-1-other-confounds-may-exist.",
    "href": "posts/2015-08-24-caveats-confounds-correlational-designs/index.html#caveat-1-other-confounds-may-exist.",
    "title": "Controlling for confounding variables in correlational research: Four caveats",
    "section": "Caveat 1: Other confounds may exist.",
    "text": "Caveat 1: Other confounds may exist.\nThis is the easiest caveat: perhaps the correlation is simply due to confounds that haven’t been accounted for in the study. It’s can be frustratingly easy to come up with a number of factors that conceivably could have contributed to the correlation but weren’t considered in the analysis. This caveat, of course, is what makes experimental designs with random assignment of participants to conditions so appealing: on average, proper randomisation cancels out the effects of confound variables – both of those that we considered and of those that we didn’t think of."
  },
  {
    "objectID": "posts/2015-08-24-caveats-confounds-correlational-designs/index.html#caveat-2-the-effect-of-the-covariate-may-not-be-linear.",
    "href": "posts/2015-08-24-caveats-confounds-correlational-designs/index.html#caveat-2-the-effect-of-the-covariate-may-not-be-linear.",
    "title": "Controlling for confounding variables in correlational research: Four caveats",
    "section": "Caveat 2: The effect of the covariate may not be linear.",
    "text": "Caveat 2: The effect of the covariate may not be linear.\nWhen you enter a control covariate to a regression model, correlation analysis or ANCOVA, you model a linear (i.e. straight-line) relationship between the control covariate and the outcome. But many relationships aren’t accurately captured by a straight-line function. For instance, the relationship between readers’ and listeners’ age and their comprehension of words in an unknown related language is curvilinear. A linear age term may therefore not entirely account for any confounding due to age.\nMultiple regressions and ANCOVAs can account for non-linear relationships, too, but the non-linearity has to be explicitly specified. (Options include using polynomial terms, splines or generalised additive modelling.) This is rarely done, however, and even when it’s done, the non-linear relationship is only accounted for to an approximation."
  },
  {
    "objectID": "posts/2015-08-24-caveats-confounds-correlational-designs/index.html#caveat-3-imperfect-measures-cannot-entirely-eradicate-the-confound.",
    "href": "posts/2015-08-24-caveats-confounds-correlational-designs/index.html#caveat-3-imperfect-measures-cannot-entirely-eradicate-the-confound.",
    "title": "Controlling for confounding variables in correlational research: Four caveats",
    "section": "Caveat 3: Imperfect measures cannot entirely eradicate the confound.",
    "text": "Caveat 3: Imperfect measures cannot entirely eradicate the confound.\nEven when a control covariate has been accurately modelled in the analysis, this doesn’t guarantee that the confound has been entirely accounted for. The culprit is measurement error: what we’d like to do is to account for differences in phonological short-term memory (a psychometric construct); what we have at our disposal is a pretty rough measure of phonological short-term memory (e.g. the classic digit-span measure). The more accurately the measure reflects the construct, the better we can account for differences at the construct level, “but fallibility in a covariate usually implies that there would be more adjustment if the variable were measured without error” (Huitema 2011: 569).\nSome variables are both highly accurate and easy to collect, e.g. the participants’ age and sex. Others are more involved, however, including psychometric constructs and variables such as language skills or socio-economic status (SES): a language test necessarily yields as approximation of the participants’ actual proficiency, gross income only approximates a person’s SES, and different operationalisations yield different approximations. One way to deal with the problem of measurement error is to adopt a latent-variable approach (see, e.g., Conway et al. 2005). This involves taking several measures of the confound construct and aggregate these using a technique known as confirmatory factor analysis. Needless to say, however, taking several measures of several potential confounds is bound to be an arduous and time-consuming task – especially considering that it doesn’t make Caveats 1 and 2 go away.\nIn sum, when a report says that a confound was statistically controlled for, what that actually means is that a measure of this confound was statistically controlled for. This usually undercorrects for the actual confound."
  },
  {
    "objectID": "posts/2015-08-24-caveats-confounds-correlational-designs/index.html#caveat-4-the-confound-may-itself-be-an-effect-of-the-predictor.",
    "href": "posts/2015-08-24-caveats-confounds-correlational-designs/index.html#caveat-4-the-confound-may-itself-be-an-effect-of-the-predictor.",
    "title": "Controlling for confounding variables in correlational research: Four caveats",
    "section": "Caveat 4: The ‘confound’ may itself be an effect of the predictor.",
    "text": "Caveat 4: The ‘confound’ may itself be an effect of the predictor.\nLet’s say that researchers investigating the link between musical ability and L2 skills notice that the effect disappears once accounting for differences in the amount of L2 exposure, which they take to suggest that there is no causal link between musical ability and L2 skills. However, it is possible that L2 exposure itself is influenced by musical ability (e.g. due to a greater tendency of musically gifted learners to sing and listen to L2 songs):\n\n\n\nOveradjusting\n\n\nIn this case, concluding that musical ability does not enter into a causal relationship with L2 skills would be an oversimplification: musical ability contributes, albeit indirectly, to differences in L2 skills. However, this relationship is mediated by L2 exposure. So, in much the same way that the presence of a correlation after controlling for a covariate doesn’t prove causality, so the absence of a correlation after controlling for a covariate doesn’t disprove causality, either."
  },
  {
    "objectID": "posts/2015-08-24-caveats-confounds-correlational-designs/index.html#conclusion-and-further-reading",
    "href": "posts/2015-08-24-caveats-confounds-correlational-designs/index.html#conclusion-and-further-reading",
    "title": "Controlling for confounding variables in correlational research: Four caveats",
    "section": "Conclusion and further reading",
    "text": "Conclusion and further reading\nThere aren’t any quick statistical fixes to the problems discussed above, but I think it’s important to be aware of these caveats when interpreting causal claims in correlational studies. These caveats, which are also discussed by Christenfeld et al. (2004) and Huitema (2011), shouldn’t be taken to suggest that adjusting for confounding variables is a waste of time and effort. On the contrary, in Christenfeld et al.’s (2004) words, it’s “necessary, but [just] not sufficient”."
  },
  {
    "objectID": "posts/2015-02-21-thinking-about-graphs/index.html",
    "href": "posts/2015-02-21-thinking-about-graphs/index.html",
    "title": "Thinking about graphs",
    "section": "",
    "text": "I firmly believe that research results are best communicated graphically. Straightforward scatterplots, for instance, tend to be much more informative than correlation coefficients to both novices and dyed-in-the-wool scholars alike. Often, however, it’s more challenging to come up with a graph that highlights the patterns (or lack thereof) that you want to highlight in your discussion and that your readership will understand both readily and accurately. In this post, I ask myself whether I could have presented the results from an experiment I carried out last year any better in a paper to be published soon."
  },
  {
    "objectID": "posts/2015-02-21-thinking-about-graphs/index.html#the-plot",
    "href": "posts/2015-02-21-thinking-about-graphs/index.html#the-plot",
    "title": "Thinking about graphs",
    "section": "The plot",
    "text": "The plot\nEarlier this week, I received the page proofs of an article in which I report on a learning experiment in a context of receptive multilingualism (preprint). In the experiment, 80 speakers of German were asked to translate words from a closely related language, viz. Dutch. One of the goals of the experiment was to find out whether the participants could translate certain words (‘ij cognates’ and ‘oe cognates’; it doesn’t matter much for this blog post) more accurately when they occurred in the last part of the experiment (‘Block 3’) compared to in the first part (‘Block 1’). Here is the code that you can use to reproduce the plot showing the results (‘Figure 2’):\n\n# Read in data from my institutional webpage:\ndat &lt;- read.csv(\"http://homeweb.unifr.ch/VanhoveJ/Pub/Data/CorrespondenceRules_Blocks.csv\")\n\n# 'Block' contains number but it's more useful to consider it a factor here:\ndat$Block &lt;- factor(dat$Block)\n\n\n# Load the ggplot2 package for graphics\nlibrary(ggplot2)\n\n# Draw plot of 'CorrectVowel' by 'Block':\nggplot(dat, aes(x = Block, y = CorrectVowel)) + \n  # draw boxplot, but don't draw outliers\n  # because next line plots all datapoints individually\n  geom_boxplot(outlier.shape = NA) + \n  # add individual datapoints and label them\n  # and jitter them horizontally (less overlap)\n  geom_text(size = 5, aes(label = Item),  \n            position = position_jitter(height = 0, width = 0.3)) + \n  # separately for each category\n  facet_wrap(~ Category) +\n  # label axis\n  scale_y_continuous(name = \"Correct vowel choices (%)\") \n\n\n\n\nFigure 2: Percentage of correct vowel choices for each ‹ij› and ‹oe› cognate depending on whether it occurred in Block 1 or 3. The boxplots mark the quartiles of each distribution. Only answers on untrained correspondences were considered, i.e., ‹ij› cognates for ‹oe› participants and vice versa.\n\n\n\n\nSome of the labels overlap, and I used a vector graphics editor (Inkscape) to move the labels horizontally to sort that out. I think that this plot contains a lot of useful information, especially compared to a run-of-the-mill barplot: not only are the individual datapoints plotted, they are also labelled so that it’s relatively easy to see that schoen was a particulary difficult word and that wijze saw a 10pp-increase in accuracy from Block 1 to Block 3."
  },
  {
    "objectID": "posts/2015-02-21-thinking-about-graphs/index.html#other-options",
    "href": "posts/2015-02-21-thinking-about-graphs/index.html#other-options",
    "title": "Thinking about graphs",
    "section": "Other options?",
    "text": "Other options?\n‘Relatively’ is the operative word in the preceeding paragraph, though: I don’t think I drew an awful plot, but I now think I could’ve presented my readership with some better alternatives. For instance, how about plotting the ‘before’ and ‘after’ scores in a slightly embellished scatterplot?\nUpdate (2023-08-26): Nowadays, I’d use pivot_wider() in the tidyr package (part of the tidyverse) instead of dcast().\n\n# We need the data in a slightly different format;\n# use dcast from the reshape2 package:\nlibrary(reshape2)\n# see http://seananderson.ca/2013/10/19/reshape.html\ndat2 &lt;- dcast(dat,\n              Item + Category ~ Block,\n              value.var = \"CorrectVowel\")\n# Rename the columns\ncolnames(dat2) &lt;- c(\"Item\", \"Category\", \"Block1\", \"Block3\")\n# dat2 # uncomment to see the results\n\n# Scatterplot \nggplot(dat2, aes(x = Block1, y = Block3)) +\n  # label the datapoints\n  geom_text(size = 5, aes(label = Item)) + \n  # separate scatterplots for each category\n  facet_wrap(~ Category) +\n  # label axes\n  scale_x_continuous(name = \"Correct vowel choices (%) in Block 1\") + \n  scale_y_continuous(name = \"Correct vowel choices (%) in Block 3\") +\n  # add 'x = y' line\n  geom_abline(intercept = 0, slope = 1, linetype = \"dashed\") \n\n\n\n\nThe dashed line divides the scatterplot such that stimuli appearing above the line are translated more accurately in Block 3 than in Block 1 and those that appear below it are translated less accurately in Block 3 than in Block 1. This can give a rough-and-ready impression of how many items showed an increase in accuracy as well as by what margin (vertical distance to the dashed line).\nInstead of plotting the two stimulus categories separately, we could of course also plot them in the same graph. Here’s the R code to draw such a plot (plot not displayed here).\n\nggplot(dat2, aes(x = Block1, y = Block3, colour = Category)) +\n  geom_text(size = 5, aes(label = Item)) +\n  scale_x_continuous(name = \"Correct vowel choices (%) in Block 1\") +\n  scale_y_continuous(name = \"Correct vowel choices (%) in Block 3\") +\n  geom_abline(intercept = 0, slope = 1, linetype = \"dashed\")\n\nI think these scatterplots are a step up from the original boxplot comparisons, but they can still be criticised. First, I think that the dashed ‘x = y’ line might somehow come across as the fit line from a linear regression. We could obviously spell it out in the caption that this isn’t the case, but it’d be better if it were immediately obvious. Second, it is difficult to gauge distances with respect to a diagonal line: the vertical distance from the word groet to the dashed line is fairly large, but its perpendicular distance to the line is necessarily smaller, which might convey a false impression. Again, we could spell this out in the caption, but it’d be better if we didn’t have to.\nIn view of these two criticisms, the following graph shows the improvement in accuracy in Block 3 compared to Block 1 plotted against the baseline accuracies.\n\n# Calculate difference between 3 and 1\ndat2$Difference &lt;- dat2$Block3 - dat2$Block1\n\n# Draw scatterplot of baseline vs improvement\nggplot(dat2, aes(x = Block1, y = Difference)) +\n  geom_text(size = 5, aes(label = Item)) +\n  facet_wrap(~ Category) +\n  scale_x_continuous(name = \"Correct vowel choices (%) in Block 1\") +\n  scale_y_continuous(name = \"Improvement in vowel choice accuracy (pp)\\nin Block 3 compared to Block 1\") +\n  geom_abline(intercept = 0, slope = 0, linetype = \"solid\")\n\n\n\n\nThe solid line again divides the stimuli that showed an improvement in accuracy and those that showed a decrease, but I think that a perfectly horizontal line is less readily mistaken for a regression fit line. Additionally, the improvement in accuracy for each item can be gauged straightforwardly as the distance to the dividing line – without the need to specify that it’s the vertical distance rather than the perpendicular distance that’s important (they’re both the same).\nAn additional benefit of this plot is that it can readily reveal peculiar patterns that might merit further investigation. For instance, the effect of ‘Block’ on accuracy seems to depend on baseline accuracy (i.e. it doesn’t seem to be additive): there is a decreasing shape to the scatterplot. We could highlight this trend by adding scatterplot smoothers to the panels, as in the graph below.\n\nggplot(dat2, aes(x = Block1, y = Difference)) +\n  geom_text(size = 5, aes(label = Item)) +\n  facet_wrap(~ Category) +\n  scale_x_continuous(name = \"Correct vowel choices (%) in Block 1\") +\n  scale_y_continuous(name = \"Improvement in vowel choice accuracy (pp)\\nin Block 3 compared to Block 1\") +\n  geom_abline(intercept = 0, slope = 0, linetype = \"solid\") +\n  # add loess scatterplot smoother; suppress confidence bands\n  geom_smooth(method = \"loess\", se = FALSE) \n\n`geom_smooth()` using formula = 'y ~ x'\n\n\n\n\n\nA last alternative that I’ll mention is to plot boxplots of the accuracy improvements rather than of the raw accuracy scores separately for Blocks 1 and 3:\n\nggplot(dat2, aes(x = Category, y = Difference)) +\n  geom_boxplot(outlier.shape = NA) +\n  geom_text(aes(label = Item), size = 5, position = position_jitter(height=0, width=0.3)) +\n  scale_y_continuous(name = \"Improvement in vowel choice accuracy (pp)\\nin Block 3 compared to Block 1\")\n\n\n\n\nI don’t really like this plot as it throws away much of the information we could have gained by using scatterplots. That said, highlighting a measure of the central tendency of the increases could be useful, so perhaps a combination of the scatterplots above and these boxplots could serve both purposes."
  },
  {
    "objectID": "posts/2015-02-21-thinking-about-graphs/index.html#open-questions",
    "href": "posts/2015-02-21-thinking-about-graphs/index.html#open-questions",
    "title": "Thinking about graphs",
    "section": "Open questions",
    "text": "Open questions\nThis blog posts merely serves as an illustration of how the same data can be visualised in sundry ways and what factors go into choosing one graphic rather than the other options. One hugely important factor that perhaps isn’t always fully considered is how the graph is likely to be perceived by the audience.\nThis entails anticipating misinterpretations that result from a cursory reading of the caption and the surrounding text (as I’ve tried to show above), but on a more basic level it means taking into account the statistical literacy of your audience. Quantile–quantile plots, for instance, are useful for comparing two samples, but I don’t think the concept of quantiles is presently common currency for most readers.\nWhat is sorely lacking in this discussion is concrete evidence about how easily and accurately graphical displays are interpreted by their intended audiences. For instance, while I think that a diagonal line in a scatterplot showing pre- and post-test data is often interpreted as a regression fit, I don’t actually know. This lack of empirical data is also the reason why I’m hesitant to make sweeping recommendations about ‘best practices’ when it comes to drawing graphs. A Bland–Altman (or Tukey mean-difference) plot, for instance, is easy enough to draw and explain (and similar to my second scatterplot; see R code below) and seems to be pretty common in the biomedical sciences. But I don’t recall ever coming across one when when reading journals in linguistics, applied linguistics or psychology, so most readers aren’t going to be familiar with it. It might turn out that readers of applied linguistics journals can readily and accurately interpret Bland–Altman plots – but it’s also possible that the additional – if simple – step of plotting differences against means rather than against baseline scores creates some sort of cognitive barrier.\nActual studies on how applied linguists, educators, policy-makers etc. interpret common and not-so-common visualisations could only serve to improve the communicative quality of our research papers, and it’s something I might look into in the months to come.\n\n# A Bland-Altmann plot plots the mean of two paired observations\n# against the difference between them.\n\n# First compute mean:\ndat2$Mean &lt;- (dat2$Block1 + dat2$Block3)/2\n\n# Then plot Bland-Altmann\nggplot(dat2, aes(x = Mean, y = Difference)) +\n  geom_text(size = 5, aes(label = Item)) +\n  facet_wrap(~ Category) +\n  scale_x_continuous(name = \"Overall correct vowel choices (mean %) in Blocks 1 and 3\") +\n  scale_y_continuous(name = \"Improvement in vowel choice accuracy (pp)\\nin Block 3 compared to Block 1\") +\n  geom_abline(intercept = 0, slope = 0, linetype = \"solid\") +\n  ggtitle(\"Tukey mean-difference plot\")"
  },
  {
    "objectID": "posts/2015-02-21-thinking-about-graphs/index.html#software-versions",
    "href": "posts/2015-02-21-thinking-about-graphs/index.html#software-versions",
    "title": "Thinking about graphs",
    "section": "Software versions",
    "text": "Software versions\n\ndevtools::session_info()\n\n─ Session info ───────────────────────────────────────────────────────────────\n setting  value\n version  R version 4.3.1 (2023-06-16)\n os       Ubuntu 22.04.3 LTS\n system   x86_64, linux-gnu\n ui       X11\n language en_US\n collate  en_US.UTF-8\n ctype    en_US.UTF-8\n tz       Europe/Zurich\n date     2023-08-26\n pandoc   3.1.1 @ /usr/lib/rstudio/resources/app/bin/quarto/bin/tools/ (via rmarkdown)\n\n─ Packages ───────────────────────────────────────────────────────────────────\n package     * version date (UTC) lib source\n cachem        1.0.6   2021-08-19 [2] CRAN (R 4.2.0)\n callr         3.7.3   2022-11-02 [1] CRAN (R 4.3.1)\n cli           3.6.1   2023-03-23 [1] CRAN (R 4.3.0)\n colorspace    2.1-0   2023-01-23 [1] CRAN (R 4.3.0)\n crayon        1.5.2   2022-09-29 [1] CRAN (R 4.3.1)\n devtools      2.4.5   2022-10-11 [1] CRAN (R 4.3.1)\n digest        0.6.29  2021-12-01 [2] CRAN (R 4.2.0)\n dplyr         1.1.2   2023-04-20 [1] CRAN (R 4.3.0)\n ellipsis      0.3.2   2021-04-29 [2] CRAN (R 4.2.0)\n evaluate      0.15    2022-02-18 [2] CRAN (R 4.2.0)\n fansi         1.0.4   2023-01-22 [1] CRAN (R 4.3.1)\n farver        2.1.1   2022-07-06 [1] CRAN (R 4.3.0)\n fastmap       1.1.0   2021-01-25 [2] CRAN (R 4.2.0)\n fs            1.5.2   2021-12-08 [2] CRAN (R 4.2.0)\n generics      0.1.3   2022-07-05 [1] CRAN (R 4.3.0)\n ggplot2     * 3.4.2   2023-04-03 [1] CRAN (R 4.3.0)\n glue          1.6.2   2022-02-24 [2] CRAN (R 4.2.0)\n gtable        0.3.3   2023-03-21 [1] CRAN (R 4.3.0)\n htmltools     0.5.5   2023-03-23 [1] CRAN (R 4.3.0)\n htmlwidgets   1.6.2   2023-03-17 [1] CRAN (R 4.3.1)\n httpuv        1.6.11  2023-05-11 [1] CRAN (R 4.3.1)\n jsonlite      1.8.7   2023-06-29 [1] CRAN (R 4.3.1)\n knitr         1.39    2022-04-26 [2] CRAN (R 4.2.0)\n labeling      0.4.2   2020-10-20 [1] CRAN (R 4.3.0)\n later         1.3.1   2023-05-02 [1] CRAN (R 4.3.1)\n lattice       0.21-8  2023-04-05 [4] CRAN (R 4.3.0)\n lifecycle     1.0.3   2022-10-07 [1] CRAN (R 4.3.0)\n magrittr      2.0.3   2022-03-30 [1] CRAN (R 4.3.0)\n Matrix        1.6-0   2023-07-08 [4] CRAN (R 4.3.1)\n memoise       2.0.1   2021-11-26 [2] CRAN (R 4.2.0)\n mgcv          1.9-0   2023-07-11 [4] CRAN (R 4.3.1)\n mime          0.10    2021-02-13 [2] CRAN (R 4.0.2)\n miniUI        0.1.1.1 2018-05-18 [1] CRAN (R 4.3.1)\n munsell       0.5.0   2018-06-12 [1] CRAN (R 4.3.0)\n nlme          3.1-162 2023-01-31 [4] CRAN (R 4.2.2)\n pillar        1.9.0   2023-03-22 [1] CRAN (R 4.3.0)\n pkgbuild      1.4.2   2023-06-26 [1] CRAN (R 4.3.1)\n pkgconfig     2.0.3   2019-09-22 [2] CRAN (R 4.2.0)\n pkgload       1.3.2.1 2023-07-08 [1] CRAN (R 4.3.1)\n plyr          1.8.8   2022-11-11 [1] CRAN (R 4.3.1)\n prettyunits   1.1.1   2020-01-24 [2] CRAN (R 4.2.0)\n processx      3.8.2   2023-06-30 [1] CRAN (R 4.3.1)\n profvis       0.3.8   2023-05-02 [1] CRAN (R 4.3.1)\n promises      1.2.0.1 2021-02-11 [1] CRAN (R 4.3.1)\n ps            1.7.5   2023-04-18 [1] CRAN (R 4.3.1)\n purrr         1.0.1   2023-01-10 [1] CRAN (R 4.3.0)\n R6            2.5.1   2021-08-19 [2] CRAN (R 4.2.0)\n Rcpp          1.0.11  2023-07-06 [1] CRAN (R 4.3.1)\n remotes       2.4.2   2021-11-30 [2] CRAN (R 4.2.0)\n reshape2    * 1.4.4   2020-04-09 [1] CRAN (R 4.3.1)\n rlang         1.1.1   2023-04-28 [1] CRAN (R 4.3.0)\n rmarkdown     2.21    2023-03-26 [1] CRAN (R 4.3.0)\n rstudioapi    0.14    2022-08-22 [1] CRAN (R 4.3.0)\n scales        1.2.1   2022-08-20 [1] CRAN (R 4.3.0)\n sessioninfo   1.2.2   2021-12-06 [2] CRAN (R 4.2.0)\n shiny         1.7.4.1 2023-07-06 [1] CRAN (R 4.3.1)\n stringi       1.7.12  2023-01-11 [1] CRAN (R 4.3.1)\n stringr       1.5.0   2022-12-02 [1] CRAN (R 4.3.0)\n tibble        3.2.1   2023-03-20 [1] CRAN (R 4.3.0)\n tidyselect    1.2.0   2022-10-10 [1] CRAN (R 4.3.0)\n urlchecker    1.0.1   2021-11-30 [1] CRAN (R 4.3.1)\n usethis       2.2.2   2023-07-06 [1] CRAN (R 4.3.1)\n utf8          1.2.3   2023-01-31 [1] CRAN (R 4.3.1)\n vctrs         0.6.3   2023-06-14 [1] CRAN (R 4.3.0)\n withr         2.5.0   2022-03-03 [2] CRAN (R 4.2.0)\n xfun          0.39    2023-04-20 [1] CRAN (R 4.3.0)\n xtable        1.8-4   2019-04-21 [1] CRAN (R 4.3.1)\n yaml          2.3.5   2022-02-21 [2] CRAN (R 4.2.0)\n\n [1] /home/jan/R/x86_64-pc-linux-gnu-library/4.3\n [2] /usr/local/lib/R/site-library\n [3] /usr/lib/R/site-library\n [4] /usr/lib/R/library\n\n──────────────────────────────────────────────────────────────────────────────"
  },
  {
    "objectID": "posts/2017-01-31-automatise-repetitive-tasks/index.html",
    "href": "posts/2017-01-31-automatise-repetitive-tasks/index.html",
    "title": "Automatise repetitive tasks",
    "section": "",
    "text": "Research often involves many repetitive tasks. For a ongoing project, for instance, we needed to replace all stylised apostrophes (’) with straight apostrophes (’) in some 3,000 text files when preparing the texts for the next step. As another example, you may need to split up a bunch of files into different directories depending on, say, the character in the file name just before the extension. When done by hand, such tasks are as mind-numbing and time-consuming as they sound – perhaps you would do them on a Friday afternoon while listening to music or outsource them to a student assistant. My advice, though, is this: Try to automatise repetitive tasks.\nDoing repetitive tasks is what computers are for, so rather than spending several hours learning nothing, I suggest you spend that time writing a script or putting together a command line call that does the task for you. If you have little experience doing this, this will take time at first. In fact, I reckon I often spend roughly same amount of time trying to automatise menial tasks as it would have cost me to do them by hand. But in the not-so-long run, automatisation is a time-saver: Once you have a working script, you can tweak and reuse it. Additionally, while you’re figuring out how to automatise a menial chore, you’re actually learning something useful. The chores become more of a challenge and less mind-numbing. I’m going to present an example or two of what I mean and I will conclude by giving some general pointers."
  },
  {
    "objectID": "posts/2017-01-31-automatise-repetitive-tasks/index.html#example-1-replacing-stylised-apostrophes-by-straight-apostrophes",
    "href": "posts/2017-01-31-automatise-repetitive-tasks/index.html#example-1-replacing-stylised-apostrophes-by-straight-apostrophes",
    "title": "Automatise repetitive tasks",
    "section": "Example 1: Replacing stylised apostrophes by straight apostrophes",
    "text": "Example 1: Replacing stylised apostrophes by straight apostrophes\nThis is the example cited above: for an ongoing project, we needed to replace all stylised apostrophes (i.e., ’) with straight apostrophes (i.e., ‘) in some 3,000 text files. This task is so menial you shouldn’t even think about doing it by hand, i.e., by opening all 3,000 text files and using ‘Find and replace’. I work on Linux so I Googled something like “find and replace in all files Linux” and found a post on StackOverflow very much like this one that provided the basic solution: To replace occurrences of “foo” with “bar”” in all files in a directory, open a Linux terminal (command-line) and use\ncd /path/to/your/folder\nsed -i 's/foo/bar/g' *\n\nThe first line navigates you to the directory containing the files you want to edit, so I needed to replace that by, say,\ncd Documents/ProjectX/files\n\nThe second line is a sed command where I needed to replace ‘foo’ with a stylised apostrophe and ‘bar’ with a straight apostrophe. To get it to work, I needed to replace the outer single quotation marks by double quotation marks:\nsed -i \"s/’/'/g\" *\n\nI later learnt that the ‘-i’ means that the files are edited in place; removing the ‘-i’ shows the content of the files with all ’s changes to ‘s in the terminal, but doesn’t affect the files themselves. The ’g’ causes the command to replace all occurrences, not just the first one, and by using ’*’ at the end, the sed command is applied to all files in the directory.\nI don’t think this command will work out of the box on Windows, but that’s not the point (I’m sure you can easily find a similar solution for Windows). Rather, the point is that, given a choice between doing a repetitive task and trying to find an automatic solution, invest your time in the second.\nIncidentally, an alternative to using the command line for such tasks would be to use a program with a graphical user interface that allows you to find and replace strings in all files in a directory. The advantage to using the command line is that you can simply save the commands so that you have an unambiguous record of what steps you took when preparing materials or data."
  },
  {
    "objectID": "posts/2017-01-31-automatise-repetitive-tasks/index.html#example-2-concatenating-pairs-of-text-files-based-on-their-name",
    "href": "posts/2017-01-31-automatise-repetitive-tasks/index.html#example-2-concatenating-pairs-of-text-files-based-on-their-name",
    "title": "Automatise repetitive tasks",
    "section": "Example 2: Concatenating pairs of text files based on their name",
    "text": "Example 2: Concatenating pairs of text files based on their name\nThe files mentioned above contain short texts by children at different points in time. At each point in time, each child wrote two texts: a narrative and an argumentative one. For the purposes of an analysis, we needed to combine the narrative and argumentative text into one file for each child at each point in time.\nThis task would again be mind-numbing, but helpfully, the files were named in such a way that this step could be automatised. Each file name started with a string that uniquely identified the child, followed by either ‘narr’ or ‘arg’ (narrative vs. argumentative text), followed by the point in time when the text was written and the file extension. By Googling something like “merge text files based on name”, I found another helpful StackOverflow post. I needed to tweak this a bit, and while I bet the solution below looks pig-ugly to people more knowledgeable about scripting, it does the job:\n# Merge text files based on name\n\n# Find all files in directory containing 'narr'\nFILESNARR=$(find . -type f -name '*narr*')\n# Find all files in directory containing 'arg'\nFILESARG=$(find . -type f -name '*arg*')\n\n# The following loops through all files with narr and arg found above.\n# It extracts the first part of the filename (child’s unique ID)\n# (filename1: up till 'narr' or 'arg')\n# as well as the second part (time of data collection + extension)\n# (filename2: after 'narr' or 'arg').\n# First, the contents of the narr files are written to a new file\n# called filename1_filename2 (i.e., unique ID_Time.txt).\n# Then, the contents of the arg files are appended to this file.\n\n# First write the narrative texts to file.\nfor file in $FILESNARR; do\n # take 'file', remove everything after and including _narr\n filename1=${file%_narr_*}\n # take 'file', remove everything before and including _narr\n filename2=${file#*_narr_}\n cat \"$file\" &gt;&gt; \"${filename1}_${filename2}\"\ndone\n\n# Then add the argumentative texts to the same file.\nfor file in $FILESARG; do\n filename1=${file%_arg_*}\n filename2=${file#*_arg_}\n cat \"$file\" &gt;&gt; \"${filename1}_${filename2}\"\ndone\n\nNote that if we decided we would rather have the argumentative first and the narrative texts second, we just change the order of the last two blocks. It would also be easy to tweak this script so that we would have some filler text before or after each text, for instance. Copious comments increases the script’s reuse potential."
  },
  {
    "objectID": "posts/2017-01-31-automatise-repetitive-tasks/index.html#general-pointers",
    "href": "posts/2017-01-31-automatise-repetitive-tasks/index.html#general-pointers",
    "title": "Automatise repetitive tasks",
    "section": "General pointers",
    "text": "General pointers\n\nI have found StackOverflow to be the most useful resource for finding commands such as those above.\nIf you’re working with sound files and need to perform menial tasks on them, make use of Praat’s scripting possibilities. You can find some very useful scripts at http://www.linguistics.ucla.edu/faciliti/facilities/acoustic/praat.html, and even if they don’t do what you need, you can use them to teach yourself how to script in Praat. (It’s pretty intuitive.)\nWhen the menial steps involve data that you need to preprocess or squeeze into the right format, don’t manually edit the original data files. Instead, do your data wrangling in your statistics program and use a script, not a graphical user interface. This way, any changes are documented and easily reversible.\nDouble-check a couple of edited files to see if everything went according to plan.\nMost imporantly, make a back-up of the files you want to edit. Don’t run a command on batch of text files only to find out that all exclamation marks have been replaced by spaces rather than dots. You can’t click “undo previous” when working at the prompt."
  },
  {
    "objectID": "posts/2016-04-01-multiple-comparisons-scenarios/index.html",
    "href": "posts/2016-04-01-multiple-comparisons-scenarios/index.html",
    "title": "On correcting for multiple comparisons: Five scenarios",
    "section": "",
    "text": "Daniël Lakens recently blogged about a topic that crops up every now and then: Do you need to correct your p-values when you’ve run several significance tests? The blog post is worth a read, and I feel this quote sums it up well:\nI think so, too, which is why in this blog post, I present five scenarios and discuss how I feel about correcting for multiple comparisons in each of them."
  },
  {
    "objectID": "posts/2016-04-01-multiple-comparisons-scenarios/index.html#the-multiple-comparisons-problem",
    "href": "posts/2016-04-01-multiple-comparisons-scenarios/index.html#the-multiple-comparisons-problem",
    "title": "On correcting for multiple comparisons: Five scenarios",
    "section": "The multiple comparisons problem",
    "text": "The multiple comparisons problem\nIn case the phrase “multiple comparisons” doesn’t ring a bell, I’ll give a rundown of what’s involved. If you run a significance test, you accept a 5% risk of finding a significant result if, in fact, no real pattern exists. This is referred to as a “Type-I error”, a “false positive” or a “false alarm”. The 5% figure itself is a matter of convention: you could decide to accept a lower risk of finding something that doesn’t exist, but then there’s a lower chance of finding true relationships.\nBut let’s say you run two significance tests (on unrelated data). For each of the tests, you still accept a 5% risk of finding a false positive. That is, if no real relationship existed in the data, your first test has a 5% chance of returning a positive result, and so does your second test. But if you consider the two tests simultaneously, your chances of any test returning a positive result wouldn’t be 5% but 9.75%: in 5% of cases, the first test will be positive; in the remaining 95% of cases, the second test will sometimes be positive. The graph below shows how the total (“familywise”) Type-I error rate increases as the number of tests increases.\n\n\n\n\n\nThis increase in overall Type-I error rate is a problem if you run several tests, find any positive result, and interpret that result as though your chance of finding it by accident were 5%. There exist several approaches to solve this “multiple comparisons” problem which involve recalibrating the Type-I error rate of each single test so that the overall Type-I error rate is lowered to 5%. In what follows, I’m not going to discuss these approaches, as I will instead argue that the multiple comparisons problem can often be avoided."
  },
  {
    "objectID": "posts/2016-04-01-multiple-comparisons-scenarios/index.html#five-scenarios",
    "href": "posts/2016-04-01-multiple-comparisons-scenarios/index.html#five-scenarios",
    "title": "On correcting for multiple comparisons: Five scenarios",
    "section": "Five scenarios",
    "text": "Five scenarios\nI’m going to discuss five scenarios and examine whether they present a multiple comparisons problem. The scenarios are not exhausitive, but they should cover a fair range of situations in which researchers are in doubt as to whether they should correct for multiple comparisons. For all five scenarios, I’ll make a couple of crucial assumptions, all of which are variations on the same theme:\n\nNo selective reporting. Whichever tests you set out to run were ran and are reported.\nNo data snooping. All tests you ran were decided on before seeing the data; you’ve not looked at interesting patterns in the data and then decided to formally test for their existence using those same data. More subtly, you didn’t base your decisions about removing outliers, combining variables etc. on the data at hand.\nNo post-hoc hypothesising passed off for a priori predictions (HARKing). What you report to have been your prediction before you collected the data actually was your prediction before seeing the data.\n\nWhat this boils down to is that you had a specific idea of what to test in your study, then tested it, and then reported what you tested. You may think that all of these assumptions hold true for your own research, but if you try to pre-register your next study, you may be surprised at how many decisions you take only after seeing the data first (I was!).\n\nScenario 1: ‘Differences exist’.\nScenario: You want to investigate whether the font a text is written in affects readers’ comprehension of the text. You devise a reading comprehension task and prepare six versions of the target text: one is printed in Arial, one in Times, one in Courier, one in Helvetica, one in Gentium, and one in LM mono. You have no particular expectation about which fonts will hamper readers’ understanding of the text and which fonts will enhance it. Participants are randomly assigned to one of the six versions, and the outcome measure is their performance on the comprehension task.\nComment: With one outcome (dependent) variable and a predictor (independent) variable with six levels, you could run 15 t-tests: Arial vs. Times, Arial vs. Courier, Times vs. Courier etc. In that case, multiple comparisons would be a problem. But the problem is easily avoided if you analyse the data in one model, though the choice of the model could be a matter of discussion:\n\nIf your precise research question is “Is readers’ comprehension of a text different depending on whether the text is written in Arial, Times, Courier, Helvetica, Gentium or LM mono?”, then a simple one-way ANOVA with text font as the independent variable gives you a single F-test that you can use to answer your research question. Since you run one significance test to answer one research question, no multiple comparisons are involved.\nIf your precise research question is “Is readers’ comprehension of a text different depending on the font the text is written in?”, and your choice of fonts in the experiment is just a selection of possible fonts, then it seems more appropriate to model text font as a random effect. You could then test if the random effect for text font represents a significant improvement over a null model (Faraway recommends parametric bootstrapping for this), and you’d again have one significance test for one research question, and no multiple comparisons.\n\nIn both cases, the multiple comparisons problem disappears once you align the significance test with your research question.\nIncidentally, this scenario concerned mean differences between groups, but a similar logic applies when you’re interested in correlations between variables. If you collect, say, five variables and your research question is “Are any of these variables correlated with one another?”, then you could run 10 bivariate correlation tests. In this case, a correction for multiple comparisons would be necessary. But it may be better to tailor the test to the research question and test for all correlations simultaneously.\n\n\nScenario 2: One comparison is critical.\nScenario: You want to investigate whether a moderate intake of alcohol boosts learners’ fluency in the L2. Participants are randomly assigned to one of three groups and drink a pint of (real) beer, alcohol-free beer or water. They then describe a video clip in the L2, and the number of syllables per minute is counted for each participant.\nComment: In principle, three comparisons could be carried out (beer vs. alcohol-free, beer vs. water, alcohol-free vs. water), and at first blush one might want to correct for multiple comparisons. However, the alcohol-free vs. water comparison is irrelevant for the research question (which concerns the intake of actual alcohol). Furthermore, the beer vs. water comparison is a much less stringent test of the researcher’s hypothesis than the beer vs. alcohol-free comparison as it confounds ‘intake of alcohol’ with ‘assumed intake of alcohol’ such that differences between the beer and water groups could come about due to a placebo effect.\nFor these reasons, I don’t think correcting for multiple comparisons is necessary here as – again – no multiple comparisons are needed for testing the researcher’s hypothesis: comparing the beer and alcohol-free groups suffices. The additional alcohol-free vs. water comparison would be useful to test for the different hypothesis that assumed intake of alcohol influences L2 fluency, but I don’t think this should affect the way the first hypothesis should be tested.\n\n\nScenario 3: Different theories are subjected to one different test each.\nScenario: Same as Scenario 2, only this time you’re interested in the effects on fluency of (a) actual intake of alcohol and (b) perceived intake of alcohol.\nComment: Out of the three possible tests (beer vs. alcohol-free, beer vs. water, alcohol-free vs. water), only two are relevant. The beer vs. alcohol-free comparison addresses the first research question; the alcohol-free vs. water comparison addresses the second one. If you find a positive result for the first comparison, you’d tentatively conclude that the actual intake of alcohol affected fluency. If you find a positive result for the second comparison, you’d tentatively conclude that the perceived intake of alcohol affected fluency. No correction for multiple comparisons is needed as you have one significance test for each research question.\n\n\nScenario 4: Theory survives if any out of a number of possible tests is positive.\nScenario: A variation on Scenario 2. You want to investigate whether a moderate intake of alcohol affects learners’ L2 behaviour. The theory behind the study predicts that a moderate of intake of alcohol does affect learners’ L2 behavior, but nothing more specific than that. Participants are randomly assigned to one of two groups and drink a pint of (real) or alcohol-free beer. They then describe a video clip in the L2. You measure several outcome variables:\n\nfluency: the number of syllables per minute is counted for each participant;\nlexical diversity: the number of different words divided by total number of words;\nsyntactic complexity: the number of clauses per T-unit.\n\nComment: The first thing to note is that the research question is pretty vague as the theory behind it is rather non-committal. With this vague a theory, multiple comparisons are an issue if you’d run three separate tests and conclude from any one positive result that your theory had been vindicated. In such cases, it may be more appropriate to try to align the significance test with the vague theory and analyse fluency, lexical diversity and syntactic complexity in one multivariate model instead.\nAnother possibility may be to try to adopt (a) more precise theory (theories) before the start of the study, but this would put us either in Scenario 3 (separate theories) or in Scenario 5 (one theory).\n\n\nScenario 5: Several specific positive tests are needed to corroborate theory.\nScenario: The opposite of Scenario 4. You have a theory that predicts that a moderate intake of alcohol affects learners’ L2 production in terms of fluency, lexical diversity and syntactic complexity. Participants are randomly assigned to one of two groups and drink a pint of (real) or alcohol-free beer. They then describe a video clip in the L2, and you measure the same three outcome variables as above.\nComment: This time, the theory behind the study is considerably more specific and makes three testable predictions. If you’d only conclude the theory had been vindicated if all three predictions panned out, you’d be shooting yourself in the foot power-wise if you corrected for multiple comparisons. I’m not sure how I’d deal with such a scenario (it hasn’t really come up so far), but I’d probably take the bus to the library of our Statistics Department and take it from there."
  },
  {
    "objectID": "posts/2016-04-01-multiple-comparisons-scenarios/index.html#conclusion",
    "href": "posts/2016-04-01-multiple-comparisons-scenarios/index.html#conclusion",
    "title": "On correcting for multiple comparisons: Five scenarios",
    "section": "Conclusion",
    "text": "Conclusion\nIn this blog post, I’ve tried to argue that correcting for multiple comparisons may not always be necessary if the way in which the data will be analysed and the theoretical inferences that will be drawn from the results are clearly specified in advance and all analyses are reported. In such cases, the challenge is to align the significance test used with the theory. Once selective reporting and analytical and theoretical flexibility are thrown into the mix, things are different. But in such cases, it would seem to me that multiple comparison procedures are of limited use as the number of actual and implicit comparisons may be impossible to determine.\nOne reason I blog is to help me organise my thoughts about topics such as these, so I’m looking forward to your comments."
  },
  {
    "objectID": "posts/2020-09-02-cluster-covar-sim/index.html",
    "href": "posts/2020-09-02-cluster-covar-sim/index.html",
    "title": "Capitalising on covariates in cluster-randomised experiments",
    "section": "",
    "text": "In cluster-randomised experiments, participants are assigned to the conditions randomly but not on an individual basis. Instead, entire batches (‘clusters’) of participants are assigned in such a way that each participant in the same cluster is assigned to the same condition. A typical example would be an educational experiment in which all pupils in the same class get assigned to the same experimental condition. Crucially, the analysis should take into account the fact that the random assignment took place at the cluster level rather than at the individual level.\nAlso typically in educational experiments, researchers have some information about the participants’ performance before the intervention took place. This information can come in the form of a covariate, for instance the participants’ performance on a pretest or some self-assessment of their skills. Even in experiments that use random assignment, including such covariates in the analysis is useful as they help to reduce the error variance. Lots of different methods for including covariates in the analysis of cluster-randomised experiments are discussed in the literature, but I couldn’t find any discussion about the merits and drawbacks of these different methods.\nIn order to provide such discussion, I ran a series of simulations to compare 25 (!) different ways of including a covariate in the analysis of a cluster-randomised experiment in terms of their Type-I error and their power. The article outlining these simulations and the findings is available from PsyArXiv; the R code used for the simulations as well as its output is available from the Open Science Framework. In the remainder of this post, I’ll discuss how these simulations may be useful to you if you’re planning to run a cluster-randomised experiment."
  },
  {
    "objectID": "posts/2020-09-02-cluster-covar-sim/index.html#so-whats-the-upshot",
    "href": "posts/2020-09-02-cluster-covar-sim/index.html#so-whats-the-upshot",
    "title": "Capitalising on covariates in cluster-randomised experiments",
    "section": "So what’s the upshot?",
    "text": "So what’s the upshot?\nPlease read pages 1–3 and pages 40–42 of the article :)"
  },
  {
    "objectID": "posts/2020-09-02-cluster-covar-sim/index.html#i-know-of-some-analytical-method-that-you-didnt-consider.",
    "href": "posts/2020-09-02-cluster-covar-sim/index.html#i-know-of-some-analytical-method-that-you-didnt-consider.",
    "title": "Capitalising on covariates in cluster-randomised experiments",
    "section": "I know of some analytical method that you didn’t consider.",
    "text": "I know of some analytical method that you didn’t consider.\nAh, interesting! It took a long time to run these simulations (about 36 hours), during which I couldn’t use my computer for anything else, so I’m not exactly gung-ho about rerunning them just to include one additional analytic method.\nBut here’s what you can do. Go to the OSF page and download the files functions/generate_clustered_data.R and scripts/additional_simulations.Rmd. The latter file contains some smaller-scale simulations that don’t take as long to run. Adapt the simulations there and check if the analytical method you know of has an acceptable Type-I error rate for a variety of parameter settings. (Two examples are available, but if you can’t make sense of them, let me know.) If its Type-I error rate is acceptable, run another batch of simulations to assess its power and compare it to the power for the best-performing methods in my simulation.\nIf your method compares well to these best-performing methods in terms of both its Type-I error rate and its power, drop me a line, and perhaps I’ll get round to rerunning the large-scale simulations. Better still, download the other functions and scripts, include your method in functions/analyse_clustered_data.R, and adjust the other files accordingly. Then run the simulation yourself :)"
  },
  {
    "objectID": "posts/2020-09-02-cluster-covar-sim/index.html#i-dont-think-your-parameter-choices-are-relevant-for-my-study.",
    "href": "posts/2020-09-02-cluster-covar-sim/index.html#i-dont-think-your-parameter-choices-are-relevant-for-my-study.",
    "title": "Capitalising on covariates in cluster-randomised experiments",
    "section": "I don’t think your parameter choices are relevant for my study.",
    "text": "I don’t think your parameter choices are relevant for my study.\nPerhaps your study will feature clusters that vary more strongly in size than was the case in my simulations. Or perhaps you suspect that the intracluster correlation will be quite different from the ones that I considered. Or perhaps etc., etc., etc. It’d be better if the results of the simulations were more directly relevant to what you suspect your study will look like.\nBut here’s the beauty. You can go to the OSF page, download all the functions and scripts, and tailor the simulation parameters to your liking. In script/simulation_type_I_error.R and script/simulation_power.R, you can change the cluster sizes, the number of clusters, the strength of the covariate, the ICC, the effect, and the randomisation scheme. Then run these scripts and figure out which analysis method will likely retain its nominal Type-I error while maximising power."
  },
  {
    "objectID": "posts/2020-09-02-cluster-covar-sim/index.html#when-generating-the-data-youre-making-some-assumptions-im-not-willing-to-make.",
    "href": "posts/2020-09-02-cluster-covar-sim/index.html#when-generating-the-data-youre-making-some-assumptions-im-not-willing-to-make.",
    "title": "Capitalising on covariates in cluster-randomised experiments",
    "section": "When generating the data, you’re making some assumptions I’m not willing to make.",
    "text": "When generating the data, you’re making some assumptions I’m not willing to make.\nThe assumptions are outlined in the article on pp. 23–24, and they are made more explicit in the function that generates the data (functions/generate_clustered_data.R). Perhaps they’re unrealistic. For instance, the data are all drawn from normal distributions, the covariate is linearly related to the outcome, etc. If you want to revise these assumptions, you’ll have to edit this function. (Test the function extensively afterwards!) Then re-run the simulations, with the simulation parameters tailored to your study."
  },
  {
    "objectID": "posts/2020-09-02-cluster-covar-sim/index.html#which-journal-will-this-article-be-published-in",
    "href": "posts/2020-09-02-cluster-covar-sim/index.html#which-journal-will-this-article-be-published-in",
    "title": "Capitalising on covariates in cluster-randomised experiments",
    "section": "Which journal will this article be published in?",
    "text": "Which journal will this article be published in?\nAt the moment, I don’t intend to submit this article to any journal. The main reason is that anyone who may be interested in it already has free access to it. If anyone has any feedback, I’d be happy to hear it, but I don’t currently feel like jumping through a series of hoops in some drawn-out reviewing process."
  },
  {
    "objectID": "posts/2016-12-20-bootstrapping/index.html",
    "href": "posts/2016-12-20-bootstrapping/index.html",
    "title": "A few examples of bootstrapping",
    "section": "",
    "text": "This post illustrates a statistical technique that becomes particularly useful when you want to calculate the sampling variation of some custom statistic when you start to dabble in mixed-effects models. This technique is called bootstrapping and I will first illustrate its use in constructing confidence intervals around a custom summary statistic. Then I’ll illustrate three bootstrapping approaches when constructing confidence intervals around a regression coefficient, and finally, I will show how bootstrapping can be used to compute p-values.\nThe goal of this post is not to argue that bootstrapping is superior to the traditional alternatives—in the examples discussed, they are pretty much on par—but merely to illustrate how it works. The main advantage of bootstrapping, as I understand it, is that it can be applied in situation where the traditional alternatives are not available, where you don’t understand how to use them or where their assumptions are questionable, but I think it’s instructive to see how its results compare to those of traditional approaches where both can readily be applied.\nThose interested can download the R code for this post."
  },
  {
    "objectID": "posts/2016-12-20-bootstrapping/index.html#a-confidence-interval-around-an-asymmetrically-trimmed-mean",
    "href": "posts/2016-12-20-bootstrapping/index.html#a-confidence-interval-around-an-asymmetrically-trimmed-mean",
    "title": "A few examples of bootstrapping",
    "section": "A confidence interval around an asymmetrically trimmed mean",
    "text": "A confidence interval around an asymmetrically trimmed mean\nThe histogram below shows how well 159 participants performed on a L1 German vocabulary test (WST). While the distribution is clearly left-skewed, we still want to characterise its central tendency. Let’s say, for the sake of this example, that we decide to calculate the sample mean, but we want to disregard the lowest 10% of scores. For this sample, this yields a value of 32.7.\n\n\n\n\n\nThe distribution of 159 WST (German vocabulary test) scores. Disregarding the bottom 10% of this sample (16 values), its mean is 32.7.\n\n\n\n\nEasy enough, but let’s say we want to express the sampling variation of this number (32.7) using an 80% confidence interval? Constructing confidence intervals around regular means can be done using the appropriate t-distribution, but we may not feel comfortable applying this approach to a mean based on the top 90% of the data only or we may not be sure how many degrees of freedom the t-distribution should have (158 or 142?). (In practice, this won’t make much of a difference, but verifying this would be reassuring.)\nOne possible solution is to proceed as follows. We take the sample that we actually observed (the green histogram below) and use it to create a new sample with the same number of observations (in this case: 159). We do this by taking a random observation from the original dataset, jotting down its value, putting it back in the dataset, sampling again etc., until we have the number of observations required (resampling with replacement). This new sample will be fairly similar in shape to our original sample, but won’t be identical to it (the purple histogram below): Some values will occur more often than in the original sample, others less often, and some may not occur at all in the new sample. We then compute the statistic of interest for the new sample (here: the mean after disregarding the lowest 10% of observations; the dashed red line) and jot down its value. We can do this, say, 20,000 times to see to what extent the trimmed means vary in the new, randomly created samples (blue histogram below).\nUpdate (2023-08-08): I reran the code with 20,000 bootstrap runs; in the original blog post, I only used 1,000 bootstrap runs.\n\n\n\n\n\nTop row: The observed distribution of WST scores; the dashed red line indicates the asymmetrically trimmed mean (disregarding the bottom 10% of the data). Middle row: Three new samples obtained by resampling with replacement from the observed WST distribution and their asymmetrically trimmed means. Bottom row: The distribution of asymmetrically trimmed means in 20,000 such new samples.\n\n\n\n\nTo construct a 80% confidence interval around the asymmetrically trimmed mean of 32.7, we can sort the values in the blue histogram from low to high and look up the values that demarcate the middle 80% (roughly 31.9 and 33.6).\nThe technique used above—generating new samples based on the observed data to assess how much a statistic varies in similar but different samples—is known as bootstrapping. It is particularly useful when you want to gauge the variability of a sample statistic\n\nwhen you don’t have an analytic means of doing so (e.g., based on a theoretical distribution such as the t-distribution for sample means)\n\nor if you fear that the assumptions behind the analytic approach may be violated to such an extent that it may not provide good enough approximations.\n\nWhile I’ve described how bootstrapping works, it’s less easy to see why it works. To assess the sampling variation of a given sample statistic, we would ideally have access to the population from which the sample at hand was drawn. Needless to say, we rarely have this luxury, and the sample at hand typically represents our best guess of how the population might look like, so we use our sample as a stand-in for the population.\nPerhaps you feel that it requires too big a leap of faith to use the sample we have as a stand-in for the population. After all, by chance, the sample we have may poorly reflect the population from which it was drawn. I think this is a reasonable concern, but rather than to alleviate it, I will just point out that analytical approaches operate on similar assumptions: t-values, for instance, use the sample standard deviations as stand-ins for the population standard deviations, and their use in smaller samples is predicated on the fiction that the samples were drawn from normal distributions. Assumptions, then, are needed to get a handle on inferential problems, whether you choose to solve them analytically, using bootstrapping or by another means."
  },
  {
    "objectID": "posts/2016-12-20-bootstrapping/index.html#a-confidence-interval-for-a-regression-coefficient-non-parametric-parametric-and-semi-parametric-bootstrapping",
    "href": "posts/2016-12-20-bootstrapping/index.html#a-confidence-interval-for-a-regression-coefficient-non-parametric-parametric-and-semi-parametric-bootstrapping",
    "title": "A few examples of bootstrapping",
    "section": "A confidence interval for a regression coefficient: Non-parametric, parametric and semi-parametric bootstrapping",
    "text": "A confidence interval for a regression coefficient: Non-parametric, parametric and semi-parametric bootstrapping\nThe procedure outlined in the example above is only one kind of bootstrapping. While it’s usually just called ‘bootstrapping’, it is sometimes called non-parametric bootstrapping to distinguish it from two other flavours of bootstrapping: parametric bootstrapping and semi-parametric bootstrapping. In this section, I will briefly explain the difference between these three techniques using a regression example.\nWe have a dataset with 159 observations of four variables, whose intercorrelations are outlined below. The outcome variable is TotCorrect; the other three variables serve as predictors.\n\n\n\n\n\nAfter standardising the three predictors, we fit a multiple regression model on these data, which yields the following estimated coefficients:\n\n\n                Estimate\n(Intercept)        16.50\nz.WST               1.15\nz.Raven             1.89\nz.English.Cloze     1.60\n\n\nTo express the variabilitity of z.WST’s coefficient, we want to construct a 90% confidence interval around it. We could use the regression model’s output to construct a 90% confidence interval using the appropriate t-distribution, but for the sake of the argument, let’s pretend that option is off the table.\n\nNon-parametric bootstrap\nFor the non-parametric bootstrap, we do essentially the same as in the previous example, but rather than computing the trimmed means for 20,000 new samples, we fit a regression model on them and jot down the estimated coefficient for z.WST.\n\nResample 159 rows with replacement from the original dataset.\nFit the multiple regression model on the new dataset.\nJot down the estimated coefficient for z.WST.\nPerform steps 1–3 20,000 times.\n\n(The results are shown further down.)\n\n\nParametric bootstrap\nThe regression model above is really just an equation:\n\\[\n\\textrm{TotCorrect}_i = 16.50 + 1.15\\times \\textrm{z.WST}_i + 1.89\\times\\textrm{z.Raven}_i+1.60\\times\\textrm{z.English.Cloze}_i + \\textrm{random error}_i,\n\\]\nwhere the random errors are assumed to be normally distributed around 0 with a standard deviation that is estimated from the data.\nInstead of randomly resampling cases from the original dataset, we could use this regression equation to generate new outcome values. Due to the random error term, new simulated samples will differ from each other. We can then refit the regression model on the new values to see how much the estimated coefficients vary.\n\nSimulate 159 new outcome values on the basis of the regression model (prediction + random error from normal distribution) and the original predictors.\nFit the multiple regression model on the new outcome values using the old predictors.\nJot down the estimated coefficient for z.WST.\nPerform steps 1–3 20,000 times.\n\nThis approach is called parametric bootstrapping because it relies on the original model’s estimated regression coefficients (parameters), its error term estimate as well as the assumption that the errors are drawn from a normal distribution.\n\n\nSemi-parametric bootstrap\nAs its name suggests, the semi-parametric bootstrap is a compromise between the non-parametric and the parametric bootstrap. The new outcome values are partly based on the original regression model, but instead of drawing the errors randomly from a normal distribution, the original model’s errors are resampled with replacement and added to the model’s predictions.\n\nExtract the residuals (errors) from the original regression model.\nPredict 159 new outcome values on the basis of the regression model (prediction only) and the original predictors.\nResample 159 values with replacement from the errors extracted in step 1 and add these to the predictions from step 2.\nFit the multiple regression model on the new outcome values (prediction + resampled errors) using the old predictors.\nJot down the estimated coefficient for z.WST.\nPerform steps 3–5 20,000 times.\n\nThis approach is called semi-parametric bootstrapping because, while it relies on the original model’s estimated regression coefficients, the error distribution is assumed to be normal but is instead approximated by sampling from the original model’s errors.\n\n\nResults\nThe histograms below show the sampling variability of the estimated z.WST coefficient according to each method.\n\n\n\n\n\nTo construct 90% confidence intervals, we pick the 5th and 95th percentiles of the bootstrapped estimates. The resulting intervals are plotted below alongside the 90% confidence interval constructed from the model summary. In this case, the confidence interval obtained by nonparametric bootstrapping is a bit wider."
  },
  {
    "objectID": "posts/2016-12-20-bootstrapping/index.html#testing-null-hypotheses-using-bootstrapping",
    "href": "posts/2016-12-20-bootstrapping/index.html#testing-null-hypotheses-using-bootstrapping",
    "title": "A few examples of bootstrapping",
    "section": "Testing null hypotheses using bootstrapping",
    "text": "Testing null hypotheses using bootstrapping\nAnother use for parametric and semi-parametric bootstrapping is to compute p-values if they can’t be computed analytically or if the assumptions of the analytical approach aren’t adequately met. The basic logic is the same in either case: You generate a large number of datasets in which you know the null hypothesis (e.g., ‘no difference’ or ‘the coefficient is 0’ or perhaps even ‘the difference/coefficient is 0.3’) to be true. You then compute whichever statistic you’re interested in for all of these new datasets and you calculate the proportion of cases where the statistic in the new datasets exceeds the statistic in your original sample. This proportion is a p-value as it expresses the (approximate) probability of observing a sample statistic of at least this magnitude if the null hypothesis is actually true.\n(Example for more advanced users: Likelihood ratio tests used for comparing nested mixed effects models are based on the assumption that likelihood ratios follow a \\(\\chi^2\\) distribution if the null hypothesis is true. This is a good enough assumption for large samples, but when you’re fitting mixed effects models, it isn’t always clear exactly what a large sample is. When in doubt, you can resort to bootstrapping in such cases: You generate new datasets in which the null hypothesis is true and compute likelihood ratios for them. These likelihood ratios then serve as distribution of likelihood ratios under the null hypothesis against which you can then compare the likelihood ratio computed for your original dataset.)\nFor illustrative purposes, we’re going to compute the p-value for the estimated z.WST coefficient from the previous example using parametric and semi-parametric bootstrapping, although there are no reasons to suspect that the t-distribution-based p-value in the model output (0.012) is completely off. As such, we’d expect the three methods to yield similar results. For both the parametric and semi-parametric bootstraps, we need to generate datasets in which we know the null hypothesis to be true (i.e., the coefficient for z.WST = 0). This we do by first fitting a null model to the data, that is, a model that is similar to the full model we want to fit but without the z.WST term we want to compute the p-value for. Then we generate new datasets on the basis of this null model. This guarantees that the null hypothesis is actually true in all newly generated samples, so that the distribution of the estimated z.WST coefficients for the newly generated samples can serve as an approximation of the estimated z.WST coefficient’s distribution under the null hypothesis.\nThe steps for computing p-values using a parametric and semi-parametric bootstrap are pretty similar; the only real difference is whether we’re more comfortable assuming that the residuals are drawn from a normal distribution or that their distribution is similar to that of the null model’s residuals.\n\nParametric bootstrapping\n\nFit a null model, i.e., a model without whichever term you want to compute the p-value for (here: z.WST).\nSimulate 159 new outcome values on the basis of the null model (prediction + random error from normal distribution) and the predictors in the null model.\nFit the full multiple regression model, i.e., with the term we want to compute the p-value for, on the new outcome values using the old predictors.\nJot down the esetimated coefficient for z.WST.\nPerform steps 2–4 20,000 times.\nSee how often the estimated z.WST coefficients in the 20,000 samples generated under the null hypothesis are larger in magnitude than the estimated z.WST coefficient in the original sample. This is the p-value.\n\n(Results below.)\n\n\nSemi-parametric bootstrapping\n\nFit a null model, i.e., a model without whichever term you want to compute the p-value for (here: z.WST).\nExtract the null model’s residuals (errors).\nPredict 159 new outcome values on the basis of the null model (prediction only) and the predictors in the null model.\nResample 159 values with replacement from the errors extracted in step 2 and add these to the predictions from step 3.\nFit the full multiple regression model, i.e., with the term we want to compute the p-value for, on the new outcome values (prediction + resampled error) using the old predictors.\nJot down the estimated coefficient for z.WST.\nPerform steps 4–6 20,000 times.\nSee how often the estimated z.WST coefficients in the 20,000 samples generated under the null hypothesis are larger in magnitude than the estimated z.WST coefficient in the original sample. This is the p-value.\n\n\n\nResults\nThe distribution of the z.WST looks pretty similar whether we use parametric or semi-parametric bootstrapping.\n\n\n\n\n\nThe p-value obtained by parametric bootstrapping is 0.0146 (i.e., 291 out of 20,000 estimated z.WST coefficients have absolute values larger than 1.15), the one obtained by semi-parametric bootstrapping is 0.0122, whereas the t-distribution-based p-value was 0.012. As expected, then, there is hardly any difference between them. The bootstrapped p-values are themselves subject to random variation, and different runs of the bootstrap will produce slightly different results. We could increase the number of bootstrap runs for greater accuracy, but I don’t think this is really necessary here."
  },
  {
    "objectID": "posts/2016-12-20-bootstrapping/index.html#concluding-remarks",
    "href": "posts/2016-12-20-bootstrapping/index.html#concluding-remarks",
    "title": "A few examples of bootstrapping",
    "section": "Concluding remarks",
    "text": "Concluding remarks\nMy goal was to illustrate how bootstrapping works rather than to discuss at length when to use which approach. Indeed, in the regression example above, the results of the bootstraps hardly differ from the t-distribution-based results, so the choice of method doesn’t really matter much.\nThat said, bootstrapping is a technique that’s useful to at least know about as it comes in handy if you want to estimate the sampling variation of a summary statistic in some funky distribution or if you want to verify, say, a likelihood ratio test for nested mixed-effects models."
  },
  {
    "objectID": "posts/2016-12-20-bootstrapping/index.html#software-versions",
    "href": "posts/2016-12-20-bootstrapping/index.html#software-versions",
    "title": "A few examples of bootstrapping",
    "section": "Software versions",
    "text": "Software versions\n\ndevtools::session_info()\n\n─ Session info ───────────────────────────────────────────────────────────────\n setting  value\n version  R version 4.3.1 (2023-06-16 ucrt)\n os       Windows 10 x64 (build 18363)\n system   x86_64, mingw32\n ui       RTerm\n language (EN)\n collate  English_United Kingdom.utf8\n ctype    English_United Kingdom.utf8\n tz       Europe/Zurich\n date     2023-08-08\n pandoc   3.1.1 @ C:/Program Files/RStudio/resources/app/bin/quarto/bin/tools/ (via rmarkdown)\n\n─ Packages ───────────────────────────────────────────────────────────────────\n package      * version date (UTC) lib source\n cachem         1.0.8   2023-05-01 [1] CRAN (R 4.3.1)\n callr          3.7.3   2022-11-02 [1] CRAN (R 4.3.1)\n cli            3.6.1   2023-03-23 [1] CRAN (R 4.3.1)\n colorspace     2.1-0   2023-01-23 [1] CRAN (R 4.3.1)\n crayon         1.5.2   2022-09-29 [1] CRAN (R 4.3.1)\n devtools       2.4.5   2022-10-11 [1] CRAN (R 4.3.1)\n digest         0.6.33  2023-07-07 [1] CRAN (R 4.3.1)\n dplyr        * 1.1.2   2023-04-20 [1] CRAN (R 4.3.1)\n ellipsis       0.3.2   2021-04-29 [1] CRAN (R 4.3.1)\n evaluate       0.21    2023-05-05 [1] CRAN (R 4.3.1)\n fansi          1.0.4   2023-01-22 [1] CRAN (R 4.3.1)\n farver         2.1.1   2022-07-06 [1] CRAN (R 4.3.1)\n fastmap        1.1.1   2023-02-24 [1] CRAN (R 4.3.1)\n forcats      * 1.0.0   2023-01-29 [1] CRAN (R 4.3.1)\n fs             1.6.3   2023-07-20 [1] CRAN (R 4.3.1)\n generics       0.1.3   2022-07-05 [1] CRAN (R 4.3.1)\n ggplot2      * 3.4.2   2023-04-03 [1] CRAN (R 4.3.1)\n glue           1.6.2   2022-02-24 [1] CRAN (R 4.3.1)\n gridExtra      2.3     2017-09-09 [1] CRAN (R 4.3.1)\n gtable         0.3.3   2023-03-21 [1] CRAN (R 4.3.1)\n hms            1.1.3   2023-03-21 [1] CRAN (R 4.3.1)\n htmltools      0.5.5   2023-03-23 [1] CRAN (R 4.3.1)\n htmlwidgets    1.6.2   2023-03-17 [1] CRAN (R 4.3.1)\n httpuv         1.6.11  2023-05-11 [1] CRAN (R 4.3.1)\n jsonlite       1.8.7   2023-06-29 [1] CRAN (R 4.3.1)\n knitr          1.43    2023-05-25 [1] CRAN (R 4.3.1)\n labeling       0.4.2   2020-10-20 [1] CRAN (R 4.3.0)\n later          1.3.1   2023-05-02 [1] CRAN (R 4.3.1)\n lifecycle      1.0.3   2022-10-07 [1] CRAN (R 4.3.1)\n lubridate    * 1.9.2   2023-02-10 [1] CRAN (R 4.3.1)\n magrittr     * 2.0.3   2022-03-30 [1] CRAN (R 4.3.1)\n memoise        2.0.1   2021-11-26 [1] CRAN (R 4.3.1)\n mime           0.12    2021-09-28 [1] CRAN (R 4.3.0)\n miniUI         0.1.1.1 2018-05-18 [1] CRAN (R 4.3.1)\n munsell        0.5.0   2018-06-12 [1] CRAN (R 4.3.1)\n pillar         1.9.0   2023-03-22 [1] CRAN (R 4.3.1)\n pkgbuild       1.4.2   2023-06-26 [1] CRAN (R 4.3.1)\n pkgconfig      2.0.3   2019-09-22 [1] CRAN (R 4.3.1)\n pkgload        1.3.2.1 2023-07-08 [1] CRAN (R 4.3.1)\n prettyunits    1.1.1   2020-01-24 [1] CRAN (R 4.3.1)\n processx       3.8.2   2023-06-30 [1] CRAN (R 4.3.1)\n profvis        0.3.8   2023-05-02 [1] CRAN (R 4.3.1)\n promises       1.2.0.1 2021-02-11 [1] CRAN (R 4.3.1)\n ps             1.7.5   2023-04-18 [1] CRAN (R 4.3.1)\n purrr        * 1.0.1   2023-01-10 [1] CRAN (R 4.3.1)\n R6             2.5.1   2021-08-19 [1] CRAN (R 4.3.1)\n RColorBrewer * 1.1-3   2022-04-03 [1] CRAN (R 4.3.0)\n Rcpp           1.0.11  2023-07-06 [1] CRAN (R 4.3.1)\n readr        * 2.1.4   2023-02-10 [1] CRAN (R 4.3.1)\n remotes        2.4.2.1 2023-07-18 [1] CRAN (R 4.3.1)\n rlang          1.1.1   2023-04-28 [1] CRAN (R 4.3.1)\n rmarkdown      2.23    2023-07-01 [1] CRAN (R 4.3.1)\n rstudioapi     0.15.0  2023-07-07 [1] CRAN (R 4.3.1)\n scales         1.2.1   2022-08-20 [1] CRAN (R 4.3.1)\n sessioninfo    1.2.2   2021-12-06 [1] CRAN (R 4.3.1)\n shiny          1.7.4.1 2023-07-06 [1] CRAN (R 4.3.1)\n stringi        1.7.12  2023-01-11 [1] CRAN (R 4.3.0)\n stringr      * 1.5.0   2022-12-02 [1] CRAN (R 4.3.1)\n tibble       * 3.2.1   2023-03-20 [1] CRAN (R 4.3.1)\n tidyr        * 1.3.0   2023-01-24 [1] CRAN (R 4.3.1)\n tidyselect     1.2.0   2022-10-10 [1] CRAN (R 4.3.1)\n tidyverse    * 2.0.0   2023-02-22 [1] CRAN (R 4.3.1)\n timechange     0.2.0   2023-01-11 [1] CRAN (R 4.3.1)\n tzdb           0.4.0   2023-05-12 [1] CRAN (R 4.3.1)\n urlchecker     1.0.1   2021-11-30 [1] CRAN (R 4.3.1)\n usethis        2.2.2   2023-07-06 [1] CRAN (R 4.3.1)\n utf8           1.2.3   2023-01-31 [1] CRAN (R 4.3.1)\n vctrs          0.6.3   2023-06-14 [1] CRAN (R 4.3.1)\n withr          2.5.0   2022-03-03 [1] CRAN (R 4.3.1)\n xfun           0.39    2023-04-20 [1] CRAN (R 4.3.1)\n xtable         1.8-4   2019-04-21 [1] CRAN (R 4.3.1)\n yaml           2.3.7   2023-01-23 [1] CRAN (R 4.3.0)\n\n [1] C:/Users/VanhoveJ/AppData/Local/R/win-library/4.3\n [2] C:/Program Files/R/R-4.3.1/library\n\n──────────────────────────────────────────────────────────────────────────────"
  },
  {
    "objectID": "posts/2015-01-07-some-alternatives-to-barplots/index.html",
    "href": "posts/2015-01-07-some-alternatives-to-barplots/index.html",
    "title": "Some alternatives to bar plots",
    "section": "",
    "text": "Bar plots. They often deliver the main result of empirical studies, be it at conferences or in journal articles, by proudly showing that the mean score, reaction time etc. in one group is a notch higher than in the other. But the information that bar plots convey is limited and sometimes downright misleading. In this post, I suggest some alternatives you can use in your next presentation or paper."
  },
  {
    "objectID": "posts/2015-01-07-some-alternatives-to-barplots/index.html#whats-wrong-with-bar-plots",
    "href": "posts/2015-01-07-some-alternatives-to-barplots/index.html#whats-wrong-with-bar-plots",
    "title": "Some alternatives to bar plots",
    "section": "What’s wrong with bar plots?",
    "text": "What’s wrong with bar plots?\nJust to be clear: the bar plots I’m arguing against in this post are those that show the average result for each group, possibly complemented with confidence bars, but nothing more. Below are three examples of such bar plots. They summarise the results of one part of a learning experiment in which participants were assigned to one of two learning conditions (‘ij’ or ‘oe’, but it doesn’t really matter for this post) and then tried to translate a number of critical words in an unknown but related language. The question of interest is whether participants in one learning condition outperform participants in the other.\nThe bar plot on the left shows the mean percentage of correctly translated critical words per participant for each learning condition. The plot in the middle additionally shows the upper limits of the means’ 95% confidence intervals; this type of plot is often referred to as a dynamite plot. The one on the right, finally, shows both the upper and lower limits of the 95% CIs.\nUpdate (2023-08-26): I didn’t include the R code in the original blog post but instead linked to a separate document. I’ve now slightly modified the code so that it runs without hiccoughs with the present software versions and I now just show the code in the blog post. The first code snippet isn’t too important, as I will suggest that you don’t draw such plots.\n\n# Load packages\nlibrary(tidyverse) # for working with datasets and plotting\nlibrary(gridExtra) # for creating side-by-side plots\n\n# Read in data\ndat &lt;- read_csv(\"http://janhove.github.io/datasets/correspondencerules.csv\")\n\n# Rename learning condition levels\ndat &lt;- dat |&gt; \n  mutate(LearningCondition = ifelse(LearningCondition == \"ij-ei\", \"&lt;ij&gt;\", \"&lt;oe&gt;\"))\n\n# Divide data into training and test part\n# only retaing critical items on non-training part\ndat_post_critical &lt;- dat |&gt; \n  filter(Block != \"Training\") |&gt; \n  filter(Category %in% c(\"oe cognate\", \"ij cognate\"))\n\n# Figure 1\n# Step 1: summarise percentages per category per subject in a slightly different way\nperSubjectCorrect &lt;- dat_post_critical |&gt; \n  group_by(Subject, Category, LearningCondition) |&gt; \n  summarise(\n    Correct = 100 * mean(Correct == \"yes\"),\n    CorrectVowel = 100 * mean(CorrectVowel == \"yes\"),\n    .groups = \"drop\"\n  ) |&gt; \n  mutate(Category = ifelse(Category == \"ij cognate\",\n                           \"&lt;ij&gt; cognates\",\n                           \"&lt;oe&gt; cognates\"))\n# Step 2: draw plot\ngp &lt;- perSubjectCorrect |&gt; \n  filter(Category == \"&lt;oe&gt; cognates\") |&gt; \n  ggplot(aes(x = LearningCondition, y = Correct)) +\n  guides(fill = \"none\", colour = \"none\") +\n  theme(axis.text.x = element_text(size = 12),\n        axis.title.x = element_text(size = 13),\n        axis.text.y = element_text(size = 12),\n        axis.title.y = element_text(size = 13),\n        strip.text.x = element_text(size = 16)) +\n  ylim(0, 100)\n\ngp1 &lt;- gp + \n  stat_summary(fun = mean, geom = \"bar\", position = \"dodge\", \n               mapping = aes(fill = LearningCondition), \n               color = \"black\", linewidth = 1) +\n  guides(fill = \"none\", colour = \"none\") +\n  xlab(\"Learning condition\") + \n  ylab(\"Percentage correct translations\")\n\n# Compute confidence bounds\nlowCI &lt;- function(x) {t.test(x)$conf.int[1]}\nhiCI &lt;- function(x) {t.test(x)$conf.int[2]}\n\ngp2 &lt;- gp + \n  stat_summary(fun = mean, geom = \"bar\", position = \"dodge\", \n               mapping = aes(fill = LearningCondition), \n               color = \"black\", linewidth = 0.8) +\n  stat_summary(fun = mean, fun.min = lowCI, fun.max = hiCI, \n               geom = \"errorbar\", position = \"dodge\", \n               colour = \"black\", width = 0.3) +\n  xlab(\"Learning condition\") + \n  ylab(\"Percentage correct translations\")\n\ngp3 &lt;- gp + \n  stat_summary(fun = mean, geom = \"bar\", position = \"dodge\", \n               mapping = aes(fill = LearningCondition), \n               color = \"black\", linewidth = 0.8) +\n  stat_summary(fun = mean, fun.min = mean, fun.max = hiCI, \n               geom = \"errorbar\", position = \"dodge\", \n               colour = \"black\", width = 0.3) +\n  xlab(\"Learning condition\") + \n  ylab(\"Percentage correct translations\")\n\ngrid.arrange(gp1, gp3, gp2, ncol = 3)\n\n\n\n\nWhile the tidiness of these three plots is appealing (you run your study, calculate the means and CIs, et voilà), the information they convey is limited to the group means and a measure of uncertainty about these means. The bar plots don’t disclose several snippets of crucial information: How many participants were there in each group? Roughly how large is the overlap between the two groups? And does the mean accurately convey the central tendency of the data?\nAfter all, mean percentages of roughly 30 and 40 can correspond to any of a well-nigh unlimited number of wildly different patterns in the data: in principle, the bar plot on the left could correspond to a situation in which all participants have translated either 30 or 40% of the items correctly, to a situation in which 30 or 40% of the participants translated all items correctly and 70 or 60% none of them, and to pretty much anything in-between. Even the variants with the error bars can correspond to pretty much anything from normally distributed data centred around 30 and 40%, over distributions with a couple of outliers to multimodal patterns (e.g. many participants with no correct translations, several participants around 50% and a handful with perfect scores). The dynamite plot is particular is a perennial target of criticism for a host of additional reasons (Update (2023-08-26): Link broken.). (Fittingly, it’s pretty difficult to get the ggplot2 graphical package for R to draw dynamite plots.)"
  },
  {
    "objectID": "posts/2015-01-07-some-alternatives-to-barplots/index.html#plot-the-raw-data",
    "href": "posts/2015-01-07-some-alternatives-to-barplots/index.html#plot-the-raw-data",
    "title": "Some alternatives to bar plots",
    "section": "Plot the raw data",
    "text": "Plot the raw data\nThe bar plot’s drawbacks can be avoided by adhering to one straightforward principle: show the raw data. You spent quite a bit of effort to collect it, so why not show it? If your data set is fairly small, you could just plot the raw values, as in the plot on the left; here, the data points are jittered horizontally to reduce the amount of overlap.\nAn alternative that I quite like is shown on the right. First, I draw boxplots (shown in the middle panel) for both groups and then I plot the individual data points on top of them. (If you do this, make sure you don’t plot outliers twice!) At a single glance, readers get an accurate sense of where the individual data points lie as well as where the quartiles of the data are situated.\n\ngp4 &lt;- gp + \n  geom_point(position = position_jitter(h = 0, w = 0.2), \n             size = 1.5, \n             aes(color = LearningCondition),\n             shape = 1)  +\n  xlab(\"Learning condition\") + \n  ylab(\"Percentage correct translations\")\n\ngp5 &lt;- gp + \n  geom_boxplot(aes(color = LearningCondition))  +\n  xlab(\"Learning condition\") + \n  ylab(\"Percentage correct translations\")\n\ngp6 &lt;- gp + \n  geom_boxplot(outlier.shape = NA, # don't plot outliers twice\n               aes(color = LearningCondition)) +\n  geom_point(position = position_jitter(h = 0, w = 0.2), \n             size = 1.5, aes(color = LearningCondition),\n             shape = 1)  +\n  xlab(\"Learning condition\") + \n  ylab(\"Percentage correct translations\")\n\ngrid.arrange(gp4, gp5, gp6, ncol = 3)\n\n\n\n\nThe reason why I prefer to plot the individual data points rather than the naked box plots that more readers will be familiar with is that, much like bar plots, box plots can be deceptive. As an example, consider the box plots below (data courtesy of my colleague Ladina). (Update (2023-08-26): See the published article (Stocker 2017) for background.) With median scores of about 45-50, first and third quartiles more or less symmetrically around the median, dito hinges and no outlying points, these box plots evoke symmetric, unimodal and well-behaved (if not quite normal) distributions. On the basis of this plot, we’d be inclined to move on to t-tests and the like for our inferences.\n\nladina &lt;- read_csv(\"http://janhove.github.io/datasets/ladina.csv\")\n\n# Create SubjectID\n# Only consider French survey and statements of unknown truthfulness\nladina &lt;- ladina |&gt; \n  mutate(SubjectID = paste0(L1, ID),\n         Survey = ifelse(L1 == \"French\", \"French survey\", \"German survey\")) |&gt; \n  filter(truthfulness == \"notknown\") |&gt; \n  filter(Survey == \"French survey\") |&gt; \n  filter(accent %in% c(\"French\", \"German\"))\n\nlad1 &lt;- ggplot(ladina,\n             aes(x = accent,\n                 y = score,\n                 colour = accent)) +\n  guides(fill = \"none\", colour = \"none\") + \n  geom_boxplot() +\n  xlab(\"Condition\") + \n  ylab(\"Score\")\n\nlad1\n\n\n\n\nHowever, the idea that these data are symmetrically and unimodally distributed is utterly wrong. In fact, the box plots fail to convey the most striking aspect of these data, viz. that scores around 50% are entirely atypical! As the histograms shown below reveal, scores of 0 and 100 occur far more often than anything in-between, so that both the mean and the median are poor summaries of the patterns in the data.\n\nlad2 &lt;- ggplot(ladina,\n               aes(score)) +\n  geom_histogram(aes(fill = accent),\n                 bins = 101) +\n  facet_wrap(~ accent, ncol = 2) + \n  theme_bw() +\n  xlab(\"Score\") +\n  ylab(\"Frequency\") +\n  guides(fill = FALSE)\nlad2\n\n\n\n\nWith nearly 3,000 observations, this data set is rather large, and plotting individual dots for each data point is probably inadvisable. However, we don’t need to restrict ourselves to computing summary data and plotting those: here are some alternatives that stay true(r) to the original data. The top left plot shows violin plots (which show the kernel density of the data) as well as the raw data as transparent dots. This plot requires a bit of fiddling (edit: This wasn’t intended as a pun, but it’s so bad I’ll keep it.) to underscore the heaps of observations at 0 and 100.\nThe top right consists of frequency polygons. These are essentially histograms in which the tops of the bars are connected and with the bars themselves removed. What I like about them in this case is that they convey the striking patterns in the data (the spikes at 0 and 100) and allow for a much better direct comparison of the two distributions than side-by-side histograms would.\nOn the bottom row, the empirical cumulative density functions of both conditions are directly compared. The left plot shows the default way of plotting ECDFs. The spikes at 0 and 100 in the histogram show themselves as steep (in fact: vertical) increases in the graphs. From this graph, you can also glean that a score of 20 corresponds to roughly the 25th quantile of the data in Condition 2 (turquoise, or as we say in Flanders: ‘apple-blue-sea-green’), i.e. 25% of the observations in Condition 2 are lower than 20. The red line is a bit higher at this point, indicating that about 29% of the observations in Condition 1 are lower than 20. Note that this means that the scores in Condition 1 tend to be lower than those in Condition 2.\nAs I find it counterintuitive that lower scores correspond to higher lines, the right plot shows the same ECDFs, but with the x- and y-axes flipped. From this graph, you can easily glean that the median (cumulative density of 0.50) is higher in Condition 2, and that this is in fact pretty much the case for all quantiles: the red line lies consistently under the apple-blue-see-green one. The spikes in the data now show themselves as flat (vertical) parts of the curves.\n\nlad3 &lt;- ggplot(ladina,\n             aes(x = accent,\n                 y = score,\n                 colour = accent)) +\n  guides(fill = \"none\", colour = \"none\") +\n  geom_violin(adjust = 0.3, trim = FALSE) +\n  geom_point(position = position_jitter(h = 0, w = 0.2), \n             size = 1.5, aes(color = accent), alpha = 0.2) +\n  xlab(\"Condition\") + \n  ylab(\"Score\")\n\nlad4 &lt;- ggplot(ladina,\n            aes(score, \n                after_stat(density), \n                colour = accent)) +\n  geom_freqpoly(binwidth = 5, origin = -2.5) +\n  xlab(\"Score\") +\n  ylab(\"Density\") +\n  guides(fill = \"none\", colour = \"none\")\n\nlad5 &lt;- ggplot(ladina,\n             aes(x = score, colour = accent)) +\n  guides(fill = \"none\", colour = \"none\") +\n  xlab(\"Score\") + ylab(\"Cumulative density\") +\n  stat_ecdf()\n\nlad6 &lt;- ggplot(ladina,\n             aes(x = score, colour = accent)) +\n  guides(fill = FALSE, colour = FALSE) +\n  xlab(\"Score\") + ylab(\"Cumulative density\") +\n  stat_ecdf() +\n  coord_flip()\n\ngrid.arrange(lad3, lad4, lad5, lad6, ncol = 2)\n\n\n\n\nThese graphs probably aren’t optimal, but they illustrate that it isn’t necessary to stick to bar and box plots to visually report results."
  },
  {
    "objectID": "posts/2015-01-07-some-alternatives-to-barplots/index.html#wrap-up",
    "href": "posts/2015-01-07-some-alternatives-to-barplots/index.html#wrap-up",
    "title": "Some alternatives to bar plots",
    "section": "Wrap-up",
    "text": "Wrap-up\nWhile tidy and familiar, bar plots reveal little information about the data we’ve worked hard to collect, and even the trusted box plot can conceal their most striking aspects. I don’t think we should to stick to the default ways of graphically conveying our results. As a rule, plots that stay true to the original data – ideally by plotting the raw data points – are better plots."
  },
  {
    "objectID": "posts/2015-01-07-some-alternatives-to-barplots/index.html#software-versions",
    "href": "posts/2015-01-07-some-alternatives-to-barplots/index.html#software-versions",
    "title": "Some alternatives to bar plots",
    "section": "Software versions",
    "text": "Software versions\n\ndevtools::session_info()\n\n─ Session info ───────────────────────────────────────────────────────────────\n setting  value\n version  R version 4.3.1 (2023-06-16)\n os       Ubuntu 22.04.3 LTS\n system   x86_64, linux-gnu\n ui       X11\n language en_US\n collate  en_US.UTF-8\n ctype    en_US.UTF-8\n tz       Europe/Zurich\n date     2023-08-26\n pandoc   3.1.1 @ /usr/lib/rstudio/resources/app/bin/quarto/bin/tools/ (via rmarkdown)\n\n─ Packages ───────────────────────────────────────────────────────────────────\n package     * version date (UTC) lib source\n bit           4.0.5   2022-11-15 [1] CRAN (R 4.3.0)\n bit64         4.0.5   2020-08-30 [1] CRAN (R 4.3.0)\n cachem        1.0.6   2021-08-19 [2] CRAN (R 4.2.0)\n callr         3.7.3   2022-11-02 [1] CRAN (R 4.3.1)\n cli           3.6.1   2023-03-23 [1] CRAN (R 4.3.0)\n colorspace    2.1-0   2023-01-23 [1] CRAN (R 4.3.0)\n crayon        1.5.2   2022-09-29 [1] CRAN (R 4.3.1)\n curl          5.0.1   2023-06-07 [1] CRAN (R 4.3.1)\n devtools      2.4.5   2022-10-11 [1] CRAN (R 4.3.1)\n digest        0.6.29  2021-12-01 [2] CRAN (R 4.2.0)\n dplyr       * 1.1.2   2023-04-20 [1] CRAN (R 4.3.0)\n ellipsis      0.3.2   2021-04-29 [2] CRAN (R 4.2.0)\n evaluate      0.15    2022-02-18 [2] CRAN (R 4.2.0)\n fansi         1.0.4   2023-01-22 [1] CRAN (R 4.3.1)\n farver        2.1.1   2022-07-06 [1] CRAN (R 4.3.0)\n fastmap       1.1.0   2021-01-25 [2] CRAN (R 4.2.0)\n forcats     * 1.0.0   2023-01-29 [1] CRAN (R 4.3.0)\n fs            1.5.2   2021-12-08 [2] CRAN (R 4.2.0)\n generics      0.1.3   2022-07-05 [1] CRAN (R 4.3.0)\n ggplot2     * 3.4.2   2023-04-03 [1] CRAN (R 4.3.0)\n glue          1.6.2   2022-02-24 [2] CRAN (R 4.2.0)\n gridExtra   * 2.3     2017-09-09 [1] CRAN (R 4.3.0)\n gtable        0.3.3   2023-03-21 [1] CRAN (R 4.3.0)\n hms           1.1.3   2023-03-21 [1] CRAN (R 4.3.0)\n htmltools     0.5.5   2023-03-23 [1] CRAN (R 4.3.0)\n htmlwidgets   1.6.2   2023-03-17 [1] CRAN (R 4.3.1)\n httpuv        1.6.11  2023-05-11 [1] CRAN (R 4.3.1)\n jsonlite      1.8.7   2023-06-29 [1] CRAN (R 4.3.1)\n knitr         1.39    2022-04-26 [2] CRAN (R 4.2.0)\n labeling      0.4.2   2020-10-20 [1] CRAN (R 4.3.0)\n later         1.3.1   2023-05-02 [1] CRAN (R 4.3.1)\n lifecycle     1.0.3   2022-10-07 [1] CRAN (R 4.3.0)\n lubridate   * 1.9.2   2023-02-10 [1] CRAN (R 4.3.0)\n magrittr      2.0.3   2022-03-30 [1] CRAN (R 4.3.0)\n memoise       2.0.1   2021-11-26 [2] CRAN (R 4.2.0)\n mime          0.10    2021-02-13 [2] CRAN (R 4.0.2)\n miniUI        0.1.1.1 2018-05-18 [1] CRAN (R 4.3.1)\n munsell       0.5.0   2018-06-12 [1] CRAN (R 4.3.0)\n pillar        1.9.0   2023-03-22 [1] CRAN (R 4.3.0)\n pkgbuild      1.4.2   2023-06-26 [1] CRAN (R 4.3.1)\n pkgconfig     2.0.3   2019-09-22 [2] CRAN (R 4.2.0)\n pkgload       1.3.2.1 2023-07-08 [1] CRAN (R 4.3.1)\n prettyunits   1.1.1   2020-01-24 [2] CRAN (R 4.2.0)\n processx      3.8.2   2023-06-30 [1] CRAN (R 4.3.1)\n profvis       0.3.8   2023-05-02 [1] CRAN (R 4.3.1)\n promises      1.2.0.1 2021-02-11 [1] CRAN (R 4.3.1)\n ps            1.7.5   2023-04-18 [1] CRAN (R 4.3.1)\n purrr       * 1.0.1   2023-01-10 [1] CRAN (R 4.3.0)\n R6            2.5.1   2021-08-19 [2] CRAN (R 4.2.0)\n Rcpp          1.0.11  2023-07-06 [1] CRAN (R 4.3.1)\n readr       * 2.1.4   2023-02-10 [1] CRAN (R 4.3.0)\n remotes       2.4.2   2021-11-30 [2] CRAN (R 4.2.0)\n rlang         1.1.1   2023-04-28 [1] CRAN (R 4.3.0)\n rmarkdown     2.21    2023-03-26 [1] CRAN (R 4.3.0)\n rstudioapi    0.14    2022-08-22 [1] CRAN (R 4.3.0)\n scales        1.2.1   2022-08-20 [1] CRAN (R 4.3.0)\n sessioninfo   1.2.2   2021-12-06 [2] CRAN (R 4.2.0)\n shiny         1.7.4.1 2023-07-06 [1] CRAN (R 4.3.1)\n stringi       1.7.12  2023-01-11 [1] CRAN (R 4.3.1)\n stringr     * 1.5.0   2022-12-02 [1] CRAN (R 4.3.0)\n tibble      * 3.2.1   2023-03-20 [1] CRAN (R 4.3.0)\n tidyr       * 1.3.0   2023-01-24 [1] CRAN (R 4.3.0)\n tidyselect    1.2.0   2022-10-10 [1] CRAN (R 4.3.0)\n tidyverse   * 2.0.0   2023-02-22 [1] CRAN (R 4.3.1)\n timechange    0.2.0   2023-01-11 [1] CRAN (R 4.3.0)\n tzdb          0.4.0   2023-05-12 [1] CRAN (R 4.3.0)\n urlchecker    1.0.1   2021-11-30 [1] CRAN (R 4.3.1)\n usethis       2.2.2   2023-07-06 [1] CRAN (R 4.3.1)\n utf8          1.2.3   2023-01-31 [1] CRAN (R 4.3.1)\n vctrs         0.6.3   2023-06-14 [1] CRAN (R 4.3.0)\n vroom         1.6.3   2023-04-28 [1] CRAN (R 4.3.0)\n withr         2.5.0   2022-03-03 [2] CRAN (R 4.2.0)\n xfun          0.39    2023-04-20 [1] CRAN (R 4.3.0)\n xtable        1.8-4   2019-04-21 [1] CRAN (R 4.3.1)\n yaml          2.3.5   2022-02-21 [2] CRAN (R 4.2.0)\n\n [1] /home/jan/R/x86_64-pc-linux-gnu-library/4.3\n [2] /usr/local/lib/R/site-library\n [3] /usr/lib/R/site-library\n [4] /usr/lib/R/library\n\n──────────────────────────────────────────────────────────────────────────────"
  },
  {
    "objectID": "posts/2016-07-05-classification-algorithm/index.html",
    "href": "posts/2016-07-05-classification-algorithm/index.html",
    "title": "Classifying second-language learners as native- or non-nativelike: Don’t neglect classification error rates",
    "section": "",
    "text": "I’d promised to write another installment on drawing graphs, but instead I’m going to write about something that I had to exclude, for reasons of space, from a recently published book chapter on age effects in second language (L2) acquisition: classifying observations (e.g., L2 learners) and estimating error rates.\nI’m going to illustrate the usefulness of classification algorithms for addressing some problems in L2 acquisition research, but my broader aim is to show that there’s more to statistics than running significance tests and to encourage you to explore—even if superficially—what else is out there."
  },
  {
    "objectID": "posts/2016-07-05-classification-algorithm/index.html#background-classifying-l2-learners-as-native--or-non-nativelike",
    "href": "posts/2016-07-05-classification-algorithm/index.html#background-classifying-l2-learners-as-native--or-non-nativelike",
    "title": "Classifying second-language learners as native- or non-nativelike: Don’t neglect classification error rates",
    "section": "Background: classifying L2 learners as native- or non-nativelike",
    "text": "Background: classifying L2 learners as native- or non-nativelike\nIn the field of second language acquisition, there are a couple of theories that predict that L2 learners who begin learning the L2 after a certain age will never be ‘native-like’ in the L2. The ‘certain age’ differs between studies, and what the prediction boils down to in some versions is that no L2 learner will ever be fully ‘native-like’ in the L2.\nI, for one, don’t think that ‘nativelikeness’ is a useful scientific construct, but that doesn’t matter for this post: Some researchers obviously do consider it useful, and for them the question is how they can test their prediction.\nResearchers interested in nativelikeness usually administer a battery of linguistic tasks to a sample of L2 learners as well as to a ‘control’ sample of L1 speakers. On the basis of the L1 speakers’ results, they then define a nativelikeness criterion—an interval that is considered typical of L1 speakers’ performance. Common intervals are (a) the L1 speakers’ mean ± two standard deviations or (b) the range of the L1 speakers’ results. L2 speakers whose results fall outside this interval are considered non-nativelike, and the goal of the study is often to establish from which age of L2 acquisition onwards no nativelike L2 speakers can be found."
  },
  {
    "objectID": "posts/2016-07-05-classification-algorithm/index.html#the-problem-misclassifications",
    "href": "posts/2016-07-05-classification-algorithm/index.html#the-problem-misclassifications",
    "title": "Classifying second-language learners as native- or non-nativelike: Don’t neglect classification error rates",
    "section": "The problem: Misclassifications",
    "text": "The problem: Misclassifications\nThe procedure I’ve just sketched is pretty common but it’s statistically naïve. One problem with it is that it may misclassify non-nativelike speakers as nativelike. I think most researchers are aware of this problem, as it’s sometimes implied that fewer L2 learners would’ve qualified as nativelike if only more and more reliable data were available. This may well be true. But the other side of the coin is rarely considered: not all L1 speakers may pass the nativelikeness criterion either!\nTo my knowledge, no paper in L2 acquisition provides an error-rate estimate, i.e., a quantitative appraisal of how well the nativelikeness criterion would distinguish between L2 and L1 speakers other than those used for defining the criterion. Nonetheless, I think this is precisely what is needed if we are to sensibly interpret such studies. Let me illustrate."
  },
  {
    "objectID": "posts/2016-07-05-classification-algorithm/index.html#illustration",
    "href": "posts/2016-07-05-classification-algorithm/index.html#illustration",
    "title": "Classifying second-language learners as native- or non-nativelike: Don’t neglect classification error rates",
    "section": "Illustration",
    "text": "Illustration\nAbrahamsson and Hyltenstam subjected 41 advanced Spanish-speaking learners of L2 Swedish as well as 15 native speakers of Swedish to a battery of linguistic tasks. From these tasks, 14 variables were extracted; the details don’t matter much here, but you can look them up in the paper (see Table 6 on page 280). Abrahamsson and Hyltenstam defined the minimum criterion of nativelikeness as the lowest native-speaker result on each measure, but I’m going to define it as the range of native-speaker results (i.e., between lowest and highest; it doesn’t really matter much).\nThe original raw data aren’t available, but I’ve simulated some placeholder data to illustrate my point.\n(For the 15 native speakers, I simulated 14 variables from normal distributions with the same mean as in Abrahamsson & Hyltenstam’s Table 6; the standard deviation was estimated by taking the range and dividing it by 4. For the 41 non-native speakers, I simulated the same 14 variables but with generally lower means and larger standard deviations. None of the variables were systematically correlated. This simulation obviously represent a huge simplification; life would be easier if people put their data online.)\nUsing these simulated data, we can compute the range of the native-speaker results. Don’t be intimidated by the R code, the comments say what it accomplishes, which is really all you need to know.\nUpdate (2023-08-08): I’ve reran this code using newer software versions. I now also rely on the tidyverse suite of packages instead of on the individual packages that later went on to form the tidyverse suite.\n\n# Read in data\ndat &lt;- read.csv(\"http://homeweb.unifr.ch/VanhoveJ/Pub/Data/nativelikeness.csv\",\n                stringsAsFactors = TRUE)\nstr(dat)\n\n'data.frame':   56 obs. of  15 variables:\n $ Pred1 : int  18 15 17 13 22 21 15 20 18 20 ...\n $ Pred2 : int  12 17 14 16 16 12 19 13 22 14 ...\n $ Pred3 : int  18 11 16 15 20 14 20 18 18 14 ...\n $ Pred4 : int  -5 11 0 -3 2 -9 2 7 16 6 ...\n $ Pred5 : int  19 11 21 8 20 24 13 9 14 19 ...\n $ Pred6 : int  29 22 21 24 19 24 28 27 19 21 ...\n $ Pred7 : int  -10 -8 -5 -9 -8 -6 -8 -7 -4 -2 ...\n $ Pred8 : int  16 15 16 17 16 17 17 16 16 15 ...\n $ Pred9 : int  73 74 70 64 70 72 64 76 63 63 ...\n $ Pred10: int  71 70 74 67 73 65 73 66 72 66 ...\n $ Pred11: int  7852 6977 7246 7768 8106 8457 7680 8156 7672 8417 ...\n $ Pred12: int  38 34 35 33 36 35 36 37 33 36 ...\n $ Pred13: int  45 41 45 42 46 40 36 38 39 48 ...\n $ Pred14: int  41 40 36 36 40 41 41 44 38 36 ...\n $ Class : Factor w/ 2 levels \"L1 speaker\",\"L2 speaker\": 1 1 1 1 1 1 1 1 1 1 ...\n\n# Load packages\nlibrary(tidyverse)\n\n# Retain L1 speakers\ndat.L1 &lt;- dat |&gt;\n  filter(Class == \"L1 speaker\")\n# Compute minimum and maximum for numeric data\nmin.L1 &lt;- dat.L1 |&gt;\n  summarise_if(is.numeric, min)\nmax.L1 &lt;- dat.L1 |&gt;\n  summarise_if(is.numeric, max)\n\nWe can then take a look at the L2 speakers’ results and filter out the L2 speakers whose results aren’t all within the native speakers’ range:\n\n# Retain L2 speakers\ndat.L2 &lt;- dat |&gt; filter(Class == \"L2 speaker\")\n\n# Retain only L2 speakers whose results lie within L1 speakers' range\ndat.nativelikeL2 &lt;- dat.L2 |&gt;\n  filter(Pred1 &gt;= min.L1[[1]], Pred1 &lt;= max.L1[[1]]) |&gt;\n  filter(Pred2 &gt;= min.L1[[2]], Pred2 &lt;= max.L1[[2]]) |&gt;\n  filter(Pred3 &gt;= min.L1[[3]], Pred3 &lt;= max.L1[[3]]) |&gt;\n  filter(Pred4 &gt;= min.L1[[4]], Pred4 &lt;= max.L1[[4]]) |&gt;\n  filter(Pred5 &gt;= min.L1[[5]], Pred5 &lt;= max.L1[[5]]) |&gt;\n  filter(Pred6 &gt;= min.L1[[6]], Pred6 &lt;= max.L1[[6]]) |&gt;\n  filter(Pred7 &gt;= min.L1[[7]], Pred7 &lt;= max.L1[[7]]) |&gt;\n  filter(Pred8 &gt;= min.L1[[8]], Pred8 &lt;= max.L1[[8]]) |&gt;\n  filter(Pred9 &gt;= min.L1[[9]], Pred9 &lt;= max.L1[[9]]) |&gt;\n  filter(Pred10 &gt;= min.L1[[10]], Pred10 &lt;= max.L1[[10]]) |&gt;\n  filter(Pred11 &gt;= min.L1[[11]], Pred11 &lt;= max.L1[[11]]) |&gt;\n  filter(Pred12 &gt;= min.L1[[12]], Pred12 &lt;= max.L1[[12]]) |&gt;\n  filter(Pred13 &gt;= min.L1[[13]], Pred13 &lt;= max.L1[[13]]) |&gt;\n  filter(Pred14 &gt;= min.L1[[14]], Pred14 &lt;= max.L1[[14]])\ndat.nativelikeL2 |&gt; nrow()\n\n[1] 0\n\n# = empty dataset\n\nSure enough, none of the L2 learners classify as nativelike. (With more realistic data, a handful probably would have, cf. Abrahamsson & Hyltenstam’s results.)\nBy contrast, and quite obviously, all fifteen native speakers are classified as nativelike:\n\ndat.L1 |&gt;\n  filter(Pred1 &gt;= min.L1[[1]], Pred1 &lt;= max.L1[[1]]) |&gt;\n  filter(Pred2 &gt;= min.L1[[2]], Pred2 &lt;= max.L1[[2]]) |&gt;\n  filter(Pred3 &gt;= min.L1[[3]], Pred3 &lt;= max.L1[[3]]) |&gt;\n  filter(Pred4 &gt;= min.L1[[4]], Pred4 &lt;= max.L1[[4]]) |&gt;\n  filter(Pred5 &gt;= min.L1[[5]], Pred5 &lt;= max.L1[[5]]) |&gt;\n  filter(Pred6 &gt;= min.L1[[6]], Pred6 &lt;= max.L1[[6]]) |&gt;\n  filter(Pred7 &gt;= min.L1[[7]], Pred7 &lt;= max.L1[[7]]) |&gt;\n  filter(Pred8 &gt;= min.L1[[8]], Pred8 &lt;= max.L1[[8]]) |&gt;\n  filter(Pred9 &gt;= min.L1[[9]], Pred9 &lt;= max.L1[[9]]) |&gt;\n  filter(Pred10 &gt;= min.L1[[10]], Pred10 &lt;= max.L1[[10]]) |&gt;\n  filter(Pred11 &gt;= min.L1[[11]], Pred11 &lt;= max.L1[[11]]) |&gt;\n  filter(Pred12 &gt;= min.L1[[12]], Pred12 &lt;= max.L1[[12]]) |&gt;\n  filter(Pred13 &gt;= min.L1[[13]], Pred13 &lt;= max.L1[[13]]) |&gt;\n  filter(Pred14 &gt;= min.L1[[14]], Pred14 &lt;= max.L1[[14]]) |&gt;\n  nrow()\n\n[1] 15\n\n\nThis comes as no surprise: the nativelikeness criterion was based on these speakers’ scores, so of course they should pass it with flying colours.\nBut what happens when we test a new sample of native speakers using the old nativelikeness criterion? I simulated data for another 10,000 native speakers using the same procedure I used to create the first 15 native speakers’ data.\n\n# Read in data for *new* L1 speakers\nnew.L1 &lt;- read.csv(\"http://homeweb.unifr.ch/VanhoveJ/Pub/Data/new_nativelikeness.csv\",\n                   stringsAsFactors = TRUE)\nstr(new.L1)\n\n'data.frame':   10000 obs. of  14 variables:\n $ Pred1 : int  14 15 21 17 17 18 18 17 23 18 ...\n $ Pred2 : int  14 18 16 12 16 15 15 10 15 15 ...\n $ Pred3 : int  17 18 14 16 23 13 20 21 22 21 ...\n $ Pred4 : int  1 21 -17 3 -2 1 7 0 -1 16 ...\n $ Pred5 : int  23 15 14 12 11 15 6 14 17 20 ...\n $ Pred6 : int  24 24 26 26 26 29 17 29 24 25 ...\n $ Pred7 : int  -10 -6 -8 -6 -10 -3 -6 -8 -6 -9 ...\n $ Pred8 : int  16 13 15 14 17 17 15 14 18 16 ...\n $ Pred9 : int  64 77 69 64 66 73 67 70 71 66 ...\n $ Pred10: int  67 73 70 63 74 74 75 62 66 75 ...\n $ Pred11: int  7159 8098 8344 7113 7142 7808 7388 8062 8124 7864 ...\n $ Pred12: int  38 32 28 32 38 32 39 38 35 31 ...\n $ Pred13: int  41 42 44 49 40 37 44 47 43 52 ...\n $ Pred14: int  36 38 38 40 36 45 41 40 43 39 ...\n\n# Retain only participants whose results lie within *original* L1 speakers' range\nnew.L1 |&gt;\n  filter(Pred1 &gt;= min.L1[[1]], Pred1 &lt;= max.L1[[1]]) |&gt;\n  filter(Pred2 &gt;= min.L1[[2]], Pred2 &lt;= max.L1[[2]]) |&gt;\n  filter(Pred3 &gt;= min.L1[[3]], Pred3 &lt;= max.L1[[3]]) |&gt;\n  filter(Pred4 &gt;= min.L1[[4]], Pred4 &lt;= max.L1[[4]]) |&gt;\n  filter(Pred5 &gt;= min.L1[[5]], Pred5 &lt;= max.L1[[5]]) |&gt;\n  filter(Pred6 &gt;= min.L1[[6]], Pred6 &lt;= max.L1[[6]]) |&gt;\n  filter(Pred7 &gt;= min.L1[[7]], Pred7 &lt;= max.L1[[7]]) |&gt;\n  filter(Pred8 &gt;= min.L1[[8]], Pred8 &lt;= max.L1[[8]]) |&gt;\n  filter(Pred9 &gt;= min.L1[[9]], Pred9 &lt;= max.L1[[9]]) |&gt;\n  filter(Pred10 &gt;= min.L1[[10]], Pred10 &lt;= max.L1[[10]]) |&gt;\n  filter(Pred11 &gt;= min.L1[[11]], Pred11 &lt;= max.L1[[11]]) |&gt;\n  filter(Pred12 &gt;= min.L1[[12]], Pred12 &lt;= max.L1[[12]]) |&gt;\n  filter(Pred13 &gt;= min.L1[[13]], Pred13 &lt;= max.L1[[13]]) |&gt;\n  filter(Pred14 &gt;= min.L1[[14]], Pred14 &lt;= max.L1[[14]]) |&gt;\n  nrow()\n\n[1] 1048\n\n\nOnly 1048 of the 10,000 new native speakers pass the nativelikeness criterion! And these 10,000 new native speakers were sampled from the exact same population as the fifteen speakers used to establish the nativelikeness criterion—factors that would matter in real life such as social status, age, region, linguistic background, and what not don’t matter here; these would only make matters worse (see this paper on the selection of native-speaker controls by Sible Andringa).\nClearly, the finding that none of the L2 speakers are classified as nativelike carries considerably less weight now that we know that most L1 speakers wouldn’t have, either. Such information about the error rate associated with the nativelikeness criterion is therefore crucial to properly interpret studies relying on such a criterion. In practice, the bias against being classified as nativelike may not be huge as in this simulated example, but without an error-rate estimate (or access to the raw data), we’ve no way of knowing."
  },
  {
    "objectID": "posts/2016-07-05-classification-algorithm/index.html#estimating-error-rates-using-classification-algorithms",
    "href": "posts/2016-07-05-classification-algorithm/index.html#estimating-error-rates-using-classification-algorithms",
    "title": "Classifying second-language learners as native- or non-nativelike: Don’t neglect classification error rates",
    "section": "Estimating error rates using classification algorithms",
    "text": "Estimating error rates using classification algorithms\nIf researchers want to classify L2 learners as nativelike or non-nativelike and sensibly interpret their results, I suggest they stop defining nativelikeness criteria as intervals based on native speakers’ scores. Instead, they can turn to tools developed in a field specialised in such matters: machine learning, or predictive modelling. There’s an astounding number of algorithms out there that were developed for taking a set of predictor variables (e.g., task scores) on the one hand and a set of class labels (e.g., L1 speaker vs. L2 speaker) on the other hand, deriving a classification model from these data, and estimating the error rate of the classifications.\nI won’t provide a detailed introduction—Kuhn & Johnson’s Applied Predictive Modeling seems excellent—but I’ll just illustrate one such classification algorithm, random forests. In fact, the precise workings of this algorithm, which was developed in 2001 by Leo Breiman, needn’t really concern us here—you can read about them from the horse’s mouth, so to speak, in my thesis, or in tutorials by Tagliamonte & Baayen or Strobl and colleagues. What’s important is that it often produces excellent classification models and that it computes an error-rate estimate as a matter of course.\nThe randomForest function in the randomForest package implements the algorithm. There are a couple of settings that the user can tweak; again these needn’t concern us here—you can read about these in the articles referred to above.\n\n# Load the randomForest package;\n# you may need to run 'install.packages(\"randomForest\")' first.\nlibrary(randomForest)\n\n# Random forest have built-in random variability;\n# by setting the random seed, you'll get the same result as me.\n# You can try setting a different seed or not setting one\n# and see what happens\nset.seed(5-7-2016)\n\n# Use a random forest to predict Class (L1 vs. L2 speaker)\n# by means of all other variables in the dataset.\nnativelike.rf &lt;- randomForest(Class ~ ., data = dat)\n\n# Output, including confusion matrix\nnativelike.rf\n\n\nCall:\n randomForest(formula = Class ~ ., data = dat) \n               Type of random forest: classification\n                     Number of trees: 500\nNo. of variables tried at each split: 3\n\n        OOB estimate of  error rate: 16.07%\nConfusion matrix:\n           L1 speaker L2 speaker class.error\nL1 speaker          8          7  0.46666667\nL2 speaker          2         39  0.04878049\n\n\nThe output shows the estimated classification error that was computed on the basis of the original (simulated) data with 15 L1 and 41 L2 speakers (OBB estimate of error rate): an estimated 16.1% of observations will be misclassified by this algorithm. With more data (more observations, more predictors, more reliable predictors), this estimated error rate may become more accurate.\nMore interesting for our present purposes is the confusion matrix: The algorithm wrongly classifies two out of 41 L2 speakers as L1 speakers—these could perhaps be considered to have passed an updated ‘nativelikeness criterion’ inasmuch as they ‘fooled’ the algorithm. But it also misclassifies 7 of the 15 L1 speakers as L2 speakers. In this case, then, the 5% ‘nativelikeness incidence’ among L2 speakers may be an underestimate, as the algorithm seems to be biased against classifying participants as L1 speakers. This is likely due to the imbalance in the data: there are about 3 times more L2 than L1 speakers, so the algorithm naturally defaults to L2 speakers. (Take-home message if you want to conduct a study on nativelikeness: include more native speakers.)\nThe same random forest can also be applied to the 10,000 new L1 speakers, which gives a better estimate of how much the odds are stacked against classifying a participant as an L1 speaker:\n\nnew.predictions &lt;- predict(nativelike.rf, newdata = new.L1)\nsummary(new.predictions)\n\nL1 speaker L2 speaker \n      7745       2255 \n\n\nWhile the random forest doesn’t classify all L1 speakers in the original control sample as L1 speakers (as the naïve nativelikeness procedure did), it performs much better on new L1 data, classifying 77% of new L1 speakers as L1 speakers. Evidently, in a real study, we wouldn’t have a sample of 10,000 participants on the side to check the estimated classification error rate."
  },
  {
    "objectID": "posts/2016-07-05-classification-algorithm/index.html#conclusions",
    "href": "posts/2016-07-05-classification-algorithm/index.html#conclusions",
    "title": "Classifying second-language learners as native- or non-nativelike: Don’t neglect classification error rates",
    "section": "Conclusions",
    "text": "Conclusions\n\nBy using common definitions of nativelikeness criteria, L2 acquisition studies are likely to stack the odds against findings of nativelikeness and yield generally uninterpretable results.\nRandom forests and other classification algorithms will yield considerably better classifications than ad-hoc criteria, but they may be far from perfect. Their imperfection, unlike that of ad-hoc criteria, can be quantified, however, which is crucial for interpreting the results.\nYou’re unlikely to learn about such algorithms in an introductory course to statistics, but it’s useful to simply know that they exist. This is how you build up your statistical toolbox: when you know that these tools exist and have a vague sense of what they’re for, you can brush up on them when you need them. There’s a world beyond t-tests, ANOVA and Pearson’s r."
  },
  {
    "objectID": "posts/2016-07-05-classification-algorithm/index.html#software-versions",
    "href": "posts/2016-07-05-classification-algorithm/index.html#software-versions",
    "title": "Classifying second-language learners as native- or non-nativelike: Don’t neglect classification error rates",
    "section": "Software versions",
    "text": "Software versions\n\ndevtools::session_info()\n\n─ Session info ───────────────────────────────────────────────────────────────\n setting  value\n version  R version 4.3.1 (2023-06-16)\n os       Ubuntu 22.04.2 LTS\n system   x86_64, linux-gnu\n ui       X11\n language en_US\n collate  en_US.UTF-8\n ctype    en_US.UTF-8\n tz       Europe/Zurich\n date     2023-08-08\n pandoc   3.1.1 @ /usr/lib/rstudio/resources/app/bin/quarto/bin/tools/ (via rmarkdown)\n\n─ Packages ───────────────────────────────────────────────────────────────────\n package      * version date (UTC) lib source\n cachem         1.0.6   2021-08-19 [2] CRAN (R 4.2.0)\n callr          3.7.3   2022-11-02 [1] CRAN (R 4.3.1)\n cli            3.6.1   2023-03-23 [1] CRAN (R 4.3.0)\n colorspace     2.1-0   2023-01-23 [1] CRAN (R 4.3.0)\n crayon         1.5.2   2022-09-29 [1] CRAN (R 4.3.1)\n devtools       2.4.5   2022-10-11 [1] CRAN (R 4.3.1)\n digest         0.6.29  2021-12-01 [2] CRAN (R 4.2.0)\n dplyr        * 1.1.2   2023-04-20 [1] CRAN (R 4.3.0)\n ellipsis       0.3.2   2021-04-29 [2] CRAN (R 4.2.0)\n evaluate       0.15    2022-02-18 [2] CRAN (R 4.2.0)\n fansi          1.0.4   2023-01-22 [1] CRAN (R 4.3.1)\n fastmap        1.1.0   2021-01-25 [2] CRAN (R 4.2.0)\n forcats      * 1.0.0   2023-01-29 [1] CRAN (R 4.3.0)\n fs             1.5.2   2021-12-08 [2] CRAN (R 4.2.0)\n generics       0.1.3   2022-07-05 [1] CRAN (R 4.3.0)\n ggplot2      * 3.4.2   2023-04-03 [1] CRAN (R 4.3.0)\n glue           1.6.2   2022-02-24 [2] CRAN (R 4.2.0)\n gtable         0.3.3   2023-03-21 [1] CRAN (R 4.3.0)\n hms            1.1.3   2023-03-21 [1] CRAN (R 4.3.0)\n htmltools      0.5.5   2023-03-23 [1] CRAN (R 4.3.0)\n htmlwidgets    1.6.2   2023-03-17 [1] CRAN (R 4.3.1)\n httpuv         1.6.11  2023-05-11 [1] CRAN (R 4.3.1)\n jsonlite       1.8.7   2023-06-29 [1] CRAN (R 4.3.1)\n knitr          1.39    2022-04-26 [2] CRAN (R 4.2.0)\n later          1.3.1   2023-05-02 [1] CRAN (R 4.3.1)\n lifecycle      1.0.3   2022-10-07 [1] CRAN (R 4.3.0)\n lubridate    * 1.9.2   2023-02-10 [1] CRAN (R 4.3.0)\n magrittr       2.0.3   2022-03-30 [1] CRAN (R 4.3.0)\n memoise        2.0.1   2021-11-26 [2] CRAN (R 4.2.0)\n mime           0.10    2021-02-13 [2] CRAN (R 4.0.2)\n miniUI         0.1.1.1 2018-05-18 [1] CRAN (R 4.3.1)\n munsell        0.5.0   2018-06-12 [1] CRAN (R 4.3.0)\n pillar         1.9.0   2023-03-22 [1] CRAN (R 4.3.0)\n pkgbuild       1.4.2   2023-06-26 [1] CRAN (R 4.3.1)\n pkgconfig      2.0.3   2019-09-22 [2] CRAN (R 4.2.0)\n pkgload        1.3.2.1 2023-07-08 [1] CRAN (R 4.3.1)\n prettyunits    1.1.1   2020-01-24 [2] CRAN (R 4.2.0)\n processx       3.8.2   2023-06-30 [1] CRAN (R 4.3.1)\n profvis        0.3.8   2023-05-02 [1] CRAN (R 4.3.1)\n promises       1.2.0.1 2021-02-11 [1] CRAN (R 4.3.1)\n ps             1.7.5   2023-04-18 [1] CRAN (R 4.3.1)\n purrr        * 1.0.1   2023-01-10 [1] CRAN (R 4.3.0)\n R6             2.5.1   2021-08-19 [2] CRAN (R 4.2.0)\n randomForest * 4.7-1.1 2022-05-23 [1] CRAN (R 4.3.1)\n Rcpp           1.0.11  2023-07-06 [1] CRAN (R 4.3.1)\n readr        * 2.1.4   2023-02-10 [1] CRAN (R 4.3.0)\n remotes        2.4.2   2021-11-30 [2] CRAN (R 4.2.0)\n rlang          1.1.1   2023-04-28 [1] CRAN (R 4.3.0)\n rmarkdown      2.21    2023-03-26 [1] CRAN (R 4.3.0)\n rstudioapi     0.14    2022-08-22 [1] CRAN (R 4.3.0)\n scales         1.2.1   2022-08-20 [1] CRAN (R 4.3.0)\n sessioninfo    1.2.2   2021-12-06 [2] CRAN (R 4.2.0)\n shiny          1.7.4.1 2023-07-06 [1] CRAN (R 4.3.1)\n stringi        1.7.12  2023-01-11 [1] CRAN (R 4.3.1)\n stringr      * 1.5.0   2022-12-02 [1] CRAN (R 4.3.0)\n tibble       * 3.2.1   2023-03-20 [1] CRAN (R 4.3.0)\n tidyr        * 1.3.0   2023-01-24 [1] CRAN (R 4.3.0)\n tidyselect     1.2.0   2022-10-10 [1] CRAN (R 4.3.0)\n tidyverse    * 2.0.0   2023-02-22 [1] CRAN (R 4.3.1)\n timechange     0.2.0   2023-01-11 [1] CRAN (R 4.3.0)\n tzdb           0.4.0   2023-05-12 [1] CRAN (R 4.3.0)\n urlchecker     1.0.1   2021-11-30 [1] CRAN (R 4.3.1)\n usethis        2.2.2   2023-07-06 [1] CRAN (R 4.3.1)\n utf8           1.2.3   2023-01-31 [1] CRAN (R 4.3.1)\n vctrs          0.6.3   2023-06-14 [1] CRAN (R 4.3.0)\n withr          2.5.0   2022-03-03 [2] CRAN (R 4.2.0)\n xfun           0.39    2023-04-20 [1] CRAN (R 4.3.0)\n xtable         1.8-4   2019-04-21 [1] CRAN (R 4.3.1)\n yaml           2.3.5   2022-02-21 [2] CRAN (R 4.2.0)\n\n [1] /home/jan/R/x86_64-pc-linux-gnu-library/4.3\n [2] /usr/local/lib/R/site-library\n [3] /usr/lib/R/site-library\n [4] /usr/lib/R/library\n\n──────────────────────────────────────────────────────────────────────────────"
  },
  {
    "objectID": "posts/2016-11-16-common-language-effect-sizes/index.html",
    "href": "posts/2016-11-16-common-language-effect-sizes/index.html",
    "title": "Common-language effect sizes",
    "section": "",
    "text": "The goal of this blog post is to share with you a simple R function that may help you to better communicate the extent to which two groups differ and overlap by computing common-language effect sizes."
  },
  {
    "objectID": "posts/2016-11-16-common-language-effect-sizes/index.html#what-is-the-common-language-effect-size",
    "href": "posts/2016-11-16-common-language-effect-sizes/index.html#what-is-the-common-language-effect-size",
    "title": "Common-language effect sizes",
    "section": "What is the ‘common-language effect size’?",
    "text": "What is the ‘common-language effect size’?\nIn 1992, McGraw and Wong introduced the common-language effect size, which they defined as\n\nthe probability that a score sampled at random from one distribution will be greater than a score sampled from some other distribution.\n\nFor instance, if you have scores on an English reading comprehension task for both French- and German-speaking learners, you can compute the probability that a randomly chosen French-speaking learner will have a higher score than a randomly chosen German-speaking learner. This gives you an idea of how much the groups’ scores overlap, and the number can more easily be communicated to an audience that has no firm notion of what quantiles are or of what standardised effect sizes such as d = 0.3 mean."
  },
  {
    "objectID": "posts/2016-11-16-common-language-effect-sizes/index.html#computing-common-language-effect-sizes-in-r",
    "href": "posts/2016-11-16-common-language-effect-sizes/index.html#computing-common-language-effect-sizes-in-r",
    "title": "Common-language effect sizes",
    "section": "Computing common-language effect sizes in R",
    "text": "Computing common-language effect sizes in R\nBelow I first generate some data: 40 data points in a group creatively called A vs. 30 data points in group B.\n\n# Generate data\nset.seed(16-11-2016)\ndf &lt;- data.frame(Outcome = c(rbeta(40, 4, 14),   # 40 observations in A\n                             rbeta(30, 2, 4)),   # 30 in B\n                 Group = c(rep(\"A\", 40), rep(\"B\", 30)))\n\nA couple of boxplots to show the spread and central tendencies:\n\n\n\n\n\nAnd the key summary statistics:\n\n\n# A tibble: 2 × 3\n  Group  Mean Standard_Deviation\n  &lt;chr&gt; &lt;dbl&gt;              &lt;dbl&gt;\n1 A     0.244             0.0841\n2 B     0.301             0.172 \n\n\nOn the basis of the group means and standard deviations, McGraw and Wong’s common-language effect size can be computed as follows:\n\npnorm(0, 0.24 - 0.30, sqrt(0.084^2 + 0.172^2), lower.tail = FALSE)\n\n[1] 0.38\n\n\nI.e., there’s a 38% chance that if you put an observation from Group A and one from Group B together at random, the one from Group A will be greater.\nStrictly speaking, McGraw and Wong’s method assumes normally distributed, continuous data. While they point out that their measure is quite robust with respect to this assumption, you can use a brute-force method that doesn’t make this assumption to see if that yields different results.\nEdit: On Twitter, Guillaume Rousselet suggested a quicker and mor exhaustive brute-force method for computing common-language effect sizes. I’ve updated the code and post to implement his suggestion.\nOn https://janhove.github.io/RCode/CommonLanguageEffectSizes.R, I provide a function, cles.fnc(), that pairs each observation from the first group to each observation from the second group and then checks how often the observation from the first group is larger than the one from the second group. Ties are also taken into account.\nHere’s how the cles.fnc() function works:\n\n# Read in the function\nsource(\"https://janhove.github.io/RCode/CommonLanguageEffectSizes.R\")\n\n# Set variable you want to compare between the groups,\n# the group name,\n# the baseline level,\n# and the dataset:\ncles &lt;- cles.fnc(variable = \"Outcome\", group = \"Group\", baseline = \"A\", data = df)\n\nCommon-language effect size:\n\nThe probability that a random Outcome observation from group A\nis higher/larger than a random Outcome observation from the other group(s):\n\n    Algebraic method:   0.38\n    Brute-force method: 0.4\n\n\nThe results for both methods aren’t identical (38% vs. 40%), but they’re in the same ballpark. This is more often the case than not.\nYou can turn off the output by setting the parameter print to FALSE:\n\n## (not run)\n# cles &lt;- cles.fnc(variable = \"Outcome\", group = \"Group\", baseline = \"A\", data = df, print = FALSE)\n\nYou can also extract information from the cles object if you want to pass it on to other functions:\n\ncles$algebraic # McGraw & Wong's method\n\n[1] 0.38\n\ncles$brute     # brute-force method\n\n[1] 0.4"
  },
  {
    "objectID": "posts/2016-11-16-common-language-effect-sizes/index.html#an-example-with-non-overlapping-distributions",
    "href": "posts/2016-11-16-common-language-effect-sizes/index.html#an-example-with-non-overlapping-distributions",
    "title": "Common-language effect sizes",
    "section": "An example with non-overlapping distributions",
    "text": "An example with non-overlapping distributions\nThe code below generates a dataset with two non-overlapping groups.\n\n# Generate data\nset.seed(16-11-2016)\ndf &lt;- data.frame(Outcome = c(sample(0:20, 10, replace = TRUE),\n                             sample(21:25, 10, replace = TRUE)),\n                 Group = c(rep(\"A\", 10), rep(\"B\", 10)))\n\nBoxplots:\n\n\n\n\n\nMcGraw & Wong’s (1992) method suggests that there’s a 6% chance that a random observation in A will be higher than one in B. This may well be true at the population level, but it’s clearly not true at the sample level. The brute-force method pegs this probability at 0%, which may be wrong at the population level, but it’s clearly correct at the sample level.\n\ncles &lt;- cles.fnc(variable = \"Outcome\", group = \"Group\", baseline = \"A\", data = df)\n\nCommon-language effect size:\n\nThe probability that a random Outcome observation from group A\nis higher/larger than a random Outcome observation from the other group(s):\n\n    Algebraic method:   0.04\n    Brute-force method: 0"
  },
  {
    "objectID": "posts/2016-11-16-common-language-effect-sizes/index.html#for-use-with-more-complex-datasets",
    "href": "posts/2016-11-16-common-language-effect-sizes/index.html#for-use-with-more-complex-datasets",
    "title": "Common-language effect sizes",
    "section": "For use with more complex datasets",
    "text": "For use with more complex datasets\nLet’s say you have data from a longitudinal study in which you collected data for Groups A and B at Times 1, 2 and 3, and you want to compare the groups at each time:\n\n# Generate data\nset.seed(16-11-2016)\ndf &lt;- data.frame(Time = c(rep(1, 40), rep(2, 40), rep(3, 40)),\n                 Group = rep(c(rep(\"A\", 25), rep(\"B\", 15)), 3),\n                 Outcome = c(rnorm(25, 1.1, 1.0), rnorm(15, 1.2, 0.3),\n                             rnorm(25, 1.2, 0.6), rnorm(15, 1.2, 0.8),\n                             rnorm(25, 1.8, 0.4), rnorm(15, 1.7, 1.0)))\n\nThe boxplots:\n\n\n\n\n\nUsing the by() function, you can run cles.fnc() separately for each Time. For more complex datasets, you can include more variables in the INDICES list.\n\ncles &lt;- with(df, # select dataframe df\n             by(data = df, # group dataframe df\n                INDICES = list(Time = Time), # by Time\n                # and run cles.fnc() within each group\n                FUN = function(x) cles.fnc(\"Outcome\", \"Group\", \"A\", data = x, print = FALSE)))\ncles\n\nTime: 1\n$algebraic\n[1] 0.45\n\n$brute\n[1] 0.55\n\n------------------------------------------------------------ \nTime: 2\n$algebraic\n[1] 0.46\n\n$brute\n[1] 0.5\n\n------------------------------------------------------------ \nTime: 3\n$algebraic\n[1] 0.75\n\n$brute\n[1] 0.77"
  },
  {
    "objectID": "posts/2016-11-16-common-language-effect-sizes/index.html#software-versions",
    "href": "posts/2016-11-16-common-language-effect-sizes/index.html#software-versions",
    "title": "Common-language effect sizes",
    "section": "Software versions",
    "text": "Software versions\n\ndevtools::session_info()\n\n─ Session info ───────────────────────────────────────────────────────────────\n setting  value\n version  R version 4.3.1 (2023-06-16 ucrt)\n os       Windows 10 x64 (build 18363)\n system   x86_64, mingw32\n ui       RTerm\n language (EN)\n collate  English_United Kingdom.utf8\n ctype    English_United Kingdom.utf8\n tz       Europe/Zurich\n date     2023-08-08\n pandoc   3.1.1 @ C:/Program Files/RStudio/resources/app/bin/quarto/bin/tools/ (via rmarkdown)\n\n─ Packages ───────────────────────────────────────────────────────────────────\n package     * version date (UTC) lib source\n cachem        1.0.8   2023-05-01 [1] CRAN (R 4.3.1)\n callr         3.7.3   2022-11-02 [1] CRAN (R 4.3.1)\n cli           3.6.1   2023-03-23 [1] CRAN (R 4.3.1)\n colorspace    2.1-0   2023-01-23 [1] CRAN (R 4.3.1)\n crayon        1.5.2   2022-09-29 [1] CRAN (R 4.3.1)\n devtools      2.4.5   2022-10-11 [1] CRAN (R 4.3.1)\n digest        0.6.33  2023-07-07 [1] CRAN (R 4.3.1)\n dplyr       * 1.1.2   2023-04-20 [1] CRAN (R 4.3.1)\n ellipsis      0.3.2   2021-04-29 [1] CRAN (R 4.3.1)\n evaluate      0.21    2023-05-05 [1] CRAN (R 4.3.1)\n fansi         1.0.4   2023-01-22 [1] CRAN (R 4.3.1)\n farver        2.1.1   2022-07-06 [1] CRAN (R 4.3.1)\n fastmap       1.1.1   2023-02-24 [1] CRAN (R 4.3.1)\n forcats     * 1.0.0   2023-01-29 [1] CRAN (R 4.3.1)\n fs            1.6.3   2023-07-20 [1] CRAN (R 4.3.1)\n generics      0.1.3   2022-07-05 [1] CRAN (R 4.3.1)\n ggplot2     * 3.4.2   2023-04-03 [1] CRAN (R 4.3.1)\n glue          1.6.2   2022-02-24 [1] CRAN (R 4.3.1)\n gtable        0.3.3   2023-03-21 [1] CRAN (R 4.3.1)\n hms           1.1.3   2023-03-21 [1] CRAN (R 4.3.1)\n htmltools     0.5.5   2023-03-23 [1] CRAN (R 4.3.1)\n htmlwidgets   1.6.2   2023-03-17 [1] CRAN (R 4.3.1)\n httpuv        1.6.11  2023-05-11 [1] CRAN (R 4.3.1)\n jsonlite      1.8.7   2023-06-29 [1] CRAN (R 4.3.1)\n knitr         1.43    2023-05-25 [1] CRAN (R 4.3.1)\n labeling      0.4.2   2020-10-20 [1] CRAN (R 4.3.0)\n later         1.3.1   2023-05-02 [1] CRAN (R 4.3.1)\n lifecycle     1.0.3   2022-10-07 [1] CRAN (R 4.3.1)\n lubridate   * 1.9.2   2023-02-10 [1] CRAN (R 4.3.1)\n magrittr      2.0.3   2022-03-30 [1] CRAN (R 4.3.1)\n memoise       2.0.1   2021-11-26 [1] CRAN (R 4.3.1)\n mime          0.12    2021-09-28 [1] CRAN (R 4.3.0)\n miniUI        0.1.1.1 2018-05-18 [1] CRAN (R 4.3.1)\n munsell       0.5.0   2018-06-12 [1] CRAN (R 4.3.1)\n pillar        1.9.0   2023-03-22 [1] CRAN (R 4.3.1)\n pkgbuild      1.4.2   2023-06-26 [1] CRAN (R 4.3.1)\n pkgconfig     2.0.3   2019-09-22 [1] CRAN (R 4.3.1)\n pkgload       1.3.2.1 2023-07-08 [1] CRAN (R 4.3.1)\n prettyunits   1.1.1   2020-01-24 [1] CRAN (R 4.3.1)\n processx      3.8.2   2023-06-30 [1] CRAN (R 4.3.1)\n profvis       0.3.8   2023-05-02 [1] CRAN (R 4.3.1)\n promises      1.2.0.1 2021-02-11 [1] CRAN (R 4.3.1)\n ps            1.7.5   2023-04-18 [1] CRAN (R 4.3.1)\n purrr       * 1.0.1   2023-01-10 [1] CRAN (R 4.3.1)\n R6            2.5.1   2021-08-19 [1] CRAN (R 4.3.1)\n Rcpp          1.0.11  2023-07-06 [1] CRAN (R 4.3.1)\n readr       * 2.1.4   2023-02-10 [1] CRAN (R 4.3.1)\n remotes       2.4.2.1 2023-07-18 [1] CRAN (R 4.3.1)\n rlang         1.1.1   2023-04-28 [1] CRAN (R 4.3.1)\n rmarkdown     2.23    2023-07-01 [1] CRAN (R 4.3.1)\n rstudioapi    0.15.0  2023-07-07 [1] CRAN (R 4.3.1)\n scales        1.2.1   2022-08-20 [1] CRAN (R 4.3.1)\n sessioninfo   1.2.2   2021-12-06 [1] CRAN (R 4.3.1)\n shiny         1.7.4.1 2023-07-06 [1] CRAN (R 4.3.1)\n stringi       1.7.12  2023-01-11 [1] CRAN (R 4.3.0)\n stringr     * 1.5.0   2022-12-02 [1] CRAN (R 4.3.1)\n tibble      * 3.2.1   2023-03-20 [1] CRAN (R 4.3.1)\n tidyr       * 1.3.0   2023-01-24 [1] CRAN (R 4.3.1)\n tidyselect    1.2.0   2022-10-10 [1] CRAN (R 4.3.1)\n tidyverse   * 2.0.0   2023-02-22 [1] CRAN (R 4.3.1)\n timechange    0.2.0   2023-01-11 [1] CRAN (R 4.3.1)\n tzdb          0.4.0   2023-05-12 [1] CRAN (R 4.3.1)\n urlchecker    1.0.1   2021-11-30 [1] CRAN (R 4.3.1)\n usethis       2.2.2   2023-07-06 [1] CRAN (R 4.3.1)\n utf8          1.2.3   2023-01-31 [1] CRAN (R 4.3.1)\n vctrs         0.6.3   2023-06-14 [1] CRAN (R 4.3.1)\n withr         2.5.0   2022-03-03 [1] CRAN (R 4.3.1)\n xfun          0.39    2023-04-20 [1] CRAN (R 4.3.1)\n xtable        1.8-4   2019-04-21 [1] CRAN (R 4.3.1)\n yaml          2.3.7   2023-01-23 [1] CRAN (R 4.3.0)\n\n [1] C:/Users/VanhoveJ/AppData/Local/R/win-library/4.3\n [2] C:/Program Files/R/R-4.3.1/library\n\n──────────────────────────────────────────────────────────────────────────────"
  },
  {
    "objectID": "posts/2015-12-04-perks-data-sharing/index.html",
    "href": "posts/2015-12-04-perks-data-sharing/index.html",
    "title": "Some advantages of sharing your data and code",
    "section": "",
    "text": "I believe researchers should put their datasets and any computer code used to analyse them online as a matter of principle, without being asked to, and many researchers (Dorothy Bishop, Rolf Zwaan, Jelte Wicherts, and certainly dozens more) have cogently argued that such ‘open data’ should be the norm. However, it seems to me that some aren’t too enthused at the prospect of putting their data and code online as they fear that some curmudgeon will go over them with the fine-toothed comb to find some tiny error and call the whole study into question. Why else would anyone be interested in their data and code?\nI would. The scientific ideals of transparency and allowing yourself to be corrected are important arguments for open data. But I can see plenty of advantages to open data other than spotting errors and running alternative analyses whose results may contradict those of the original study. In the hopes that researchers will be more willing to share their data and code if they better appreciate such perks, I discuss four of them."
  },
  {
    "objectID": "posts/2015-12-04-perks-data-sharing/index.html#realistic-examples-for-teaching-statistics",
    "href": "posts/2015-12-04-perks-data-sharing/index.html#realistic-examples-for-teaching-statistics",
    "title": "Some advantages of sharing your data and code",
    "section": "Realistic examples for teaching statistics",
    "text": "Realistic examples for teaching statistics\nWhen teaching statistics, real datasets are great for going beyond the formulae and discussing other crucial aspects of quantitative research. The data might not follow a textbook distribution – should we be worrying? Some values may be missing – does this matter in this case? Some variables may be ambiguously labelled – a reminder to properly label them yourselves. There may be several ways to transform a variable – how strongly does our choice affect the results? And you can actually draw some subject-matter conclusions on the basis of real data, which could increase interest in the exercise.\nI have a few datasets that I can incorporate in our own introductory statistics course, but for many examples and exercises I have to resort to datasets from other fields or even simulated data. While the latter are often inspired by real studies, they’d be much instructive if they were based on the actual datasets with all their imperfections. Now, it’s of course possible that in working through a real dataset, we could stumble on some errors or arbitrary decisions that have a major impact on the original study’s conclusions. But that’s just a by-product of wanting to learn more about dealing with data."
  },
  {
    "objectID": "posts/2015-12-04-perks-data-sharing/index.html#making-analyses-and-results-more-intelligible",
    "href": "posts/2015-12-04-perks-data-sharing/index.html#making-analyses-and-results-more-intelligible",
    "title": "Some advantages of sharing your data and code",
    "section": "Making analyses and results more intelligible",
    "text": "Making analyses and results more intelligible\nHaving access to the data behind research papers would also be great when teaching subject-matter courses. For a typical class in an MA seminar, I ask my students to read a research paper, which we then dissect in class. Naturally, we devote considerable time to how the data are analysed and to how the conclusions follow from the results.\nInferential statistics aren’t very intuitive, however, and I like to refer as much as possible to descriptive statistics during the discussion – ideally in the form of graphs showing the raw data (e.g. scatterplots). When the paper doesn’t contain such graphs, it’d be incredibly useful if I could draw them myself using the original data – and show the students what “t(46) = 2.1, p = 0.04” actually looks like.\nThere are other things you could do when you have the raw data behind a paper. For instance, you could try to explain sampling error, signal detection and p-values using graphical inference – it doesn’t hurt to revisit these concepts outside of the statistics class. The same goes for more involved analyses: While I could have a crack at describing what hierarchical regression does, I think it’d be much more instructive to draw a scatterplot matrix of the variables involved or plots where the residuals of the previous step are regressed against the next predictor. For this, having the original data would be great.\nAgain, we might well end up concluding that the original study glosses over some unbecoming patterns in the data. But that too would be a by-product of trying to really understand a study.\nOf course, having the original data and code wouldn’t only be useful for teaching students, but also for teaching yourself as a researcher. This may include learning about complex techniques such as importing data from online corpora or visualising a logistic regression model, but it could equally well include more mundane things like finding a more straightforward method for sorting a data frame in R, computing accuracy scores per participant per experimental condition, or even simply checking how others organise their datasets. Either way, your data and code can be instructive to others."
  },
  {
    "objectID": "posts/2015-12-04-perks-data-sharing/index.html#irrelevant-to-you-interesting-to-me",
    "href": "posts/2015-12-04-perks-data-sharing/index.html#irrelevant-to-you-interesting-to-me",
    "title": "Some advantages of sharing your data and code",
    "section": "Irrelevant to you, interesting to me",
    "text": "Irrelevant to you, interesting to me\nFor my Ph.D. project I designed a pretty simple task in which people had to guess the meaning of words in a language they didn’t know but that was genealogically related to their native language. Naturally I wondered whether it would matter whether I tested my German-speaking participants in Swedish, Danish, Norwegian, Frisian, Dutch etc., so it would’ve been interesting to see how strongly translation performance correlated across target languages in previous studies. Only in one study that I knew of were the participants actually tested in several related languages with similar tasks. But the intercorrelations weren’t mentioned in the report as they simply hadn’t been of interest to the authors. I requested access to the data in order to compute them myself (easier than asking the authors to compute them for me, I thought), but my request was turned down on the grounds that the data were “too complex”.\nTo be clear, I fully understand why these intercorrelations were irrelevant to these authors, but with their data (about 1800 participants!) I could have pretty easily found an answer that was interesting to me. This, I think, applies to other datasets, too – there may be some aspect of your data that doesn’t interest you but may well help someone else design a study or refine a research question. By sharing your data without being asked to, you allow others to build on your work in ways you might not have conceived of."
  },
  {
    "objectID": "posts/2015-12-04-perks-data-sharing/index.html#more-focused-writing",
    "href": "posts/2015-12-04-perks-data-sharing/index.html#more-focused-writing",
    "title": "Some advantages of sharing your data and code",
    "section": "More focused writing",
    "text": "More focused writing\nOne thing I find difficult when writing is catering for the diverse backgrounds of my intended readership. Consider logistic mixed-effects models. I have pretty good reasons for using such models for analysing my data, but I’m often unsure in how much detail I should discuss them in papers. For instance, the ‘random correlations’ between the random intercepts and the random slopes in these models may be of interest to some readers, but probably not to most. Worse, the jargon may convince some readers that the paper isn’t meant for them. This would be a shame if these random correlations aren’t of too much importance for the paper’s conclusion. Similarly, does anyone really care that I use cubic regression splines rather than thin plate regression splines to model non-linear trends? Most readers won’t, and some may be confused by what I consider to be an unimportant detail. But I can’t rule out that some readers may be genuinely interested in it.\nHowever, I’m willing to assume that anyone knowing enough about statistics to care about random correlations or regression spline classes will be able to make sense of my R script, so I just put it online alongside the data. (For future projects, I consider putting the R script on RPubs so that interested readers can look up the details without having to actually run the analyses. See this example.) That way, I can focus on what I consider to be the important aspects of the analysis, which makes for clearer writing."
  },
  {
    "objectID": "posts/2015-12-04-perks-data-sharing/index.html#conclusions",
    "href": "posts/2015-12-04-perks-data-sharing/index.html#conclusions",
    "title": "Some advantages of sharing your data and code",
    "section": "Conclusions",
    "text": "Conclusions\nTo me, open data isn’t just about preventing fraud and fixing honest mistakes. It’s also about getting most mileage out of your data by allowing it to be used for educating future scientists, helping colleagues, and catering to your readers’ interests and background."
  },
  {
    "objectID": "posts/2014-08-20-adjusted-pvalues-breakpoint-regression/index.html",
    "href": "posts/2014-08-20-adjusted-pvalues-breakpoint-regression/index.html",
    "title": "Calibrating p-values in ‘flexible’ piecewise regression models",
    "section": "",
    "text": "Last year, I published an article in which I critiqued the statistical tools used to assess the hypothesis that ultimate second-language (L2) proficiency is non-linearly related to the age of onset of L2 acquisition. Rather than using ANOVAs or correlation coefficient comparisons, I argued, breakpoint (or piecewise) regression should be used. But unless the breakpoint is specified in advance, such models are demonstrably anti-conservative. Here I outline a simulation-based approach for calibrating the p-values of such models to take their anti-conservatism into account."
  },
  {
    "objectID": "posts/2014-08-20-adjusted-pvalues-breakpoint-regression/index.html#piecewise-regression",
    "href": "posts/2014-08-20-adjusted-pvalues-breakpoint-regression/index.html#piecewise-regression",
    "title": "Calibrating p-values in ‘flexible’ piecewise regression models",
    "section": "Piecewise regression",
    "text": "Piecewise regression\nPiecewise (or breakpoint) regression is a pretty self-descriptive term: it’s a regression model with an elbow in the function. For instance, in the graph below, the function relating x to y flattens for x values higher than 0.5.\n\nWhat is important here is that the analyst has to specify at which x point the regression curve is allowed to change slopes – in the graph above, I had to specify the breakpoint parameter myself (see my article on how this can be done)."
  },
  {
    "objectID": "posts/2014-08-20-adjusted-pvalues-breakpoint-regression/index.html#the-problem-increased-type-i-error-rates",
    "href": "posts/2014-08-20-adjusted-pvalues-breakpoint-regression/index.html#the-problem-increased-type-i-error-rates",
    "title": "Calibrating p-values in ‘flexible’ piecewise regression models",
    "section": "The problem: increased Type-I error rates",
    "text": "The problem: increased Type-I error rates\nLanguage researchers may be familiar with piecewise regression thanks to Harald Baayen’s excellent Analyzing linguistic data, in which he illustrates a method to automate the selection of the breakpoint. Briefly, this method consists of looping through all possible breakpoints and fitting a piecewise regression model for each of them. Then, the best-fitting model is retained.\nIn my article on analyses in critical period research, I too adopted this procedure in order to illustrate that the data in the two studies that I scrutinised were not characterised by the predicted non-linearities. What I may have underemphasised in that article, however, is that this breakpoint selection procedure yields a higher-than-nominal Type-I error rate of finding a non-linearity. In plain language, looping through several possible breakpoints increases your risk of finding non-linearities when in fact none exist.\nThe intuition behind this is probably (hopefully) dealt with in every introductory statistics course. If you run a two-samples t-test with an alpha level of 0.05, you accept a 5% risk to find a significant difference between the two groups even if the populations do not differ. If you run five t-tests on unrelated data, each with an alpha level of 0.05, the risk of finding a non-existing difference is still 5% for each test considered individually – but the global risk (‘familywise Type-I error rate’) is now about 23%:\n\n1 - (1 - 0.05)^5\n\n[1] 0.2262191\n\n\nSimilarly, if you loop through, say, 5 possible breakpoints to establish whether a non-linearity exists using piecewise regression, your Type-I error rate goes up. But it won’t be as high as 23% since you’re not conducting the same analysis on unrelated data. It’d be nice if we could calibrate the p-values from a piecewise regression model that suffers from this multiple comparison problem, but I wouldn’t know how to go about it analytically. Instead, I suggest a simulation-based approach. But first, let’s make the problem a bit more concrete by turning to the actual datasets."
  },
  {
    "objectID": "posts/2014-08-20-adjusted-pvalues-breakpoint-regression/index.html#the-data",
    "href": "posts/2014-08-20-adjusted-pvalues-breakpoint-regression/index.html#the-data",
    "title": "Calibrating p-values in ‘flexible’ piecewise regression models",
    "section": "The data",
    "text": "The data\nIn my CPH article, I re-analysed two datasets from DeKeyser et al. (2010). The specifics of these two datasets aren’t too important for our present purposes, so I’ll skip them. The two datasets are available here (Israel data) and here (North America data); the following plots show the relationship between the AOA (age of onset of L2 acquisition) and GJT (a measure of L2 proficiency) variables in both datasets.\n\nPiecewise regressions carried out using the a priori cut-off AOA values specified by DeKeyser et al. (AOA = 18) did not yield significant improvements over straightforward linear models ( p = 0.98 and 0.07 for the Israel and North America data, respectively). In a second series of analyses, I determined the optimal breakpoints following the approach suggested by Baayen. For the Israel data, the optimal breakpoint was located at AOA 6 but a piecewise model still failed to yield a significant improvement over a linear model ( p = 0.62). For the North America data, fitting a breakpoint at AOA 16 did yield such an improvement ( p = 0.047).\nAs I looped through several breakpoints (from AOA 5 till 19), however, these p-values will be anti-conservative."
  },
  {
    "objectID": "posts/2014-08-20-adjusted-pvalues-breakpoint-regression/index.html#computing-adjusted-p-values",
    "href": "posts/2014-08-20-adjusted-pvalues-breakpoint-regression/index.html#computing-adjusted-p-values",
    "title": "Calibrating p-values in ‘flexible’ piecewise regression models",
    "section": "Computing adjusted p-values",
    "text": "Computing adjusted p-values\nAs always, a p-value is supposed to express the probability of finding a significant result if the null hypothesis were true. When fitting piecewise regressions, the null hypothesis is that the relationship between the two variables is linear. We don’t know precisely how a linear relationship between our two variables looks like under the null hypothesis (intercept, slope and dispersion parameters) but a reasonable starting point is to fit a linear model to the data we have and take its parameters as the population parameters. After all, they’re our best guess. We’re then going to simulate new datasets based on this model.\nBy way of illustration, the following R code reads in the Israel data and uses it to create 9 linear AOA-GJT relationships with similar properties.\n\n# Read in Israel data\ndat &lt;- read.csv(\"http://homeweb.unifr.ch/VanhoveJ/Pub/papers/CPH/DeKeyser2010Israel.csv\")\n# Fit linear model\nmod &lt;- lm(GJT ~ AOA, data = dat)\n# Create grid for 9 plots\npar(mfrow=c(3,3),\n    oma=c(0, 0, 2, 0), # room for title\n    bty=\"l\", las=\"1\")\n# Draw 9 plots with simulated data\nplots &lt;- replicate(9,\n                   plot(dat$AOA, unlist(simulate(mod)),\n                        xlab=\"AOA\", ylab=\"simulated GJT\"))\ntitle(main=\"9 simulated datasets\", outer=TRUE)\n\n\n\n\nAs you can see, some of the simulated GJT values lie outside the range of the original GJT data. DeKeyser et al. used a test that consisted of 204 yes/no items, so values higher than 204 are impossible whereas a score of 102 corresponds to random guessing. Our regression model doesn’t know that 102 and 204 represent the de facto limits of the GJT variable, and while we could try to use censored regression models to deal with that, I’m just going to tell R to set values higher than 204 to 204 and values lower than 102 to 102. The following function, generateBreakpointData.fnc, does just that and, while we’re at it, rounds off the simulated values to integers.\n\nstraight.lm &lt;- lm(GJT ~ AOA, dat)\nAOAvals &lt;- dat$AOA\ngenerateBreakpointData.fnc &lt;- function(AOA = AOAvals) {\n  UA &lt;- round(unlist(simulate(straight.lm)))\n  UA[UA&gt;204] &lt;- 204\n  UA[UA&lt;102] &lt;- 102\n  return(data.frame(AOA = AOA,\n                    UA = UA))\n}\n\nNext, we’re going to fit a piecewise model with a specified breakpoint to the simulated data. Here, the breakpoint is at AOA 12.\n\nfitBreakpoint.fnc &lt;- function(Data = Data,\n                              Breakpoint = 12) {\n  with.lm &lt;- lm(UA ~ AOA + I(pmax(AOA-Breakpoint, 0)), Data)\n  pval &lt;- anova(with.lm)$'Pr(&gt;F)'[2]\n  dev &lt;- deviance(with.lm)\n  return(data.frame(pval = pval,\n                    dev = dev))\n}\n\nHere’s the outcome of running the two commands after each other:\n\ndat &lt;- read.csv(\"http://homeweb.unifr.ch/VanhoveJ/Pub/papers/CPH/DeKeyser2010Israel.csv\")\nstraight.lm &lt;- lm(GJT ~ AOA, dat)\nAOAvals &lt;- dat$AOA\nData &lt;- generateBreakpointData.fnc(AOA = AOAvals)\nfitBreakpoint.fnc(Data = Data, Breakpoint = 12)\n\n       pval      dev\n1 0.3074955 11913.35\n\n\nThe values that are returned are the p-value of the test whether the slope is different before from after the breakpoint and the model’s deviance (i.e. lack of fit).\nNext, we’re going to loop through all possible breakpoints while jotting down the p-values and deviances. I say ‘loop’, but in R you really want to vectorise your repeated calculations as much as possible. This is what the mapply line below does. The generateFitBreakpoint.fnc (among other programming-related stuff, I’m terrible at naming functions) takes two arguments, the lowest considered breakpoint value and the higher considered breakpoint value, and returns the p-value associated with the optimal breakpoint model as well as the location of said breakpoint.\n\ngenerateFitBreakpoint.fnc &lt;- function(MinBreakpoint = min(AOAvals) + 1,\n                                      MaxBreakpoint = 19) {\n  Data &lt;- generateBreakpointData.fnc()\n  Results &lt;- mapply(fitBreakpoint.fnc,\n                  Breakpoint = MinBreakpoint:MaxBreakpoint,\n                  MoreArgs=list(Data = Data))\n  Summary &lt;- data.frame(Breakpoint = MinBreakpoint:MaxBreakpoint,\n                    PValues = unlist(Results['pval',]),\n                    Deviances = unlist(Results['dev',]))\n  BestP = Summary$PValues[which.min(Summary$Deviances)]\n  BestBP = Summary$Breakpoint[which.min(Summary$Deviances)]\n  return(list(OptimalBP = BestBP,\n              OptimalP = BestP))\n}\n\nAll that is left to do now is to repeat that process a couple of thousand times and inspect the distribution of the recorded p-values. The code below simulates 10,000 datasets based on DeKeyser et al.’s Israel data, loops through each of them to determine the individual best-fitting breakpoint models (possible breakpoints between AOA 5 and 19) and saves the associated p-values.\n(I ran this code on a Linux terminal. It may not work on other systems, in which case you can let me know.)\n\n# Load 'parallel' package for faster computing.\nlibrary(parallel)\n# Distribute the job among a couple of CPU cores\ncl &lt;- makeCluster(detectCores() - 1)\n# Supply the other cores with the information necessary \nclusterExport(cl, c(\"dat\", \"straight.lm\", \"AOAvals\",\n                    \"generateBreakpointData.fnc\",\n                    \"fitBreakpoint.fnc\",\n                    \"generateFitBreakpoint.fnc\"))\n# Run 10000 times\nReplicateResults &lt;- parSapply(cl, 1:10000,\n                              function(i, ...) generateFitBreakpoint.fnc())\n# Save to disk\nwrite.csv(ReplicateResults, \"breakpointsimulationIsrael.csv\", row.names=FALSE)"
  },
  {
    "objectID": "posts/2014-08-20-adjusted-pvalues-breakpoint-regression/index.html#results",
    "href": "posts/2014-08-20-adjusted-pvalues-breakpoint-regression/index.html#results",
    "title": "Calibrating p-values in ‘flexible’ piecewise regression models",
    "section": "Results",
    "text": "Results\nIf the null hypothesis is true, p-values should be distributed uniformly between 0 and 1. As the following graph shows, however, by looping through several possible breakpoints, the p-value distribution is skewed towards 0. Out of 10,000 datasets simulated under the null, 1350 produced a significant but spurious non-linearity – more than double the nominal Type-I error rate of 5%.\n\n# Set graphical parameters\npar(mfrow=c(1,1), bty=\"l\", las=\"1\")\n# Read in data (and transpose for clarity's sake)\nresIsr &lt;- t(read.csv(\"breakpointsimulationIsrael.csv\"))\n# Draw histogram of p-values\nhist(resIsr[,2], col=\"lightblue\", breaks=seq(0,1,0.05),\n     xlim=c(0,1),\n     xlab=\"p value\", ylab=\"Frequency\",\n     main=\"Distribution of p-values under the null\\n(Israel data)\")\n\n\n\ntable(resIsr[,2] &lt;= 0.05)\n\n\nFALSE  TRUE \n 8650  1350 \n\n\nTo calibrate the observed p-value against this skewed distribution, we just look up the proportion of p-values generated under the null equal to or lower than the observed p-value. I observed a p-value of 0.62 after looping through breakpoints from AOA 5 till 19. 0.62 lies in the 95.6th quantile of the simulated p-value distribution (i.e. about 95% of p-values are lower than 0.62), so the calibrated p-value is 0.96 rather than 0.62:\n\nplot(ecdf(resIsr[,2]),\n     xlab=\"observed p-value\",\n     ylab=\"cumulative probability\",\n     main=\"Cumulative probability of p-values under the null\\n(Israel data)\")\nabline(v=0.62, col=\"lightblue\", lwd=3)\ntable(resIsr[,2] &lt;= 0.62)\n\n\nFALSE  TRUE \n  436  9564 \n\nabline(h=sum(resIsr[,2]&lt;=0.62)/10000, col=\"lightblue\", lwd=3)\n\n\n\n\nI’ll skip the code for simulating the p-value distribution for the North America analyses (just read in the other dataset and go through the same steps) and get straight to the simulated distribution:\n\n\n\n\n\nFor the North America data, the estimated Type-I error rate is about 10.4%, which is pretty similar to the one for the Israel data. (Not surprisingly, since the linear models for both datasets are remarkably similar.) The observed p-value of 0.047 lies in the 9.8th quantile of the simulated distribution, so the calibrated p-value is 0.098 rather than 0.047."
  },
  {
    "objectID": "posts/2014-08-20-adjusted-pvalues-breakpoint-regression/index.html#conclusions",
    "href": "posts/2014-08-20-adjusted-pvalues-breakpoint-regression/index.html#conclusions",
    "title": "Calibrating p-values in ‘flexible’ piecewise regression models",
    "section": "Conclusions",
    "text": "Conclusions\n\nLooping through different breakpoints increases the Type-I error rate of piecewise regression models.\nThe observed p-values can be calibrated against the distribution of p-values under the simulated null.\n\nThe distribution of simulated p-values will vary from dataset to dataset and will depend on the range of breakpoints considered (the smaller the range, the smaller the skew). Lastly, multiple comparison problem doesn’t only apply when following the automatised procedure suggested by Baayen: even if you look at the data first and then decide which breakpoint to fit, your p-values will be off if you don’t take into account on which breakpoints you could have decided had the data come out differently (see Gelman and Loken for a treatment of a more general version of this problem)."
  },
  {
    "objectID": "posts/2014-08-20-adjusted-pvalues-breakpoint-regression/index.html#software-versions",
    "href": "posts/2014-08-20-adjusted-pvalues-breakpoint-regression/index.html#software-versions",
    "title": "Calibrating p-values in ‘flexible’ piecewise regression models",
    "section": "Software versions",
    "text": "Software versions\n\ndevtools::session_info()\n\n─ Session info ───────────────────────────────────────────────────────────────\n setting  value\n version  R version 4.3.1 (2023-06-16)\n os       Ubuntu 22.04.3 LTS\n system   x86_64, linux-gnu\n ui       X11\n language en_US\n collate  en_US.UTF-8\n ctype    en_US.UTF-8\n tz       Europe/Zurich\n date     2023-08-25\n pandoc   3.1.1 @ /usr/lib/rstudio/resources/app/bin/quarto/bin/tools/ (via rmarkdown)\n\n─ Packages ───────────────────────────────────────────────────────────────────\n package     * version date (UTC) lib source\n cachem        1.0.6   2021-08-19 [2] CRAN (R 4.2.0)\n callr         3.7.3   2022-11-02 [1] CRAN (R 4.3.1)\n cli           3.6.1   2023-03-23 [1] CRAN (R 4.3.0)\n crayon        1.5.2   2022-09-29 [1] CRAN (R 4.3.1)\n devtools      2.4.5   2022-10-11 [1] CRAN (R 4.3.1)\n digest        0.6.29  2021-12-01 [2] CRAN (R 4.2.0)\n ellipsis      0.3.2   2021-04-29 [2] CRAN (R 4.2.0)\n evaluate      0.15    2022-02-18 [2] CRAN (R 4.2.0)\n fastmap       1.1.0   2021-01-25 [2] CRAN (R 4.2.0)\n fs            1.5.2   2021-12-08 [2] CRAN (R 4.2.0)\n glue          1.6.2   2022-02-24 [2] CRAN (R 4.2.0)\n htmltools     0.5.5   2023-03-23 [1] CRAN (R 4.3.0)\n htmlwidgets   1.6.2   2023-03-17 [1] CRAN (R 4.3.1)\n httpuv        1.6.11  2023-05-11 [1] CRAN (R 4.3.1)\n jsonlite      1.8.7   2023-06-29 [1] CRAN (R 4.3.1)\n knitr         1.39    2022-04-26 [2] CRAN (R 4.2.0)\n later         1.3.1   2023-05-02 [1] CRAN (R 4.3.1)\n lifecycle     1.0.3   2022-10-07 [1] CRAN (R 4.3.0)\n magrittr      2.0.3   2022-03-30 [1] CRAN (R 4.3.0)\n memoise       2.0.1   2021-11-26 [2] CRAN (R 4.2.0)\n mime          0.10    2021-02-13 [2] CRAN (R 4.0.2)\n miniUI        0.1.1.1 2018-05-18 [1] CRAN (R 4.3.1)\n pkgbuild      1.4.2   2023-06-26 [1] CRAN (R 4.3.1)\n pkgload       1.3.2.1 2023-07-08 [1] CRAN (R 4.3.1)\n prettyunits   1.1.1   2020-01-24 [2] CRAN (R 4.2.0)\n processx      3.8.2   2023-06-30 [1] CRAN (R 4.3.1)\n profvis       0.3.8   2023-05-02 [1] CRAN (R 4.3.1)\n promises      1.2.0.1 2021-02-11 [1] CRAN (R 4.3.1)\n ps            1.7.5   2023-04-18 [1] CRAN (R 4.3.1)\n purrr         1.0.1   2023-01-10 [1] CRAN (R 4.3.0)\n R6            2.5.1   2021-08-19 [2] CRAN (R 4.2.0)\n Rcpp          1.0.11  2023-07-06 [1] CRAN (R 4.3.1)\n remotes       2.4.2   2021-11-30 [2] CRAN (R 4.2.0)\n rlang         1.1.1   2023-04-28 [1] CRAN (R 4.3.0)\n rmarkdown     2.21    2023-03-26 [1] CRAN (R 4.3.0)\n rstudioapi    0.14    2022-08-22 [1] CRAN (R 4.3.0)\n sessioninfo   1.2.2   2021-12-06 [2] CRAN (R 4.2.0)\n shiny         1.7.4.1 2023-07-06 [1] CRAN (R 4.3.1)\n stringi       1.7.12  2023-01-11 [1] CRAN (R 4.3.1)\n stringr       1.5.0   2022-12-02 [1] CRAN (R 4.3.0)\n urlchecker    1.0.1   2021-11-30 [1] CRAN (R 4.3.1)\n usethis       2.2.2   2023-07-06 [1] CRAN (R 4.3.1)\n vctrs         0.6.3   2023-06-14 [1] CRAN (R 4.3.0)\n xfun          0.39    2023-04-20 [1] CRAN (R 4.3.0)\n xtable        1.8-4   2019-04-21 [1] CRAN (R 4.3.1)\n yaml          2.3.5   2022-02-21 [2] CRAN (R 4.2.0)\n\n [1] /home/jan/R/x86_64-pc-linux-gnu-library/4.3\n [2] /usr/local/lib/R/site-library\n [3] /usr/lib/R/site-library\n [4] /usr/lib/R/library\n\n──────────────────────────────────────────────────────────────────────────────"
  },
  {
    "objectID": "posts/2014-11-18-neglected-covariates/index.html",
    "href": "posts/2014-11-18-neglected-covariates/index.html",
    "title": "The curious neglect of covariates in discussions of statistical power",
    "section": "",
    "text": "The last couple of years have seen a spate of articles in (especially psychology) journals and on academic blogs discussing the importance of statistical power. But the positive effects of including a couple of well-chosen covariates deserve more appreciation in such discussions. My previous blog posts have been a bit on the long side, so I’ll try to keep this one short and R-free.\nDiscussions on the importance of statistical power serve the useful purpose of reminding researchers that experiments with 20 participants in each group are seriously underpowered. For example, a true effect typical of L2 research (d = 0.7 according to Plonsky & Oswald, 2014) has a less than 60% chance of being detected in such a study. Furthermore, underpowered studies aren’t as informative as many like to believe with respect to the estimated direction and size of any significant effect they do find (Gelman & Carlin, 2014). These discussions are extremely valuable, but the typical advice they often seem to entail (‘quintuple your sample size or don’t bother doing the study’) ignores a more practical solution to the power problem: using covariates.\nIn the vast majority of studies that I read, the researchers collect humongous amounts of ‘background’ data on their participants. For instance, in research on the cognitive correlates of bilingualism, researchers collect questionnaire data on language history (e.g., age of L2 acquisition, age of active daily bilingualism) and administer tests of verbal intelligence, working memory etc. - all of this is in addition to the data that really interests them (typically some measure of cognitive control). These data are then summarised and neatly tabulated to demonstrate that the groups compared are ‘matched’ for working memory etc., so that it can be argued that whatever differences in cognitive control are found between the groups can’t be explained through differences in cognitive control. Working memory is then ignored as researchers go about their ANOVA and t-test business.\nTo me, this is statistical sacrilege. Surely, if it’s important to demonstrate that working memory capacity (to stick with that example) is comparable between the groups compared, it’s because differences in working memory tend to be related to differences in cognitive control? While the groups may have comparable average working memory capacities, their will be substantial within-group variability in working memory capacity that is likely linked to within-group variability in cognitive control. If we include working memory capacity as a covariate (or control variable), we’ll be able to explain some of the error variance in cognitive control. This, in turn, will improve the precision with which the effects of interest are estimated, i.e. greater statistical power.\nThe important thing here is that including covariates that are likely to be related to differences in the outcome variable is advantageous even if the groups are comparable with respect to them. By condemning potential ‘confound variables’ to summary tables, researchers are leaving money on the table – if they’re important enough to be collected, why not make full use of them?\nNow, taking into account covariates in the main analyses means that researchers will have to abandon their simple statistical tools (t-tests, correlations, χ²-tests) in favour of multivariate statistics (multiple regression, logistic regression, or classification algorithms), which could take some getting used to. But they can still focus on the effect they’re interested in and they shouldn’t be expected to discuss the effects of the covariates in any detail apart from mentioning that they were included in the analyses.\nI’m glossing over some details here, but the general point that researchers are shooting themselves in the foot powerwise by not making use of the rich data they collect in the actual analyses merits more attention."
  },
  {
    "objectID": "posts/2018-08-12-consider-generalisability/index.html",
    "href": "posts/2018-08-12-consider-generalisability/index.html",
    "title": "Consider generalisability",
    "section": "",
    "text": "A good question to ask yourself when designing a study is, “Who and what are any results likely to generalise to?” Generalisability needn’t always be a priority when planning a study. But by giving the matter some thought before collecting your data, you may still be able to alter your design so that you don’t have to smother your conclusions with ifs and buts if you do want to draw generalisations.\nThe generalisability question is mostly cast in terms of the study’s participants: Would any results apply just to the participants themselves or to some wider population, and if so, to which one? Important as this question is, this blog post deals with a question that is asked less often but is equally crucial: Would any results apply just to the materials used in the study or might they stand some chance of generalising to different materials?"
  },
  {
    "objectID": "posts/2018-08-12-consider-generalisability/index.html#generalising-across-listeners-texts-and-speakers",
    "href": "posts/2018-08-12-consider-generalisability/index.html#generalising-across-listeners-texts-and-speakers",
    "title": "Consider generalisability",
    "section": "Generalising across listeners, texts, and speakers",
    "text": "Generalising across listeners, texts, and speakers\nTo make this more concrete, consider the following example (inspired by Schüppert, Hilton, Gooskens & van Heuven, 2013, Stavelsebortfall i modern danska). Spoken Danish is notoriously difficult to understand, and Swedes have considerably more difficulties understanding Danish than Danes do understanding Swedish. One possible contributing factor is that Danish is spoken more quickly than its sibling languages. This may affect its intelligibility in two ways. First, a higher speech rate means that more propositions are communicated in the same time span, which may cause processing difficulties for unaccustomed listeners. Second, higher speech rates tend to lead to an increase of connected speech features. This includes assimilation (e.g., a careful pronunciation of English broadcast is ‘bro:d ka:st’, but the ‘d’ tends to become a ‘g’ if it’s pronounced more quickly, i.e., ‘bro:g ka:st’) and the elision of sounds and syllables (e.g., German haben tends to become ham).\nTo tease apart these ways in which a higher speech rate may compound Swedes’ comprehension difficulties with Danish, you could design an experiment along the following lines. Have a Dane read a text slowly and carefully. Then have her read the same text more naturally. By slowing down or speeding up these texts, four conditions can be created:\n\nslow, few connected speech features (original recording)\nquick, many connected speech features (original recording)\nslow, many connected speech features (slowed down version of (2))\nquick, few connected speech features (sped-up version of (1))\n\nThen, you might assign some Swedes to each of these four recordings and ask them a couple of questions to test their comprehension.\nFirst of all, note that you wouldn’t assign just one Swede to each recording: if you were to observe differences in the text’s intelligibility between the four conditions, this could plausibly be related to differences between the listeners more than differences between the intelligibility of fast and slow speech. That is, the results might strongly be affected by listener idiosyncrasies. So you assign a decent number of Swedes to each recording so that these idiosyncrasies would likely cancel out and your results stand a chance of perhaps applying to Swedes listening to Danish in general. (I won’t go into matters of statistical precision or power, though those would of course also be relevant.)\nBut by the same token, it is possible that any difference in the intelligibility of the four recordings may be idiosyncratic to the specific text that was recorded: perhaps speed or connected speech differences affect the intelligibility of some texts considerably more, or less, than that of this particular text. Or perhaps this text was so difficult or easy to understand that speech rate and connected speech can hardly affect its intelligibility to begin with. To guard against this possibility, you may want to consider including several texts in your study. If it’s feasible, you could have each participant listen to, say, eight texts. Ideally, each participant would be tested in all four conditions (two texts per condition), so that you can capitalise on the increased statistical precision that within-participants designs afford, and the same text would appear in different conditions across participants. This design would represent a substantial improvement over the one-text between-participants design.\nBut we’re still not there yet: What about the possibility that any result we find would only apply to our one Danish speaker? Perhaps this speaker happens to use few connected speech features when speaking quickly, or vice versa? When in doubt, it may be a good idea to consider having multiple speakers record a couple of texts. For instance, we could have eight Danes record eight texts in four conditions, for a total of 256 recordings.\nThe participants’ time is, of course, limited, and there would be little point in forcing them to listen to all 256 recordings - if only because the same texts reappear 32 times. But instead you could have each of them listen to the eight different texts (two per condition), with a different speaker each time, and vary the assignment of text to speaker and to condition between participants. If this experiment is analysed appropriately (e.g., using a mixed-effect model), you’d have more confidence that any result generalises than if you only had one text and one speaker."
  },
  {
    "objectID": "posts/2018-08-12-consider-generalisability/index.html#even-larger-samples-of-texts-and-listeners",
    "href": "posts/2018-08-12-consider-generalisability/index.html#even-larger-samples-of-texts-and-listeners",
    "title": "Consider generalisability",
    "section": "Even larger samples of texts and listeners",
    "text": "Even larger samples of texts and listeners\nIn the example above, each listener is exposed to each text and to each speaker. This may be feasible when you ‘only’ have eight texts and eight speakers, but what if you had fifty texts and 30 speakers? The key thing to appreciate is that you don’t have to have each listener listen to each text or to each speaker. Instead, you could work out a design in which each listener listens to only, say, eight different texts (two per condition). The speaker/text combinations could then simply be randomly sampled from the speaker/text combinations available, with the stipulation that no speaker or text is chosen twice (to avoid unwanted adaptation effects). You could then still analyse the data using the same mixed-effect model but the results would be even less likely to be affected by speaker- or text-related idiosyncrasies."
  },
  {
    "objectID": "posts/2018-08-12-consider-generalisability/index.html#the-upshot",
    "href": "posts/2018-08-12-consider-generalisability/index.html#the-upshot",
    "title": "Consider generalisability",
    "section": "The upshot",
    "text": "The upshot\nBy using a larger number of texts or speakers, you could increase the generalisability of any findings, in much the same way that larger participant samples tend to yield more generalisable findings. If you can’t realistically expose all participants to all texts or speakers, you can expose each of them to only a subset of texts or speakers instead."
  },
  {
    "objectID": "posts/2018-08-12-consider-generalisability/index.html#suggested-reading",
    "href": "posts/2018-08-12-consider-generalisability/index.html#suggested-reading",
    "title": "Consider generalisability",
    "section": "Suggested reading",
    "text": "Suggested reading\nI learnt a lot by reading Westfall et al.’s (2014) Statistical power and optimal design in experiments in which samples of participants respond to samples of stimuli."
  },
  {
    "objectID": "posts/2015-11-17-scatterplot-trendline/index.html",
    "href": "posts/2015-11-17-scatterplot-trendline/index.html",
    "title": "Drawing a scatterplot with a non-linear trend line",
    "section": "",
    "text": "This blog post is a step-by-step guide to drawing scatterplots with non-linear trend lines in R. It is geared towards readers who don’t have much experience with drawing statistical graphics and who aren’t entirely happy with their attempts in Excel."
  },
  {
    "objectID": "posts/2015-11-17-scatterplot-trendline/index.html#background",
    "href": "posts/2015-11-17-scatterplot-trendline/index.html#background",
    "title": "Drawing a scatterplot with a non-linear trend line",
    "section": "Background",
    "text": "Background\nWhen flicking through an issue of a journal on language research or when attending a conference, chances are you’ll harvest a fair number of unclear, uninformative, for-the-record-only graphs. This is unfortunate, as a good graph serves two important purposes:\nFirst, it can alert the researcher to aspects of the data that aren’t obvious from a purely numerical description, such as outliers, coding errors, non-linearities, and skewed distributions.\nSecond, while a good graph can be difficult to construct, it should – by virtue of being a good graph – be straightforward to comprehend with little guidance on the part of the author or presenter. In my view, a good graph provides a reasonably accurate picture of the main patterns in the data and of how the raw data relate to these patterns – i.e. is there a lot of variation or do the individual data points map closely onto the patterns? This makes graphs – rather than numerical descriptions or significance tests – essential for presenting research results to an audience, especially one that may not be familiar with advanced statistical techniques or even one that may not be entirely comfortable with concepts such as, say, standard deviations or confidence intervals (any casual definition of either of which is almost certainly wrong).\nSince knowing how to draw a good graph is bound to be a useful skill for our students – whether they’ll become researchers themselves or will have to communicate research data to policy makers and teachers – I’ve decided I’m going to stress it more in my teaching. This blog post is a step-by-step solution to an exercise I gave my students. While they were free to use whatever program they wanted, I’m going to use R in this solution. The main reason is that I’m most familiar with it myself. In addition, the plots it produces look pretty clean and professional (I often find Excel graphs to be pig ugly, but that’s me), and it’s easier to tell you which commands you have to type at the R prompt than what you have to select and click in Excel."
  },
  {
    "objectID": "posts/2015-11-17-scatterplot-trendline/index.html#installing-r-and-ggplot2",
    "href": "posts/2015-11-17-scatterplot-trendline/index.html#installing-r-and-ggplot2",
    "title": "Drawing a scatterplot with a non-linear trend line",
    "section": "Installing R and ggplot2",
    "text": "Installing R and ggplot2\n\nGo to http://r-project.org. Download and install R. R is a free but powerful environment for conducting statistical analyses and drawing graphs.\nGo to http://www.rstudio.com. Download and install RStudio. R itself is run on a command line; RStudio provides a more organised user interface for R.\nOpen RStudio. At the prompt (bottom left, the line starting with ‘&gt;’), type the following command:\n\n\ninstall.packages(\"ggplot2\")\n\nThis installs a (free) add-on package, ggplot2, that provides powerful plotting capabilities. Make sure you type (or copy-paste) the command verbatim – if you type install.package(\"ggplot2\") (without the s), R will return an error. Likewise if you type INSTALL.PACKAGES(\"ggplot2\") (in caps)."
  },
  {
    "objectID": "posts/2015-11-17-scatterplot-trendline/index.html#reading-in-data",
    "href": "posts/2015-11-17-scatterplot-trendline/index.html#reading-in-data",
    "title": "Drawing a scatterplot with a non-linear trend line",
    "section": "Reading in data",
    "text": "Reading in data\nThe data for this exercise are available from https://janhove.github.io/datasets/sinergia.csv. Download this file and save it locally. To import it into R, enter the following command at the prompt (again verbatim):\n\ndat &lt;- read.csv(file.choose())\n\nA window will now open where you can navigate to the directory where you’ve saved the dataset. Select and open the dataset.\nThe dataset is now known in R as dat. To get an outline of the dataset, you can run the str() command with dat as its argument (the lines beginning with ‘##’ show the output of the command; you don’t have to type this yourself):\n\nstr(dat)\n\n'data.frame':   163 obs. of  10 variables:\n $ Subject: int  64 78 134 230 288 326 447 527 545 550 ...\n $ Spoken : int  23 19 24 12 12 20 22 9 19 22 ...\n $ Written: int  25 20 12 26 9 24 25 15 14 24 ...\n $ Sex    : chr  \"female\" \"female\" \"male\" \"male\" ...\n $ Age    : int  27 47 33 84 28 32 53 71 16 52 ...\n $ NrLang : int  4 3 3 4 4 3 3 2 3 3 ...\n $ BWDS   : int  7 8 6 7 9 6 8 8 4 5 ...\n $ WST    : num  34 33 32 37 35 35 34 34 29 35 ...\n $ Raven  : int  28 26 24 20 24 19 16 15 21 15 ...\n $ English: num  2.394 -0.0314 -0.4751 0.4915 1.7437 ..."
  },
  {
    "objectID": "posts/2015-11-17-scatterplot-trendline/index.html#drawing-a-scatterplot",
    "href": "posts/2015-11-17-scatterplot-trendline/index.html#drawing-a-scatterplot",
    "title": "Drawing a scatterplot with a non-linear trend line",
    "section": "Drawing a scatterplot",
    "text": "Drawing a scatterplot\nThe goal is to visualise the relationship between the self-explanatory Age variable and Raven, which contains the participants’ results on a cognitive task. The workhorse plot for showing the relationship between two continuous variables such as these is the scatterplot.\nThe basic idea behind a scatterplot is simple: each pair of (Age, Raven) observations is shown in an XY plane. There’s no need to group together participants by decade; you don’t have to compute the average Raven score per age group etc. – you just show the data you have.\nThere are a couple of ways to draw a scatterplot in R. For this tutorial we’ll use the functions in the ggplot2 package. To activate these functions, run the following command:\n\nlibrary(ggplot2)\n\nBy default, ggplot2 draws plots on a grey background. Personally, I prefer a white background, so I tell ggplot2 to switch its default theme to black and white:\n\ntheme_set(theme_bw())\n\nTo draw any plot, we need to tell the program where it can find the data and which information should go where. In this case, the data can be found in the dataset called dat, and it makes sense to plot the Age information along the x axis and the Raven scores along the y axis. (As a rule of thumb, if it’s more likely that one variable affects another variable than vice versa, put the first variable along the x axis.) All of this is defined in the ggplot function (first two lines). The third line then specifies what we want to do with these data. Here, we want to plot a point for each pair of (Age, Raven) observations. The lay-out of this graph is stored as p (this is what the &lt;- in the first line does). To display the graph, simply type p at the prompt. The hashes introduce comments that are ignored by the program but that are useful for documenting what you’re doing.\n\np &lt;- ggplot(data = dat,                # specify dataset\n            aes(x = Age, y = Raven)) + # Age on x-, Raven on y-axis\n  geom_point(pch = 1)                  # plot points (pch = 1: circles, type '?pch' for other options)\np\n\n\n\n\nThis scatterplot strongly suggests the presence of a non-linear age effect: Raven performance increases up through about age 20-25 and then decreases with age. Additionally, there don’t seem to be any wildly outlying points that may be indicative of coding errors and the like. Both of these points are useful to know: the first because it indicates that common tools such as linear regression or correlations would mischaracterise the relationship between age and Raven score; the second because such outliers would’ve require us to go back to the raw data and check whether they make sense.\nWhen this scatterplot is to be used in a publication or for a presentation, it may need a bit of polishing, though. First, while the axis labels are pretty straightforward here, that’s not always the case. To change them, we can take the plot object, p, and explicitly add an xlab() and a ylab() argument to it:\n\np &lt;- p +\n  xlab(\"Age (years)\") +            # add label for x axis\n  ylab(\"Raven's Matrices (score)\") # add label for y axix\np\n\n\n\n\nSecond, it may be useful to highlight the non-linear pattern in the data. To this end, we can add a scatterplot smoother to the plot. The algorithm behind such a smoother essentially fits a number of best-fitting curves to subsets of the data and then glues them together:\n\nq1 &lt;- p +\n  geom_smooth() # add scatterplot smoother\nq1\n\n`geom_smooth()` using method = 'loess' and formula = 'y ~ x'\n\n\n\n\n\nThe message informs us that we didn’t specify any one algorithm for drawing the smoother, so it defaulted to the loess algorithm. The blue curve is the scatterplot smoother; the grey band about it is a 95% confidence band. For this exercise, I prefer to turn this confidence band off (se = FALSE) as it moves us into the realm of inferential statistics – for now, I’d rather stick to plotting and descriptive statistics. The appearence of the curve itself can be changed, too, e.g. by making it a bit thicker (width) and colouring it black.\nAdditionally, we can change the default smoothing algorithm. One alternative for drawing non-linear curves is to use a generalised additive model (see this blog post). The logic behind it is pretty similar to the one behind loess curves, though:\nUpdate (2023-08-25): When I wrote this blog post back in 2015, the parameter that governed the line width was called width instead of linewidth.\n\nq2 &lt;- p +\n  geom_smooth(colour = \"black\",   # black line\n              linewidth = 1,      # slightly thicker\n              se = FALSE,         # turn off confidence band\n              method = \"gam\",     # use 'gam' instead of default (loess)\n              formula = y ~ s(x)) # specify gam formula\nq2\n\n\n\n\nBy default, the gam function estimates the form of the curve by fitting so-called thin-plate regression splines. The details don’t matter here, but a disadvantage of this default is that it assumes that the curve has the same degree of ‘wiggliness’ everywhere. What this means is that the increased rate of deterioration around age 50 and the decreased rate of deterioration around age 60 needn’t be ‘there’: they may just be the result of the function trying to accommodate the fact that a constant degree of wiggliness was implicitly assumed. In other words, it may add wiggles to the end of the curve (where there may not be any) in order to be able to add wiggles to the start of the curve (where they’re clearly needed). If you have enough data, you can use so-called ‘adaptive’ smooths instead:\n\nq3 &lt;- p +\n  geom_smooth(colour = \"black\",\n              linewidth = 1,\n              se = FALSE,\n              method = \"gam\",\n              formula = y ~ s(x, bs = \"ad\")) # use 'adaptive' spline\nq3\n\n\n\n\nThis graph succeeds in highlighting both the average age trend in the data and showing the scatter about that trend and doesn’t sweep any unbecoming data under the rug. I’m also pretty confident that this graph can be interpreted by experts and laypeople alike: while they may not know the algorithm behind the curve, it’s the meaning of the curve that’s of interest. And its meaning is obvious."
  },
  {
    "objectID": "posts/2015-11-17-scatterplot-trendline/index.html#software-versions",
    "href": "posts/2015-11-17-scatterplot-trendline/index.html#software-versions",
    "title": "Drawing a scatterplot with a non-linear trend line",
    "section": "Software versions",
    "text": "Software versions\n\ndevtools::session_info()\n\n─ Session info ───────────────────────────────────────────────────────────────\n setting  value\n version  R version 4.3.1 (2023-06-16)\n os       Ubuntu 22.04.3 LTS\n system   x86_64, linux-gnu\n ui       X11\n language en_US\n collate  en_US.UTF-8\n ctype    en_US.UTF-8\n tz       Europe/Zurich\n date     2023-08-25\n pandoc   3.1.1 @ /usr/lib/rstudio/resources/app/bin/quarto/bin/tools/ (via rmarkdown)\n\n─ Packages ───────────────────────────────────────────────────────────────────\n package     * version date (UTC) lib source\n cachem        1.0.6   2021-08-19 [2] CRAN (R 4.2.0)\n callr         3.7.3   2022-11-02 [1] CRAN (R 4.3.1)\n cli           3.6.1   2023-03-23 [1] CRAN (R 4.3.0)\n colorspace    2.1-0   2023-01-23 [1] CRAN (R 4.3.0)\n crayon        1.5.2   2022-09-29 [1] CRAN (R 4.3.1)\n devtools      2.4.5   2022-10-11 [1] CRAN (R 4.3.1)\n digest        0.6.29  2021-12-01 [2] CRAN (R 4.2.0)\n dplyr         1.1.2   2023-04-20 [1] CRAN (R 4.3.0)\n ellipsis      0.3.2   2021-04-29 [2] CRAN (R 4.2.0)\n evaluate      0.15    2022-02-18 [2] CRAN (R 4.2.0)\n fansi         1.0.4   2023-01-22 [1] CRAN (R 4.3.1)\n farver        2.1.1   2022-07-06 [1] CRAN (R 4.3.0)\n fastmap       1.1.0   2021-01-25 [2] CRAN (R 4.2.0)\n fs            1.5.2   2021-12-08 [2] CRAN (R 4.2.0)\n generics      0.1.3   2022-07-05 [1] CRAN (R 4.3.0)\n ggplot2     * 3.4.2   2023-04-03 [1] CRAN (R 4.3.0)\n glue          1.6.2   2022-02-24 [2] CRAN (R 4.2.0)\n gtable        0.3.3   2023-03-21 [1] CRAN (R 4.3.0)\n htmltools     0.5.5   2023-03-23 [1] CRAN (R 4.3.0)\n htmlwidgets   1.6.2   2023-03-17 [1] CRAN (R 4.3.1)\n httpuv        1.6.11  2023-05-11 [1] CRAN (R 4.3.1)\n jsonlite      1.8.7   2023-06-29 [1] CRAN (R 4.3.1)\n knitr         1.39    2022-04-26 [2] CRAN (R 4.2.0)\n labeling      0.4.2   2020-10-20 [1] CRAN (R 4.3.0)\n later         1.3.1   2023-05-02 [1] CRAN (R 4.3.1)\n lattice       0.21-8  2023-04-05 [4] CRAN (R 4.3.0)\n lifecycle     1.0.3   2022-10-07 [1] CRAN (R 4.3.0)\n magrittr      2.0.3   2022-03-30 [1] CRAN (R 4.3.0)\n Matrix        1.6-0   2023-07-08 [4] CRAN (R 4.3.1)\n memoise       2.0.1   2021-11-26 [2] CRAN (R 4.2.0)\n mgcv          1.9-0   2023-07-11 [4] CRAN (R 4.3.1)\n mime          0.10    2021-02-13 [2] CRAN (R 4.0.2)\n miniUI        0.1.1.1 2018-05-18 [1] CRAN (R 4.3.1)\n munsell       0.5.0   2018-06-12 [1] CRAN (R 4.3.0)\n nlme          3.1-162 2023-01-31 [4] CRAN (R 4.2.2)\n pillar        1.9.0   2023-03-22 [1] CRAN (R 4.3.0)\n pkgbuild      1.4.2   2023-06-26 [1] CRAN (R 4.3.1)\n pkgconfig     2.0.3   2019-09-22 [2] CRAN (R 4.2.0)\n pkgload       1.3.2.1 2023-07-08 [1] CRAN (R 4.3.1)\n prettyunits   1.1.1   2020-01-24 [2] CRAN (R 4.2.0)\n processx      3.8.2   2023-06-30 [1] CRAN (R 4.3.1)\n profvis       0.3.8   2023-05-02 [1] CRAN (R 4.3.1)\n promises      1.2.0.1 2021-02-11 [1] CRAN (R 4.3.1)\n ps            1.7.5   2023-04-18 [1] CRAN (R 4.3.1)\n purrr         1.0.1   2023-01-10 [1] CRAN (R 4.3.0)\n R6            2.5.1   2021-08-19 [2] CRAN (R 4.2.0)\n Rcpp          1.0.11  2023-07-06 [1] CRAN (R 4.3.1)\n remotes       2.4.2   2021-11-30 [2] CRAN (R 4.2.0)\n rlang         1.1.1   2023-04-28 [1] CRAN (R 4.3.0)\n rmarkdown     2.21    2023-03-26 [1] CRAN (R 4.3.0)\n rstudioapi    0.14    2022-08-22 [1] CRAN (R 4.3.0)\n scales        1.2.1   2022-08-20 [1] CRAN (R 4.3.0)\n sessioninfo   1.2.2   2021-12-06 [2] CRAN (R 4.2.0)\n shiny         1.7.4.1 2023-07-06 [1] CRAN (R 4.3.1)\n stringi       1.7.12  2023-01-11 [1] CRAN (R 4.3.1)\n stringr       1.5.0   2022-12-02 [1] CRAN (R 4.3.0)\n tibble        3.2.1   2023-03-20 [1] CRAN (R 4.3.0)\n tidyselect    1.2.0   2022-10-10 [1] CRAN (R 4.3.0)\n urlchecker    1.0.1   2021-11-30 [1] CRAN (R 4.3.1)\n usethis       2.2.2   2023-07-06 [1] CRAN (R 4.3.1)\n utf8          1.2.3   2023-01-31 [1] CRAN (R 4.3.1)\n vctrs         0.6.3   2023-06-14 [1] CRAN (R 4.3.0)\n withr         2.5.0   2022-03-03 [2] CRAN (R 4.2.0)\n xfun          0.39    2023-04-20 [1] CRAN (R 4.3.0)\n xtable        1.8-4   2019-04-21 [1] CRAN (R 4.3.1)\n yaml          2.3.5   2022-02-21 [2] CRAN (R 4.2.0)\n\n [1] /home/jan/R/x86_64-pc-linux-gnu-library/4.3\n [2] /usr/local/lib/R/site-library\n [3] /usr/lib/R/site-library\n [4] /usr/lib/R/library\n\n──────────────────────────────────────────────────────────────────────────────"
  },
  {
    "objectID": "posts/2017-11-20-more-informative-replications/index.html",
    "href": "posts/2017-11-20-more-informative-replications/index.html",
    "title": "Suggestions for more informative replication studies",
    "section": "",
    "text": "In recent years, psychologists have started to run large-scale replications of seminal studies. For a variety of reasons, which I won’t go into, this welcome development hasn’t quite made it to research on language learning and bi- and multilingualism. That said, I think it can be interesting to scrutinise how these large-scale replications are conducted. In this blog post, I take a closer look at a replication attempt by O’Donnell et al. with some 4,500 participants that’s currently in press at Psychological Science and make five suggestions as to how I think similar replications could be designed to be even more informative."
  },
  {
    "objectID": "posts/2017-11-20-more-informative-replications/index.html#a-replication-attempt-of-professor-priming",
    "href": "posts/2017-11-20-more-informative-replications/index.html#a-replication-attempt-of-professor-priming",
    "title": "Suggestions for more informative replication studies",
    "section": "A replication attempt of “professor priming”",
    "text": "A replication attempt of “professor priming”\nA purported phenomenon that’s come under particular scrutiny in recent years is that it is possible to predictably influence people’s behaviour by ‘priming’ them with a concept related to the desired behaviour. In so-called “professor priming”, you would make your participants engage with the concept of intelligence by having them write down how they imagined their daily life as a university professor would look like. Relative to participants who had to engage with the concept of stupidity (by imagining their daily life as football hooligans), participants primed with intelligence would then perform better on a trivia quiz. (I don’t really understand what trivia quizzing has to do with intelligence, but you get the idea.)\nIt was this specific example that O’Donnell et al. sought to replicate in 23 labs across the globe with some 4,500 participants in total. About half of the participants imagined their daily life as professors, and the rest as hooligans. Afterwards, they completed an ostensibly unrelated trivia quiz with 30 questions.\n“Professor priming” sounds literally unbelievable, and, indeed, O’Donnell et al.’s replication did not yield much support for it. (Replication attempts by Ap Dijksterhuis, the researcher who first suggested the existence of professor priming, also failed. Or they only worked in men. Which is to say that the original result couldn’t be replicated.) In their discussion, the replication authors point out that two-thirds of the participants in fact suspected a link between the two ostensibly unrelated tasks, and duly discuss a number of factors that might explain the discrepancy between the original professor priming study and the replication. These include the possibility that “professor” and “hooligan” are associated with different concepts across cultures and that some trivia items could work better in some cultures than in others.\nWhen reading about this replication attempt, I ventured on Twitter - that oasis of nuance - that it struck me as a waste of time. That wasn’t to say that I thought it wasn’t carried well, but to me it seemed that both the finding itself (zilch) as well as the potential mitigating factors were fairly predictable. But that’s hardly a constructive attitude. So, in the hopes that they are useful for future replication attempts, here are some post-hoc suggestions for getting more out of 4,500 participants. I’ll assume throughout that “professor priming” might just be a real phenomenon."
  },
  {
    "objectID": "posts/2017-11-20-more-informative-replications/index.html#five-suggestions-for-getting-more-out-of-4500-participants",
    "href": "posts/2017-11-20-more-informative-replications/index.html#five-suggestions-for-getting-more-out-of-4500-participants",
    "title": "Suggestions for more informative replication studies",
    "section": "Five suggestions for getting more out of 4,500 participants",
    "text": "Five suggestions for getting more out of 4,500 participants\n\n(1) A larger pool of stimuli\nOne possible factor that could explain O’Donnell et al.’s null result is that “professors” and “hooligans” don’t evoke the same concepts now across the globe as they did at the time and place of the original study (The Netherlands in the 1990s). One solution to this would be to use more than one stimulus that’s associated with the concept of intelligence and more than one stimulus associated with the concept of stupidity. For instance, you could have a whole pool of brainy stereotypes and of brawny stereotypes. Each participant assigned to the “brainy” condition is then in turn assigned randomly to one of the brainy stereotypes, and similarly for the “brawny” condition.\nThe advantage of operationalising intelligence and stupidity using multiple constructs is generalisability. Arguably, “professor priming” is but one instantiation of “intelligence priming” - that is, it’s not so much that imagining your life as a professor would boost your trivia performance as engaging with the concept of intelligence is. By using multiple operationalisations, you put yourself in a position to claim that any positive or negative results aren’t just due to your choice of stereotype. Using partial pooling (mixed-effects models), you can get an estimate of how well each operationalisation “works” without falling victim to multiple comparisons.\nIf you want to run a more exact, highly powered replication of the original while at the same time exploring the generalisability of these findings, you could still use the “professor” and “hooligan” primes for 2,000 participants and, say, ten other primes for 250 participants each.\n\n\n(2) A larger pool of trivia items\nSimilarly, instead of presenting the same 30 trivia items to all participants, you could have a larger pool of trivia items from which you present 30 to each participant. This again reduces the risk that you happened to select those trivia items that are or aren’t sensitive to priming. Using mixed effects modelling, it should be fairly easy to explore which trivia items are answered more correctly following professor priming than following hooligan priming. (I don’t quite get the use of meta-analytic techniques in this replication attempt seeing as all data are available, but this may be inherent to the format.)\n(Further reading concerning this suggestion: Westfall et al. (2015).)\n\n\n(3) Multiple outcomes\nI understand that earlier replication attempts of professor priming also used IQ tests as the follow-up task, as opposed to trivia quizzes. In the same spirit of generalisability, you could have some or all participants complete a slimmed-down IQ test.\n\n\n(4) A partial pretest–posttest design (a.k.a. a Solomon four-group design)\nAssuming professor priming affects trivia quizzing, it’s natural to ask which aspects of trivia quizzing are improved by it. In addition to using a larger pool of trivia items and analysing them using mixed effects models, you could also imagine having, say, 25% of your participants complete the quiz before and after the priming intervention. (I wouldn’t do this with all participants for fear that pretest sensitisation becomes another mitigating factor.) That way, you can start to answer questions such as In what respect did the participants’ performance improve? Was it particular questions they got right afterwards, or did performance improve across all items? and Does pretest sensitisation mitigate the effect?\n\n\n(5) A pilot phase?\nTwo-thirds of the participants claimed to have suspected a link between the priming part of the experiment and the quizzing part. This finding was probably more difficult to anticipate. But perhaps future replication studies may want to use some proportion of their data, say, the initial 5%, as pilot data, which is to be analysed in full. Any striking patterns that could call into question the reliability of the replication findings, such as participant awareness, could be discussed and possibly ironed out at that stage."
  },
  {
    "objectID": "posts/2017-11-20-more-informative-replications/index.html#summary",
    "href": "posts/2017-11-20-more-informative-replications/index.html#summary",
    "title": "Suggestions for more informative replication studies",
    "section": "Summary",
    "text": "Summary\nThe informativeness of large replication attempts could be improved even further by carrying out both a maximally exact replication and looser replications that seek to generalise the postulated effect. Using mixed-effects models, you can then analyse how well the finding replicates more or less exactly and whether this generalises to other stimuli and outcomes. This would make positive findings more informative since you can start to explore potency differences between stimuli and susceptibility differences between outcomes. But it would also make negative findings more informative, since it would be more difficult to suggest that a null finding is due to ill-chosen stimuli or items and that the effect really does exist nonetheless."
  },
  {
    "objectID": "posts/2019-01-14-clustered-data-different-results/index.html",
    "href": "posts/2019-01-14-clustered-data-different-results/index.html",
    "title": "Guarantees in the long run vs. interpreting the data at hand: Two analyses of clustered data",
    "section": "",
    "text": "An analytical procedure may have excellent long-run properties but still produce nonsensical results in individual cases. I recently encountered a real-life illustration of this, but since those data aren’t mine, I’ll use simulated data with similar characteristics for this blog post."
  },
  {
    "objectID": "posts/2019-01-14-clustered-data-different-results/index.html#the-data",
    "href": "posts/2019-01-14-clustered-data-different-results/index.html#the-data",
    "title": "Guarantees in the long run vs. interpreting the data at hand: Two analyses of clustered data",
    "section": "The data",
    "text": "The data\nThe illustration occurred for a study with clustered data (pupils in classes) in which there were only one or two clusters per condition. In the dataset that I simulated, there are three classes that were assigned in their entirety to either the control condition (1 class) or the intervention condition (2 classes):\n\n# Read in data\nd &lt;- read.csv(\"http://homeweb.unifr.ch/VanhoveJ/Pub/Data/simulated_clustered_data.csv\")\n\n# Pupils per class and per condition\nxtabs(~ class + group_class, data = d)\n\n     group_class\nclass control intervention\n    1      17            0\n    2       0           13\n    3       0           18\n\n\nCrucially, the clusters in the data need to be taken into consideration during the analysis (see Vanhove 2015). Having just one or two clusters per condition is far from ideal, but this dataset is, in principle, analysable. Let’s start by plotting the data:\n\nlibrary(tidyverse)\nggplot(d,\n       aes(x = factor(class),\n           y = outcome,\n           fill = group_class)) +\n  geom_boxplot(outlier.shape = NA) +\n  geom_point(position = position_jitter(width = 0.1)) +\n  xlab(\"class\")\n\n\n\n\nFigure 1. Simulated data for a cluster-randomised experiment in which one class (= cluster) was assigned to the control condition and two were assigned to the intervention condition."
  },
  {
    "objectID": "posts/2019-01-14-clustered-data-different-results/index.html#analysis-1-a-hierarchical-model",
    "href": "posts/2019-01-14-clustered-data-different-results/index.html#analysis-1-a-hierarchical-model",
    "title": "Guarantees in the long run vs. interpreting the data at hand: Two analyses of clustered data",
    "section": "Analysis 1: A hierarchical model",
    "text": "Analysis 1: A hierarchical model\nOne option is to analyse these data in a hierarchical (mixed-effects) model in which random effects are used to account for class-to-class variability. Based on the present dataset, this variability is in fact estimated to be non-existent: the estimated standard deviation of the distribution of class effects is essentially 0 (0.00003). This seems implausible - not least of all because I simulated data with a class effect. The estimated intervention effect, meanwhile is -0.24 with a standard error of 0.31, which means that the intervention effect is not significant in this analysis (t(1) = 0.77, p = 0.58).\n\nlibrary(nlme)\nlme_fit &lt;- lme(outcome ~ group_class, random =~ 1|class, data = d)\nsummary(lme_fit)\n\nLinear mixed-effects model fit by REML\n  Data: d \n    AIC   BIC logLik\n  146.2 153.5 -69.11\n\nRandom effects:\n Formula: ~1 | class\n        (Intercept) Residual\nStdDev:   3.079e-05    1.016\n\nFixed effects:  outcome ~ group_class \n                          Value Std.Error DF t-value p-value\n(Intercept)              0.1793    0.2463 45  0.7278  0.4705\ngroup_classintervention -0.2370    0.3065  1 -0.7734  0.5809\n Correlation: \n                        (Intr)\ngroup_classintervention -0.804\n\nStandardized Within-Group Residuals:\n    Min      Q1     Med      Q3     Max \n-2.1958 -0.4982 -0.1049  0.5869  2.5309 \n\nNumber of Observations: 48\nNumber of Groups: 3 \n\n\nThis procedure is somewhat too conservative, which means that the p-values it produces tend to be too large. Alternatively, you could fit the mixed-effects model using the lmer() function from the lme4 package, but then you have to take care that your p-values aren’t too low. But in practical terms, this doesn’t matter much: with a t-value of 0.77, you’re never going to end up with a p-value anywhere close to 0.05."
  },
  {
    "objectID": "posts/2019-01-14-clustered-data-different-results/index.html#analysis-2-a-t-test-on-the-group-means",
    "href": "posts/2019-01-14-clustered-data-different-results/index.html#analysis-2-a-t-test-on-the-group-means",
    "title": "Guarantees in the long run vs. interpreting the data at hand: Two analyses of clustered data",
    "section": "Analysis 2: A t-test on the group means",
    "text": "Analysis 2: A t-test on the group means\nIf mixed-effects models aren’t your thing, you may be tempted to run an analysis that is both easy to report and perfectly accounts for the effect of clustering on inferential statistics: you’d compute the mean outcome per class and then run a t-test on the class means instead. This procedure has excellent long-run properties: it returns a significant result for the intervention effect in 5% of cases when no such effect exists. That is, it isn’t too liberal (as t-test on the individual-level data would be) nor too conservative (like the analysis with lme()). (Incidentally, an analysis on individual level data would give you pretty much the same result as the lme_fit model above for the present data – but as a procedure it’s much too liberal.)\nFor the present data, this means that we first compute our three class means and then run a t-test on these three means (i.e., a t-test with one degree of freedom).\n\n# Compute summary per class (cluster):\n#  mean outcome, standard deviation of outcome,\n#  n data points, estimated standard error of mean outcome\nd_summary &lt;- d |&gt; \n  group_by(class, group_class) |&gt; \n  summarise(mean_outcome = mean(outcome),\n            sd_outcome = sd(outcome),\n            n = n(),\n            se_mean = sd_outcome / sqrt(n()),\n            .groups = \"drop\")\nd_summary\n\n# A tibble: 3 × 6\n  class group_class  mean_outcome sd_outcome     n se_mean\n  &lt;int&gt; &lt;chr&gt;               &lt;dbl&gt;      &lt;dbl&gt; &lt;int&gt;   &lt;dbl&gt;\n1     1 control            0.179       1.30     17   0.315\n2     2 intervention      -0.0587      0.864    13   0.240\n3     3 intervention      -0.0571      0.825    18   0.194\n\n\nI write the t-test in the form of a linear model as the output is more informative.\n\n# t-test on class means\nttest_fit &lt;- lm(mean_outcome ~ group_class, data = d_summary)\nsummary(ttest_fit)\n\n\nCall:\nlm(formula = mean_outcome ~ group_class, data = d_summary)\n\nResiduals:\n        1         2         3 \n 7.69e-20 -7.87e-04  7.87e-04 \n\nCoefficients:\n                        Estimate Std. Error t value Pr(&gt;|t|)\n(Intercept)              0.17926    0.00111     161   0.0040\ngroup_classintervention -0.23717    0.00136    -174   0.0037\n\nResidual standard error: 0.00111 on 1 degrees of freedom\nMultiple R-squared:     1,  Adjusted R-squared:     1 \nF-statistic: 3.03e+04 on 1 and 1 DF,  p-value: 0.00366\n\n\nThe estimated intervention effect is still -0.24, but now the standard error is merely 0.001, yielding a t-value of a whopping 174. Even with one degree of freedom, this corresponds to a highly significant effect (p = 0.004). (The data were simulated without an intervention effect, so this is a Type-I error. But you wouldn’t know this.)"
  },
  {
    "objectID": "posts/2019-01-14-clustered-data-different-results/index.html#which-analysis-is-better",
    "href": "posts/2019-01-14-clustered-data-different-results/index.html#which-analysis-is-better",
    "title": "Guarantees in the long run vs. interpreting the data at hand: Two analyses of clustered data",
    "section": "Which analysis is better?",
    "text": "Which analysis is better?\nSo we’ve got two analyses that are both defensible but give rise to wildly different inferences. The estimate for the class-by-class variability in the lme() fit seems implausibly small, but the estimated intervention effect and its standard error wouldn’t raise any eyebrows. By contrast, the t-test on the class means seems to produce absurdly low standard errors – not to mention a ridiculous R² value of 1. Yet it is this latter procedure that has the better long-run characteristics. So which analysis is the better one?\nThe boring answer is that it depends on what you’re looking for in an analysis. The second analysis yields a very low p-value, but it would be incorrect to say that it is too low: If you simulate lots and lots of datasets similar in structure to the one analysed (clustered data with one and two clusters per condition, respectively) and without any intervention effect present, this procedure will only find a p-value lower than 0.05 in 5% of cases, and it’ll only find a p-value lower than 0.004 in 0.4% of cases. If you analyse those same simulated datasets using lme(), you may not find a single significant result (I didn’t in 20,000 simulations). So if the long-run behaviour of the analytical procedure is your chief concern, running a t-test on the class means is the better option.\nYou may, however, also be interested in estimating the intervention effect and, crucially, quantifying your uncertainty about this estimate in the form of a standard error or confidence interval. In this respect, the hierarchical model produces a more plausible result - even though for want of data, this uncertainty estimate may be highly inaccurate.\n(The actual simulation-to-simulation variation was such that 90% of the estimated intervention effects lay between -0.83 and 0.83, a range of 1.65 units. The 90% confidence interval for the intervention effect in ttest_fit ranges from -0.25 to -0.23, for a range of merely 0.02 units. For lme_fit, it ranges from -2.17 to 1.70, for a range of 3.87 units. So the true accuracy lies pretty much half-way between these two estimates. But in real life, you’d only have the confidence intervals to go by, and one spanning merely 0.02 units would be considerably more suspicious than one spanning nearly 4 units. Incidentally, if you fit the same model using lme4’s lmer() function, you get a 90% confidence interval from -0.74 to 0.26, though with a couple of warnings.)"
  },
  {
    "objectID": "posts/2019-01-14-clustered-data-different-results/index.html#whats-causing-the-problem",
    "href": "posts/2019-01-14-clustered-data-different-results/index.html#whats-causing-the-problem",
    "title": "Guarantees in the long run vs. interpreting the data at hand: Two analyses of clustered data",
    "section": "What’s causing the problem?",
    "text": "What’s causing the problem?\nThe ridiculously low standard errors and huge R² values in ttest_fit are consequences of the small residuals that this model produces:\n\nresid(ttest_fit)\n\n         1          2          3 \n 7.691e-20 -7.869e-04  7.869e-04 \n\n\nThe first value is the residual for the class that served as the single control class. The corresponding fitted value equals the class mean, so the residual is 0 (which is what the ‘e-20’ part essentially means). The second and third value are the residuals for the classes that served as the intervention classes. Their fitted values equal the mean of both class means, and the residuals are the deviations from that mean of means. Since these class means are so similar, the deviations are small, too. Crucially, the similarity in the means of classes 2 and 3 is atypical: Based on the variability within each class, we’d expect their means to differ more than they do.\nTo demonstrate this, let’s first compute the standard deviation of these two class means:\n\nsd(d_summary$mean_outcome[2:3])\n\n[1] 0.001113\n\n\nNow we take bootstrap samples within classes 2 and 3, compute their respective means, and compute the standard deviation of these means.\n\n# Split off classes 2 and 3\nclass2 &lt;- d |&gt; filter(class == 2)\nclass3 &lt;- d |&gt; filter(class == 3)\n\n# Bootstrap the outcomes of class 2 and take the mean\n# Bootstrap the outcomes of class 3 and take the mean\n# Output the standard deviation of the bootstrapped means\nsds &lt;- replicate(20000, {\n  mean_bs_class2 &lt;- mean(sample(class2$outcome, replace = TRUE))\n  mean_bs_class3 &lt;- mean(sample(class3$outcome, replace = TRUE))\n  sd(c(mean_bs_class2, mean_bs_class3))\n}\n)\n\n# Plot the standard deviations of the bootstrapped means\nhist(sds, col = \"grey\", breaks = 100)\nabline(v = sd(d_summary$mean_outcome[2:3]), col = \"red\", lty = 2)\n\n\n\n\nFigure 2. Bootstrapping the outcomes within each class demonstrates that the class means are more similar to each other than they ordinarily would be.\n\n\n\n\nAs Figure 2 shows, given the variability within each class, you would typically expect their class means to differ considerably more than they actually do. In fact, only in about 0.4% of bootstrap runs did the class means differ less than in our dataset.\n\nmean(sds &lt; sd(d_summary$mean_outcome[2:3]))\n\n[1] 0.00365\n\n\nThe ttest_fit model doesn’t know that the class means are much more similar to each other than they normally would be, so it doesn’t account for this. (Mind you, if only the long-run behaviour of the model is of importance, it doesn’t need to know this.)"
  },
  {
    "objectID": "posts/2019-01-14-clustered-data-different-results/index.html#conclusions",
    "href": "posts/2019-01-14-clustered-data-different-results/index.html#conclusions",
    "title": "Guarantees in the long run vs. interpreting the data at hand: Two analyses of clustered data",
    "section": "Conclusions",
    "text": "Conclusions\nIt isn’t unheard of that different defensible analyses of the same data lead to different inferences (see, e.g., Poarch et al., 2018, and Silberzahn et al., 2018). But I think it’s instructive to come across a case where the objectives of achieving the desired long-run behaviour and of sensibly interpreting the data that were actually observed clash.\nIf I had to analyse a dataset like this one in earnest, I would look into fitting a Bayesian model with reasonably informed priors in order to offset the lack of information present in the data. (A hierarchical model with brms’s default priors experiences convergence issues.) If that doesn’t work, I’d probably fit a frequentist hierarchical model (using lme or lmer()) as the results it yields are more plausible in the present case than that those of the class mean analysis.\nOh, and I’d rant to whoever collected these data that they should’ve dropped by before running the study :) I’d then have told them that it would be great if they could collect data in two or three more classes. Or that it might perhaps be possible to assign half of the pupils within each class to each condition. Still, they were extremely unlucky that the class means were so similar given the within-class variance."
  },
  {
    "objectID": "posts/2019-01-14-clustered-data-different-results/index.html#references",
    "href": "posts/2019-01-14-clustered-data-different-results/index.html#references",
    "title": "Guarantees in the long run vs. interpreting the data at hand: Two analyses of clustered data",
    "section": "References",
    "text": "References\nGregory J. Poarch, Jan Vanhove and Raphael Berthele. 2018. The effect of bidialectalism on executive function. International Journal of Bilingualism.\nR. Silberzahn et al.. 2018. Many analysts, one data set: Making transparent how variations in analytic choices affect results. Advances in Methods and Practices in Psychological Science 1(3). 337-356.\nJan Vanhove. 2015. Analyzing randomized controlled interventions: Three notes for applied linguists. Studies in Second Language Learning and Teaching 5(1). 135-152."
  },
  {
    "objectID": "posts/2019-01-14-clustered-data-different-results/index.html#software-versions",
    "href": "posts/2019-01-14-clustered-data-different-results/index.html#software-versions",
    "title": "Guarantees in the long run vs. interpreting the data at hand: Two analyses of clustered data",
    "section": "Software versions",
    "text": "Software versions\nPlease note that I reran the code on this page on August 6, 2023.\n\ndevtools::session_info()\n\n─ Session info ───────────────────────────────────────────────────────────────\n setting  value\n version  R version 4.3.1 (2023-06-16)\n os       Ubuntu 22.04.2 LTS\n system   x86_64, linux-gnu\n ui       X11\n language en_US\n collate  en_US.UTF-8\n ctype    en_US.UTF-8\n tz       Europe/Zurich\n date     2023-08-06\n pandoc   3.1.1 @ /usr/lib/rstudio/resources/app/bin/quarto/bin/tools/ (via rmarkdown)\n\n─ Packages ───────────────────────────────────────────────────────────────────\n package     * version date (UTC) lib source\n cachem        1.0.6   2021-08-19 [2] CRAN (R 4.2.0)\n callr         3.7.3   2022-11-02 [1] CRAN (R 4.3.1)\n cli           3.6.1   2023-03-23 [1] CRAN (R 4.3.0)\n colorspace    2.1-0   2023-01-23 [1] CRAN (R 4.3.0)\n crayon        1.5.2   2022-09-29 [1] CRAN (R 4.3.1)\n devtools      2.4.5   2022-10-11 [1] CRAN (R 4.3.1)\n digest        0.6.29  2021-12-01 [2] CRAN (R 4.2.0)\n dplyr       * 1.1.2   2023-04-20 [1] CRAN (R 4.3.0)\n ellipsis      0.3.2   2021-04-29 [2] CRAN (R 4.2.0)\n evaluate      0.15    2022-02-18 [2] CRAN (R 4.2.0)\n fansi         1.0.4   2023-01-22 [1] CRAN (R 4.3.1)\n farver        2.1.1   2022-07-06 [1] CRAN (R 4.3.0)\n fastmap       1.1.0   2021-01-25 [2] CRAN (R 4.2.0)\n forcats     * 1.0.0   2023-01-29 [1] CRAN (R 4.3.0)\n fs            1.5.2   2021-12-08 [2] CRAN (R 4.2.0)\n generics      0.1.3   2022-07-05 [1] CRAN (R 4.3.0)\n ggplot2     * 3.4.2   2023-04-03 [1] CRAN (R 4.3.0)\n glue          1.6.2   2022-02-24 [2] CRAN (R 4.2.0)\n gtable        0.3.3   2023-03-21 [1] CRAN (R 4.3.0)\n hms           1.1.3   2023-03-21 [1] CRAN (R 4.3.0)\n htmltools     0.5.5   2023-03-23 [1] CRAN (R 4.3.0)\n htmlwidgets   1.6.2   2023-03-17 [1] CRAN (R 4.3.1)\n httpuv        1.6.11  2023-05-11 [1] CRAN (R 4.3.1)\n jsonlite      1.8.7   2023-06-29 [1] CRAN (R 4.3.1)\n knitr         1.39    2022-04-26 [2] CRAN (R 4.2.0)\n labeling      0.4.2   2020-10-20 [1] CRAN (R 4.3.0)\n later         1.3.1   2023-05-02 [1] CRAN (R 4.3.1)\n lattice       0.21-8  2023-04-05 [4] CRAN (R 4.3.0)\n lifecycle     1.0.3   2022-10-07 [1] CRAN (R 4.3.0)\n lubridate   * 1.9.2   2023-02-10 [1] CRAN (R 4.3.0)\n magrittr      2.0.3   2022-03-30 [1] CRAN (R 4.3.0)\n memoise       2.0.1   2021-11-26 [2] CRAN (R 4.2.0)\n mime          0.10    2021-02-13 [2] CRAN (R 4.0.2)\n miniUI        0.1.1.1 2018-05-18 [1] CRAN (R 4.3.1)\n munsell       0.5.0   2018-06-12 [1] CRAN (R 4.3.0)\n nlme        * 3.1-162 2023-01-31 [4] CRAN (R 4.2.2)\n pillar        1.9.0   2023-03-22 [1] CRAN (R 4.3.0)\n pkgbuild      1.4.2   2023-06-26 [1] CRAN (R 4.3.1)\n pkgconfig     2.0.3   2019-09-22 [2] CRAN (R 4.2.0)\n pkgload       1.3.2.1 2023-07-08 [1] CRAN (R 4.3.1)\n prettyunits   1.1.1   2020-01-24 [2] CRAN (R 4.2.0)\n processx      3.8.2   2023-06-30 [1] CRAN (R 4.3.1)\n profvis       0.3.8   2023-05-02 [1] CRAN (R 4.3.1)\n promises      1.2.0.1 2021-02-11 [1] CRAN (R 4.3.1)\n ps            1.7.5   2023-04-18 [1] CRAN (R 4.3.1)\n purrr       * 1.0.1   2023-01-10 [1] CRAN (R 4.3.0)\n R6            2.5.1   2021-08-19 [2] CRAN (R 4.2.0)\n Rcpp          1.0.11  2023-07-06 [1] CRAN (R 4.3.1)\n readr       * 2.1.4   2023-02-10 [1] CRAN (R 4.3.0)\n remotes       2.4.2   2021-11-30 [2] CRAN (R 4.2.0)\n rlang         1.1.1   2023-04-28 [1] CRAN (R 4.3.0)\n rmarkdown     2.21    2023-03-26 [1] CRAN (R 4.3.0)\n rstudioapi    0.14    2022-08-22 [1] CRAN (R 4.3.0)\n scales        1.2.1   2022-08-20 [1] CRAN (R 4.3.0)\n sessioninfo   1.2.2   2021-12-06 [2] CRAN (R 4.2.0)\n shiny         1.7.4.1 2023-07-06 [1] CRAN (R 4.3.1)\n stringi       1.7.12  2023-01-11 [1] CRAN (R 4.3.1)\n stringr     * 1.5.0   2022-12-02 [1] CRAN (R 4.3.0)\n tibble      * 3.2.1   2023-03-20 [1] CRAN (R 4.3.0)\n tidyr       * 1.3.0   2023-01-24 [1] CRAN (R 4.3.0)\n tidyselect    1.2.0   2022-10-10 [1] CRAN (R 4.3.0)\n tidyverse   * 2.0.0   2023-02-22 [1] CRAN (R 4.3.1)\n timechange    0.2.0   2023-01-11 [1] CRAN (R 4.3.0)\n tzdb          0.4.0   2023-05-12 [1] CRAN (R 4.3.0)\n urlchecker    1.0.1   2021-11-30 [1] CRAN (R 4.3.1)\n usethis       2.2.2   2023-07-06 [1] CRAN (R 4.3.1)\n utf8          1.2.3   2023-01-31 [1] CRAN (R 4.3.1)\n vctrs         0.6.3   2023-06-14 [1] CRAN (R 4.3.0)\n withr         2.5.0   2022-03-03 [2] CRAN (R 4.2.0)\n xfun          0.39    2023-04-20 [1] CRAN (R 4.3.0)\n xtable        1.8-4   2019-04-21 [1] CRAN (R 4.3.1)\n yaml          2.3.5   2022-02-21 [2] CRAN (R 4.2.0)\n\n [1] /home/jan/R/x86_64-pc-linux-gnu-library/4.3\n [2] /usr/local/lib/R/site-library\n [3] /usr/lib/R/site-library\n [4] /usr/lib/R/library\n\n──────────────────────────────────────────────────────────────────────────────"
  },
  {
    "objectID": "posts/2017-10-24-increasing-power-precision/index.html",
    "href": "posts/2017-10-24-increasing-power-precision/index.html",
    "title": "Increasing power and precision using covariates",
    "section": "",
    "text": "A recurring theme in the writings of methodologists over the last years and indeed decades is that researchers need to increase the statistical power (and precision) of the studies they conduct. These writers have rightly stressed the necessity of larger sample sizes, but other research design characteristics that affect power and precision have received comparatively little attention. In this blog post, I discuss and demonstrate how by capitalising on information that they collect anyway, researchers can achieve more power and precision without running more participants."
  },
  {
    "objectID": "posts/2017-10-24-increasing-power-precision/index.html#tldr",
    "href": "posts/2017-10-24-increasing-power-precision/index.html#tldr",
    "title": "Increasing power and precision using covariates",
    "section": "tl;dr",
    "text": "tl;dr\nConsider whether one or a handful of variables other than the ones you’re really interested in may account for differences in the outcome between participants, and actually use them in the analysis rather than just report them in a “background variables” table."
  },
  {
    "objectID": "posts/2017-10-24-increasing-power-precision/index.html#statistical-power-and-precision",
    "href": "posts/2017-10-24-increasing-power-precision/index.html#statistical-power-and-precision",
    "title": "Increasing power and precision using covariates",
    "section": "Statistical power and precision",
    "text": "Statistical power and precision\nFirst, let’s discuss what statistical power and precision mean. Statistical power is a concept rooted in the paradigm of null hypothesis testing and refers to the probability with which a study will find a significant pattern in the data if such a pattern is indeed present in the ‘mechanism’ that generated the data. For instance, a new teaching method may really be better than the old one, but due to variability in the data, there’s always some probability that an evaluation study would not find a significant difference between the learning outcomes of pupils taught with the new vs. the old method. The smaller the probability of this happening, the greater the statistical power of the study.\nAs the example demonstrates, the immediate effect of low statistical power is that, pretty much by definition, actually existing patterns in the data may fly under the researchers’ radar. Less obviously, but not any less seriously, underpowered studies coupled with publication bias – i.e., the tendency for authors to submit and for reviewers to recommended significant findings for publication – can dramatically distort the literature on any given phenomenon. Specifically, a literature comprised largely of significant findings stemming from underpowered studies is likely to overestimate both the evidence concerning the phenomenon at hand as well as the magnitude of this phenomenon. Within a null hypothesis testing framework, then, underpowered studies are detrimental both to the conclusions that can be drawn from individual studies as well as to the ones that can be drawn from the entire body of literature.\nSo what determines the statistical power of any given study? For ease of exposition, I’ll focus on a study in which the efficacy of two teaching methods is evaluated by comparing learning outcomes between a number of pupils taught with either the old or the new method. The details don’t really matter here, so you can imagine that the pupils are tested using some standardised instrument that yields a continuous outcome of some sort. The power of the evaluation study, that is, the probability with which it will yield a significant difference between the two teaching methods if their efficacy is indeed not the same, depends on three factors:\n\nThe true mean difference in learning outcomes between the two methods: if an infinite number of pupils were tested with the old method and another infinite number of pupils with the new one, what would the difference between the mean learning outcomes in the two group be? This is known as the effect size.\nThe true variability in learning outcome within each group: again assuming an infinite number of pupils were tested, how large would the standard deviation of the learning outcomes within each group be? This is known as error variance.\nThe number of pupils tested in each group, i.e., the sample size.\n\nAs the graphs below illustrate, the power of a study is larger if the effect size it investigates is larger, if the variability within each group is smaller, and if the sample size is larger. The precise power values in each graph don’t matter much since they depend on the two determinants in the respective other graphs which I fixed at arbitrary values; the graphs are just intended to help you visualise in broad strokes the relationship between each determinant and statistical power.\n\n\n\nFigure 1: The three determinants of statistical power: effect size, error variance, and sample size.\n\n\n(Sidebar: I’m yet to be convinced that single-figure claims about the statistical power of a study, e.g., are both sensible and useful.)\nThe concept of statistical power is deeply rooted in null hypothesis testing, but a related concept exists that is more appealing to researchers who don’t want to use null hypothesis testing – perhaps because they think that the null hypothesis is so unlikely to be true in their area of research that it makes little sense to test it. This related concept is precision and quite simply refers to accurately an effect size (e.g., a mean difference) was estimated by a study. This is typically expressed by means of some interval reported around the estimated effect size, be it a standard error, a confidence interval or a credibility interval. The wider the interval, the less precise the estimate is assumed to be. Like power, precision is dependent on the within-group variability and the sample size, but unlike power, it is not affected by the effect size.\n(The latter point is true in Gaussian models, in which the variance and the mean of a variable are unrelated. In non-Gaussian models this usually isn’t true so that precision will often depend on the effect size, too.)"
  },
  {
    "objectID": "posts/2017-10-24-increasing-power-precision/index.html#going-beyond-n",
    "href": "posts/2017-10-24-increasing-power-precision/index.html#going-beyond-n",
    "title": "Increasing power and precision using covariates",
    "section": "Going beyond n",
    "text": "Going beyond n\nThe vast majority of discussions about statistical power (or precision, though there are fewer of those) focus on but one of the three (or two, respectively) determinants discussed above: the study’s sample size. For good reason, it would seem: it’s difficult to come up with a situation in which researchers can change the effect size under investigation without changing their research question. Similarly, researchers may feel that the within-group variability isn’t under their control: reducing measurement error (Update (2023-08-07): This external link appears to be broken.) is one theoretical option but it isn’t always feasible; investigating a more homogeneous sample of participants is another option but one that’s hardly desirable in a field in which samples are too homogeneous as it is, jeopardising external validity. Sample size, then, seems to be the only factor that’s really up to the researchers themselves.\nHowever, there is a long tradition in research methodology of reducing error variance, and thereby increasing power and precision, through design and analytical choices. Specifically, it is possible, and often feasible, to reduce a study’s error variance by making better use of information contained in covariates than is typically the case in linguistics and psychology.\nBy covariates (or control variables, or background variables), I here mean variables that are collected during a study, often as a matter of course, but which aren’t of primary interest as predictor or outcome variables. For instance, pretty much every researcher collects some basic demographic data about their participants: age, sex, socio-economic status etc. Often even more detailed information is available, for instance, a measure of the participants’ working memory capacity, of their IQ, of their French-language skills etc. This information is then dutifully reported in some sort of a table in order to sketch the make-up of the participant sample or to document that the random assignment of participants to conditions really “worked”, i.e., created comparable groups.\n(Sidebar: Don’t run balance tests in randomised experiments. They’re useless.)\nI assume that one reason why researchers collect these covariates is that they, or their colleagues, believe that they are presumably related to the outcome variable so that it’s important to be able to argue that these variables didn’t cause or obscure the phenomenon of interest. But if this belief is correct, researchers are underusing these covariates by not incorporating them into the study’s design and main analysis. Below, I’ll discuss how covariate information can be used during the design stage and during the analysis, and then I’ll demonstrate, on the basis of some simulations, how substantial the advantages of taking into account covariate information are.\n(I won’t cover within-subjects designs. These similarly tend to have greater power and precision than comparable between-subjects designs, but I think their advantages are already well recognised.)"
  },
  {
    "objectID": "posts/2017-10-24-increasing-power-precision/index.html#considering-covariates-when-designing-the-study",
    "href": "posts/2017-10-24-increasing-power-precision/index.html#considering-covariates-when-designing-the-study",
    "title": "Increasing power and precision using covariates",
    "section": "Considering covariates when designing the study",
    "text": "Considering covariates when designing the study\nIn a typical experiment, the participants are assigned randomly to the conditions, often in such a way that the number of participants in each condition is equal. This is known as a completely randomised design (CRD).\nBut let’s say we have some relevant information about the participants at the onset of the study. For instance, we might want to quantify the effect of providing some glosses on L2 French text comprehension and have at our disposal the results of an L2 French placement test for all participants. In that case, rather than assign the participants to conditions completely at random, we could create pairs of participants with similar placement test results and, within each pair, randomly assign one participant to each condition. These pairs are known as blocks, and the design is known as a randomised block design (RBD). The advantage of doing so is that the two groups of participants will now be alike in terms of an obviously important background variable. Combined with an appropriate analytical technique (which I will discuss below), this will improve the study’s power and precision.\nAnother possibility, suggested by Dalton and Overall (1977) is to sort the participants on their covariate score and assign them to the conditions in an ABBAABBAA… fashion. Which condition is A and which is B is then determined at random. This alternate ranks design (ARD) similarly equalises the two groups in terms of a relevant background variable, and has similar advantages to the randomised block design.\n(The RBD and ARD assume that you know all your participants before the start of the study and that you have some relevant information about them. This is obviously not the case in studies in which the participants ‘trickle in’ to the lab as the data collection progresses. I’ll come back to this in the concluding discussion.)"
  },
  {
    "objectID": "posts/2017-10-24-increasing-power-precision/index.html#considering-covariates-when-analysing-the-data",
    "href": "posts/2017-10-24-increasing-power-precision/index.html#considering-covariates-when-analysing-the-data",
    "title": "Increasing power and precision using covariates",
    "section": "Considering covariates when analysing the data",
    "text": "Considering covariates when analysing the data\nWhen analysing their data and covariate information is available, researchers have four options.\nFirst, they could ignore the covariate. This done more often than it should be, perhaps because the use of covariates makes the analysis less elegant or because of a fear that reviewers will insist on seeing the results without the covariate. As the simulations below show, however, ignoring important covariates is pretty much never desirable, and may even invalidate the results.\nThe second option is to include a blocking factor in the analysis. This is actually the standard way of analysing a randomised block design: if, say, 40 participants were first grouped into 20 pairs and then randomly assigned within each pair, we have a study with 20 blocks. The block information (but not the covariate itself), that is, a categorical variable with 20 levels, is then included as a predictor in an ANOVA. In this example, this would cost 19 degrees of freedom.\nA third option is to include the covariate as a linear term in the analysis using regression/ANCOVA. If one is willing to assume that the link between the covariate and the outcome is approximately linear, then this may be more advantageous than including a blocking factor, since it only costs 1 degree of freedom (instead of 19) and it didn’t require discretising a continuous variable.\nSome people prefer blocking factors to linear covariate terms because the latter assume a linear relationship between the covariate and the outcome. However, tools exist for modelling the covariate as a nonlinear term, for instance, using polynomial terms or in generalised additive models. So modelling the covariate nonlinearly is the fourth option."
  },
  {
    "objectID": "posts/2017-10-24-increasing-power-precision/index.html#comparison-of-different-designs-and-methods-of-analysis",
    "href": "posts/2017-10-24-increasing-power-precision/index.html#comparison-of-different-designs-and-methods-of-analysis",
    "title": "Increasing power and precision using covariates",
    "section": "Comparison of different designs and methods of analysis",
    "text": "Comparison of different designs and methods of analysis\nSo which design and which method of analysis are to be preferred in terms of power and precision? There are some published simulation results (e.g., Maxwell et al. 1984), but for full flexibility I ran a couple of simulations of my own. The R code is absolutely atrocious, so I’ll just summarise what I did.\n\nRandomised block design. I generated a covariate from a normal distribution and used the covariate to form pairs of participants. Within each pair, one member was randomly assigned to one of two conditions. An outcome variable was then generated that was partly determined by the covariate, partly by the experimental condition, and partly by random (normal) noise. Four methods of analysis were applied to the simulated data:\n\nA linear model ignoring the covariate altogether. (lm(outcome ~ condition))\nA linear model with a blocking factor as a fixed effect. (lm(outcome ~ block + condition))\nA linear model with a linear covariate term. (lm(outcome ~ covariate + condition))\nA generalised additive model with a smooth covariate term. (gam(outcome ~ s(covariate) + condition))\n\nAlternate ranks design. I again generated a covariate from a normal distribution, ranked the participants on it, and assigned them to conditions in a ABBAABB etc. fashion. Which condition A and B referred to was decided randomly. The outcome was generated like in the previous case. Three methods of analysis were applied:\n\nA linear model ignoring the covariate altogether. (lm(outcome ~ condition))\nA linear model with a linear covariate term. (lm(outcome ~ covariate + condition))\nA generalised additive model with a smooth covariate term. (gam(outcome ~ s(covariate) + condition))\n\nCompletely randomised design. I again generated a covariate from a normal distribution. Half of the participants were assigned to each condition, but the covariate wasn’t referred to during this assignment. The outcome was generated like in the previous cases. Three methods of analysis were applied:\n\nA linear model ignoring the covariate altogether. (lm(outcome ~ condition))\nA linear model with a linear covariate term. (lm(outcome ~ covariate + condition))\nA generalised additive model with a smooth covariate term. (gam(outcome ~ s(covariate) + condition))\n\n\nFor each simulation, I varied * the total number of participants (20 vs. 50), that is, either 10 or 25 participants per condition. * the size of the condition effect (0 vs. 0.8 units). The 0 effect simulations will be useful to compare the Type-I error rates between the different methods, whereas the 0.8 effect simulations are useful for comparing their power. (Both are equally useful for comparing their precision.) * the relationship between the covariate and the outcome. This relationship was either linear or nonlinear (cosine). I chose the cosine function because the correlation between a normal variable with mean 0 and its cosine is 0. (By contrast, the correlation between a normal variable with mean 0 and its sine is quite high.) The cosine relationship, then, represents the worse-case scenario for the analytical methods assuming a linear relationship.\nIgnoring the effect of the experimental condition, half of the variance in the outcomes was due to the covariate and half due to random noise. (This may seem like a lot of variance due to the covariate, but I think that pretest scores routinely account for more than that.)\n(Incidentally, I found out by accident that var(sqrt(5)*cos(N(0, 1))) ~= 1.)\nEach simulation was run 1,000 times. From each run, I extracted the estimated coefficient of the condition effect, its p-value, and the width of its 95% confidence interval. And now, for the results.\n\nType-I error\nLet’s first see how often each design/analysis returned a significant effect for condition when this was in fact set to 0 in the simulation. The first plot shows the results for the simulations in which the covariate was linearly related to the outcome.\n\n\n\nFigure 2: The proportion of p-values below 0.05 for simulations in which the condition effect was actually 0 and the covariate was linearly related to the outcome. These numbers should hover around the 0.05 mark; the dotted vertical lines the 95% probability region around 0.05. (CRD = completely randomised design; ARD = alternate ranks design; RBD = randomized block design)\n\n\nImportantly, not all design/analysis combinations retain their nominal Type-I error rate of 0.05. On the one hand, modelling a truly linear variable as potentially nonlinear in a generalised additive model (‘nonlinear covariate’) seems to return slightly too many significant findings, especially for pretty small samples. (I used thin plate regression splines for these simulations; it’s worse when using cubic regression splines.)\nOn the other hand, when covariate information is used in the design of the study (i.e., for designs RBD and ARD) but then ignored altogether in the analysis, the Type-I error rate is considerably too low. What happens is that analysis overestimates the variability of the condition estimate, which thanks to the design doesn’t vary that much.\nCompare this to a scenario where the covariate is maximally nonlinear:\n\n\n\nFigure 3: The proportion of p-values below 0.05 for simulations in which the condition effect was actually 0 and the covariate was nonlinearly related to the outcome. These numbers should hover around the 0.05 mark; the dotted vertical lines the 95% probability region around 0.05. (CRD = completely randomised design; ARD = alternate ranks design; RBD = randomized block design)\n\n\nAgain, the generalised additive model is anti-conservative, particularly for smaller samples, and like in the linear case, the Type-I error rate is too low when the covariate was used in the design but not in the analysis. Additionally, when a nonlinear covariate was used in the design but analysed as a linear covariate, this is associated with the same problem as when the covariate is ignored altogether. (Note again that in this simulation the nonlinearity isn’t even distantly approximated by some linearity. This problem won’t be as pronounced in more realistic scenarios.)\nTentative conclusions: 1. GAM-ANCOVAs are somewhat too optimistic (solution?). 2. Using covariates in the design and then ignoring them in the analysis is not a good idea. 3. When the covariate is strongly nonlinearly related to the outcome but used in the design, using a blocking factor in the analysis seems best.\n\n\nPower\nHow often does the analysis return a significant result when the effect of condition is, in fact, 0.8 units? (For reference, the remaining variance in the data would on average be 2, i.e., the within-group standard deviation would be 1.4.)\n\n\n\nFigure 4: The proportion of p-values below 0.05 for simulations in which the condition effect was 0.8 and the covariate was linearly related to the outcome. (CRD = completely randomised design; ARD = alternate ranks design; RBD = randomized block design)\n\n\nWhen the covariate is linearly related to the outcome, the analyses in which the covariate is actually considered blow the analyses in which the covariate is ignored utterly out of the water. This is true regardless of whether the covariate was considered in the design itself. In fact, the power of n = 20 designs that do consider the covariate is only somewhat worse than that of n = 50 designs that ignore the covariate.\nBetween the different assignment schemes (RBD, ARD, CRD), there’s only minor differences as long as the covariate is included in the analysis. The ARD and RBD schemes are slightly more powerful, which is due to the covariate being guaranteed to have similar distributions in each and every case. (ANCOVA is more powerful when in such cases.) But these differences are minute.\nIn the RBD, researchers have the choice between including the covariate using a single term or using a blocking factor with multiple levels. The former has somewhat greater power since it uses fewer degrees of freedom to accomplish essentially the same thing. This difference in degrees of freedom is especially pronounced for smaller samples, but all in all small.\nDiscerning readers may notice that treating the covariate as potentially nonlinear ever so slightly pips treating it as linear. This, however, is an artefact from this analytical method’s larger Type-I error rate (i.e., it’s a tad too optimistic).\nAnd now for a nonlinear covariate:\n\n\n\nFigure 5: The proportion of p-values below 0.05 for simulations in which the condition effect was 0.8 and the covariate was nonlinearly related to the outcome. (CRD = completely randomised design; ARD = alternate ranks design; RBD = randomized block design)\n\n\nTreating the covariate as potentially nonlinear when it in fact is strongly nonlinear unsurprisingly beats the other methods in terms of power. This is partly due to the inherent optimism of this method (as discussed above), but mostly because this method makes best use of the information in the data. Including the covariate information using a blocking factor in a RBD is less powerful due to the loss of degrees of freedom, particularly in small samples. For strongly nonlinear covariates, treating the covariate as linear or not considering it at all yield essentially the same results.\n\n\nPrecision\nIf instead of power, you’re interested in getting tight confidence intervals, the following two plots may be of interest to you. The conclusions, however, are pretty much the same as above: ignoring an important covariate is shooting yourself in the foot precision-wise; modelling it as linear when it’s in fact strongly nonlinear is equally as bad; taking it into account using a blocking factor rather than as a continuous variable is slightly worse due to the loss of degrees of freedom; and the CRD is ever so slightly worse than the RBD and ARD.\n\n\n\nFigure 6: The 90th percentiles of the width of the 95% confidence intervals around the condition effect when the covariate was linearly related to the outcome. In other words, 90% of the confidence intervals are narrower than the numbers plotted above. (CRD = completely randomised design; ARD = alternate ranks design; RBD = randomized block design)\n\n\n\n\n\nFigure 7: The 90th percentiles of the width of the 95% confidence intervals around the condition effect when the covariate was nonlinearly related to the outcome. (CRD = completely randomised design; ARD = alternate ranks design; RBD = randomized block design)\n\n\nNote that both of the plots above show the 90th percentile of the widths of the entire confidence interval."
  },
  {
    "objectID": "posts/2017-10-24-increasing-power-precision/index.html#discussion-and-conclusion",
    "href": "posts/2017-10-24-increasing-power-precision/index.html#discussion-and-conclusion",
    "title": "Increasing power and precision using covariates",
    "section": "Discussion and conclusion",
    "text": "Discussion and conclusion\nIncluding important covariates in the analysis is useful power- and precision-wise. Such covariates are often collected anyway but then disregarded during the analysis, which is akin to throwing away a good proportion of your participants for no good reason.\nConsidering such important covariates during the design of the study typically requires knowing your entire participant sample beforehand, which is rarely the case. That said, performing the random assignment of participants to conditions in the absence of covariate information but using this information during the analysis is only slightly worse than considering the covariates during both assignment and analysis.\nInterestingly, some methods have been developed for “on-the-fly” blocking, that is, for blocking participants as they trickle in to the lab (e.g., Moore and Moore 2013; Kapelner and Krieger 2013). It seems to be me that little is to be gained from implementing these methods, but if the algorithms were to be implemented in experimental software, the slight gain in efficiency would obviously be welcome.\n(The simulations reported above all featured normally distributed covariates, so things may be different were skewed covariates are concerned. In fact, it would seem that the BRD/ARD schemes coupled with ANCOVA are more powerful/precise that the other alternatives when the covariate is non-normally distributed (McAweeny and Klockars 1998).)\nThe broader point that I want to make is that when discussing power/precision, it should be realised that studies with fairly few participants can still be powerful/precise. Particularly for researchers who don’t have the luxury of having hordes of undergrads in search of credits at their disposal, I think it would be helpful if discussions of power/precision (or laments about its absence) would treat more fully ways of achieving power other than upping the sample size. Using covariate information more efficiently is one way, but there are, of course, others, some of which I’ve discussed before on this blog.\nLastly, I’ll repeat a recurring theme on this blog, namely that for any discussions concerning power that won’t automatically end in “triple your sample size” to be meaningful, I think it’s necessary to ditch standardised effect sizes. (I’d put that last bit in Latin if I could.)"
  },
  {
    "objectID": "posts/2022-02-17-levenshtein/index.html",
    "href": "posts/2022-02-17-levenshtein/index.html",
    "title": "An R function for computing Levenshtein distances between texts using the word as the unit of comparison",
    "section": "",
    "text": "For a new research project, we needed a way to tabulate the changes that were made to a text when correcting it. Since we couldn’t find a suitable tool, I wrote an R function that uses the Levenshtein algorithm to determine both the smallest number of words that need to be changed to transform one version of a text into another and what these changes are.\nYou can download the obtain_edits() function from https://janhove.github.io/RCode/obtainEdits.R or source it directly:\n\nsource(\"https://janhove.github.io/RCode/obtainEdits.R\")\n\nThe function recognises words that were deleted or inserted, words that were substituted for other words, and cases where one word was split into two words or where two words were merged to one word. All these changes count as one operation. The algorithm determines both the smallest number of operations needed to transform one version of the text into the other and outputs a data frame that lists what these operations are.\nHere’s an example:\n\noriginal  &lt;- \"Check howmany changes need be made in order to change the first tekst in to the second one.\"\ncorrected &lt;- \"Check how many changes need to be made in order to change the first text into the second one.\"\nobtain_edits(original, corrected)\n\n[[1]]\n[1] 4\n\n[[2]]\n  change_position  change_type change_from change_to\n1              14       merger       in to      into\n2              13 substitution       tekst      text\n3               5    insertion                    to\n4               2        split     howmany  how many\n\n\nNote that while the minimal operation count is uniquely determined, the list of changes that were made isn’t. Consider this example:\n\ntextA &lt;- \"first secondthird\"\ntextB &lt;- \"second third\"\nobtain_edits(textA, textB)\n\n[[1]]\n[1] 2\n\n[[2]]\n  change_position change_type change_from    change_to\n1               2       split secondthird second third\n2               1    deletion       first             \n\n\nThe algorithm identifies the difference from textA to textB as a matter of deleting ‘first’ and splitting up ‘secondthird’. But we could also consider it a matter of substituting ‘second’ for ‘first’ and ‘third’ for ‘secondthird’.\nNothing about stats or research design in this post, but perhaps this function is useful to someone somewhere!"
  },
  {
    "objectID": "posts/2019-08-07-interactions-logistic/index.html",
    "href": "posts/2019-08-07-interactions-logistic/index.html",
    "title": "Interactions in logistic regression models",
    "section": "",
    "text": "When you want to know if the difference between two conditions is larger in one group than in another, you’re interested in the interaction between ‘condition’ and ‘group’. Fitting interactions statistically is one thing, and I will assume in the following that you know how to do this. Interpreting statistical interactions, however, is another pair of shoes. In this post, I discuss why this is the case and how it pertains to interactions fitted in logistic regression models."
  },
  {
    "objectID": "posts/2019-08-07-interactions-logistic/index.html#the-problem-nonlinear-mappings",
    "href": "posts/2019-08-07-interactions-logistic/index.html#the-problem-nonlinear-mappings",
    "title": "Interactions in logistic regression models",
    "section": "The problem: Nonlinear mappings",
    "text": "The problem: Nonlinear mappings\nThe crux of the problem was discussed from a psychologist’s perspective by Loftus (1978; see also Wagenmakers et al. 2012): Even if you find a statistical interaction between ‘condition’ and ‘group’, this doesn’t necessarily mean that one group is more sensitive to the difference in conditions than the other group. By the same token, even if you don’t find a statistical interaction between ‘condition’ and ‘group’, this doesn’t necessarily suggest that both groups are equally sensitive to the difference in conditions. The reason for this ambiguity is that the measured outcome variable (e.g., test score, reaction time, etc.) need not map linearly onto the latent variable of interest (e.g., proficiency, effort, etc.). For example, a 3-point increase on an arbitrary scale from one condition to another may correspond to a greater gain in actual proficiency on the higher end of the scale than on the lower end. If one group progresses from 7 to 10 points and the other from 15 to 18 points, then, the statistical analysis won’t reveal an interaction between ‘condition’ and ‘group’ in terms of the measured outcome variable, even though there is a greater gain in proficiency for the latter group than for the former. By the same token, a 4-point increase for the first group may correspond to a 3-point increase for the second group in terms of proficiency gained. If this doesn’t make much sense to you, I suggest you read the first seven pages of Wagenmakers et al. (2012).\nUnfortunately, the problem with interpreting interactions doesn’t vanish if you don’t want to infer latent psychological variables and you stick strictly to observable quantities. For instance, if you want to compare the efficiency of two car makes (red vs. blue) depending on fuel type (petrol vs. diesel), you may not find an interaction between car make and fuel type when you express fuel efficiency in litres per 100 kilometres: There may be, say, a one-litre difference between both fuel types for both car makes (Figure 1, left). But if you express fuel consumption in miles per gallon (or kilometres per litre), you’d observe that the difference in distance travelled per amount of fuel consumed between diesel and petrol is larger for one car make than for the other (Figure 1, right). The reason is that, while both measures (litres/100 km and miles/gallon) are perfectly reasonable and use exactly the same information (distance travelled and fuel burnt), the mapping between them is nonlinear.\n\n\n\n\n\nFigure 1. You may not observe an interaction between fuel type and car make when fuel consumption is expressed as fuel burnt per distance covered (litres per 100 km, left) even though you would observe such an interaction when fuel consumption is expressed as distance covered per unit of fuel burnt (miles per gallon, right)."
  },
  {
    "objectID": "posts/2019-08-07-interactions-logistic/index.html#log-odds-odds-and-proportions",
    "href": "posts/2019-08-07-interactions-logistic/index.html#log-odds-odds-and-proportions",
    "title": "Interactions in logistic regression models",
    "section": "Log-odds, odds, and proportions",
    "text": "Log-odds, odds, and proportions\nYou encounter the same problem when you fit interactions in a logistic model. The coefficients in logistic models are estimated on the log-odds scale, but such models are more easily interpreted when the coefficients or its predictions are converted to odds (by exponentiating the log-odds) or to proportions (by applying the logistic function to predictions obtained in log-odds). Both the exponential and the logistic function are nonlinear, so that you end up with the same problem as above: Whether or not you observe an interaction may depend on how you express the outcome variable.\n(Sidebar: A proportion of 0.80 corresponds to odds of 0.80/(1-0.80) = 4 against 1. By taking the natural logarithm of the odds, you end up with the log-odds of ln(0.80) = 1.39. To arrive back at the odds, exponentiate: exp(1.39) = 4. To arrive back at the proportion, apply the logistic function: 1/(1+exp(-1.39)) = 0.80. All these functions are nonlinear.)\nFigure 2 illustrates that a non-crossover interaction if the results are expressed in, say, log-odds needn’t remain an interaction if you express the results in another way.\n\n\n\n\n\nFigure 2. Top row: The difference in proportions between conditions A and B is the same in groups C and D. When the same results are expressed in odds or log-odds, however, the difference is larger in group D than in group C. Middle row: This time, the difference in odds is the same in both groups, but when expressed in proportions or log-odds, the difference is larger in group C than in group D. Bottom row: The difference in log-odds is the same in both groups. When expressed in odds, it is larger in group D, but when expressed in proportions, it is smaller in group D.\n\n\n\n\nWhat’s the practical upshot of all this? First, before you interpret a non-crossover interaction, read Wagenmakers et al. (2012). Second, if you’re working with binary data and you predict a non-crossover interaction in a logistic model, be aware that a significant interaction in terms of the log-odds output by your model needn’t correspond to an interaction in terms of proportions, and vice versa."
  },
  {
    "objectID": "posts/2019-08-07-interactions-logistic/index.html#how-to-check-for-interactions-in-terms-of-proportions",
    "href": "posts/2019-08-07-interactions-logistic/index.html#how-to-check-for-interactions-in-terms-of-proportions",
    "title": "Interactions in logistic regression models",
    "section": "How to check for interactions in terms of proportions",
    "text": "How to check for interactions in terms of proportions\nAt this point, you may be wondering how you can check whether a significant interaction in terms of log-odds in your own model corresponds to a significant interaction in terms of proportions, too. (If you aren’t, check out this random Garfield comic instead.) In the following, I show two ways how you can do this: using parametric bootstrapping and by going Bayesian.\nWe’ll work with a simple made-up dataset. The question is whether variables AB and CD interact with respect to the outcome variable Successes. For each combination of predictor variables, 1000 cases were observed, among which variable numbers of Successes:\n\n# Create a dataset\ndf &lt;- expand.grid(\n  AB = c(\"A\", \"B\"),\n  CD = c(\"C\", \"D\")\n)\ndf$Total &lt;- 1000\ndf$Successes &lt;- c(473, 524, 898, 945)\ndf\n\n  AB CD Total Successes\n1  A  C  1000       473\n2  B  C  1000       524\n3  A  D  1000       898\n4  B  D  1000       945\n\n\n\nUsing parametric bootstrapping\nFirst, fit the logistic regression model. Unsurprisingly (since this is a made-up dataset), the interaction effect is significant when expressed in log-odds (0.46, 95% confidence interval: [0.08, 0.85], z = 2.38, p = 0.017):\n\n# Fit a logistic regression model\nm &lt;- glm(cbind(Successes, Total - Successes) ~ AB*CD, data = df, family = \"binomial\")\nsummary(m)$coefficients\n\n            Estimate Std. Error z value Pr(&gt;|z|)\n(Intercept)   -0.108     0.0633   -1.71 8.79e-02\nABB            0.204     0.0896    2.28 2.26e-02\nCDD            2.283     0.1222   18.69 6.29e-78\nABB:CDD        0.464     0.1954    2.38 1.74e-02\n\nconfint(m)\n\nWaiting for profiling to be done...\n\n\n              2.5 % 97.5 %\n(Intercept) -0.2324 0.0159\nABB          0.0287 0.3799\nCDD          2.0478 2.5271\nABB:CDD      0.0849 0.8519\n\n\nWhen expressed in proportions, however, the difference between C and D in condition A is pretty much as large as the difference between C and D in condition B. For this example, we could just as well compute this directly from dataset df, but a more general method is to ask the model to predict the proportions and then compute the differences between them:\n\n# Predicted proportions in condition A\npredict(m, type = \"response\", newdata = data.frame(AB = \"A\", CD = \"C\"))\n\n    1 \n0.473 \n\npredict(m, type = \"response\", newdata = data.frame(AB = \"A\", CD = \"D\"))\n\n    1 \n0.898 \n\n# difference:\n0.898 - 0.473\n\n[1] 0.425\n\n# Predicted proportions in condition B\npredict(m, type = \"response\", newdata = data.frame(AB = \"B\", CD = \"C\"))\n\n    1 \n0.524 \n\npredict(m, type = \"response\", newdata = data.frame(AB = \"B\", CD = \"D\"))\n\n    1 \n0.945 \n\n# difference:\n0.945 - 0.524\n\n[1] 0.421\n\n# difference between differences:\n0.425 - 0.421\n\n[1] 0.004\n\n\n0.004 is the estimated interaction effect when expressed on the proportion scale. Now we need to express our uncertainty about this number. Here I do this by means of a parametric bootstrap. If you’ve never heard of bootstrapping, you can read my blog post Some illustrations of bootstrapping. In a nutshell, we’re going to use the model to simulate new datasets, refit the model on those simulate datasets, and calculate the interaction effect according to these refitted models in the same way as we did above. This will yield a large number of estimated interaction effects (some larger than 0.004, others smaller) on the basis of which we can estimate how large our estimation error could be:\n\n# The following code implements a parametric bootstrap\n#   with 'bs_runs' runs.\nbs_runs &lt;- 20000\n\n# Preallocate matrix to contain predicted proportions per cell\nbs_proportions &lt;- matrix(ncol = 4, nrow = bs_runs)\n\n# Simulate new data from model, refit model, and\n#   obtain predicted proportions a large number of times\nfor (i in 1:bs_runs) {\n  # Simulate a new outcome from the model\n  bs_outcome &lt;- simulate(m)$sim_1\n  \n  # Refit model on simulated outcome\n  bs_m &lt;- glm(bs_outcome ~ AB*CD, data = df, family = \"binomial\")\n  \n  # Predicted proportion (\"response\") for each cell according to model bs_m\n  bs_proportions[i, 1] &lt;- predict(bs_m, type = \"response\",\n                                  newdata = data.frame(AB = \"A\", CD = \"C\"))\n  bs_proportions[i, 2] &lt;- predict(bs_m, type = \"response\",\n                                  newdata = data.frame(AB = \"A\", CD = \"D\"))\n  bs_proportions[i, 3] &lt;- predict(bs_m, type = \"response\",\n                                  newdata = data.frame(AB = \"B\", CD = \"C\"))\n  bs_proportions[i, 4] &lt;- predict(bs_m, type = \"response\",\n                                  newdata = data.frame(AB = \"B\", CD = \"D\"))\n}\n\n# Take the difference between D and C for condition A\nbs_differences_A &lt;- bs_proportions[, 2] - bs_proportions[, 1]\n\n# Take the difference between D and C for condition B\nbs_differences_B &lt;- bs_proportions[, 4] - bs_proportions[, 3]\n\n# Take the difference between these differences\nbs_differences_diff &lt;- bs_differences_B - bs_differences_A\n\n# Plot\nhist(bs_differences_diff, col = \"tomato\",\n     main = \"bootstrapped interaction effects\\n(in proportions)\",\n     xlab = \"estimates\")\n\n\n\n\nFigure 3. Distribution of the bootstrapped interaction effect in proportions. 95% of this distribution falls between -0.054 and 0.046.\n\n\n\n\nThe confidence interval about the number 0.004 can be obtained as follows:\n\n# 95% confidence interval using the percentile approach\nquantile(bs_differences_diff, probs = c(0.025, 0.975))\n\n  2.5%  97.5% \n-0.054  0.046 \n\n\nSo while the interaction effect is significant when expressed in log-odds, its uncertainty interval nicely straddles 0 when expressed in proportions. (Obviously, the example was contrived to yield this result.)\n\n\nGoing the Bayesian route\nAlternatively, you can fit the data in a Bayesian model. I’ve used the brm() function from the brms package in a previous blog post, but its syntax should be fairly transparent. The only real difference (in terms of syntax) between brm() and glm() is in how you specify the outcome variable in a binomial logistic model. In this model (a Bayesian binomial logistic model with uninformative priors), too, the interaction effect is ‘significant’ (not a Bayesian term) when expressed in log-odds (0.47, 95% credible interval: [0.08, 0.86]).\n\nlibrary(brms)\nm.brm &lt;- brm(Successes | trials(Total) ~ AB*CD, data = df, \n             family = \"binomial\", \n             cores = 4,\n             # large number of iter(ations) to reduce\n             #   effect of randomness on inferences\n             iter = 10000, warmup = 1000,\n             # suppress messages\n             silent = 2, refresh = 0)\nsummary(m.brm)\n\n Family: binomial \n  Links: mu = logit \nFormula: Successes | trials(Total) ~ AB * CD \n   Data: df (Number of observations: 4) \n  Draws: 4 chains, each with iter = 10000; warmup = 1000; thin = 1;\n         total post-warmup draws = 36000\n\nPopulation-Level Effects: \n          Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS\nIntercept    -0.11      0.06    -0.23     0.01 1.00    24798    28727\nABB           0.20      0.09     0.03     0.38 1.00    21190    23420\nCDD           2.29      0.12     2.05     2.53 1.00    19101    22207\nABB:CDD       0.47      0.20     0.09     0.86 1.00    18793    21908\n\nDraws were sampled using sampling(NUTS). For each parameter, Bulk_ESS\nand Tail_ESS are effective sample size measures, and Rhat is the potential\nscale reduction factor on split chains (at convergence, Rhat = 1).\n\n\nBayesian models generate not just estimates but entire distributions of estimates (the ‘posterior distribution’). We can use these to to output distributions of predicted proportions and of the differences between them:\n\n# Draw predictions from posterior distribution (in log-odds):\nbrm_predictions &lt;- posterior_linpred(m.brm,\n                                     newdata = expand.grid(\n                                       AB = c(\"A\", \"B\"),\n                                       CD = c(\"C\", \"D\"),\n                                       Total = 1000\n                                       )\n                                     )\n# Instead of using newdata = expand.grid(...),\n#   you could also generate the 4 distributions\n#   one at a time, e.g.:\n# brm_predictions_AC &lt;- posterior_linpred(m.brm,\n#                                         newdata = data.frame(\n#                                           AB = \"A\",\n#                                           CD = \"C\",\n#                                           Total = 1000\n#                                           )\n#                                         )\n\n# Convert predictions in log-odds to predictions in proportions:\nbrm_proportions &lt;- plogis(brm_predictions)\n\n# Take the difference between D and C for condition A\nbrm_difference_A &lt;- brm_proportions[, 3] - brm_proportions[, 1]\n\n# Take the difference between D and C for condition B\nbrm_difference_B &lt;- brm_proportions[, 4] - brm_proportions[, 2]\n\n# Take the difference between these differences\nbrm_difference_diff &lt;- brm_difference_B - brm_difference_A\nhist(brm_difference_diff, col = \"steelblue\",\n     main = \"posterior distribution of interaction\\n(in proportions)\",\n     xlab = \"estimates\")\n\n\n\n\nFigure 4. Posterior distribution of the interaction effect in proportions. 95% of this distribution falls between -0.054 and 0.047.\n\n\n\n\nThe credible interval about the number 0.004 can be obtained as follows:\n\n# 95% credible interval\nquantile(brm_difference_diff, probs = c(0.025, 0.975))\n\n   2.5%   97.5% \n-0.0533  0.0460 \n\n\nThe parametric and the bootstrap approach thus yield essentially the same results in this case."
  },
  {
    "objectID": "posts/2019-08-07-interactions-logistic/index.html#references",
    "href": "posts/2019-08-07-interactions-logistic/index.html#references",
    "title": "Interactions in logistic regression models",
    "section": "References",
    "text": "References\nLoftus, Geffrey R. 1978. On interpretation of interactions. Memory & Cognition 6(3). 312-319.\nWagenmakers, Eric-Jan, Angelos-Miltiadis Krypotos, Amy H. Criss and Geoff Iverson. 2012. On the interpretation of removable interactions: A survey of the field 33 years after Loftus. Memory & Cognition 40(2). 145-160."
  },
  {
    "objectID": "posts/2019-08-07-interactions-logistic/index.html#software-versions",
    "href": "posts/2019-08-07-interactions-logistic/index.html#software-versions",
    "title": "Interactions in logistic regression models",
    "section": "Software versions",
    "text": "Software versions\nPlease note that I reran the code on this page on August 6, 2023.\n\ndevtools::session_info()\n\n─ Session info ───────────────────────────────────────────────────────────────\n setting  value\n version  R version 4.3.1 (2023-06-16)\n os       Ubuntu 22.04.2 LTS\n system   x86_64, linux-gnu\n ui       X11\n language en_US\n collate  en_US.UTF-8\n ctype    en_US.UTF-8\n tz       Europe/Zurich\n date     2023-08-06\n pandoc   3.1.1 @ /usr/lib/rstudio/resources/app/bin/quarto/bin/tools/ (via rmarkdown)\n\n─ Packages ───────────────────────────────────────────────────────────────────\n package        * version date (UTC) lib source\n abind            1.4-5   2016-07-21 [1] CRAN (R 4.3.1)\n backports        1.4.1   2021-12-13 [1] CRAN (R 4.3.0)\n base64enc        0.1-3   2015-07-28 [1] CRAN (R 4.3.0)\n bayesplot        1.10.0  2022-11-16 [1] CRAN (R 4.3.1)\n bridgesampling   1.1-2   2021-04-16 [1] CRAN (R 4.3.1)\n brms           * 2.19.0  2023-03-14 [1] CRAN (R 4.3.1)\n Brobdingnag      1.2-9   2022-10-19 [1] CRAN (R 4.3.1)\n cachem           1.0.6   2021-08-19 [2] CRAN (R 4.2.0)\n callr            3.7.3   2022-11-02 [1] CRAN (R 4.3.1)\n checkmate        2.2.0   2023-04-27 [1] CRAN (R 4.3.1)\n cli              3.6.1   2023-03-23 [1] CRAN (R 4.3.0)\n coda             0.19-4  2020-09-30 [1] CRAN (R 4.3.1)\n codetools        0.2-19  2023-02-01 [4] CRAN (R 4.2.2)\n colorspace       2.1-0   2023-01-23 [1] CRAN (R 4.3.0)\n colourpicker     1.2.0   2022-10-28 [1] CRAN (R 4.3.1)\n crayon           1.5.2   2022-09-29 [1] CRAN (R 4.3.1)\n crosstalk        1.2.0   2021-11-04 [1] CRAN (R 4.3.1)\n curl             5.0.1   2023-06-07 [1] CRAN (R 4.3.1)\n devtools         2.4.5   2022-10-11 [1] CRAN (R 4.3.1)\n digest           0.6.29  2021-12-01 [2] CRAN (R 4.2.0)\n distributional   0.3.2   2023-03-22 [1] CRAN (R 4.3.1)\n dplyr          * 1.1.2   2023-04-20 [1] CRAN (R 4.3.0)\n DT               0.28    2023-05-18 [1] CRAN (R 4.3.1)\n dygraphs         1.1.1.6 2018-07-11 [1] CRAN (R 4.3.1)\n ellipsis         0.3.2   2021-04-29 [2] CRAN (R 4.2.0)\n evaluate         0.15    2022-02-18 [2] CRAN (R 4.2.0)\n fansi            1.0.4   2023-01-22 [1] CRAN (R 4.3.1)\n farver           2.1.1   2022-07-06 [1] CRAN (R 4.3.0)\n fastmap          1.1.0   2021-01-25 [2] CRAN (R 4.2.0)\n forcats        * 1.0.0   2023-01-29 [1] CRAN (R 4.3.0)\n fs               1.5.2   2021-12-08 [2] CRAN (R 4.2.0)\n generics         0.1.3   2022-07-05 [1] CRAN (R 4.3.0)\n ggplot2        * 3.4.2   2023-04-03 [1] CRAN (R 4.3.0)\n glue             1.6.2   2022-02-24 [2] CRAN (R 4.2.0)\n gridExtra        2.3     2017-09-09 [1] CRAN (R 4.3.0)\n gtable           0.3.3   2023-03-21 [1] CRAN (R 4.3.0)\n gtools           3.9.4   2022-11-27 [1] CRAN (R 4.3.1)\n hms              1.1.3   2023-03-21 [1] CRAN (R 4.3.0)\n htmltools        0.5.5   2023-03-23 [1] CRAN (R 4.3.0)\n htmlwidgets      1.6.2   2023-03-17 [1] CRAN (R 4.3.1)\n httpuv           1.6.11  2023-05-11 [1] CRAN (R 4.3.1)\n igraph           1.5.0.1 2023-07-23 [1] CRAN (R 4.3.1)\n inline           0.3.19  2021-05-31 [1] CRAN (R 4.3.1)\n jsonlite         1.8.7   2023-06-29 [1] CRAN (R 4.3.1)\n knitr            1.39    2022-04-26 [2] CRAN (R 4.2.0)\n later            1.3.1   2023-05-02 [1] CRAN (R 4.3.1)\n lattice          0.21-8  2023-04-05 [4] CRAN (R 4.3.0)\n lifecycle        1.0.3   2022-10-07 [1] CRAN (R 4.3.0)\n loo              2.6.0   2023-03-31 [1] CRAN (R 4.3.1)\n lubridate      * 1.9.2   2023-02-10 [1] CRAN (R 4.3.0)\n magrittr         2.0.3   2022-03-30 [1] CRAN (R 4.3.0)\n markdown         1.7     2023-05-16 [1] CRAN (R 4.3.0)\n MASS             7.3-60  2023-05-04 [4] CRAN (R 4.3.1)\n Matrix           1.6-0   2023-07-08 [4] CRAN (R 4.3.1)\n matrixStats      1.0.0   2023-06-02 [1] CRAN (R 4.3.1)\n memoise          2.0.1   2021-11-26 [2] CRAN (R 4.2.0)\n mime             0.10    2021-02-13 [2] CRAN (R 4.0.2)\n miniUI           0.1.1.1 2018-05-18 [1] CRAN (R 4.3.1)\n munsell          0.5.0   2018-06-12 [1] CRAN (R 4.3.0)\n mvtnorm          1.2-2   2023-06-08 [1] CRAN (R 4.3.1)\n nlme             3.1-162 2023-01-31 [4] CRAN (R 4.2.2)\n pillar           1.9.0   2023-03-22 [1] CRAN (R 4.3.0)\n pkgbuild         1.4.2   2023-06-26 [1] CRAN (R 4.3.1)\n pkgconfig        2.0.3   2019-09-22 [2] CRAN (R 4.2.0)\n pkgload          1.3.2.1 2023-07-08 [1] CRAN (R 4.3.1)\n plyr             1.8.8   2022-11-11 [1] CRAN (R 4.3.1)\n posterior        1.4.1   2023-03-14 [1] CRAN (R 4.3.1)\n prettyunits      1.1.1   2020-01-24 [2] CRAN (R 4.2.0)\n processx         3.8.2   2023-06-30 [1] CRAN (R 4.3.1)\n profvis          0.3.8   2023-05-02 [1] CRAN (R 4.3.1)\n promises         1.2.0.1 2021-02-11 [1] CRAN (R 4.3.1)\n ps               1.7.5   2023-04-18 [1] CRAN (R 4.3.1)\n purrr          * 1.0.1   2023-01-10 [1] CRAN (R 4.3.0)\n R6               2.5.1   2021-08-19 [2] CRAN (R 4.2.0)\n Rcpp           * 1.0.11  2023-07-06 [1] CRAN (R 4.3.1)\n RcppParallel     5.1.7   2023-02-27 [1] CRAN (R 4.3.1)\n readr          * 2.1.4   2023-02-10 [1] CRAN (R 4.3.0)\n remotes          2.4.2   2021-11-30 [2] CRAN (R 4.2.0)\n reshape2         1.4.4   2020-04-09 [1] CRAN (R 4.3.1)\n rlang            1.1.1   2023-04-28 [1] CRAN (R 4.3.0)\n rmarkdown        2.21    2023-03-26 [1] CRAN (R 4.3.0)\n rstan            2.26.22 2023-08-01 [1] local\n rstantools       2.3.1.1 2023-07-18 [1] CRAN (R 4.3.1)\n rstudioapi       0.14    2022-08-22 [1] CRAN (R 4.3.0)\n scales           1.2.1   2022-08-20 [1] CRAN (R 4.3.0)\n sessioninfo      1.2.2   2021-12-06 [2] CRAN (R 4.2.0)\n shiny            1.7.4.1 2023-07-06 [1] CRAN (R 4.3.1)\n shinyjs          2.1.0   2021-12-23 [1] CRAN (R 4.3.1)\n shinystan        2.6.0   2022-03-03 [1] CRAN (R 4.3.1)\n shinythemes      1.2.0   2021-01-25 [1] CRAN (R 4.3.1)\n StanHeaders      2.26.27 2023-06-14 [1] CRAN (R 4.3.1)\n stringi          1.7.12  2023-01-11 [1] CRAN (R 4.3.1)\n stringr        * 1.5.0   2022-12-02 [1] CRAN (R 4.3.0)\n tensorA          0.36.2  2020-11-19 [1] CRAN (R 4.3.1)\n threejs          0.3.3   2020-01-21 [1] CRAN (R 4.3.1)\n tibble         * 3.2.1   2023-03-20 [1] CRAN (R 4.3.0)\n tidyr          * 1.3.0   2023-01-24 [1] CRAN (R 4.3.0)\n tidyselect       1.2.0   2022-10-10 [1] CRAN (R 4.3.0)\n tidyverse      * 2.0.0   2023-02-22 [1] CRAN (R 4.3.1)\n timechange       0.2.0   2023-01-11 [1] CRAN (R 4.3.0)\n tzdb             0.4.0   2023-05-12 [1] CRAN (R 4.3.0)\n urlchecker       1.0.1   2021-11-30 [1] CRAN (R 4.3.1)\n usethis          2.2.2   2023-07-06 [1] CRAN (R 4.3.1)\n utf8             1.2.3   2023-01-31 [1] CRAN (R 4.3.1)\n V8               4.3.0   2023-04-08 [1] CRAN (R 4.3.0)\n vctrs            0.6.3   2023-06-14 [1] CRAN (R 4.3.0)\n withr            2.5.0   2022-03-03 [2] CRAN (R 4.2.0)\n xfun             0.39    2023-04-20 [1] CRAN (R 4.3.0)\n xtable           1.8-4   2019-04-21 [1] CRAN (R 4.3.1)\n xts              0.13.1  2023-04-16 [1] CRAN (R 4.3.1)\n yaml             2.3.5   2022-02-21 [2] CRAN (R 4.2.0)\n zoo              1.8-12  2023-04-13 [1] CRAN (R 4.3.1)\n\n [1] /home/jan/R/x86_64-pc-linux-gnu-library/4.3\n [2] /usr/local/lib/R/site-library\n [3] /usr/lib/R/site-library\n [4] /usr/lib/R/library\n\n──────────────────────────────────────────────────────────────────────────────"
  },
  {
    "objectID": "posts/2014-09-12-a-graphical-explanation-of-p-values/index.html",
    "href": "posts/2014-09-12-a-graphical-explanation-of-p-values/index.html",
    "title": "A purely graphical explanation of p-values",
    "section": "",
    "text": "p-values are confusing creatures–not just to students taking their first statistics course but to seasoned researchers, too. To fully understand how p-values are calculated, you need to know a good deal of mathematical and statistical concepts (e.g. probability distributions, standard deviations, the central limit theorem, null hypotheses), and these take a while to settle in. But to explain the meaning of p-values, I think we can get by without such sophistication. So here you have it, a purely graphical explanation of p-values."
  },
  {
    "objectID": "posts/2014-09-12-a-graphical-explanation-of-p-values/index.html#a-graphical-t-test-anova",
    "href": "posts/2014-09-12-a-graphical-explanation-of-p-values/index.html#a-graphical-t-test-anova",
    "title": "A purely graphical explanation of p-values",
    "section": "A graphical t-test / ANOVA",
    "text": "A graphical t-test / ANOVA\nThe statistical tests you’re most likely to run into are the two-sample t-test and its big brother, analysis of variance (ANOVA). These tests are used to establish whether the difference between two or more group averages are due to chance or whether their is some underlying system to them.\nFor this example, I use data from my Ph.D. project. The goal of this analysis (not of my thesis, though) is to establish whether men and women differ in their ability to understand spoken words in an unknown but related foreign language. The following R code reads in the data, tabulates the number of men and women (table()) and shows the distribution of the participants’ scores on the listening test (hist()). To reproduce this analysis, you’ll need to install R, which is freely available from http://r-project.org/–and while you’re at it, download a graphical user interface like RStudio.\n\ndat &lt;- read.csv(\"http://janhove.github.io/datasets/sinergia.csv\")\ntable(dat$Sex)\n\n\nfemale   male \n    90     73 \n\n\n\nhist(dat$Spoken, main=\"Histogram\", xlab=\"Score\", col=\"lightblue\")\n\n\n\n\nTo establish whether men and women differ ‘significantly’ from one another, we’d usually plot the data, run a t-test, and check the reported p-value, but an alternative approach is discussed by Wickham and colleagues.\nThe basic idea is as follows. If there is a systematic relationship between two variables (e.g. a difference in group averages), then we ought to be able to tell the real dataset from fake datasets that were randomly generated without an underlying systematic relationship. To generate fake but ‘realistic’ datasets, we can permute the real data, i.e., randomly switch the datapoints between the groups. Then, we organise a line-up (like in the movies!) with the real dataset hidden among several fake datasets. If we can distinguish the real dataset from the fake ones, we have some evidence that there is some degree of system in the real data that is not present in the fake data.\nThis procedure is implemented in the nullabor package for R, which relies on the ggplot2 package. Make sure you have these two packages installed on your machine (install.packages()).\n\n# Remove the # before the two next lines to install the ggplot2 and nullabor packages.\n# install.packages(\"ggplot2\")\n# install.packages(\"nullabor\")\n# Load packages\nlibrary(ggplot2)\nlibrary(nullabor)\n\nThe following code plots the listening scores for all participants according to whether they’re male or female, but the real dataset is hidden among 19 fake (permuted) datasets. The crosshairs mark the means for each sex in each dataset. Can you pick out the real dataset? (The code to generate this plot is somewhat complicated but is of no consequence to the argument; don’t lose yourself in it.)\nUpdate (2023-08-25): Compared to the original blog post, I replaced the fun.y argument, which is now deprecated, by fun.\n\np &lt;- ggplot(dat, aes(Sex, Spoken)) %+%\n      lineup(null_permute(\"Spoken\"), dat) +\n      scale_y_continuous(limits = c(0, 30)) + \n      #geom_boxplot(outlier.shape = NA, na.rm = TRUE) +\n      geom_point(size = 1,\n                 aes(col=Sex),\n                 position = position_jitter(w=0.3, h = 0)) +\n      stat_summary(fun = mean, \n                   geom = \"point\", \n                   shape = 10, size = 4) +\n      guides(colour = \"none\") +\n      facet_wrap(~ .sample)\n\ndecrypt(\"sFZM ldbd 32 PN43b3N2 5g\")\n\np\n\n\n\n\nMost sample means are pretty similar, but not entirely identical, to each other. Due to the randomness inherent in generating permutations, some differences between the sample means for men and women are in fact expected. This is known as sampling error. If men’s and women’s listening scores didn’t differ systematically from one another, we wouldn’t be able to pick out the real dataset with above-chance accuracy, i.e., we would only have a 1 in 20 (5%) chance of picking the right answer.\nIf there were some systematic difference between the two groups, however, we would be able to beat those odds. Eye-balling these plots, I’d say that Panel 16 stands out in that the difference between men and women is larger than in the other panels. Using the encrypted code provided above, we can check whether this hunch is correct:\n\ndecrypt(\"sFZM ldbd 32 PN43b3N2 5g\")\n\n[1] \"True data in position  16\"\n\n\nIt is. If there were no systematic difference between the two groups (``if the null hypothesis were correct’‘, in statistics parlance), I’d only have had a 1-in-20 chance of picking the right answer, yet I managed to do so. Put differently, it’s pretty unlikely (5%) to observe this noticeable a difference between the two groups if no systematic pattern existed, and from this we’d usually conclude that there is some systematic pattern. This, in effect, is what is expressed by’p = 0.05’. To establish ‘p = 0.01’ using this procedure, you’d have to be able to pick out the actual dataset when it’s hidden among 99 fake datasets – but that wouldn’t work now since you already know how it looks.\nA cautionary note, though: While we’ve established with some confidence that women have differerent (higher) listening scores from men, we’ve not demonstrated that it’s the sex difference that causes this difference. In this study, the men were on average somewhat older than the women, for instance."
  },
  {
    "objectID": "posts/2014-09-12-a-graphical-explanation-of-p-values/index.html#a-graphical-correlation-test",
    "href": "posts/2014-09-12-a-graphical-explanation-of-p-values/index.html#a-graphical-correlation-test",
    "title": "A purely graphical explanation of p-values",
    "section": "A graphical correlation test",
    "text": "A graphical correlation test\nThe graphical test can also be applied to the relationship between two continuous variables. Usually, we’d compute a correlation test or fit a regression model to get a p-value for such a situation.\nThe following plot shows the relationship between a measure of intelligence (‘Raven’) and the participants’ performance on a reading task. The real dataset is again hidden among 19 fake datasets that were generated without an underlying pattern. Can you spot the real dataset?\n\nq &lt;- ggplot(dat, aes(Raven, Written)) %+%\n      lineup(null_permute(\"Raven\"), dat, n = 20) +\n      geom_point(size = 1, shape = 1) +\n      facet_wrap(~ .sample)\n\ndecrypt(\"sFZM ldbd 32 PN43b3N2 ua\")\n\nq\n\n\n\n\nYou might be able to spot the real dataset, but I had to peek at the right answer to know it was Panel 7. In this case, I wasn’t able to beat the odds since the real data look too similar to the fake data, suggesting that ‘no systematic pattern’ is a more parsimonious relationship.\nCompare this to the following plot, which shows the participants’ reading scores in function of their English skills.\n\nr &lt;- ggplot(dat, aes(English, Written)) %+%\n      lineup(null_permute(\"English\"), dat, n = 20) +\n      geom_point(size = 1, shape = 1) +\n      facet_wrap(~ .sample)\n\ndecrypt(\"sFZM ldbd 32 PN43b3N2 uJ\")\n\nr\n\n\n\n\nIf you picked Panel 8 because its discernible positive trend, you’d be right. A statistical test would likely yield a p-value lower than 0.05. In fact, in this case, you’d probably be able to pick out the real data from among a much larger number of fake datasets."
  },
  {
    "objectID": "posts/2014-09-12-a-graphical-explanation-of-p-values/index.html#concluding-remarks",
    "href": "posts/2014-09-12-a-graphical-explanation-of-p-values/index.html#concluding-remarks",
    "title": "A purely graphical explanation of p-values",
    "section": "Concluding remarks",
    "text": "Concluding remarks\nObviously there’s some subjectivity inherent in this type of analysis, and the procedure breaks down when you already know the shape of the actual data. To increase its reliability, you could try to enlist the help of a panel of independent judges who are not involved in the research project. That said, the goal of this post was to clarify what enigmatic phrases like ‘p &lt; 0.05’ express: the degree of unusualness of the data at hand compared to data with no underlying systematic pattern."
  },
  {
    "objectID": "posts/2014-09-12-a-graphical-explanation-of-p-values/index.html#software-versions",
    "href": "posts/2014-09-12-a-graphical-explanation-of-p-values/index.html#software-versions",
    "title": "A purely graphical explanation of p-values",
    "section": "Software versions",
    "text": "Software versions\n\ndevtools::session_info()\n\n─ Session info ───────────────────────────────────────────────────────────────\n setting  value\n version  R version 4.3.1 (2023-06-16)\n os       Ubuntu 22.04.3 LTS\n system   x86_64, linux-gnu\n ui       X11\n language en_US\n collate  en_US.UTF-8\n ctype    en_US.UTF-8\n tz       Europe/Zurich\n date     2023-08-25\n pandoc   3.1.1 @ /usr/lib/rstudio/resources/app/bin/quarto/bin/tools/ (via rmarkdown)\n\n─ Packages ───────────────────────────────────────────────────────────────────\n package     * version date (UTC) lib source\n cachem        1.0.6   2021-08-19 [2] CRAN (R 4.2.0)\n callr         3.7.3   2022-11-02 [1] CRAN (R 4.3.1)\n class         7.3-22  2023-05-03 [4] CRAN (R 4.3.1)\n cli           3.6.1   2023-03-23 [1] CRAN (R 4.3.0)\n cluster       2.1.4   2022-08-22 [4] CRAN (R 4.2.1)\n colorspace    2.1-0   2023-01-23 [1] CRAN (R 4.3.0)\n crayon        1.5.2   2022-09-29 [1] CRAN (R 4.3.1)\n DEoptimR      1.1-0   2023-07-12 [1] CRAN (R 4.3.1)\n devtools      2.4.5   2022-10-11 [1] CRAN (R 4.3.1)\n digest        0.6.29  2021-12-01 [2] CRAN (R 4.2.0)\n diptest       0.76-0  2021-05-04 [1] CRAN (R 4.3.1)\n dplyr         1.1.2   2023-04-20 [1] CRAN (R 4.3.0)\n ellipsis      0.3.2   2021-04-29 [2] CRAN (R 4.2.0)\n evaluate      0.15    2022-02-18 [2] CRAN (R 4.2.0)\n fansi         1.0.4   2023-01-22 [1] CRAN (R 4.3.1)\n farver        2.1.1   2022-07-06 [1] CRAN (R 4.3.0)\n fastmap       1.1.0   2021-01-25 [2] CRAN (R 4.2.0)\n flexmix       2.3-19  2023-03-16 [1] CRAN (R 4.3.1)\n fpc           2.2-10  2023-01-07 [1] CRAN (R 4.3.1)\n fs            1.5.2   2021-12-08 [2] CRAN (R 4.2.0)\n generics      0.1.3   2022-07-05 [1] CRAN (R 4.3.0)\n ggplot2     * 3.4.2   2023-04-03 [1] CRAN (R 4.3.0)\n glue          1.6.2   2022-02-24 [2] CRAN (R 4.2.0)\n gtable        0.3.3   2023-03-21 [1] CRAN (R 4.3.0)\n htmltools     0.5.5   2023-03-23 [1] CRAN (R 4.3.0)\n htmlwidgets   1.6.2   2023-03-17 [1] CRAN (R 4.3.1)\n httpuv        1.6.11  2023-05-11 [1] CRAN (R 4.3.1)\n jsonlite      1.8.7   2023-06-29 [1] CRAN (R 4.3.1)\n kernlab       0.9-32  2023-01-31 [1] CRAN (R 4.3.1)\n knitr         1.39    2022-04-26 [2] CRAN (R 4.2.0)\n labeling      0.4.2   2020-10-20 [1] CRAN (R 4.3.0)\n later         1.3.1   2023-05-02 [1] CRAN (R 4.3.1)\n lattice       0.21-8  2023-04-05 [4] CRAN (R 4.3.0)\n lifecycle     1.0.3   2022-10-07 [1] CRAN (R 4.3.0)\n magrittr      2.0.3   2022-03-30 [1] CRAN (R 4.3.0)\n MASS          7.3-60  2023-05-04 [4] CRAN (R 4.3.1)\n mclust        6.0.0   2022-10-31 [1] CRAN (R 4.3.1)\n memoise       2.0.1   2021-11-26 [2] CRAN (R 4.2.0)\n mime          0.10    2021-02-13 [2] CRAN (R 4.0.2)\n miniUI        0.1.1.1 2018-05-18 [1] CRAN (R 4.3.1)\n modeltools    0.2-23  2020-03-05 [1] CRAN (R 4.3.1)\n moments       0.14.1  2022-05-02 [1] CRAN (R 4.3.1)\n munsell       0.5.0   2018-06-12 [1] CRAN (R 4.3.0)\n nnet          7.3-19  2023-05-03 [4] CRAN (R 4.3.1)\n nullabor    * 0.3.9   2020-02-25 [1] CRAN (R 4.3.1)\n pillar        1.9.0   2023-03-22 [1] CRAN (R 4.3.0)\n pkgbuild      1.4.2   2023-06-26 [1] CRAN (R 4.3.1)\n pkgconfig     2.0.3   2019-09-22 [2] CRAN (R 4.2.0)\n pkgload       1.3.2.1 2023-07-08 [1] CRAN (R 4.3.1)\n prabclus      2.3-2   2020-01-08 [1] CRAN (R 4.3.1)\n prettyunits   1.1.1   2020-01-24 [2] CRAN (R 4.2.0)\n processx      3.8.2   2023-06-30 [1] CRAN (R 4.3.1)\n profvis       0.3.8   2023-05-02 [1] CRAN (R 4.3.1)\n promises      1.2.0.1 2021-02-11 [1] CRAN (R 4.3.1)\n ps            1.7.5   2023-04-18 [1] CRAN (R 4.3.1)\n purrr         1.0.1   2023-01-10 [1] CRAN (R 4.3.0)\n R6            2.5.1   2021-08-19 [2] CRAN (R 4.2.0)\n Rcpp          1.0.11  2023-07-06 [1] CRAN (R 4.3.1)\n remotes       2.4.2   2021-11-30 [2] CRAN (R 4.2.0)\n rlang         1.1.1   2023-04-28 [1] CRAN (R 4.3.0)\n rmarkdown     2.21    2023-03-26 [1] CRAN (R 4.3.0)\n robustbase    0.99-0  2023-06-16 [1] CRAN (R 4.3.1)\n rstudioapi    0.14    2022-08-22 [1] CRAN (R 4.3.0)\n scales        1.2.1   2022-08-20 [1] CRAN (R 4.3.0)\n sessioninfo   1.2.2   2021-12-06 [2] CRAN (R 4.2.0)\n shiny         1.7.4.1 2023-07-06 [1] CRAN (R 4.3.1)\n stringi       1.7.12  2023-01-11 [1] CRAN (R 4.3.1)\n stringr       1.5.0   2022-12-02 [1] CRAN (R 4.3.0)\n tibble        3.2.1   2023-03-20 [1] CRAN (R 4.3.0)\n tidyr         1.3.0   2023-01-24 [1] CRAN (R 4.3.0)\n tidyselect    1.2.0   2022-10-10 [1] CRAN (R 4.3.0)\n urlchecker    1.0.1   2021-11-30 [1] CRAN (R 4.3.1)\n usethis       2.2.2   2023-07-06 [1] CRAN (R 4.3.1)\n utf8          1.2.3   2023-01-31 [1] CRAN (R 4.3.1)\n vctrs         0.6.3   2023-06-14 [1] CRAN (R 4.3.0)\n withr         2.5.0   2022-03-03 [2] CRAN (R 4.2.0)\n xfun          0.39    2023-04-20 [1] CRAN (R 4.3.0)\n xtable        1.8-4   2019-04-21 [1] CRAN (R 4.3.1)\n yaml          2.3.5   2022-02-21 [2] CRAN (R 4.2.0)\n\n [1] /home/jan/R/x86_64-pc-linux-gnu-library/4.3\n [2] /usr/local/lib/R/site-library\n [3] /usr/lib/R/site-library\n [4] /usr/lib/R/library\n\n──────────────────────────────────────────────────────────────────────────────"
  },
  {
    "objectID": "posts/2023-02-09-julia-levenshtein/index.html",
    "href": "posts/2023-02-09-julia-levenshtein/index.html",
    "title": "Adjusting to Julia: The Levenshtein algorithm",
    "section": "",
    "text": "In this second blog post about Julia, I’ll share with you a Julia implementation of the Levenshtein algorithm."
  },
  {
    "objectID": "posts/2023-02-09-julia-levenshtein/index.html#the-levenshtein-algorithm",
    "href": "posts/2023-02-09-julia-levenshtein/index.html#the-levenshtein-algorithm",
    "title": "Adjusting to Julia: The Levenshtein algorithm",
    "section": "The Levenshtein algorithm",
    "text": "The Levenshtein algorithm\nThe basic Levenshtein algorithm is used to count the minimum number of insertions, deletions and substitutions that are needed to convert one string into another. For instance, to convert English doubt into French doute, you need at least two operations. You could replace the b by a t and then replace the t by an e; or you could delete the b and then insert the e. As this example shows, there may be more than one way to convert one string into another using the minimum number of required operations, but this minimum number itself is unique for each pair of strings."
  },
  {
    "objectID": "posts/2023-02-09-julia-levenshtein/index.html#implementation-in-julia",
    "href": "posts/2023-02-09-julia-levenshtein/index.html#implementation-in-julia",
    "title": "Adjusting to Julia: The Levenshtein algorithm",
    "section": "Implementation in Julia",
    "text": "Implementation in Julia\nI won’t cover the logic of the Levenshtein algorithm here. The following is a straightforward Julia implementation of the pseudocode found on Wikipedia, assuming a cost of 1 for all operations. The function takes two inputs (a string a that is to be converted to a string b) and outputs an array with the Levenshtein distances between all substrings of a on the one hand and all substrings of b on the other hand. The entry in the bottom right corner of this array is the Levenshtein distances between the full strings and this is output separately as well.\n\nfunction levenshtein(a::String, b::String)\n    # Initialise table\n    distances = zeros(Int, length(a) + 1, length(b) + 1)\n    distances[:, 1] = 0:length(a)\n    distances[1, :] = 0:length(b)\n\n    # Levenshtein logic\n    for row in 2:(length(a) + 1)\n        for col in 2:(length(b) + 1)\n            distances[row, col] = min(\n                distances[row - 1, col - 1] + Int(a[row - 1] != b[col - 1] ? 1 : 0)\n                , distances[row, col - 1] + 1\n                , distances[row - 1, col] + 1\n            )\n        end\n    end\n\n    return distances, distances[length(a) + 1, length(b) + 1]\nend\n\nlevenshtein (generic function with 1 method)\n\n\nLet’s compute the Levenshtein distance between the German word Zyklus (‘cycle’) and its Swedish counterpart cykel. Note the use of ; at the end of the line to suppress the output.\n\ndist_matrix, lev_cost = levenshtein(\"zyklus\", \"cykel\");\ndisplay(dist_matrix)\n\n7×6 Matrix{Int64}:\n 0  1  2  3  4  5\n 1  1  2  3  4  5\n 2  2  1  2  3  4\n 3  3  2  1  2  3\n 4  4  3  2  2  2\n 5  5  4  3  3  3\n 6  6  5  4  4  4\n\n\nThis checks out: you do indeed need four operations to transform Zyklus into cykel."
  },
  {
    "objectID": "posts/2023-02-09-julia-levenshtein/index.html#a-vectorised-function",
    "href": "posts/2023-02-09-julia-levenshtein/index.html#a-vectorised-function",
    "title": "Adjusting to Julia: The Levenshtein algorithm",
    "section": "A vectorised function",
    "text": "A vectorised function\nBut what if we wanted to apply our new functions to several pairs of strings? Let’s first define three Dutch-German word pairs:\n\ndutch = (\"boek\", \"zuster\", \"sneeuw\");\ngerman = (\"buch\", \"schwester\", \"schnee\");\n\nWe can run our levenshtein() on these three word pairs without introducing for-loops by simply appending a dot to the function name:\n\nlevenshtein.(dutch, german)\n\n(([0 1 … 3 4; 1 0 … 2 3; … ; 3 2 … 2 3; 4 3 … 3 3], 3), ([0 1 … 8 9; 1 1 … 8 9; … ; 5 4 … 5 6; 6 5 … 6 5], 5), ([0 1 … 5 6; 1 0 … 4 5; … ; 5 4 … 4 3; 6 5 … 5 4], 4))\n\n\nHowever, since the levenshtein() function outputs two pieces of information (both the matrix with the distances between the substrings as well as the final Levenshtein distance), this vectorised call yields a tuple of three subtuples, each subtuple containing both a matrix and the corresponding final Levenshtein distance. This is why the output above looks so messy. If we wanted to obtain just the Levenshtein distances, we could write a for-loop to extract them. But I think an easier solution is to first write a wrapper around the levenshtein() function that outputs only the final Levenshtein distance and use the vectorised version of this wrapper instead:\n\nfunction lev_dist(a::String, b::String)\n  return levenshtein(a, b)[2]\nend\n\nlev_dist (generic function with 1 method)\n\n\nNow use the vectorised version of lev_dist():\n\nlev_dist.(dutch, german)\n\n(3, 5, 4)\n\n\nNice!"
  },
  {
    "objectID": "posts/2023-02-09-julia-levenshtein/index.html#obtaining-the-operations",
    "href": "posts/2023-02-09-julia-levenshtein/index.html#obtaining-the-operations",
    "title": "Adjusting to Julia: The Levenshtein algorithm",
    "section": "Obtaining the operations",
    "text": "Obtaining the operations\nWe now know that we need four operations to transform Zyklus into cykel and five to transform zuster into Schwester. But which are the operations that you need for these transformations? The function lev_alignment() defined below outputs one possible set of operations that would do the job. (Unlike the minimum number of operations required to transform one string into another, the set of operations needed isn’t uniquely defined.)\n\nfunction lev_alignment(a::String, b::String)\n    source = Vector{Char}()\n    target = Vector{Char}()\n    operations = Vector{Char}()\n    \n    lev_matrix = levenshtein(a, b)[1]\n    \n    row = size(lev_matrix, 1)\n    col = size(lev_matrix, 2)\n\n    while (row &gt; 1 && col &gt; 1)\n        if lev_matrix[row - 1, col - 1] == lev_matrix[row, col] &&\n            lev_matrix[row - 1, col - 1] &lt;= min(\n              lev_matrix[row - 1, col]\n              , lev_matrix[row, col - 1]\n              )\n            row = row - 1\n            col = col - 1\n            pushfirst!(source, a[row])\n            pushfirst!(target, b[col])\n            pushfirst!(operations, ' ')\n        else \n            if lev_matrix[row - 1, col] &lt;= min(lev_matrix[row - 1, col - 1], lev_matrix[row, col - 1])\n                row = row - 1\n                pushfirst!(source, a[row])\n                pushfirst!(target, ' ')\n                pushfirst!(operations, 'D')\n            elseif lev_matrix[row, col - 1] &lt;= lev_matrix[row - 1, col - 1]\n                col = col - 1\n                pushfirst!(source, ' ')\n                pushfirst!(target, b[col])\n                pushfirst!(operations, 'I')\n            else\n                row = row - 1\n                col = col - 1\n                pushfirst!(source, a[row])\n                pushfirst!(target, b[col])\n                pushfirst!(operations, 'S')\n            end\n        end\n    end\n\n    # If first column reached, move up.\n    while (row &gt; 1)\n        row = row - 1\n        pushfirst!(source, a[row])\n        pushfirst!(target, ' ')\n        pushfirst!(operations, 'D')\n    end\n\n    # If first row reached, move left.\n    while (col &gt; 1)\n        col = col - 1\n        pushfirst!(source, ' ')\n        pushfirst!(target, b[col])\n        pushfirst!(operations, 'I')\n    end\n    \n    return vcat(\n        reshape(source, (1, :))\n        , reshape(target, (1, :))\n        , reshape(operations, (1, :))\n    )\nend\n\nlev_alignment (generic function with 1 method)\n\n\nI won’t cover the logic behind the algorithm as this is more about learning Julia that the Levenshtein algorithm. On the Julia side, note first how empty character vectors can be initialised. Moreover, notice that the pushfirst!()\nfunction is decorated with a ! (a ‘bang’). This communicates to whoever is reading the code that this function changes some of its input. For instance, pushfirst!(source, a[row]) means that the current character of a (i.e., a[row]) is added to the front of the source vector. That is, this command changes the source vector. Finally, the source, target and operations vectors are all column vectors. In order to display them somewhat nicely, I converted each of them to a single-row matrix using reshape(). Then, the three resulting rows are concatenated vertically using vcat() to show how the two strings can be aligned and which operations are needed to transform one into the other.\nLet’s see how we can transform Zyklus into cykel:\n\nlev_alignment(\"zyklus\", \"cykel\")\n\n3×7 Matrix{Char}:\n 'z'  'y'  'k'  ' '  'l'  'u'  's'\n 'c'  'y'  'k'  'e'  'l'  ' '  ' '\n 'S'  ' '  ' '  'I'  ' '  'D'  'D'\n\n\nSo we substitute c for z, insert an e and delete the u and s. As I mentioned, this set of operations isn’t uniquely defined. Indeed, we could have also substituted c for z, e for l and l for u and then deleted the s. This also corresponds to a Levenshtein distance of four operations."
  },
  {
    "objectID": "posts/2023-02-09-julia-levenshtein/index.html#normalised-levenshtein-distances",
    "href": "posts/2023-02-09-julia-levenshtein/index.html#normalised-levenshtein-distances",
    "title": "Adjusting to Julia: The Levenshtein algorithm",
    "section": "Normalised Levenshtein distances",
    "text": "Normalised Levenshtein distances\nAbove, we computed raw Levenshtein distances. The problem with these is that longer string pairs will tend to have larger raw Levenshtein distances than shorter string pairs, even if they do seem more similar. To correct for this, we can computed normalised Levenshtein distances instead. There are various ways to compute these; one option is to divide the raw Levenshtein distance by the length of the alignment:\n\nfunction norm_lev_dist(a::String, b::String)\n  raw_dist = lev_dist(a, b)\n  alignment_length = size(lev_alignment(a, b), 2)\n  return raw_dist / alignment_length\nend\n\nnorm_lev_dist (generic function with 1 method)\n\n\n(Behind the scenes, we run the Levenshtein algorithm twice: once in lev_dist() and again in lev_alignment(). This seems wasteful - unless the Julia compiler is able to clean up the double work? I’m not sure.)\nWe obtain a normalised Levenshtein distance of about 0.57 for Zyklus - cykel:\n\nnorm_lev_dist(\"zyklus\", \"cykel\")\n\n0.5714285714285714\n\n\nWe can use a vectorised version of this function, too:\n\nnorm_lev_dist.(dutch, german)\n\n(0.75, 0.5555555555555556, 0.5)\n\n\nOf course, normalised Levenshtein distances are symmetric, so we obtain the same result when running the following command:\n\nnorm_lev_dist.(german, dutch)\n\n(0.75, 0.5555555555555556, 0.5)"
  },
  {
    "objectID": "posts/2015-06-18-preparing-data-for-analysis/index.html",
    "href": "posts/2015-06-18-preparing-data-for-analysis/index.html",
    "title": "Some tips on preparing your data for analysis",
    "section": "",
    "text": "How to prepare your spreadsheets so that they can be analysed efficiently is something you tend to learn on the job. In this blog post, I give some tips for organising datasets that I hope will be of use to students and researchers starting out in quantitative research."
  },
  {
    "objectID": "posts/2015-06-18-preparing-data-for-analysis/index.html#use-self-explanatory-names-and-labels-but-keep-them-short",
    "href": "posts/2015-06-18-preparing-data-for-analysis/index.html#use-self-explanatory-names-and-labels-but-keep-them-short",
    "title": "Some tips on preparing your data for analysis",
    "section": "Use self-explanatory names and labels, but keep them short",
    "text": "Use self-explanatory names and labels, but keep them short\nTry to minimise the need to consult the project codebook during the analysis. To this end, name your variables and their values sensibly. If you’re analysing questionnaire data, for instance, a variable name like Q3c means little to the analyst and requires them to refer back to the questionnaire or the codebook. Renaming this variable DialectUse makes it immediately clear that the variable likely encodes the degree to which the informant claims to use a dialect.\nSimilarly, a series of 0s and 1s in the Sex variable is uninformative. Sure, one of them will mean ‘man’ and the other ‘woman’, but which is which? Using the labels man and woman instead avoids any misunderstandings. Also note that values don’t have to be labelled numerically.\nLastly, use a consistent and unambiguous label to identify missing data. NA is usually a good choice, but don’t use 0 or -99 etc. Especially if you’re analysing numeric data, it may not be immediately obvious to the analyst that 0 or -99 don’t refer to actual measurements. (See exercises 2 and 3 on page 77 in my introduction to statistics, if you can read German.)\nTransparent variables names and data labels facilitate the analysis because you don’t have to go back and forth between dataset and codebook to make sense of the data. A further benefit is that the code used for the analysis tends to be more readable, even months after the analysis has been conducted.\nThat said, when coming up with descriptive names and labels, try to keep them short. A variable name like HowOftenDoYouSpeakTheLocalDialectWithYourChild is self-explanatory but cumbersome to type repeatedly when analysing the data. DialectChild might be less self-explanatory but is easier to use.\nLastly, try to keep the following guidelines in mind:\n\n\nCapitalisation matters. In R, an informant whose Sex is man is of a different gender than one whose Sex is Man. Any such confusions become clear soon enough when analysing the data and they’re pretty easy to take care of. But avoiding them is better for the analyst’s grumpiness level :)\n\n\nSpecial characters are tricky. German Umlaut characters and French accents may show up without problems on your computer, but may require the manual specification of a character encoding on a different system. Try to either avoid special characters or know which character encoding you saved the dataset in (see the MS Office support page for Excel; LibreOffice users can just use the ‘Save as’ function and will be asked which encoding they prefer).\n\n\nDon’t use spaces, question marks or exclamation marks in variable names.\n\n\nColour-coding is fine for your own use, but the colours get lost when the dataset is imported into a statistics program.\n\n\n‘Empty’ rows or columns may contain lingering spaces that mess up the dataset when it’s imported into a statistics program. By way of example, the csv file FormantMeasurements1.csv (available here) looks fine when it’s opened in a spreadsheet program:\n\n\n\nExample superfluous spaces\n\n\nWhen the dataset is imported into R, however, there seem to be two additional variables (X and X.1), some missing values (NA) and a Speaker without an ID:\n\ndat &lt;- read.csv(\"http://janhove.github.io/downloads/FormantMeasurements1.csv\",\n                stringsAsFactors = TRUE)\nsummary(dat)\n\n Speaker     Trial            Word          F1               F2        \n   : 2   Min.   : 1.00   baat   : 8   Min.   : 197.0   Min.   : 581.9  \n S1:24   1st Qu.: 6.75   biet   : 8   1st Qu.: 278.5   1st Qu.: 899.7  \n S2:24   Median :12.50   buut   : 8   Median : 350.2   Median :1544.8  \n S3:24   Mean   :12.50   daat   : 8   Mean   : 488.0   Mean   :1557.3  \n S4:24   3rd Qu.:18.25   diet   : 8   3rd Qu.: 772.2   3rd Qu.:2309.4  \n         Max.   :24.00   duut   : 8   Max.   :1031.1   Max.   :2773.7  \n         NA's   :2       (Other):50   NA's   :2        NA's   :2       \n    X             X.1         \n Mode:logical   Mode:logical  \n NA's:98        NA's:98       \n                              \n                              \n                              \n                              \n                              \n\n\nThe reason is that there are superfluous spaces in cells D99 and G9.\n\n\nSimilarly, a value label with a trailing space will be inconspicuous in your spreadsheet program but will affect how the dataset is read in into your statistics program. The csv file FormantMeasurements2.csv contains the same data as FormantMeasurements1.csv, but when read into R, there seem to be two Speakers with the same ID (S2):\n\ndat &lt;- read.csv(\"http://janhove.github.io/downloads/FormantMeasurements2.csv\",\n                stringsAsFactors = TRUE)\nsummary(dat)\n\n Speaker      Trial            Word          F1               F2        \n S1 :24   Min.   : 1.00   baat   : 8   Min.   : 197.0   Min.   : 581.9  \n S2 :23   1st Qu.: 6.75   biet   : 8   1st Qu.: 278.5   1st Qu.: 899.7  \n S2 : 1   Median :12.50   buut   : 8   Median : 350.2   Median :1544.8  \n S3 :24   Mean   :12.50   daat   : 8   Mean   : 488.0   Mean   :1557.3  \n S4 :24   3rd Qu.:18.25   diet   : 8   3rd Qu.: 772.2   3rd Qu.:2309.4  \n          Max.   :24.00   duut   : 8   Max.   :1031.1   Max.   :2773.7  \n                          (Other):48                                    \n\n\nThe reason is that there’s a trailing space in cell A28.\nLess obvious (because it doesn’t show up in the summary() output) is that there’s also a superfluous space in the Word column (the carrier word tiet occurs twice, once with and once without trailing space):\n\nlevels(dat$Word)\n\n [1] \"baat\"  \"biet\"  \"buut\"  \"daat\"  \"diet\"  \"duut\"  \"paat\"  \"piet\"  \"puut\" \n[10] \"taat\"  \"tiet\"  \"tiet \" \"tuut\" \n\n\nAs you can check for yourself, the trailing space is in cell C2.\nProblems like inconsistent capitalisation or trailing spaces are pretty easy to fix when you know they’re there, but they aren’t always obvious."
  },
  {
    "objectID": "posts/2015-06-18-preparing-data-for-analysis/index.html#a-long-data-format-is-usually-easier-to-manage-than-a-wide-one",
    "href": "posts/2015-06-18-preparing-data-for-analysis/index.html#a-long-data-format-is-usually-easier-to-manage-than-a-wide-one",
    "title": "Some tips on preparing your data for analysis",
    "section": "A ‘long’ data format is usually easier to manage than a ‘wide’ one",
    "text": "A ‘long’ data format is usually easier to manage than a ‘wide’ one\nAn example of a ‘wide’ dataset is Cognates_wide.csv (available here). The column Subject contains the participants’ IDs and every other column expresses whether the participant translated a given stimulus (e.g. ‘ägg’, ‘alltid’, ‘äta’ etc.) correctly. Every participant has their own row:\n\ndat &lt;- read.csv(\"http://janhove.github.io/downloads/Cognates_wide.csv\",\n                fileEncoding = \"UTF-8\",\n                stringsAsFactors = TRUE)\nhead(dat)\n\n  Subject agg alltid alska ata avskaffa bäbis bakgrund barn behärska bliva\n1      64   0      0     1   1        0     0        1    0        0     0\n2      78   0      0     0   0        0     0        1    0        0     0\n3     134   0      0     0   0        0     0        1    0        0     0\n4     230   0      0     0   0        0     0        1    0        0     0\n5     288   0      0     0   0        0     0        1    0        0     0\n6     326   0      0     0   0        0     1        1    1        0     0\n  blomma borgmästare borja branna butelj byrå choklad cyckel egenskap elev\n1      1           0     0      1      0    0       0      0        0    0\n2      1           0     0      1      0    0       0      1        0    0\n3      1           0     0      1      0    0       0      0        0    0\n4      0           1     0      1      0    0       0      0        0    0\n5      0           0     0      1      0    0       0      0        0    0\n6      1           0     0      1      0    0       0      1        0    1\n  ensam fåtölj fiende flicka fonster försiktig forsoka forst forsvinna\n1     0      0      1      0       1         1       0     0         1\n2     0      0      0      0       1         1       0     0         1\n3     1      0      0      0       1         0       0     0         1\n4     0      0      1      0       1         1       0     0         1\n5     0      0      0      0       1         1       0     0         0\n6     0      0      1      0       1         1       0     0         1\n  förutsättning fotboll fraga frasch full ga grupp hård häst hemlig ingenjor\n1             1       1     1      1    1  1     0    1    0      0        1\n2             0       1     1      1    1  1     0    1    0      0        1\n3             0       1     0      1    0  0     1    0    0      1        1\n4             0       1     1      0    1  0     0    1    0      0        1\n5             0       1     0      0    0  0     0    1    0      0        1\n6             0       1     0      1    1  0     0    1    0      0        1\n  intryck is kagel kanel karnkraftverk kejsar kniv konst korruption kung kyrka\n1       0  0     0     0             0      0    1     1          0    0     1\n2       0  0     0     0             1      0    1     1          0    0     1\n3       0  0     0     0             1      0    1     1          0    0     0\n4       0  0     0     0             0      1    1     1          0    0     1\n5       0  0     0     0             1      0    0     1          0    0     0\n6       0  0     0     0             1      1    0     1          0    0     1\n  kyssa lang larm leka löpa markvardig mjölk möjlig mycket nackdel öppna ost\n1     1    1    1    0    0          1     0      0      0       0     1   1\n2     1    1    0    0    0          0     1      0      0       0     1   1\n3     0    0    1    0    0          1     1      0      0       1     1   1\n4     1    1    0    0    0          0     1      1      0       0     1   1\n5     0    0    1    0    0          1     0      0      0       0     0   0\n6     1    1    0    0    0          1     1      0      0       0     0   1\n  overraska översätta paraply passiv potatis rådhus rytmisk saliv sitta\n1         0         0       1      1       1      1       1     1     1\n2         0         0       1      1       1      1       1     0     1\n3         0         1       1      1       1      1       1     0     1\n4         0         0       1      0       0      1       1     0     1\n5         0         0       0      1       0      0       0     0     0\n6         0         0       0      1       1      1       1     0     1\n  sjalvstandig skarm skola skön skriva skrubba skyskrapa smart smink söka\n1            1     0     1    1      0       1         1     1     0    0\n2            0     0     1    0      0       0         0     0     1    0\n3            1     0     0    0      0       0         1     0     1    0\n4            0     0     1    0      0       0         1     1     0    1\n5            1     0     0    0      0       0         1     1     0    0\n6            0     0     1    1      0       0         0     1     1    0\n  spegel språk städa stjärn sverige tanka tårta torsdag trakig tunga tvivla\n1      1     1     0      0       0     1     1       1      0     0      1\n2      1     1     0      1       0     0     1       0      0     0      0\n3      0     0     0      1       0     1     0       1      0     0      0\n4      1     1     0      1       0     0     1       1      0     0      0\n5      1     0     0      1       0     0     0       1      0     0      0\n6      1     1     0      1       0     1     1       1      0     1      0\n  tydlig ursprung värld varm vaxla viktig ytterst\n1      0        1     0    1     1      1       0\n2      0        1     0    1     1      1       0\n3      0        1     0    1     1      0       0\n4      0        1     0    1     0      1       0\n5      0        0     0    1     1      1       0\n6      0        1     0    1     1      1       0\n\n\nIn a ‘long’ dataset (e.g. Cognates_long.csv), the data are arranged differently, typically with one row per ‘observation unit’:\n\ndat &lt;- read.csv(\"http://janhove.github.io/downloads/Cognates_long.csv\",\n                fileEncoding = \"UTF-8\", stringsAsFactors = TRUE)\nhead(dat); tail(dat)\n\n  Subject Stimulus Correct\n1    1034      agg       0\n2    2151      agg       0\n3    9022      agg       0\n4    7337      agg       0\n5    8477      agg       0\n6    6544      agg       0\n\n\n      Subject Stimulus Correct\n16295    5290  ytterst       0\n16296    3125  ytterst       0\n16297    4137  ytterst       0\n16298    1967  ytterst       0\n16299    8942  ytterst       0\n16300    9913  ytterst       0\n\n\nI prefer long datasets for the following reasons:\n\nThe statistical tools I usually work with (mixed-effects models) require data in a long format.\nIt’s easier to add information to a long than to a wide dataset. For instance, if I wanted to add the order in which the participants saw the stimuli to the wide format, I’d have to add 100 columns to save the trial numbers (i.e. aggTrial, alltidTrial etc.). I’d only have to add one column (Trial) in the case of long data.\nWhile wide datasets are useful for analytical techniques such as principal component analysis, factor analysis or structural equation modeling as well as paired t-tests, I find it easier to convert a long dataset to a wide dataset than vice versa, especially if the dataset contains additional information about the stimuli or trials.\n\nA useful package for converting long data to wide data and vice versa is reshape2, whose use is helpfully explained by Sean Anderson."
  },
  {
    "objectID": "posts/2015-06-18-preparing-data-for-analysis/index.html#managing-several-smallish-datasets-and-then-joining-them-before-the-analysis-is-easier-than-handling-one-large-dataset",
    "href": "posts/2015-06-18-preparing-data-for-analysis/index.html#managing-several-smallish-datasets-and-then-joining-them-before-the-analysis-is-easier-than-handling-one-large-dataset",
    "title": "Some tips on preparing your data for analysis",
    "section": "Managing several smallish datasets and then joining them before the analysis is easier than handling one large dataset",
    "text": "Managing several smallish datasets and then joining them before the analysis is easier than handling one large dataset\nComplex datasets often contain repeated data. For instance, we could add the participants’ age to Cognates_long.csv above, but we’d have to repeat each participant’s age 100 times. Similarly, we could add information about the stimuli to Cognates_long.csv (e.g. their length in phonemes), but we’d have to repeat each stimulus’s length 163 times (once for each participant). In such cases, I find it easiest to compile several smaller datasets with as little information repeated as possible and to combine them when needed. That way, if I make a mistake when entering the data, I’ll only have to correct it once and then recombine the datasets.\nHere’s an illustration of what I mean. The participants’ accuracy per stimulus is stored in Cognates_long.csv.\n\ndat &lt;- read.csv(\"http://janhove.github.io/downloads/Cognates_long.csv\",\n                fileEncoding = \"UTF-8\", stringsAsFactors = TRUE)\n\nBackground information about the participants is available in ParticipantInformation.csv:\n\nparticipants &lt;- read.csv(\"http://janhove.github.io/downloads/ParticipantInformation.csv\",\n                         fileEncoding = \"UTF-8\", stringsAsFactors = TRUE)\nsummary(participants)\n\n    Subject         Sex          Age            NrLang         DS.Span     \n Min.   :  64   female:90   Min.   :10.00   Min.   :1.000   Min.   :2.000  \n 1st Qu.:2990   male  :73   1st Qu.:16.00   1st Qu.:2.000   1st Qu.:4.000  \n Median :5731               Median :39.00   Median :3.000   Median :4.000  \n Mean   :5317               Mean   :40.28   Mean   :3.067   Mean   :4.613  \n 3rd Qu.:7769               3rd Qu.:59.50   3rd Qu.:4.000   3rd Qu.:5.000  \n Max.   :9913               Max.   :86.00   Max.   :9.000   Max.   :8.000  \n                                                                           \n    DS.Total          GmVoc           Raven       EnglishScore  \n Min.   : 2.000   Min.   : 4.00   Min.   : 0.0   Min.   : 3.00  \n 1st Qu.: 5.000   1st Qu.:29.00   1st Qu.:12.0   1st Qu.:20.75  \n Median : 6.000   Median :34.00   Median :19.0   Median :31.00  \n Mean   : 6.374   Mean   :30.24   Mean   :17.8   Mean   :28.30  \n 3rd Qu.: 7.500   3rd Qu.:36.00   3rd Qu.:24.0   3rd Qu.:37.00  \n Max.   :12.000   Max.   :41.00   Max.   :35.0   Max.   :44.00  \n                  NA's   :1                      NA's   :3      \n\n\nThe entries in the Subject column in Cognates_long.csv correspond to those in the same column in ParticipantInformation.csv. Using merge(), we can add the information in the latter dataset to the corresponding rows in the former:\n\ndat &lt;- merge(dat, participants, \"Subject\")\nhead(dat); tail(dat)\n\n  Subject      Stimulus Correct    Sex Age NrLang DS.Span DS.Total GmVoc Raven\n1      64         forst       0 female  27      4       6        7    34    28\n2      64          löpa       0 female  27      4       6        7    34    28\n3      64           ost       1 female  27      4       6        7    34    28\n4      64         tunga       0 female  27      4       6        7    34    28\n5      64 förutsättning       1 female  27      4       6        7    34    28\n6      64         ensam       0 female  27      4       6        7    34    28\n  EnglishScore\n1           42\n2           42\n3           42\n4           42\n5           42\n6           42\n\n\n      Subject  Stimulus Correct    Sex Age NrLang DS.Span DS.Total GmVoc Raven\n16295    9913     grupp       1 female  40      3       5        8    29    17\n16296    9913      kung       0 female  40      3       5        8    29    17\n16297    9913     städa       0 female  40      3       5        8    29    17\n16298    9913     tanka       1 female  40      3       5        8    29    17\n16299    9913 skyskrapa       0 female  40      3       5        8    29    17\n16300    9913   rytmisk       0 female  40      3       5        8    29    17\n      EnglishScore\n16295           17\n16296           17\n16297           17\n16298           17\n16299           17\n16300           17\n\n\nSimilarly, we can add information about the stimuli, available in StimulusInformation.csv, to dat.\n\nstimuli &lt;- read.csv(\"http://janhove.github.io/downloads/StimulusInformation.csv\",\n                    fileEncoding = \"UTF-8\", stringsAsFactors = TRUE)\nsummary(stimuli)\n\n     Stimulus      Status      Duration          Swedish          German  \n agg     : 1   profile:10   Min.   : 516.0   2st     : 1   abschaffen: 1  \n alltid  : 1   target :90   1st Qu.: 708.8   2v@raska: 1   ai        : 1  \n alska   : 1                Median : 869.5   alltid  : 1   aig@nSaft : 1  \n ata     : 1                Mean   : 843.4   avskaffa: 1   aindruk   : 1  \n avskaffa: 1                3rd Qu.: 929.8   b2rja   : 1   ainzam    : 1  \n bäbis   : 1                Max.   :1264.0   babis   : 1   (Other)   :75  \n (Other) :94                NA's   :50       (Other) :94   NA's      :20  \n       English        French       LevGer           LevEng      \n @larm     : 1   alarm   : 1   Min.   :0.1429   Min.   :0.0000  \n ais       : 1   bebe    : 1   1st Qu.:0.4000   1st Qu.:0.4000  \n b2rn      : 1   bureau  : 1   Median :0.5000   Median :1.0000  \n baby      : 1   butEj   : 1   Mean   :0.5859   Mean   :0.6997  \n background: 1   cannelle: 1   3rd Qu.:0.8333   3rd Qu.:1.0000  \n (Other)   :42   (Other) :18   Max.   :1.0000   Max.   :1.0000  \n NA's      :53   NA's    :77                                    \n     LevFre         FreqGerman        FreqEnglish       FreqFrench   \n Min.   :0.0000   Min.   :   0.000   Min.   :   0.0   Min.   :    0  \n 1st Qu.:1.0000   1st Qu.:   0.932   1st Qu.:   0.0   1st Qu.:    0  \n Median :1.0000   Median :  20.020   Median :   0.0   Median :    0  \n Mean   :0.8529   Mean   : 109.714   Mean   : 119.4   Mean   :  237  \n 3rd Qu.:1.0000   3rd Qu.:  60.138   3rd Qu.:  43.6   3rd Qu.:    0  \n Max.   :1.0000   Max.   :3460.090   Max.   :3793.0   Max.   :22823  \n                                                                     \n\n\nThe shared denominator of both datasets is stored in the column Stimulus:\n\ndat &lt;- merge(dat, stimuli, \"Stimulus\")\nhead(dat); tail(dat)\n\n  Stimulus Subject Correct    Sex Age NrLang DS.Span DS.Total GmVoc Raven\n1      agg    7479       0   male  25      4       8       11    38    34\n2      agg    7337       0 female  49      3       4        5    36    19\n3      agg    2846       0 female  11      3       4        6     4    15\n4      agg    5153       0 female  51      3       4        6    37    18\n5      agg    6510       0 female  41      2       5        8    34    30\n6      agg    6329       0 female  56      3       5        6    37    21\n  EnglishScore Status Duration Swedish German English French LevGer LevEng\n1           43 target      547      Eg     ai      Eg   &lt;NA&gt;      1      0\n2           30 target      547      Eg     ai      Eg   &lt;NA&gt;      1      0\n3            6 target      547      Eg     ai      Eg   &lt;NA&gt;      1      0\n4           37 target      547      Eg     ai      Eg   &lt;NA&gt;      1      0\n5           40 target      547      Eg     ai      Eg   &lt;NA&gt;      1      0\n6           42 target      547      Eg     ai      Eg   &lt;NA&gt;      1      0\n  LevFre FreqGerman FreqEnglish FreqFrench\n1      1      24.06       26.04          0\n2      1      24.06       26.04          0\n3      1      24.06       26.04          0\n4      1      24.06       26.04          0\n5      1      24.06       26.04          0\n6      1      24.06       26.04          0\n\n\n      Stimulus Subject Correct    Sex Age NrLang DS.Span DS.Total GmVoc Raven\n16295  ytterst    1350       0   male  43      4       7       10    38    31\n16296  ytterst    7337       0 female  49      3       4        5    36    19\n16297  ytterst    8578       0   male  11      2       4        6     9    20\n16298  ytterst     717       0 female  15      2       5        6    29    21\n16299  ytterst    8805       0   male  70      4       4        4    35    22\n16300  ytterst    9912       0   male  10      2       4        4     9    14\n      EnglishScore Status Duration Swedish   German English French LevGer\n16295           38 target       NA ytterst ausserst    &lt;NA&gt;   &lt;NA&gt;    0.5\n16296           30 target       NA ytterst ausserst    &lt;NA&gt;   &lt;NA&gt;    0.5\n16297           17 target       NA ytterst ausserst    &lt;NA&gt;   &lt;NA&gt;    0.5\n16298           25 target       NA ytterst ausserst    &lt;NA&gt;   &lt;NA&gt;    0.5\n16299           35 target       NA ytterst ausserst    &lt;NA&gt;   &lt;NA&gt;    0.5\n16300            9 target       NA ytterst ausserst    &lt;NA&gt;   &lt;NA&gt;    0.5\n      LevEng LevFre FreqGerman FreqEnglish FreqFrench\n16295      1      1      23.19           0          0\n16296      1      1      23.19           0          0\n16297      1      1      23.19           0          0\n16298      1      1      23.19           0          0\n16299      1      1      23.19           0          0\n16300      1      1      23.19           0          0"
  },
  {
    "objectID": "posts/2015-06-18-preparing-data-for-analysis/index.html#reading-tip",
    "href": "posts/2015-06-18-preparing-data-for-analysis/index.html#reading-tip",
    "title": "Some tips on preparing your data for analysis",
    "section": "Reading tip",
    "text": "Reading tip\nHadley Wickham’s Tidy data, while primarily geared towards software developers, is well worth a read."
  },
  {
    "objectID": "posts/2015-06-18-preparing-data-for-analysis/index.html#software-versions",
    "href": "posts/2015-06-18-preparing-data-for-analysis/index.html#software-versions",
    "title": "Some tips on preparing your data for analysis",
    "section": "Software versions",
    "text": "Software versions\n\ndevtools::session_info()\n\n─ Session info ───────────────────────────────────────────────────────────────\n setting  value\n version  R version 4.3.1 (2023-06-16)\n os       Ubuntu 22.04.3 LTS\n system   x86_64, linux-gnu\n ui       X11\n language en_US\n collate  en_US.UTF-8\n ctype    en_US.UTF-8\n tz       Europe/Zurich\n date     2023-08-26\n pandoc   3.1.1 @ /usr/lib/rstudio/resources/app/bin/quarto/bin/tools/ (via rmarkdown)\n\n─ Packages ───────────────────────────────────────────────────────────────────\n package     * version date (UTC) lib source\n cachem        1.0.6   2021-08-19 [2] CRAN (R 4.2.0)\n callr         3.7.3   2022-11-02 [1] CRAN (R 4.3.1)\n cli           3.6.1   2023-03-23 [1] CRAN (R 4.3.0)\n crayon        1.5.2   2022-09-29 [1] CRAN (R 4.3.1)\n devtools      2.4.5   2022-10-11 [1] CRAN (R 4.3.1)\n digest        0.6.29  2021-12-01 [2] CRAN (R 4.2.0)\n ellipsis      0.3.2   2021-04-29 [2] CRAN (R 4.2.0)\n evaluate      0.15    2022-02-18 [2] CRAN (R 4.2.0)\n fastmap       1.1.0   2021-01-25 [2] CRAN (R 4.2.0)\n fs            1.5.2   2021-12-08 [2] CRAN (R 4.2.0)\n glue          1.6.2   2022-02-24 [2] CRAN (R 4.2.0)\n htmltools     0.5.5   2023-03-23 [1] CRAN (R 4.3.0)\n htmlwidgets   1.6.2   2023-03-17 [1] CRAN (R 4.3.1)\n httpuv        1.6.11  2023-05-11 [1] CRAN (R 4.3.1)\n jsonlite      1.8.7   2023-06-29 [1] CRAN (R 4.3.1)\n knitr         1.39    2022-04-26 [2] CRAN (R 4.2.0)\n later         1.3.1   2023-05-02 [1] CRAN (R 4.3.1)\n lifecycle     1.0.3   2022-10-07 [1] CRAN (R 4.3.0)\n magrittr      2.0.3   2022-03-30 [1] CRAN (R 4.3.0)\n memoise       2.0.1   2021-11-26 [2] CRAN (R 4.2.0)\n mime          0.10    2021-02-13 [2] CRAN (R 4.0.2)\n miniUI        0.1.1.1 2018-05-18 [1] CRAN (R 4.3.1)\n pkgbuild      1.4.2   2023-06-26 [1] CRAN (R 4.3.1)\n pkgload       1.3.2.1 2023-07-08 [1] CRAN (R 4.3.1)\n prettyunits   1.1.1   2020-01-24 [2] CRAN (R 4.2.0)\n processx      3.8.2   2023-06-30 [1] CRAN (R 4.3.1)\n profvis       0.3.8   2023-05-02 [1] CRAN (R 4.3.1)\n promises      1.2.0.1 2021-02-11 [1] CRAN (R 4.3.1)\n ps            1.7.5   2023-04-18 [1] CRAN (R 4.3.1)\n purrr         1.0.1   2023-01-10 [1] CRAN (R 4.3.0)\n R6            2.5.1   2021-08-19 [2] CRAN (R 4.2.0)\n Rcpp          1.0.11  2023-07-06 [1] CRAN (R 4.3.1)\n remotes       2.4.2   2021-11-30 [2] CRAN (R 4.2.0)\n rlang         1.1.1   2023-04-28 [1] CRAN (R 4.3.0)\n rmarkdown     2.21    2023-03-26 [1] CRAN (R 4.3.0)\n rstudioapi    0.14    2022-08-22 [1] CRAN (R 4.3.0)\n sessioninfo   1.2.2   2021-12-06 [2] CRAN (R 4.2.0)\n shiny         1.7.4.1 2023-07-06 [1] CRAN (R 4.3.1)\n stringi       1.7.12  2023-01-11 [1] CRAN (R 4.3.1)\n stringr       1.5.0   2022-12-02 [1] CRAN (R 4.3.0)\n urlchecker    1.0.1   2021-11-30 [1] CRAN (R 4.3.1)\n usethis       2.2.2   2023-07-06 [1] CRAN (R 4.3.1)\n vctrs         0.6.3   2023-06-14 [1] CRAN (R 4.3.0)\n xfun          0.39    2023-04-20 [1] CRAN (R 4.3.0)\n xtable        1.8-4   2019-04-21 [1] CRAN (R 4.3.1)\n yaml          2.3.5   2022-02-21 [2] CRAN (R 4.2.0)\n\n [1] /home/jan/R/x86_64-pc-linux-gnu-library/4.3\n [2] /usr/local/lib/R/site-library\n [3] /usr/lib/R/site-library\n [4] /usr/lib/R/library\n\n──────────────────────────────────────────────────────────────────────────────"
  },
  {
    "objectID": "posts/2015-02-26-explaining-key-concepts-using-permutation-tests/index.html",
    "href": "posts/2015-02-26-explaining-key-concepts-using-permutation-tests/index.html",
    "title": "Explaining key concepts using permutation tests",
    "section": "",
    "text": "It’s that time of year again where I have the honour of explaining the nuts and bolts of inferential statistics in the optional introductory statistics class that I teach. Two of my objectives are to familiarise my students with core concepts of statistical inference and to make sure that they don’t fall into the trap of reading too much into p-values. In addition to going through the traditional route based on the Central Limit Theorem, I think that several key concepts can be also illustrated in a less math-intensive way – namely by exploring the logic of permutation tests.\nMy goal isn’t to make the case for a wholesale adoption of permuation tests (which I explain in more detail below). Rather, I hope that by discussing permutation tests we can gain a better understanding of some key concepts in inferential statistics that are often neglected, and that we can do so without too many distracting equations and assumptions. Some of the points that a discussion of permutation tests raises are\nBut first, we need some fictitious data."
  },
  {
    "objectID": "posts/2015-02-26-explaining-key-concepts-using-permutation-tests/index.html#a-small-scale-randomised-experiment",
    "href": "posts/2015-02-26-explaining-key-concepts-using-permutation-tests/index.html#a-small-scale-randomised-experiment",
    "title": "Explaining key concepts using permutation tests",
    "section": "A small-scale randomised experiment",
    "text": "A small-scale randomised experiment\nLet’s say we want to investigate whether the kind of music you happen to listen to affects how intellectually fulfilling you perceive your life to be. We design an experiment with two conditions: in one condition, people are asked to listen to John Coltrane’s A Love Supreme; in the other condition, they are asked to endure mind-numbing muzak, viz. Kenny G dubbing himself over Louis Armstrong’s What a Wonderful World. After listening to the song, they are asked to rate how intellectually fulfilling their life is on a scale from 1 (very unfulfilling) to 7 (very fulfilling).\nWe recruit 6 participants – that’s on the small side, of course, but it’ll keep things tractable. We want to assign three participants to the Coltrane song and three to the Kenny G one, and to avoid a systematic bias in the results, we randomly divide the participant sample into two equal parts.\nThe answers to the questionnaire item are shown in this dotplot:\n\n\n\n\n\nIt surely seems that listening to John Coltrane is associated with higher intellectual fulfillment ratings than listening to Kenny G. Moreover, since we got these results in a randomised experiment, we could even make causal claims, namely that listening to Coltrane vs Kenny G causes higher intellectual fulfillment ratings."
  },
  {
    "objectID": "posts/2015-02-26-explaining-key-concepts-using-permutation-tests/index.html#the-null-hypothesis",
    "href": "posts/2015-02-26-explaining-key-concepts-using-permutation-tests/index.html#the-null-hypothesis",
    "title": "Explaining key concepts using permutation tests",
    "section": "The null hypothesis",
    "text": "The null hypothesis\nBut before we draw such a conclusion, we need to address a more trivial explanation of these data: perhaps they are just a fluke – perhaps the participants in the Coltrane condition happened to be the participants who tended to perceive their life to be more intellectually fulfilling anyway and we were just lucky to assign them to the Coltane condition.\nFundamentally, it is this objection – that sheer randomness might also account for these data – that inferential statistics seeks to address. This proposed explanation is known as the null hypothesis.\n(I’m delibrerately cutting corners here: the null hypothesis doesn’t always have to be ‘no effect – randomness only’. In practice, though, you’d be hard-pressed to find other null hypotheses.)\nThe way how inferential (frequentist) statistics usually proceeds is by arguing by contradiction: we try to calculate how surprising our results would be if randomness alone – and no systematic effect – were at play. In other words, we compute the probability (p) of our results (and even more staggering results) under the assumption that the null hypothesis is actually true. If this probability is small (say smaller than an arbitrary probability of 10%), we’d conclude that ‘randomness alone’ isn’t a viable explanation of our results and that systematic effects are at play, too."
  },
  {
    "objectID": "posts/2015-02-26-explaining-key-concepts-using-permutation-tests/index.html#why-random-assignment-works",
    "href": "posts/2015-02-26-explaining-key-concepts-using-permutation-tests/index.html#why-random-assignment-works",
    "title": "Explaining key concepts using permutation tests",
    "section": "Why random assignment works",
    "text": "Why random assignment works\nTypically, we would run an off-the-shelf statistical test (e.g. a t-test) to calculate this probability. Most of these tests derive from the Central Limit Theorem (CLT): if we’re willing to make some assumptions, the CLT tells us how the means from data points sampled randomly from a larger population are distributed. From this knowledge, we can derive the probability that the means of our two groups would differ by at least as much as they do if randomness were the only factor at play. For the purposes of this blog post, however, I want to focus on an inferential technique that makes fewer assumptions than CLT-derived tests and that isn’t restricted to differences between means.\nIts logic is as follows. If listening to the Coltrane vs Kenny G song didn’t affect the intellectual fulfillment ratings (= the null hypothesis), then any difference between the two groups must be due to the random assignment of participants to conditions. If this were the case, the data that we happened to observe (difference between group means: 2.67):\n\n\n\n\n\nwere as likely to occur as those where the assignment of participants to condition woud have turned out differently, say (difference between group means: 1.33):\n\n\n\n\n\nRather than relying on the Central Limit Theorem, we could write down every possible recombination of the scores of 2, 7, 6, 1, 1 and 5 into two groups, calculate the difference between the two group means for each possibility, and see how unusual the result we actually got is. (This, incidentally, is also the logic behind the graphical statistical inference tests I blogged about a while ago.)"
  },
  {
    "objectID": "posts/2015-02-26-explaining-key-concepts-using-permutation-tests/index.html#exhaustive-recombining",
    "href": "posts/2015-02-26-explaining-key-concepts-using-permutation-tests/index.html#exhaustive-recombining",
    "title": "Explaining key concepts using permutation tests",
    "section": "Exhaustive recombining",
    "text": "Exhaustive recombining\nFor small samples, it is possible to list every possible combination of scores into two groups of equal size:\n2 observations can be allocated in 2 ways:\n\nA / B\nB / A\n\n4 observations can be allocated in 6 ways (note that the order within the groups doesn’t matter):\n\nA, B / C, D\nA, C / B, D\nA, D / B, C\nB, C / A, D\nB, D / A, C\nC, D / A, B\n\n6 observations can be allocated in 20 ways:\n\nA, B, C / D, E, F\nA, B, D / C, E, F\netc.\n\n8 observations can be allocated in 70 ways, etc.\nWe can compute the number of possible combinations using R’s choose() function, e.g.:\n\n# How many ways to allocate 12 observations\n# to two groups of equal size (i.e. 6):\nchoose(12, 6)\n\n[1] 924\n\n\nAs you can imagine, the number of possible combinations sky-rockets as the sample size increases, and it quickly becomes prohibitively computionally expensive to generate every single combination. For small samples, though, it’s still pretty easy. Using some relatively easy R code, we can recombine our 6 observations into two groups of 3 in each of 20 possible ways and compute the difference between the group means. If you’re not into R, feel free to skip the next few paragraphs – the rationale is more important than the actual computer code.\n\nSkippable – R code: Computing mean difference for every possible recombination\nFirst I define a function that takes a number of data points (‘vector’ in R-speak) and a number of ‘indices’ that indicate which data points belong to Group 1 (the others belong to Group 2):\n\n# Define a function that computes the difference\n# in means (adaptable to other functions)\n# between one part of a vector (indices in Group1)\n# and the remaining part (indices NOT in Group1)\nmean.diff &lt;- function(data, Group1) {\n  diff.mean &lt;- mean(data[Group1]) -\n                mean(data[- Group1])\n  return(diff.mean)\n}\n\nTo illustrate how this function works, we read in our actual data and specify which data points belong to Group 1:\n\n# Read in actual data\nactual.data &lt;- c(2, 7, 6, 1, 1, 5)\n# The first, second and third data points\n# are in the 'Coltrane' group:\ncoltrane &lt;- c(1, 2, 3)\n# Compute mean difference\n# between Coltrane group and rest:\nmean.diff(actual.data, coltrane)\n\n[1] 2.666667\n\n\nOf course, we could’ve computed this difference in an easier way, but defining this function makes the next couple of steps easier.\nNext, we generate a list of every possible way in which the Coltrane group could’ve been made up. For this, I use the combn() function; the numbers refer to the 1st, 2nd, …, 6th data point, not to the values of the data points themselves:\n\n# For the 1st, 2nd ... 6th data points\ncombinations &lt;- combn(1:6,\n# Allocate 3 data points to Group 1\n                      3,\n# (and return output as a list)\n                      simplify = FALSE)\n\n# uncomment next line to show all 20 combinations\n# combinations\n\nNow, we apply the mean.diff() function that we wrote earlier to our data set (actual.data) for every possible combination of indices listed in combinations (i.e., 1-2-3, 1-2-4, 1-2-5 etc.):\n\n# apply function mean.diff\ndiffs &lt;- mapply(mean.diff,\n# for every combination of indices in combinations\n                Group1 = combinations,\n# apply to actual.data\n                MoreArgs = list(data = actual.data))\n\n\n\nResults\nWe have now calculated every possible difference in group means possible. Here they are, sorted from lowest to highest:\n\n\n\n\n\nWe actually observed a difference of 2.67 between the two group means; the vertical red lines indicate an absolute group mean difference of 2.67. As you can see, four out of 20 possible group mean differences have absolute values of 2.67 or higher. Put differently, the probability to observe an absolute group mean difference of 2.67 or higher if randomness alone were at play is 4/20 = 20%.\nThis is our p-value: the probability with which we would have obtained our observed difference (or even more extreme ones) if randomness alone were at play. So even if no systematic effect exists, we would still obtain an absolute mean difference of 2.67 or larger in 20% of cases. This isn’t terribly unlikely (and higher than the 10% threshold we specified above), so the null hypothesis that randomness alone is at play is still a viable explanation of our results. Note that this doesn’t necessarily mean that the null hypothesis is the correct explanation, i.e. we haven’t in any way proven that there isn’t a systematic effect."
  },
  {
    "objectID": "posts/2015-02-26-explaining-key-concepts-using-permutation-tests/index.html#what-i-like-about-these-permutation-tests",
    "href": "posts/2015-02-26-explaining-key-concepts-using-permutation-tests/index.html#what-i-like-about-these-permutation-tests",
    "title": "Explaining key concepts using permutation tests",
    "section": "What I like about these ‘permutation tests’",
    "text": "What I like about these ‘permutation tests’\n\nLess restrictive assumptions\nFirst, from a practical perspective, these permutation tests are relatively assumption-free. Statistical tests derived from the Central Limit Theorem all need to assume that the sample mean distribution is well approximated by a normal distribution. This needn’t be a problem for large samples, but it usually requires a leap of faith for small samples. Permutation tests, by contrast, rely on the much weaker assumption that the data points are mutually interchangeable under the null hypothesis (i.e. come from the same distribution). This may sound somewhat esoteric, but it essentially means that a participant who happened to be part of Group 1 could as easily have been part of Group 2.\n\n\nFlexibility with respect to what is being compared\nSecond, while we focused on the difference between the group means, we could easily have use the difference between the group medians as a test statistic, or pretty much every other test statistic imaginable – i.e. permutation tests are extremely flexible. This code, for instance, operates along exactly the same lines, but compares the variances of the groups:\n\n# Define a function to compute\n# difference in group variances\nvar.diff &lt;- function(data, Group1) {\n  diff.var &lt;- var(data[Group1]) -\n               var(data[- Group1])\n  return(diff.var)\n}\n\n# apply function median.diff\ndiffs &lt;- mapply(var.diff,\n# for every combination of indices in combinations\n                Group1 = combinations,\n# apply to actual.data\n                MoreArgs = list(data = actual.data))\n\ndotchart(sort(diffs), pch = 16,\n         xlab = \"Group variance difference\",\n         main = \"Group variance differences\\nfor all possible combinations\")\nabline(v = var.diff(actual.data, coltrane),\n       col = \"red\", lty = 2)\nabline(v = -var.diff(actual.data, coltrane),\n       col = \"red\", lty = 2)\n\n\n\n\nSo in 18 out of 20 cases, we would have observed variance differences as large as or larger than the difference we actually obtained.\n\n\nThe test follows from the design\nThe first two points are primarily of practical importance. But, more importantly, permutation tests also illustrate all-important theoretical concepts. Specifically, they stress that there is a clear logical link between the statistical test we use and the experimental design we opted for: using a permutation test is entirely warranted by the random assignment of participants to two equal-sized groups. Stressing the link between experimental design and statistical inference – rather than considering them separately – is of huge pedagogical, as well as practical, use, I believe.\nIn fact, this link between design and analysis means that it’s not so much the case that permutation tests are a substitute for more commonly used tests such as the t-test, it’s the other way round. As Fisher (1936; quoted by Ernst 2004) put it,\n\nthe statistician does not carry out this very simple and very tedious process [i.e. running a permutation test, JV], but his conclusions have no justification beyond the fact that they agree with those which could have been arrived at by this elementary method.\n\nIn class, this point could segue into an interesting discussion about the value of p-values in non-randomised (quasi-)experiments where randomisation isn’t available as the ultimate rationale for conducting statistical tests.\n\n\nThe test has to follow from the design\nPermutation tests are eminently useful for illustrating why experimental design features matter. Let’s say that, in the example above, Elaine and Puddy, and Kramer and Newman were so inseparable that they wanted to listen to the same song. Rather than being combinable in 20 different ways, the six data points would be combinable in only four ways:\n\nGeorge, Newman, Kramer / Elaine, Puddy, Jerry (difference in means: 2.67)\nGeorge, Elaine, Puddy / Newman, Kramer, Jerry (difference: -4.67)\nElaine, Puddy, Jerry / George, Newman, Kramer (difference: -2.67)\nNewman, Kramer, Jerry / George, Elaine, Puddy (difference: 4.67)\n\nThe 14 combinations where Newman and Kramer, and Elaine and Puddy aren’t in the same condition wouldn’t be part of the range of possible randomisations. The observed absolute difference in group means of 2.67 wouldn’t then be associated with a p-value of 20%, but with one of 100%!\nLess prohibitive restricting would also affect our inferences. If, for instance, Jerry and Newman would refuse to listen to the same song, only 12 possible randomisations remain, and we would have to draw our conclusions with respect to these 12 possibilities – not with respect to the full set of 20 possibilities.\nIn both cases, some data points weren’t independent of one another: the occurrence of one implied the occurrence (or absence) of some others, which can drastically affect our inferences. This point is hugely relevant to many studies in applied linguistics where the participants cannot be allocated to experimental conditions on an individual basis but only in whole groups, e.g. classes or schools (so-called ‘cluster-randomised designs’). This design feature, in fact, even occurs in some studies in not-so-applied linguistics and psychology, and it has to be taken explicitly into account in the analysis. Thus, not only can design features motivate the specification of a statistical test, they should do so – even when not using permutation tests (cf. the quote from Fisher above).\nA case more subtle than cluster-randomisation occurs when, for perfectly valid reasons, researchers insist on equating two groups with respect to one variable. If, for instance, 12 men and 20 women are available for an experiment, the researcher can randomise the participants to two groups with the restriction that each group should have 6 men and 10 women so as to balance out the sex variable (so-called ‘block designs’). This design feature, too, needs to be taken into account.\nFor a more in-depth discussion, see my Analyzing randomized controlled interventions: Three notes for applied linguists (see pages 6-7 for block designs and pages 11ff. for cluster randomisation).\n\n\nRandom assignment vs random sampling\nFinally, the justification of the permutation test above was the random assignment of participants to experimental conditions – not that the participants were randomly sampled from a larger population. Consequently, the validity of the conclusions based on this permutation test (and, following Fisher, of other statistical tests, too) is restricted to the sample in question.\nWhile permutation tests can also be used when random sampling was used, they require a different sort of justification (see Ernst 2004). This is beyond the scope of this (already way too long) blog post, but it can be useful for driving home the point that drawing inferences about an effect in a sample and drawing inferences about an effect in a larger population require different kinds of justification. In plain and somewhat simplified terms, unless our participants were randomly sampled from a larger population – and they rarely if ever are – we are technically restricted to inferring whether an effect can be observed in the sample we’ve got."
  },
  {
    "objectID": "posts/2015-02-26-explaining-key-concepts-using-permutation-tests/index.html#conclusion",
    "href": "posts/2015-02-26-explaining-key-concepts-using-permutation-tests/index.html#conclusion",
    "title": "Explaining key concepts using permutation tests",
    "section": "Conclusion",
    "text": "Conclusion\nAll in all, I think there are a lot of important principles that a discussion of permutation tests can generate. Of course, the example I gave was rather simple (small sample size, straightforward design), but for pedagogical purposes, I think it is meaty enough."
  },
  {
    "objectID": "posts/2015-02-26-explaining-key-concepts-using-permutation-tests/index.html#references-and-further-reading",
    "href": "posts/2015-02-26-explaining-key-concepts-using-permutation-tests/index.html#references-and-further-reading",
    "title": "Explaining key concepts using permutation tests",
    "section": "References and further reading",
    "text": "References and further reading\n\nR. A. Fisher’s Design of experiments discusses randomisation as the ‘physical basis of the validity of the test’ (see Chapter 2 for the ‘lady tasting tea’ example).\nMichael Ernst’s Permutation methods: A basis for exact inference illustrates basic permutation tests and covers the distinction between random assignment and random sampling.\nGary Oehlert’s First course in design and analysis of experiments amply covers the rationale for random assignment and the importance of considering design features when analysing experiments.\nMy paper on Analyzing randomized controlled interventions covers many of the points raised here, though not within a permutation-test framework."
  },
  {
    "objectID": "posts/2015-02-26-explaining-key-concepts-using-permutation-tests/index.html#software-versions",
    "href": "posts/2015-02-26-explaining-key-concepts-using-permutation-tests/index.html#software-versions",
    "title": "Explaining key concepts using permutation tests",
    "section": "Software versions",
    "text": "Software versions\n\ndevtools::session_info()\n\n─ Session info ───────────────────────────────────────────────────────────────\n setting  value\n version  R version 4.3.1 (2023-06-16)\n os       Ubuntu 22.04.3 LTS\n system   x86_64, linux-gnu\n ui       X11\n language en_US\n collate  en_US.UTF-8\n ctype    en_US.UTF-8\n tz       Europe/Zurich\n date     2023-08-26\n pandoc   3.1.1 @ /usr/lib/rstudio/resources/app/bin/quarto/bin/tools/ (via rmarkdown)\n\n─ Packages ───────────────────────────────────────────────────────────────────\n package     * version date (UTC) lib source\n cachem        1.0.6   2021-08-19 [2] CRAN (R 4.2.0)\n callr         3.7.3   2022-11-02 [1] CRAN (R 4.3.1)\n cli           3.6.1   2023-03-23 [1] CRAN (R 4.3.0)\n crayon        1.5.2   2022-09-29 [1] CRAN (R 4.3.1)\n devtools      2.4.5   2022-10-11 [1] CRAN (R 4.3.1)\n digest        0.6.29  2021-12-01 [2] CRAN (R 4.2.0)\n ellipsis      0.3.2   2021-04-29 [2] CRAN (R 4.2.0)\n evaluate      0.15    2022-02-18 [2] CRAN (R 4.2.0)\n fastmap       1.1.0   2021-01-25 [2] CRAN (R 4.2.0)\n fs            1.5.2   2021-12-08 [2] CRAN (R 4.2.0)\n glue          1.6.2   2022-02-24 [2] CRAN (R 4.2.0)\n htmltools     0.5.5   2023-03-23 [1] CRAN (R 4.3.0)\n htmlwidgets   1.6.2   2023-03-17 [1] CRAN (R 4.3.1)\n httpuv        1.6.11  2023-05-11 [1] CRAN (R 4.3.1)\n jsonlite      1.8.7   2023-06-29 [1] CRAN (R 4.3.1)\n knitr         1.39    2022-04-26 [2] CRAN (R 4.2.0)\n later         1.3.1   2023-05-02 [1] CRAN (R 4.3.1)\n lifecycle     1.0.3   2022-10-07 [1] CRAN (R 4.3.0)\n magrittr      2.0.3   2022-03-30 [1] CRAN (R 4.3.0)\n memoise       2.0.1   2021-11-26 [2] CRAN (R 4.2.0)\n mime          0.10    2021-02-13 [2] CRAN (R 4.0.2)\n miniUI        0.1.1.1 2018-05-18 [1] CRAN (R 4.3.1)\n pkgbuild      1.4.2   2023-06-26 [1] CRAN (R 4.3.1)\n pkgload       1.3.2.1 2023-07-08 [1] CRAN (R 4.3.1)\n prettyunits   1.1.1   2020-01-24 [2] CRAN (R 4.2.0)\n processx      3.8.2   2023-06-30 [1] CRAN (R 4.3.1)\n profvis       0.3.8   2023-05-02 [1] CRAN (R 4.3.1)\n promises      1.2.0.1 2021-02-11 [1] CRAN (R 4.3.1)\n ps            1.7.5   2023-04-18 [1] CRAN (R 4.3.1)\n purrr         1.0.1   2023-01-10 [1] CRAN (R 4.3.0)\n R6            2.5.1   2021-08-19 [2] CRAN (R 4.2.0)\n Rcpp          1.0.11  2023-07-06 [1] CRAN (R 4.3.1)\n remotes       2.4.2   2021-11-30 [2] CRAN (R 4.2.0)\n rlang         1.1.1   2023-04-28 [1] CRAN (R 4.3.0)\n rmarkdown     2.21    2023-03-26 [1] CRAN (R 4.3.0)\n rstudioapi    0.14    2022-08-22 [1] CRAN (R 4.3.0)\n sessioninfo   1.2.2   2021-12-06 [2] CRAN (R 4.2.0)\n shiny         1.7.4.1 2023-07-06 [1] CRAN (R 4.3.1)\n stringi       1.7.12  2023-01-11 [1] CRAN (R 4.3.1)\n stringr       1.5.0   2022-12-02 [1] CRAN (R 4.3.0)\n urlchecker    1.0.1   2021-11-30 [1] CRAN (R 4.3.1)\n usethis       2.2.2   2023-07-06 [1] CRAN (R 4.3.1)\n vctrs         0.6.3   2023-06-14 [1] CRAN (R 4.3.0)\n xfun          0.39    2023-04-20 [1] CRAN (R 4.3.0)\n xtable        1.8-4   2019-04-21 [1] CRAN (R 4.3.1)\n yaml          2.3.5   2022-02-21 [2] CRAN (R 4.2.0)\n\n [1] /home/jan/R/x86_64-pc-linux-gnu-library/4.3\n [2] /usr/local/lib/R/site-library\n [3] /usr/lib/R/site-library\n [4] /usr/lib/R/library\n\n──────────────────────────────────────────────────────────────────────────────"
  },
  {
    "objectID": "posts/2015-11-02-unequal-sample-sized/index.html",
    "href": "posts/2015-11-02-unequal-sample-sized/index.html",
    "title": "Causes and consequences of unequal sample sizes",
    "section": "",
    "text": "In this blog post, I want to dispel a myth that’s reasonably common among students: the notion that there’s something wrong about a study that compares groups of different sizes."
  },
  {
    "objectID": "posts/2015-11-02-unequal-sample-sized/index.html#goal",
    "href": "posts/2015-11-02-unequal-sample-sized/index.html#goal",
    "title": "Causes and consequences of unequal sample sizes",
    "section": "Goal",
    "text": "Goal\nThere is something aesthetically pleasing about studies that compare two equal-sized groups. An experiment with two conditions with 30 participants each looks ‘cleaner’ than one with 27 participants in one condition and 34 in the other. Whatever the reasons for this aesthetic appeal may be, I’m going to argue that there’s nothing un-kosher about unequal sample sizes per se. This post is geared first and foremost to our MA students, primarily to help them get rid of the idea that they should throw away data in order to perfectly balance their datasets.\nThere are three main causes of unequal sample sizes: simple random assignment of participants to conditions; planned imbalances; and drop-outs and missing data. I will discuss these in order."
  },
  {
    "objectID": "posts/2015-11-02-unequal-sample-sized/index.html#simple-randomisation-as-the-cause-of-sample-size-imbalance",
    "href": "posts/2015-11-02-unequal-sample-sized/index.html#simple-randomisation-as-the-cause-of-sample-size-imbalance",
    "title": "Causes and consequences of unequal sample sizes",
    "section": "Simple randomisation as the cause of sample size imbalance",
    "text": "Simple randomisation as the cause of sample size imbalance\nThe random assignment of participants to the different conditions is the hallmark of a ‘true experiment’ and distinguishes it from ‘quasi-experiments’. Random assignment can be accomplished in essentially two different ways. The first technique is complete randomisation: first, sixty participants are recruited; then, half of them are randomly assigned to the control and half to the experimental condition. This technique guarantees that an equal number of participants is assigned to both conditions. The second technique is simple randomisation: for each participant that volunteers for the experiment, there’s a 50/50 chance that she ends up in the control or in the experimental condition – regardless of how large either sample already is. Simple randomisation causes unequal sample sizes: you’re not guaranteed to get exactly 30 heads in 60 coin flips, and similarly you’re not guaranteed to get exactly 30 participants in either condition.\n(Note: Some refer to ‘simple randomisation’ as ‘complete randomisation’, so check how the procedures are described when reading about randomisation techniques.)\nUnequal sample sizes, then, may be the consequence of using simple rather than complete randomisation. And there can be good reasons for choosing simple rather than complete randomisation as your allocation technique, notably a reduced potential for selection bias (see Kahan et al. 2015) and ease of planning (it’s easier to let the experimental software take care of the randomisation than to keep track of the number of participants in each condition as participants find their way to the lab).\nCompared to complete randomisation, simple randomisation seems to have a distinct disadvantage, however: an experiment with 60 participants in total has more power, i.e. a better chance to find systematic differences between the conditions, if the participants are distributed evenly across the conditions (i.e. 30/30, complete randomisation) than if they’re distributed unevenly (e.g. 20/40). (This is assuming that the variability in both conditions is comparable.) For this reason, it’s usually much better to have 50 participants in both conditions rather than 20 in one condition and 200 in the other – even though the total number of participants is much greater in the second set-up. Simple randomisation, however, can cause such imbalances. In fact, it’s possible to end up with no participants in one of the groups.\nBut. While stark imbalances are possible when using simple randomisation, they’re also pretty improbable. Figure 1 shows the probability of ending up with any number of participants in one condition when 60 participants are randomly assigned to one of two conditions with equal probability. As this graph illustrates, it’s highly improbable to end up with 10 participants in the first condition (and 50 in the other). In fact, in 999 out of 1,000 cases, you’ll end up with between 17 and 43 participants in each group.\n\n\n\n\n\nFigure 1: Probability that simple randomisation will give rise to a specific numbers of participants in one condition if the total number of participants is 60 and they are assigned to one of two conditions with equal probability and independently of one another.\n\n\n\n\nAdditionally, while equal-sized groups maximise statistical power, the advantage is easily overstated. An experiment with 30+30 participants has a 76% chance to detect a systematic difference of 0.7 standard deviations between the two group means; for an experiment with 20+40 participants, this probability is 71%. On average, an experiment in which 60 participants are assigned to the conditions according to a simple randomisation procedure has 75% power to detect a difference of 0.7 standard deviations. The difference in power between complete randomisation (guaranteeing equal sample sizes) and simple randomisation, then, is minimal. Table 1 compares a couple of additional set-ups, and all point to the same conclusion: the loss of power associated with simple vs. complete randomisation is negligible.\n\nTable 1: Comparison of the power of an experiment using complete randomisation (equal sample sizes) and the average power of an experiment using simple randomisation (possibly unequal sample sizes). The R code for the comparison is at the bottom of this post.\n\n\n\n\n\n\n\n\nTotal number of participants\nDifference (sd)\nPower complete randomisation (t-test)\nPower simple randomisation (t-test)\n\n\n\n\n20\n0.7\n0.32\n0.30\n\n\n60\n0.7\n0.76\n0.75\n\n\n120\n0.7\n0.97\n0.97\n\n\n20\n0.3\n0.10\n0.10\n\n\n60\n0.3\n0.21\n0.21\n\n\n120\n0.3\n0.37\n0.37\n\n\n\nSimple randomisation and the unequal sample sizes it gives rise to, then, aren’t much of a problem when comparing the means of two groups. For more complex (factorial) designs, however, they do present some complications. Specifically, cell size imbalances in factorial designs force the analyst to decide whether the effects should be estimated by means of Type I, Type II or Type III sums of squares (see the explanation by Falk Scholer). I don’t feel qualified to give any definite advice in this matter other than to point out the following:\n\nWhat’s usually (though not invariably) of interest in a factorial design is the interaction between the predictors rather than their main effects.\nThe conclusions about the highest-order interaction aren’t affected by the choice between Type I, Type II or Type III sums of squares. In the most common case, if you have two independent variables, the test for the interaction between them gives the same result irrespective of your sums of squares choice.\nSo, if you’re interested in this interaction and not so much in the independent effects of the predictors, it doesn’t really matter.\n\nIn conclusion, sample size imbalances can be the result of assigning participants to conditions by means of simple randomisation. When comparing two groups, this doesn’t really present any problems. When the study has a factorial design, though, you may want to brush up on Type I/II/III sums of squares. But whatever you do, don’t throw away precious data just to make the group sizes equal."
  },
  {
    "objectID": "posts/2015-11-02-unequal-sample-sized/index.html#planned-imbalances",
    "href": "posts/2015-11-02-unequal-sample-sized/index.html#planned-imbalances",
    "title": "Causes and consequences of unequal sample sizes",
    "section": "Planned imbalances",
    "text": "Planned imbalances\nResearchers sometimes intentionally recruit a greater number of participants in one condition than in the other. This seems to be particularly the case in quasi-experiments, i.e. studies in which the allocation of participants to condition is predetermined rather than manipulated by the researcher (e.g. comparisons of native speakers to foreign language learners). From what I can tell, the (usually tacit) reasons for such imbalances include:\n\naccessibility problems and financial cost: It may be more difficult or more costly to recruit participants in secluded villages than university students;\na relative lack of interest: Comparisons within, say, the Learner group may be considered more interesting than comparisons between the Learner and the Native Speaker group;\nless variability in one group: One group may be considerably more homogeneous with respect to some linguistic behaviour than the other one, so you gain less by recruiting participants for one group than for the other.\n\nSo, even when the unequal sample sizes were planned and not due to simple randomisation, they may be a matter of sound reasoning and practicality rather than of poor design."
  },
  {
    "objectID": "posts/2015-11-02-unequal-sample-sized/index.html#drop-outs-and-missing-data",
    "href": "posts/2015-11-02-unequal-sample-sized/index.html#drop-outs-and-missing-data",
    "title": "Causes and consequences of unequal sample sizes",
    "section": "Drop-outs and missing data",
    "text": "Drop-outs and missing data\nImbalances can arise when participants drop out of the study or when data is lost due to technical glitches. These cases are different from the previous two (simple randomisation and planned imbalances) as they needn’t be due to different numbers of participants being assigned to the experiment’s conditions. Rather, at the end of the study, different numbers of participants remain there to be analysed.\nWhen data are lost due to, say, computer malfunctions, the loss of data can reasonably be expected not to skew the study’s results by occurring more often in one condition than in the other or by primarily affecting high or low performers etc. In this case, the lost data are said to be missing completely at random. While such losses of data are unfortunate as they lead to a loss of statistical power, they’re benign in that you can just discount the missing data and run your analyses on the remaining data without introducing bias. (Alternatively, the analyst could try to impute the missing data, but that’s for another time.)\nNot all missing data are missing completely at random, however. Say that an experimental study finds that children in L2 immersion classes outperform children in the control group (traditional classes) in subjects such as geography and biology. Such a finding could well be interpreted as evidence for bilingual advantages extending into scholastic performance. Now imagine that out of the 230 children starting the school year in an L2 class, 80 switched to traditional classes and dropped out of the study (35%), whereas out of the 200 children starting out in the control group, 20 went off the radar (10%). All of a sudden, the picture for L2 immersion looks bleaker: it’s plausible that it’s especially the pupils that would’ve performed poorly dropped out of the L2 immersion classes, and that the positive effect is the result of L2 immersion being more selective rather than being more effective. In such cases, the lost data are said to be not missing at random. Indeed, the very fact that more data are missing in the L2 immersion group than in the control condition is informative in its own right and should be taken into account when evaluating the efficacy of L2 immersion.\n(There’s a third kind of ‘missingness’, viz. missing at random, which describes the situation in which the missingness can be accounted for by control variables.)"
  },
  {
    "objectID": "posts/2015-11-02-unequal-sample-sized/index.html#summary",
    "href": "posts/2015-11-02-unequal-sample-sized/index.html#summary",
    "title": "Causes and consequences of unequal sample sizes",
    "section": "Summary",
    "text": "Summary\nThe main points to take away from this blog post are the following:\n\nA sample size imbalance isn’t a tell-tale sign of a poor study.\nYou don’t need equal-sized groups to compute accurate statistics.\nIf the sample size imbalance is due to drop-outs rather than due to design, simple randomisation or technical glitches, this is something to take into account when interpreting the results.\nWhatever you do, don’t throw away data."
  },
  {
    "objectID": "posts/2015-11-02-unequal-sample-sized/index.html#r-code-average-power-of-experiments-with-simple-randomisation",
    "href": "posts/2015-11-02-unequal-sample-sized/index.html#r-code-average-power-of-experiments-with-simple-randomisation",
    "title": "Causes and consequences of unequal sample sizes",
    "section": "R code: Average power of experiments with simple randomisation",
    "text": "R code: Average power of experiments with simple randomisation\n\n# Load pwr package\nlibrary(pwr)\n\n# To compute power for complete randomisation, use:\npwr.t.test(n = 30, d = 0.7)$power\n\n[1] 0.7599049\n\npwr.t.test(n = 30, d = 0.2)$power\n\n[1] 0.1186794\n\n\nFor experiments where the allocation was done using simple randomisation, we need to compute the power of each possible combination of sample sizes and weight it by its probability of occurrence. The pwr.t2n.test() function will return an error when the sample size in one condition is 0 or 1, so we’ll manually set the power for these cases to 0.\n\nweighted.power &lt;- function(total.n, d) {\n  # Compute power for n1 = 2, 3 etc. n-2\n  # And add 0 for n1 = 0, 1, n-1 and n\n  power.unbalanced &lt;- c(0, 0, pwr.t2n.test(n1 = 2:(total.n - 2), \n                                           n2 = (total.n-2):2,\n                                           d = d)$power,\n                        0, 0)\n  \n  # Compute probability of each n1/n2 combination occurring\n  weights &lt;- dbinom(0:total.n, total.n, 0.5)\n  \n  # Compute weighted average and return\n  return(weighted.mean(power.unbalanced, weights))\n}\n\nweighted.power(total.n = 60, d = 0.7)\n\n[1] 0.7527472\n\nweighted.power(total.n = 60, d = 0.2)\n\n[1] 0.1175073"
  },
  {
    "objectID": "posts/2015-11-02-unequal-sample-sized/index.html#software-versions",
    "href": "posts/2015-11-02-unequal-sample-sized/index.html#software-versions",
    "title": "Causes and consequences of unequal sample sizes",
    "section": "Software versions",
    "text": "Software versions\n\ndevtools::session_info()\n\n─ Session info ───────────────────────────────────────────────────────────────\n setting  value\n version  R version 4.3.1 (2023-06-16)\n os       Ubuntu 22.04.3 LTS\n system   x86_64, linux-gnu\n ui       X11\n language en_US\n collate  en_US.UTF-8\n ctype    en_US.UTF-8\n tz       Europe/Zurich\n date     2023-08-25\n pandoc   3.1.1 @ /usr/lib/rstudio/resources/app/bin/quarto/bin/tools/ (via rmarkdown)\n\n─ Packages ───────────────────────────────────────────────────────────────────\n package     * version date (UTC) lib source\n cachem        1.0.6   2021-08-19 [2] CRAN (R 4.2.0)\n callr         3.7.3   2022-11-02 [1] CRAN (R 4.3.1)\n cli           3.6.1   2023-03-23 [1] CRAN (R 4.3.0)\n crayon        1.5.2   2022-09-29 [1] CRAN (R 4.3.1)\n devtools      2.4.5   2022-10-11 [1] CRAN (R 4.3.1)\n digest        0.6.29  2021-12-01 [2] CRAN (R 4.2.0)\n ellipsis      0.3.2   2021-04-29 [2] CRAN (R 4.2.0)\n evaluate      0.15    2022-02-18 [2] CRAN (R 4.2.0)\n fastmap       1.1.0   2021-01-25 [2] CRAN (R 4.2.0)\n fs            1.5.2   2021-12-08 [2] CRAN (R 4.2.0)\n glue          1.6.2   2022-02-24 [2] CRAN (R 4.2.0)\n htmltools     0.5.5   2023-03-23 [1] CRAN (R 4.3.0)\n htmlwidgets   1.6.2   2023-03-17 [1] CRAN (R 4.3.1)\n httpuv        1.6.11  2023-05-11 [1] CRAN (R 4.3.1)\n jsonlite      1.8.7   2023-06-29 [1] CRAN (R 4.3.1)\n knitr         1.39    2022-04-26 [2] CRAN (R 4.2.0)\n later         1.3.1   2023-05-02 [1] CRAN (R 4.3.1)\n lifecycle     1.0.3   2022-10-07 [1] CRAN (R 4.3.0)\n magrittr      2.0.3   2022-03-30 [1] CRAN (R 4.3.0)\n memoise       2.0.1   2021-11-26 [2] CRAN (R 4.2.0)\n mime          0.10    2021-02-13 [2] CRAN (R 4.0.2)\n miniUI        0.1.1.1 2018-05-18 [1] CRAN (R 4.3.1)\n pkgbuild      1.4.2   2023-06-26 [1] CRAN (R 4.3.1)\n pkgload       1.3.2.1 2023-07-08 [1] CRAN (R 4.3.1)\n prettyunits   1.1.1   2020-01-24 [2] CRAN (R 4.2.0)\n processx      3.8.2   2023-06-30 [1] CRAN (R 4.3.1)\n profvis       0.3.8   2023-05-02 [1] CRAN (R 4.3.1)\n promises      1.2.0.1 2021-02-11 [1] CRAN (R 4.3.1)\n ps            1.7.5   2023-04-18 [1] CRAN (R 4.3.1)\n purrr         1.0.1   2023-01-10 [1] CRAN (R 4.3.0)\n pwr         * 1.3-0   2020-03-17 [1] CRAN (R 4.3.1)\n R6            2.5.1   2021-08-19 [2] CRAN (R 4.2.0)\n Rcpp          1.0.11  2023-07-06 [1] CRAN (R 4.3.1)\n remotes       2.4.2   2021-11-30 [2] CRAN (R 4.2.0)\n rlang         1.1.1   2023-04-28 [1] CRAN (R 4.3.0)\n rmarkdown     2.21    2023-03-26 [1] CRAN (R 4.3.0)\n rstudioapi    0.14    2022-08-22 [1] CRAN (R 4.3.0)\n sessioninfo   1.2.2   2021-12-06 [2] CRAN (R 4.2.0)\n shiny         1.7.4.1 2023-07-06 [1] CRAN (R 4.3.1)\n stringi       1.7.12  2023-01-11 [1] CRAN (R 4.3.1)\n stringr       1.5.0   2022-12-02 [1] CRAN (R 4.3.0)\n urlchecker    1.0.1   2021-11-30 [1] CRAN (R 4.3.1)\n usethis       2.2.2   2023-07-06 [1] CRAN (R 4.3.1)\n vctrs         0.6.3   2023-06-14 [1] CRAN (R 4.3.0)\n xfun          0.39    2023-04-20 [1] CRAN (R 4.3.0)\n xtable        1.8-4   2019-04-21 [1] CRAN (R 4.3.1)\n yaml          2.3.5   2022-02-21 [2] CRAN (R 4.2.0)\n\n [1] /home/jan/R/x86_64-pc-linux-gnu-library/4.3\n [2] /usr/local/lib/R/site-library\n [3] /usr/lib/R/site-library\n [4] /usr/lib/R/library\n\n──────────────────────────────────────────────────────────────────────────────"
  },
  {
    "objectID": "posts/2014-10-15-tautological-tests/index.html",
    "href": "posts/2014-10-15-tautological-tests/index.html",
    "title": "Silly significance tests: Tautological tests",
    "section": "",
    "text": "We can do with fewer significance tests. In my previous blog post, I argued that using significance tests to check whether random assignment ‘worked’ doesn’t make much sense. In this post, I argue against a different kind of silly significance test of which I don’t see the added value: dividing participants into a ‘high X’ and a ‘low X’ group and then testing whether the groups differ with respect to ‘X’."
  },
  {
    "objectID": "posts/2014-10-15-tautological-tests/index.html#what-do-i-mean-by-tautological-tests",
    "href": "posts/2014-10-15-tautological-tests/index.html#what-do-i-mean-by-tautological-tests",
    "title": "Silly significance tests: Tautological tests",
    "section": "What do I mean by ‘tautological tests’?",
    "text": "What do I mean by ‘tautological tests’?\nEvery so often, I stumble upon a study where it says something along the following lines:\n\nThe 70 participants were divided into three proficiency groups according to their performance on a 20-point French-language test. The high-proficiency group consisted of participants with a score of 16 or higher (n = 20); the mid-proficiency group of participants with a score between 10 and 15 (n = 37); and the low-proficiency group of participants with a score of 9 or lower (n = 13). An ANOVA showed significant differences in the test scores between the high-, mid- and low-proficiency groups (F(2, 67) = 133.5, p &lt; 0.001).\n\nThis is a fictitious example, but the general procedure is somewhat common in research papers in applied linguistics: the researcher creates groups of participants so that the groups show no overlap on a certain variable (e.g. task performance, age), and then proceeds to rubber-stamp this grouping by showing that there exist significant between-group differences on that very same variable. It’s not necessarily the grouping of participants that is rubber-stamped; every once in a while, the researchers’ choice of stimuli (e.g. high- vs.  low-frequency words) is justified in a similar fashion. For want of a better term, I’ll call this practice tautological significance testing."
  },
  {
    "objectID": "posts/2014-10-15-tautological-tests/index.html#the-problem",
    "href": "posts/2014-10-15-tautological-tests/index.html#the-problem",
    "title": "Silly significance tests: Tautological tests",
    "section": "The problem",
    "text": "The problem\nAs their name suggests, tautological tests are silly because they can’t tell us anything that’s both correct and new. Since we ourselves created participant or stimulus groups that don’t show any overlap on a particular variable, we obviously know that the groups must differ with respect to this variable. If the significance test returns a non-significant p-value, then this tells us more about the sample size than about group differences with respect to the variable in question.\nThis non-informativeness is what tautological tests have in common with the silly significant tests that I discussed previously, balance tests. In the case of balance tests, we tested for the absence of a difference (which we know doesn’t really exist); when using tautological tests, we tested for the presence of a difference which we know does exist. I don’t think that tautological tests negatively affect the study’s results, but they clutter research reports with useless – but often dauntingly looking – prose."
  },
  {
    "objectID": "posts/2014-10-15-tautological-tests/index.html#the-larger-issue-overuse-of-anova",
    "href": "posts/2014-10-15-tautological-tests/index.html#the-larger-issue-overuse-of-anova",
    "title": "Silly significance tests: Tautological tests",
    "section": "The larger issue: Overuse of ANOVA",
    "text": "The larger issue: Overuse of ANOVA\nMore problematic than tautological significance tests is what preceeds them: discretising a fine-grained variable. The point is not new but bears repeating: By carving up a fine-grained variable into groups, you throw away valuable information. As a result, you have less statistical power than you could’ve had if you’d used the original variable in your analyses. Additionally, the choice of the cut-off points is often arbitrary, and the outcome may well differ had different cut-off points been chosen.\nResearchers often seem to think that they need to form groups in order to sensibly analyse their data. The underlying idea could be that group comparisons (i.e. ANOVAs) are somehow more respectable or objective than analyses involving continuous variables (e.g. linear regression). Or perhaps researchers think that ANOVAs are necessary to deal with more complicated data, such as data with crossed dependency structures (e.g.  featuring both stimulus- and participant-related variables) or data exhibiting non-linearities. Researchers who’d like to disabuse themselves of these notions can take a look at a position paper by Harald Baayen as well as at some articles in the 2008 special issue of the Journal of Memory and Language (e.g. the Baayen et al. and Jaeger papers). If your data exhibits non-linearities that you hope to ‘cure’ by discretising a continuous variables, I suggest you first take a look at Michael Clark’s introduction to generalised additive models, which can cope with non-linearities, or perhaps look into the possibility of transforming your variables so that the relationship between them becomes approximately linear."
  },
  {
    "objectID": "posts/2014-10-15-tautological-tests/index.html#solution",
    "href": "posts/2014-10-15-tautological-tests/index.html#solution",
    "title": "Silly significance tests: Tautological tests",
    "section": "Solution",
    "text": "Solution\nThe solution to tautological tests is again straightforward. First, we ought to ask whether it’s really necessary to categorise a finer-grained variable. Often, an regression-based analysis that stays true to the original variable is feasible. Second, if for whatever reason it’s not possible to carry out a regression, just don’t carry out such tautological tests."
  },
  {
    "objectID": "posts/2014-10-15-tautological-tests/index.html#a-related-kind-of-test-use",
    "href": "posts/2014-10-15-tautological-tests/index.html#a-related-kind-of-test-use",
    "title": "Silly significance tests: Tautological tests",
    "section": "A related kind of test use",
    "text": "A related kind of test use\nA related kind of significance test (ab)use is when researchers try to ensure the comparability of stimuli or participants between conditions. For instance, when investigating the effect of word frequency on vocabulary retention, researchers often want to make sure that high- and low-frequency words are similar in respects other than frequency (e.g. word length). While I wouldn’t call this use of significance tests silly, it’s not exactly ideal either. Imai et al. discuss the use of significance tests to evaluate the success of a matching procedure (Section 7.2) and argue against it. In Section 7.3, they discuss some alternatives, but the main take-home message is quite simple: significance tests aren’t suited for this purpose."
  },
  {
    "objectID": "posts/2017-02-22-confidence-intervals-for-standardised-effect-sizes/index.html",
    "href": "posts/2017-02-22-confidence-intervals-for-standardised-effect-sizes/index.html",
    "title": "Confidence intervals for standardised mean differences",
    "section": "",
    "text": "Standardised effect sizes express patterns found in the data in terms of the variability found in the data. For instance, a mean difference in body height could be expressed in the metric in which the data were measured (e.g., a difference of 4 centimetres) or relative to the variation in the data (e.g., a difference of 0.9 standard deviations). The latter is a standardised effect size known as Cohen’s d.\nAs I’ve written before, I don’t particularly like standardised effect sizes. Nonetheless, I wondered how confidence intervals around standardised effect sizes (more specifically: standardised mean differences) are constructed. Until recently, I hadn’t really thought about it and sort of assumed you would compute them the same way as confidence intervals around raw effect sizes. But unlike raw (unstandardised) mean differences, standardised mean differences are a combination of two estimates subject to sampling error: the mean difference itself and the sample standard deviation. Moreover, the sample standard deviation is a biased estimate of the population standard deviation (it tends to be too low), which causes Cohen’s d to be an upwardly biased estimate of the population standardised mean difference. Surely both of these factors must affect how the confidence intervals around standardised effect sizes are constructed?\nIt turns out that indeed they do. When I compared the confidence intervals that I computed around a standardised effect size using a naive approach that assumed that the standard deviation wasn’t subject to sampling error and wasn’t biased, I got different results than when I used specialised R functions.\nBut these R functions all produced different results, too.\nObviously, there may well be more than one way to skin a cat, but this caused me to wonder if the different procedures for computing confidence intervals all covered the true population parameter with the nominal probability (e.g., in 95% of cases for a 95% confidence interval). I ran a simulation to find out, which I’ll report in the remainder of this post. If you spot any mistakes, please let me know."
  },
  {
    "objectID": "posts/2017-02-22-confidence-intervals-for-standardised-effect-sizes/index.html#introducing-the-contenders",
    "href": "posts/2017-02-22-confidence-intervals-for-standardised-effect-sizes/index.html#introducing-the-contenders",
    "title": "Confidence intervals for standardised mean differences",
    "section": "Introducing the contenders",
    "text": "Introducing the contenders\nBelow, I’m going to introduce three R functions for computing confidence intervals for standardised effect sizes (standardised mean differences, to be specific). To illustrate how they work, though, I’m first going to generate a two samples from normal distributions with standard deviations of 1 and means of 0.5 and 0, respectively.\nUpdate (2023-08-07): I reran all of the code in this blog post with newer software versions.\n\n# Set random seed for reproducible results\n# (today's date)\nRNGversion(\"3.5.3\")\nset.seed(2017-02-21)\n\n# Population standardised mean difference\nd &lt;- 0.5\n# Number of observations per sample\nn &lt;- 20\n\n# Generate data\ny &lt;- c(rnorm(n, d, sd = 1), \n       rnorm(n, 0, sd = 1))\nx &lt;- factor(c(rep(\"A\", n),\n              rep(\"B\", n)))\n\n\ncohen.d in the effsize package\nThe first function is cohen.d from the effsize package. It takes as its arguments the two samples you want to compare and the desired confidence level (here: 90%). You can also specify whether you want to apply hedges.correction, which causes the function to compute Hegdes’ g and confidence intervals for it. (Hedges’ g is less biased than Cohen’s d.)\n\nlibrary(effsize)\ncohen.d(y, x, conf.level = 0.9, hedges.correction = FALSE)\n\n\nCohen's d\n\nd estimate: 0.811 (large)\n90 percent confidence interval:\nlower upper \n0.257 1.366 \n\n\n90% confidence interval for Cohen’s d: [0.25, 1.37].\n\ncohen.d(y, x, conf.level = 0.9, hedges.correction = TRUE)\n\n\nHedges's g\n\ng estimate: 0.795 (medium)\n90 percent confidence interval:\nlower upper \n0.252 1.338 \n\n\n90% confidence interval for Hedges’ g: [0.25, 1.34].\n(Incidentally, cohen.d also has a parameter called noncentral, but setting it to TRUE doesn’t seem to work… (Update (2023-08-07): It does now.))\n\n\ntes in the compute.es package\nThe second function is tes from the compute.es package. It takes as its arguments the t statistic for the t test comparing the two samples, the sample sizes and the desired confidence level (as a percentage, not as a proportion):\n\n# Compute t-statistic\nt.stat &lt;- t.test(y ~ x, var.equal = TRUE)$statistic\n\n# Compute effect sizes and confidence intervals\nlibrary(compute.es)\ntes(t.stat, n.1 = 20, n.2 = 20, level = 90)\n\nMean Differences ES: \n \n d [ 90 %CI] = 0.81 [ 0.27 , 1.35 ] \n  var(d) = 0.11 \n  p-value(d) = 0.02 \n  U3(d) = 79.1 % \n  CLES(d) = 71.7 % \n  Cliff's Delta = 0.43 \n \n g [ 90 %CI] = 0.8 [ 0.26 , 1.33 ] \n  var(g) = 0.1 \n  p-value(g) = 0.02 \n  U3(g) = 78.7 % \n  CLES(g) = 71.3 % \n \n Correlation ES: \n \n r [ 90 %CI] = 0.38 [ 0.13 , 0.59 ] \n  var(r) = 0.02 \n  p-value(r) = 0.02 \n \n z [ 90 %CI] = 0.41 [ 0.13 , 0.68 ] \n  var(z) = 0.03 \n  p-value(z) = 0.02 \n \n Odds Ratio ES: \n \n OR [ 90 %CI] = 4.36 [ 1.63 , 11.6 ] \n  p-value(OR) = 0.02 \n \n Log OR [ 90 %CI] = 1.47 [ 0.49 , 2.45 ] \n  var(lOR) = 0.36 \n  p-value(Log OR) = 0.02 \n \n Other: \n \n NNT = 3.47 \n Total N = 40\n\n\nThis function outputs a lot of standardised effect sizes and their confidence intervals. Here, I’m only interested in Cohen’s d, whose 90% confidence interval now is [0.27, 1.35]. (The confidence interval for Hedges’ g is also different from that from the cohen.d function.)\nNote, incidentally, that the Cohen’s d and Hedges’ g values are the same for the tes and the cohen.d function; it’s just the confidence intervals that are different.\n\n\nci.smd in the MBESS package\nLastly, the ci.smd function from the MBESS package takes as its input a Cohen’s d, the two sample sizes, and the desired confidence level. Here I compute Cohen’s d using the cohen.d function and then feed it to ci.smd.\n\n# Compute Cohen's d\nd.stat &lt;- cohen.d(y, x, conf.level = 0.9, hedges.correction = FALSE)$estimate\n\n# Compute confidence intervals\nlibrary(MBESS)\nci.smd(smd = d.stat, n.1 = 20, n.2 = 20, conf.level = 0.9)\n\n$Lower.Conf.Limit.smd\n[1] 0.264\n\n$smd\n[1] 0.811\n\n$Upper.Conf.Limit.smd\n[1] 1.35\n\n\nThis time, the 90% confidence interval is [0.26, 1.35]. It was fairly small differences between the three functions such as these that led me to run the simulation I report below."
  },
  {
    "objectID": "posts/2017-02-22-confidence-intervals-for-standardised-effect-sizes/index.html#coverage-of-the-population-standardised-mean-difference-by-different-confidence-intervals",
    "href": "posts/2017-02-22-confidence-intervals-for-standardised-effect-sizes/index.html#coverage-of-the-population-standardised-mean-difference-by-different-confidence-intervals",
    "title": "Confidence intervals for standardised mean differences",
    "section": "Coverage of the population standardised mean difference by different confidence intervals",
    "text": "Coverage of the population standardised mean difference by different confidence intervals\n\nMethod\nFor the simulation I generated lots of samples from two normal distributions with the same standard deviation whose means where half a standard deviation apart. In other words, the population standardised mean difference was 0.5. For each sample, I computed 90% confidence intervals around the sample standardised mean difference (Cohen’s d) using the cohen.d, tes and ci.smd functions; I also computed 90% confidence intervals around Hedges’ g using the cohen.d function. I then checked how often these intervals contained the population mean standardised difference (0.5). Ideally, this should be the case in about 90% of the samples generated. If it’s fewer than that, the confidence intervals are too narrow; if it’s more than that, they’re too wide. I ran this simulation for different sample sizes, ranging from 5 observations per group to 500 per group.\nThe R code for this simulation is available at the bottom of this post.\n\n\nResults\n\n\n\n\n\nFigure 1. Coverage of the population standardised mean difference (0.5) by confidence intervals computed using the cohen.d, tes and ci.smd functions based on 10,000 simulation runs per sample size. The dashed horizontal line shows the nominal confidence level; the grey lines around it show the values between which the coverage rates should lie with 95% probability if the confidence interval had their nominal coverage rate.\n\n\n\n\nAs Figure 1 clearly shows, the coverage rates for the confidence intervals computed around Cohen’s d using the ci.smd function are at their nominal level even for small samples. The confidence intervals computed using the cohen.d and tes functions, however, are too wide for sample sizes of up to 50 observations per group."
  },
  {
    "objectID": "posts/2017-02-22-confidence-intervals-for-standardised-effect-sizes/index.html#conclusions-and-further-reading",
    "href": "posts/2017-02-22-confidence-intervals-for-standardised-effect-sizes/index.html#conclusions-and-further-reading",
    "title": "Confidence intervals for standardised mean differences",
    "section": "Conclusions and further reading",
    "text": "Conclusions and further reading\nFirst of all, I want to reiterate that I think standardised effect sizes, including standardised mean differences and correlation coefficients, are overvalued and that I think we should strive to interpret raw effect sizes instead.\nThat said, on a practical level, this simulation suggests that if you nonetheless want to express your results as a standardised mean difference and you want to compute a confidence around it, it’s a good idea to take a look at the MBESS package. The package’s vignette also has a good discussion of how exact confidence intervals can be constructed around standardised effect sizes, and the package provides a fast implementation of these methods.\nBy contrast, the effsize and compute.es packages seem to rely on overly conservative approximations to these exact methods, and differ between each other in how the variance for Cohen’s d is computed (see here and here).\nFor those of you interested in further details, Wolfgang Viechtbauer provided some links on Twitter that you may want to take a look at."
  },
  {
    "objectID": "posts/2017-02-22-confidence-intervals-for-standardised-effect-sizes/index.html#r-code",
    "href": "posts/2017-02-22-confidence-intervals-for-standardised-effect-sizes/index.html#r-code",
    "title": "Confidence intervals for standardised mean differences",
    "section": "R code",
    "text": "R code\nFor those interested, here’s the R code I used for the simulation. If you spot an error that explains the results above, please let me know.\nFirst, I defined a function, d_ci, that generates two samples from normal distributions with standard deviations of 1. In the simulation, the mean difference between these populations is 0.5, which, since both population have standard deviations of 1, means that the true standardised mean difference is 0.5. Then, four confidence intervals are computed around the sample Cohen’s d:\n\nUsing cohen.d with Hedges’ correction.\n\nUsing tes. (I only used the code relevant to Cohen’s d to speed things up.)\nUsing ci.smd.\nUsing cohen.d without Hedges’ correction.\n\nd_ci simply returns, for each of these four intervals, whether they contain the true population standardised mean difference (i.e., 0.5).\n\nd_ci &lt;- function(d = 0.5, n = 20, ci = 0.80) {\n  # Two samples with specified d\n  y &lt;- c(rnorm(n, d, sd = 1), \n         rnorm(n, 0, sd = 1))\n  x &lt;- factor(c(rep(\"A\", n),\n                rep(\"B\", n)))\n\n  # Run t-test\n  ttest &lt;- t.test(y ~ x, var.equal = TRUE, conf.level = ci)\n\n  # Effect size using effsize package ----------------------\n  # with Hedges' correction\n  es &lt;- effsize::cohen.d(y, x, conf.level = ci, \n                         hedges.correction = TRUE)\n  d.lo1 &lt;- es$conf.int[1]\n  d.hi1 &lt;- es$conf.int[2]\n\n  # Effect size using compute.es ---------------------------\n  # This is a selection from the source code for compute.es::tes.\n  df &lt;- 2 * n - 2\n  d.est2 &lt;- ttest$statistic * sqrt((2 * n)/(n * n))\n  var.d &lt;- (2 * n)/(n * n) + (d.est2^2) / (2 * (2 * n))\n  crit &lt;- qt((1 - ci)/2, df, lower.tail = FALSE)\n  d.lo2 &lt;- d.est2 - crit * sqrt(var.d)\n  d.hi2 &lt;- d.est2 + crit * sqrt(var.d)\n  \n  # MBESS package ---------------------------------------\n  # (d.est2 computed above for compute.es)\n  mbess_out &lt;- MBESS::ci.smd(smd = d.est2, n.1 = n, n.2 = n, conf.level = ci)\n  d.lo3 &lt;- mbess_out$Lower.Conf.Limit.smd\n  d.hi3 &lt;- mbess_out$Upper.Conf.Limit.smd\n  \n  # Effect size using effsize package \n  # no hedges corrections ----------------------\n  es4 &lt;- effsize::cohen.d(y, x, conf.level = ci, \n                         hedges.correction = FALSE)\n  d.lo4 &lt;- es4$conf.int[1]\n  d.hi4 &lt;- es4$conf.int[2]\n  \n  # In CI? ----------------------------------------------------\n  d.in.ci1 &lt;- (d &lt; d.hi1 && d &gt; d.lo1)      # effsize::cohen.d\n  d.in.ci2 &lt;- (d &lt; d.hi2 && d &gt; d.lo2)      # compute.es::tes\n  d.in.ci3 &lt;- (d &lt; d.hi3 && d &gt; d.lo3)      # MBESS::ci.smd\n  d.in.ci4 &lt;- (d &lt; d.hi4 && d &gt; d.lo4)      # MBESS::ci.smd\n  \n  # Return -------------------------------------------- \n  return(list(d.in.ci1, d.in.ci2, d.in.ci3, d.in.ci4))\n}\n\nThen I wrote a function, sim_es, which runs d_ci a set number of times and return the proportion of times the four confidence intervals contained the true population d.\n\n# Run d_ci a couple of times and store the coverage rates\nsim_es &lt;- function(runs = 1e1, n = 10, d = 0.5, ci = 0.8) {\n  reps &lt;- replicate(runs, d_ci(n = n, d = d, ci = ci))\n  return(list(mean(unlist(reps[1, ])),\n              mean(unlist(reps[2, ])),\n              mean(unlist(reps[3, ])),\n              mean(unlist(reps[4, ]))))\n}\n\nNext I ran sim_es 10,000 times for 9 different samples, from 5 observations per sample to 500. The desired confidence level was 90%.\n\n# Define the sample sizes\nsampleSizes &lt;- c(5, 10, 20, 30, 40, 50,\n                 100, 200, 500)\n\n# Define the confidence level and the number of runs\nci &lt;- 0.90\nruns &lt;- 10000\n\n# Evaluate sim_es for different sampleSizes\n# but with runs and ci set to the same value each time.\n# You can use mapply instead of mcmapply.\n# Remove the last line (with mc.cores) in that case.\nlibrary(parallel)\nresults &lt;- mcmapply(sim_es,\n                    n = sampleSizes,\n                    MoreArgs = list(runs = runs, ci = ci),\n                    mc.cores = detectCores())\n\nFinally, I stored the results to a dataframe and plotted them.\n\n# Store results in dataframe\nd_results &lt;- data.frame(sampleSizes)\nd_results$`cohen.d (correction) in\\neffsize package` &lt;- unlist(results[1, ])\nd_results$`tes in\\ncompute.es package` &lt;- unlist(results[2, ])\nd_results$`ci.smd in\\nMBESS package` &lt;- unlist(results[3, ])\nd_results$`cohen.d (no correction) in\\neffsize package` &lt;- unlist(results[4, ])\n\n# Load packages for plotting\nlibrary(tidyverse)\n\n# Plot\nggplot(d_results |&gt; \n         gather(\"Method\", \"Coverage\", -sampleSizes),\n       aes(x = sampleSizes,\n           y = Coverage)) +\n  # Draw lines for expected coverage rate and its 2.5 and 97.5% percentiles.\n  geom_hline(yintercept = qbinom(p = 0.025, runs, ci)/runs, colour = \"grey80\") +\n  geom_hline(yintercept = qbinom(p = 0.975, runs, ci)/runs, colour = \"grey80\") +\n  geom_hline(yintercept = ci, linetype = 2, colour = \"grey10\") +\n  geom_point() +\n  geom_line() +\n  scale_x_log10(breaks = c(5, 50, 500, 5000)) +\n  scale_y_continuous(breaks = seq(0.8, 1, 0.02)) +\n  facet_wrap(~ Method, ncol = 4) +\n  xlab(\"sample size per group\") +\n  ylab(\"Coverage of true standardised d\\nby 90% confidence interval\") +\n  theme(legend.position = \"top\", legend.direction = \"vertical\")"
  },
  {
    "objectID": "posts/2017-02-22-confidence-intervals-for-standardised-effect-sizes/index.html#software-versions",
    "href": "posts/2017-02-22-confidence-intervals-for-standardised-effect-sizes/index.html#software-versions",
    "title": "Confidence intervals for standardised mean differences",
    "section": "Software versions",
    "text": "Software versions\n\ndevtools::session_info()\n\n─ Session info ───────────────────────────────────────────────────────────────\n setting  value\n version  R version 4.3.1 (2023-06-16)\n os       Ubuntu 22.04.2 LTS\n system   x86_64, linux-gnu\n ui       X11\n language en_US\n collate  en_US.UTF-8\n ctype    en_US.UTF-8\n tz       Europe/Zurich\n date     2023-08-07\n pandoc   3.1.1 @ /usr/lib/rstudio/resources/app/bin/quarto/bin/tools/ (via rmarkdown)\n\n─ Packages ───────────────────────────────────────────────────────────────────\n package     * version date (UTC) lib source\n bit           4.0.5   2022-11-15 [1] CRAN (R 4.3.0)\n bit64         4.0.5   2020-08-30 [1] CRAN (R 4.3.0)\n cachem        1.0.6   2021-08-19 [2] CRAN (R 4.2.0)\n callr         3.7.3   2022-11-02 [1] CRAN (R 4.3.1)\n cli           3.6.1   2023-03-23 [1] CRAN (R 4.3.0)\n codetools     0.2-19  2023-02-01 [4] CRAN (R 4.2.2)\n colorspace    2.1-0   2023-01-23 [1] CRAN (R 4.3.0)\n compute.es  * 0.2-5   2020-04-01 [1] CRAN (R 4.3.1)\n crayon        1.5.2   2022-09-29 [1] CRAN (R 4.3.1)\n devtools      2.4.5   2022-10-11 [1] CRAN (R 4.3.1)\n digest        0.6.29  2021-12-01 [2] CRAN (R 4.2.0)\n dplyr       * 1.1.2   2023-04-20 [1] CRAN (R 4.3.0)\n effsize     * 0.8.1   2020-10-05 [1] CRAN (R 4.3.1)\n ellipsis      0.3.2   2021-04-29 [2] CRAN (R 4.2.0)\n evaluate      0.15    2022-02-18 [2] CRAN (R 4.2.0)\n fansi         1.0.4   2023-01-22 [1] CRAN (R 4.3.1)\n farver        2.1.1   2022-07-06 [1] CRAN (R 4.3.0)\n fastmap       1.1.0   2021-01-25 [2] CRAN (R 4.2.0)\n forcats     * 1.0.0   2023-01-29 [1] CRAN (R 4.3.0)\n fs            1.5.2   2021-12-08 [2] CRAN (R 4.2.0)\n generics      0.1.3   2022-07-05 [1] CRAN (R 4.3.0)\n ggplot2     * 3.4.2   2023-04-03 [1] CRAN (R 4.3.0)\n glue          1.6.2   2022-02-24 [2] CRAN (R 4.2.0)\n gtable        0.3.3   2023-03-21 [1] CRAN (R 4.3.0)\n hms           1.1.3   2023-03-21 [1] CRAN (R 4.3.0)\n htmltools     0.5.5   2023-03-23 [1] CRAN (R 4.3.0)\n htmlwidgets   1.6.2   2023-03-17 [1] CRAN (R 4.3.1)\n httpuv        1.6.11  2023-05-11 [1] CRAN (R 4.3.1)\n jsonlite      1.8.7   2023-06-29 [1] CRAN (R 4.3.1)\n knitr         1.39    2022-04-26 [2] CRAN (R 4.2.0)\n later         1.3.1   2023-05-02 [1] CRAN (R 4.3.1)\n lifecycle     1.0.3   2022-10-07 [1] CRAN (R 4.3.0)\n lubridate   * 1.9.2   2023-02-10 [1] CRAN (R 4.3.0)\n magrittr      2.0.3   2022-03-30 [1] CRAN (R 4.3.0)\n MBESS       * 4.9.2   2022-09-19 [1] CRAN (R 4.3.1)\n memoise       2.0.1   2021-11-26 [2] CRAN (R 4.2.0)\n mime          0.10    2021-02-13 [2] CRAN (R 4.0.2)\n miniUI        0.1.1.1 2018-05-18 [1] CRAN (R 4.3.1)\n munsell       0.5.0   2018-06-12 [1] CRAN (R 4.3.0)\n pillar        1.9.0   2023-03-22 [1] CRAN (R 4.3.0)\n pkgbuild      1.4.2   2023-06-26 [1] CRAN (R 4.3.1)\n pkgconfig     2.0.3   2019-09-22 [2] CRAN (R 4.2.0)\n pkgload       1.3.2.1 2023-07-08 [1] CRAN (R 4.3.1)\n prettyunits   1.1.1   2020-01-24 [2] CRAN (R 4.2.0)\n processx      3.8.2   2023-06-30 [1] CRAN (R 4.3.1)\n profvis       0.3.8   2023-05-02 [1] CRAN (R 4.3.1)\n promises      1.2.0.1 2021-02-11 [1] CRAN (R 4.3.1)\n ps            1.7.5   2023-04-18 [1] CRAN (R 4.3.1)\n purrr       * 1.0.1   2023-01-10 [1] CRAN (R 4.3.0)\n R6            2.5.1   2021-08-19 [2] CRAN (R 4.2.0)\n Rcpp          1.0.11  2023-07-06 [1] CRAN (R 4.3.1)\n readr       * 2.1.4   2023-02-10 [1] CRAN (R 4.3.0)\n remotes       2.4.2   2021-11-30 [2] CRAN (R 4.2.0)\n rlang         1.1.1   2023-04-28 [1] CRAN (R 4.3.0)\n rmarkdown     2.21    2023-03-26 [1] CRAN (R 4.3.0)\n rstudioapi    0.14    2022-08-22 [1] CRAN (R 4.3.0)\n scales        1.2.1   2022-08-20 [1] CRAN (R 4.3.0)\n sessioninfo   1.2.2   2021-12-06 [2] CRAN (R 4.2.0)\n shiny         1.7.4.1 2023-07-06 [1] CRAN (R 4.3.1)\n stringi       1.7.12  2023-01-11 [1] CRAN (R 4.3.1)\n stringr     * 1.5.0   2022-12-02 [1] CRAN (R 4.3.0)\n tibble      * 3.2.1   2023-03-20 [1] CRAN (R 4.3.0)\n tidyr       * 1.3.0   2023-01-24 [1] CRAN (R 4.3.0)\n tidyselect    1.2.0   2022-10-10 [1] CRAN (R 4.3.0)\n tidyverse   * 2.0.0   2023-02-22 [1] CRAN (R 4.3.1)\n timechange    0.2.0   2023-01-11 [1] CRAN (R 4.3.0)\n tzdb          0.4.0   2023-05-12 [1] CRAN (R 4.3.0)\n urlchecker    1.0.1   2021-11-30 [1] CRAN (R 4.3.1)\n usethis       2.2.2   2023-07-06 [1] CRAN (R 4.3.1)\n utf8          1.2.3   2023-01-31 [1] CRAN (R 4.3.1)\n vctrs         0.6.3   2023-06-14 [1] CRAN (R 4.3.0)\n vroom         1.6.3   2023-04-28 [1] CRAN (R 4.3.0)\n withr         2.5.0   2022-03-03 [2] CRAN (R 4.2.0)\n xfun          0.39    2023-04-20 [1] CRAN (R 4.3.0)\n xtable        1.8-4   2019-04-21 [1] CRAN (R 4.3.1)\n yaml          2.3.5   2022-02-21 [2] CRAN (R 4.2.0)\n\n [1] /home/jan/R/x86_64-pc-linux-gnu-library/4.3\n [2] /usr/local/lib/R/site-library\n [3] /usr/lib/R/site-library\n [4] /usr/lib/R/library\n\n──────────────────────────────────────────────────────────────────────────────"
  },
  {
    "objectID": "posts/2015-09-02-covariate-adjustment-mixed-logistic-regression/index.html",
    "href": "posts/2015-09-02-covariate-adjustment-mixed-logistic-regression/index.html",
    "title": "Covariate adjustment in logistic mixed models: Is it worth the effort?",
    "section": "",
    "text": "The previous post investigated whether adjusting for covariates is useful when analysing binary data collected in a randomised experiment with one observation per participant. This turned out to be the case in terms of statistical power and obtaining a more accurate estimate of the treatment effect. This posts investigates whether these benefits carry over to mixed-effect analyses of binary data collected in randomised experiments with several observations per participant. The results suggest that, while covariate adjustment may be worth it if the covariate is a very strong determinant of individual differences in the task at hand, the benefit doesn’t seem large enough to warrant collecting the covariate variable in an experiment I’m planning."
  },
  {
    "objectID": "posts/2015-09-02-covariate-adjustment-mixed-logistic-regression/index.html#background",
    "href": "posts/2015-09-02-covariate-adjustment-mixed-logistic-regression/index.html#background",
    "title": "Covariate adjustment in logistic mixed models: Is it worth the effort?",
    "section": "Background",
    "text": "Background\nI mostly deal with binary dependent variables – whether a word was translated correctly (yes or no), for instance. When each participant contributes only a single datapoint, binary data can be analysed using logistic regression models. More often than not, however, participants are asked to translate several words, i.e. several datapoints are available per participant. This ‘clustering’ needs to be taken into account in the analysis, which is where logistic mixed-effects models come in (see Jaeger 2008 for a rundown).\nFor a new experiment I’m planning, it’d be useful to know whether I should collect between-subjects variables (the participants’ language skills) that are likely to account for inter-individual differences in translation performance but that don’t interest me as such. What interests me instead is the effect of the learning condition, to which the participants will be assigned randomly. Nonetheless, as the previous post shows, accounting for important if uninteresting covariates could be beneficial in terms of power and the accuracy of the estimated treatment effect.\nHowever, when analysing an earlier experiment, I noticed that including covariates did not really affect the estimate of the treatment effect nor its standard error. Since collecting these variables will lengthen the data collection sessions, it’d be useful to know whether the additional time and effort are actually worth it from a statistical point of view. I didn’t find much in the way of readable literature that addresses this questions, so I ran some simulations to find out."
  },
  {
    "objectID": "posts/2015-09-02-covariate-adjustment-mixed-logistic-regression/index.html#set-up",
    "href": "posts/2015-09-02-covariate-adjustment-mixed-logistic-regression/index.html#set-up",
    "title": "Covariate adjustment in logistic mixed models: Is it worth the effort?",
    "section": "Set-up",
    "text": "Set-up\nThe set-up for the simulations is as follows. Sixty German-speaking participants are randomly and evenly assigned to either the experimental or the control condition. During training, the participants in the experimental condition are exposed to a series of Dutch–German word pairs, some of which feature a systematic interlingual correspondence (e.g. Dutch oe = German u as in ‘groet’–‘Gruss’). The participants in the control group are exposed to comparable word pairs without this interlingual correspondence. During testing, all participants are asked to translate previously unseen Dutch words into German. The target stimuli are those that contain Dutch oe, and the question is whether participants in the experimental condition are more likely to apply the interlingual correspondence when translating these words than the participants in the control group. After the experiment, all participants take a German vocabulary test, which a previous study had suggested to be a strong predictor of interindividual differences in this kind of task."
  },
  {
    "objectID": "posts/2015-09-02-covariate-adjustment-mixed-logistic-regression/index.html#settings",
    "href": "posts/2015-09-02-covariate-adjustment-mixed-logistic-regression/index.html#settings",
    "title": "Covariate adjustment in logistic mixed models: Is it worth the effort?",
    "section": "Settings",
    "text": "Settings\n\nThe technical detals of the simulation are available here.\n\nUpdate (2023-08-27): I slightly modified the R code and reran the simulations. You now find the R code at the bottom of this post.\nWhile I’d be chuffed if someone would go through it with the fine-toothed comb, the general idea is this. I took data from a previous experiment similar to the one I’m planning and fitted a model to this dataset. On the basis of the fitted model, I generated new datasets for which I varied the following parameters:\n\nthe number of target stimuli: 5 vs. 20 items per participant;\nthe size of the effect of the between-subjects covariate: realistic (a slope of 0.1) and hugely exaggerated (slope of 1.0). The slope of 0.1 is ‘realistic’ in that it is pretty close to the covariate effect found in the original study. The simulations with the much larger covariate effect were run in order to gauge power in situations were large inter-individual differences exist that can, however, be captured using a covariate.\n\nThe distribution of the covariate scores in the simulated datasets was similar to the one in the original one. The number of participants was fixed at 60, distributed evenly between the two conditions, and the size of the experimental effect was assumed to be equal to that of the original study. A study with more participants or investigating a larger effect size will obviously have more statistical power, but the precise power level isn’t what interests me. Rather, what I wanted to find out is, given a fixed effect size and a fixed number of participants, would it be worth it to collect a between-subjects covariate and include it in the analysis? To address this question, I compared the power of logistic mixed-effects models with and without covariates fitted to the simulated datasets. Per parameter combination, 500 simulated datasets were generated and analysed. This is not a huge number, but running logistic mixed model analyses takes a lot of time."
  },
  {
    "objectID": "posts/2015-09-02-covariate-adjustment-mixed-logistic-regression/index.html#results",
    "href": "posts/2015-09-02-covariate-adjustment-mixed-logistic-regression/index.html#results",
    "title": "Covariate adjustment in logistic mixed models: Is it worth the effort?",
    "section": "Results",
    "text": "Results\n\nTreatment estimate\nFigure 1 shows how well logistic mixed-effect models with and without the between-subjects covariate estimated the true treatment effect on average.\n\n\n\n\n\nFigure 1: Average estimated experimental effect of 500 logistic mixed-effects models without (o) and with the covariate modelled as a fixed effect (+). The vertical dashed line shows the true simulated experimental effect (0.95 log-odds).\n\n\n\n\nFor the realistic covariate slope of 0.1, the covariate-adjusted and -unadjusted models produce essentially the same and highly accurate estimate of the treatment effect. Even for the unrealistically large covariate slope of 1.0, i.e. for datasets with extreme but readily accountable inter-individual differences, the two models perform more or less on par.\nFrom this, I tentitatively conclude that, for this kind of study, accounting for known sources of inter-individual variation using covariates does not substantially affect the estimates of the experimental effect."
  },
  {
    "objectID": "posts/2015-09-02-covariate-adjustment-mixed-logistic-regression/index.html#power",
    "href": "posts/2015-09-02-covariate-adjustment-mixed-logistic-regression/index.html#power",
    "title": "Covariate adjustment in logistic mixed models: Is it worth the effort?",
    "section": "Power",
    "text": "Power\nFigure 2 shows the proportion of significant treatment effects out of 500 simulation runs for the covariate-adjusted and -unadjusted models.\n\n\n\n\n\nFigure 2: Estimated power to detect an experimental effect of 0.95 log-odds of logistic mixed-effects models without (o) and with the covariate modelled as a fixed effect (+).\n\n\n\n\nFor a realistic covariate effect (slope of 0.1), adding the between-subjects covariate improves power only marginally (by 1 to 2 percentage points). For larger covariate effects, however, the gain in power is dramatic, especially if a fair number of datapoints are available per participant."
  },
  {
    "objectID": "posts/2015-09-02-covariate-adjustment-mixed-logistic-regression/index.html#conclusion-and-outlook",
    "href": "posts/2015-09-02-covariate-adjustment-mixed-logistic-regression/index.html#conclusion-and-outlook",
    "title": "Covariate adjustment in logistic mixed models: Is it worth the effort?",
    "section": "Conclusion and outlook",
    "text": "Conclusion and outlook\nAdjusting for a between-subjects covariate in a between-subjects randomised experiment may be well worth it in terms of statistical power if the covariate is very strongly related to the outcome. For the kind of task I want to investigate, though, the relationship between the covariate and the outcome doesn’t seem to be large enough for covariate adjustment to have any noticeable effect on the results. Presumably, the model’s by-subject random intercepts do a sufficiently good job in accounting for interindividual differences in this case. In practical terms, these insights will be useful to me as not collecting the between-subjects variable should free up time elsewhere in the data collection sessions.\nLastly, some new questions that arose during this exploration:\n\nDoes accounting for a strong within-subjects covariate affect power in a between-subjects randomised experiment?\nWould by-item/by-participant variability in the covariate effects change these conclusions? Specifically, would accounting for the covariate effect using both a fixed and a random term improve power? In this dataset, the by-item variability in the between-subjects covariate was negligible, but this is probably different for other variables."
  },
  {
    "objectID": "posts/2015-09-02-covariate-adjustment-mixed-logistic-regression/index.html#r-code",
    "href": "posts/2015-09-02-covariate-adjustment-mixed-logistic-regression/index.html#r-code",
    "title": "Covariate adjustment in logistic mixed models: Is it worth the effort?",
    "section": "R code",
    "text": "R code\n\n# Read in data\n# Accompanying article: http://homeweb.unifr.ch/VanhoveJ/Pub/papers/Vanhove_CorrespondenceRules.pdf\ndat_oe &lt;- read.csv(\"http://homeweb.unifr.ch/VanhoveJ/Pub/Data/correspondences_shortened_oe.csv\")\n\n# Summarise by participant (needed for simulation)\nlibrary(tidyverse)\nperPart &lt;- dat_oe |&gt; \n  group_by(Subject) |&gt; \n  summarise(\n    c.EnglishScore = mean(c.EnglishScore),\n    c.WSTRight = mean(c.WSTRight)\n  )\n\n# Recode LearningCondition as numeric (-0.5, 0.5) (sum coding)\ndat_oe$LearningCondition &lt;- ifelse(dat_oe$LearningCondition == \"ij-ei\", -0.5, 0.5)\n\n# CorrectVowel is factor\ndat_oe$CorrectVowel &lt;- factor(dat_oe$CorrectVowel)\n\n# Fit model on real data\nlibrary(lme4)\nmod &lt;- glmer(CorrectVowel ~ LearningCondition + c.WSTRight +\n               (1 | Subject) + (1 + LearningCondition | Item), \n             data = dat_oe, family = binomial, control = glmerControl(optimizer=\"bobyqa\"))\n\n# Extract random effects\nthetas &lt;- getME(mod,\"theta\")\n\n# Extract fixed effects\nbetas &lt;- fixef(mod)\n\n# betas[1]: Intercept\n# betas[2]: LearningCondition (effect of condition)\n# betas[3]: c.WSTRight (effect of covariate)\n\n######################################\n## Function for simulating new data ##\n######################################\n\n# Function for simulating new data\nnewdata.fnc &lt;- function(         k = 80, # no. participants\n                                 m = 24, # no. items/participant\n                                 # these are the original estimates:\n                                 Intercept = -0.9332077, \n                                 eff.Condition =  0.9529923,\n                                 eff.Covariate = 0.1189162,\n                                 # multiplicator factor covariate scores\n                                 # (not relevant here)\n                                 covariate.multiple = 1) {\n  \n  betas[1] &lt;- Intercept\n  betas[2] &lt;- eff.Condition\n  betas[3] &lt;- eff.Covariate\n  \n  ### Generate new data set with k participants and m items\n  \n  # First generate k new participants,\n  # randomly assign them to the experimental/control conditions,\n  # and assign a covariate score to them.\n  # The covariate scores are drawn with replacement from the original study's\n  # covariate distribution and multiplied by the factor 'covariate.multiple'.\n  parts &lt;- data.frame(Subject = factor(1:k),\n                      LearningCondition = sample(c(rep(-0.5, k/2), # not really needed in this case\n                                                   rep(0.5, k/2))),\n                      c.WSTRight = sample(covariate.multiple*perPart$c.WSTRight, k, replace = TRUE))\n  \n  # Then generate m new items.\n  items &lt;- data.frame(Item = factor(1:m))\n  \n  # Fully cross participants and items.\n  newdat &lt;- expand.grid(Subject = factor(1:k),\n                        Item = factor(1:m))\n  newdat &lt;- merge(newdat, parts, by = \"Subject\")\n  \n  # Generate new data\n  newdat$New &lt;- factor(unlist(simulate(mod,\n                                       newdata = newdat,\n                                       allow.new.levels = TRUE,\n                                       newparams = list(theta = thetas, beta = betas))))\n  \n  ### Run models for model comparison WITHOUT covariate\n  mod1 &lt;- glmer(New ~  (1 | Subject) + (1 + LearningCondition | Item), \n                data = newdat, family = binomial, \n                control = glmerControl(optimizer=\"bobyqa\"))\n  mod2 &lt;- update(mod1, . ~ . + LearningCondition)\n  \n  ### Run models for model comparison with FIXED covariate\n  mod3 &lt;- glmer(New ~ c.WSTRight + (1 | Subject) + (1 + LearningCondition | Item), \n                data = newdat, family = binomial, \n                control = glmerControl(optimizer=\"bobyqa\"))\n  mod4 &lt;- update(mod3, . ~ . + LearningCondition)\n  \n  # Compare mod1 and mod2 and return p-value\n  pvalue.nocovar &lt;- anova(mod1, mod2)[2, 8]\n  \n  # Compare mod3 and mod4 and return p-value\n  pvalue.fixedcovar &lt;- anova(mod3, mod4)[2, 8]\n  \n  # Save estimate and se of mod2\n  est.nocovar &lt;- summary(mod2)$coef[2,1]\n  se.nocovar &lt;- summary(mod2)$coef[2,2]\n  \n  # Save estimate and se of mod4\n  est.fixedcovar &lt;- summary(mod4)$coef[3,1]\n  se.fixedcovar &lt;- summary(mod4)$coef[3,2]\n  \n  # Return p-values, estimates and standard errors\n  return(list(pvalue.nocovar, pvalue.fixedcovar,\n              est.nocovar, est.fixedcovar, \n              se.nocovar, se.fixedcovar, \n              eff.Condition,\n              eff.Covariate))\n}\n\n#################################################################################################\n## Function for running above simulation 100 times and return power (%p &lt; 0.05), average/sd ES ##\n#################################################################################################\n\npower.fnc &lt;- function(runs = 500, # number of simulation runs\n                      k = 60, \n                      m = 20,\n                      Intercept = -0.9332077,\n                      eff.Condition =  0.9529923,\n                      eff.Covariate = 0.1,\n                      covariate.multiple = 1) {\n  \n  # Run newdata.fnc a couple of times\n  sim &lt;- replicate(runs, \n                   newdata.fnc(k = k, \n                               m = m,\n                               Intercept = Intercept,\n                               eff.Condition = eff.Condition,\n                               eff.Covariate = eff.Covariate,\n                               covariate.multiple = covariate.multiple))\n  \n  # And compute power\n  power.nocovar &lt;- mean(unlist(sim[1, ]) &lt;= 0.05)\n  power.fixedcovar &lt;- mean(unlist(sim[2, ]) &lt;= 0.05)\n  \n  # Compute average effect\n  mean.est.nocovar &lt;- mean(unlist(sim[3,]))\n  mean.est.fixedcovar &lt;- mean(unlist(sim[4,]))\n  \n  # Standard deviation of effect (not reported)\n  sd.est.nocovar &lt;- sd(unlist(sim[5,]))\n  sd.est.fixedcovar &lt;- sd(unlist(sim[6,]))\n  \n  return(list(power.nocovar = power.nocovar,\n              power.fixedcovar = power.fixedcovar,\n              mean.est.nocovar = mean.est.nocovar,\n              mean.est.fixedcovar = mean.est.fixedcovar,\n              sd.est.nocovar = sd.est.nocovar,\n              sd.est.fixedcovar = sd.est.fixedcovar,\n              k = k,\n              m = m, \n              eff.Condition = eff.Condition,\n              eff.Covariate = eff.Covariate,\n              covariate.multiple = covariate.multiple))\n}\n\n# Run simulation\nparameter_combinations &lt;- expand.grid(m = c(5, 20),\n                                      eff.Covariate = c(0.1, 1.0))\nlibrary(parallel)\nresults &lt;- mcmapply(\n  power.fnc\n  , m = parameter_combinations$m\n  , eff.Covariate = parameter_combinations$eff.Covariate\n  , MoreArgs = list(k = 60, runs = 500)\n  , mc.cores = 4\n)\nresults &lt;- cbind(parameter_combinations, t(results))\nresults |&gt; \n  mutate(across(where(is.list), unlist)) |&gt; \n  write_csv(\"simulations_glmm.csv\")"
  },
  {
    "objectID": "posts/2015-09-02-covariate-adjustment-mixed-logistic-regression/index.html#software-versions",
    "href": "posts/2015-09-02-covariate-adjustment-mixed-logistic-regression/index.html#software-versions",
    "title": "Covariate adjustment in logistic mixed models: Is it worth the effort?",
    "section": "Software versions",
    "text": "Software versions\n\ndevtools::session_info()\n\n─ Session info ───────────────────────────────────────────────────────────────\n setting  value\n version  R version 4.3.1 (2023-06-16)\n os       Ubuntu 22.04.3 LTS\n system   x86_64, linux-gnu\n ui       X11\n language en_US\n collate  en_US.UTF-8\n ctype    en_US.UTF-8\n tz       Europe/Zurich\n date     2023-08-27\n pandoc   3.1.1 @ /usr/lib/rstudio/resources/app/bin/quarto/bin/tools/ (via rmarkdown)\n\n─ Packages ───────────────────────────────────────────────────────────────────\n package     * version date (UTC) lib source\n bit           4.0.5   2022-11-15 [1] CRAN (R 4.3.0)\n bit64         4.0.5   2020-08-30 [1] CRAN (R 4.3.0)\n cachem        1.0.6   2021-08-19 [2] CRAN (R 4.2.0)\n callr         3.7.3   2022-11-02 [1] CRAN (R 4.3.1)\n cli           3.6.1   2023-03-23 [1] CRAN (R 4.3.0)\n colorspace    2.1-0   2023-01-23 [1] CRAN (R 4.3.0)\n crayon        1.5.2   2022-09-29 [1] CRAN (R 4.3.1)\n devtools      2.4.5   2022-10-11 [1] CRAN (R 4.3.1)\n digest        0.6.29  2021-12-01 [2] CRAN (R 4.2.0)\n dplyr       * 1.1.2   2023-04-20 [1] CRAN (R 4.3.0)\n ellipsis      0.3.2   2021-04-29 [2] CRAN (R 4.2.0)\n evaluate      0.15    2022-02-18 [2] CRAN (R 4.2.0)\n fansi         1.0.4   2023-01-22 [1] CRAN (R 4.3.1)\n farver        2.1.1   2022-07-06 [1] CRAN (R 4.3.0)\n fastmap       1.1.0   2021-01-25 [2] CRAN (R 4.2.0)\n forcats     * 1.0.0   2023-01-29 [1] CRAN (R 4.3.0)\n fs            1.5.2   2021-12-08 [2] CRAN (R 4.2.0)\n generics      0.1.3   2022-07-05 [1] CRAN (R 4.3.0)\n ggplot2     * 3.4.2   2023-04-03 [1] CRAN (R 4.3.0)\n glue          1.6.2   2022-02-24 [2] CRAN (R 4.2.0)\n gtable        0.3.3   2023-03-21 [1] CRAN (R 4.3.0)\n hms           1.1.3   2023-03-21 [1] CRAN (R 4.3.0)\n htmltools     0.5.5   2023-03-23 [1] CRAN (R 4.3.0)\n htmlwidgets   1.6.2   2023-03-17 [1] CRAN (R 4.3.1)\n httpuv        1.6.11  2023-05-11 [1] CRAN (R 4.3.1)\n jsonlite      1.8.7   2023-06-29 [1] CRAN (R 4.3.1)\n knitr         1.39    2022-04-26 [2] CRAN (R 4.2.0)\n labeling      0.4.2   2020-10-20 [1] CRAN (R 4.3.0)\n later         1.3.1   2023-05-02 [1] CRAN (R 4.3.1)\n lifecycle     1.0.3   2022-10-07 [1] CRAN (R 4.3.0)\n lubridate   * 1.9.2   2023-02-10 [1] CRAN (R 4.3.0)\n magrittr      2.0.3   2022-03-30 [1] CRAN (R 4.3.0)\n memoise       2.0.1   2021-11-26 [2] CRAN (R 4.2.0)\n mime          0.10    2021-02-13 [2] CRAN (R 4.0.2)\n miniUI        0.1.1.1 2018-05-18 [1] CRAN (R 4.3.1)\n munsell       0.5.0   2018-06-12 [1] CRAN (R 4.3.0)\n pillar        1.9.0   2023-03-22 [1] CRAN (R 4.3.0)\n pkgbuild      1.4.2   2023-06-26 [1] CRAN (R 4.3.1)\n pkgconfig     2.0.3   2019-09-22 [2] CRAN (R 4.2.0)\n pkgload       1.3.2.1 2023-07-08 [1] CRAN (R 4.3.1)\n prettyunits   1.1.1   2020-01-24 [2] CRAN (R 4.2.0)\n processx      3.8.2   2023-06-30 [1] CRAN (R 4.3.1)\n profvis       0.3.8   2023-05-02 [1] CRAN (R 4.3.1)\n promises      1.2.0.1 2021-02-11 [1] CRAN (R 4.3.1)\n ps            1.7.5   2023-04-18 [1] CRAN (R 4.3.1)\n purrr       * 1.0.1   2023-01-10 [1] CRAN (R 4.3.0)\n R6            2.5.1   2021-08-19 [2] CRAN (R 4.2.0)\n Rcpp          1.0.11  2023-07-06 [1] CRAN (R 4.3.1)\n readr       * 2.1.4   2023-02-10 [1] CRAN (R 4.3.0)\n remotes       2.4.2   2021-11-30 [2] CRAN (R 4.2.0)\n rlang         1.1.1   2023-04-28 [1] CRAN (R 4.3.0)\n rmarkdown     2.21    2023-03-26 [1] CRAN (R 4.3.0)\n rstudioapi    0.14    2022-08-22 [1] CRAN (R 4.3.0)\n scales        1.2.1   2022-08-20 [1] CRAN (R 4.3.0)\n sessioninfo   1.2.2   2021-12-06 [2] CRAN (R 4.2.0)\n shiny         1.7.4.1 2023-07-06 [1] CRAN (R 4.3.1)\n stringi       1.7.12  2023-01-11 [1] CRAN (R 4.3.1)\n stringr     * 1.5.0   2022-12-02 [1] CRAN (R 4.3.0)\n tibble      * 3.2.1   2023-03-20 [1] CRAN (R 4.3.0)\n tidyr       * 1.3.0   2023-01-24 [1] CRAN (R 4.3.0)\n tidyselect    1.2.0   2022-10-10 [1] CRAN (R 4.3.0)\n tidyverse   * 2.0.0   2023-02-22 [1] CRAN (R 4.3.1)\n timechange    0.2.0   2023-01-11 [1] CRAN (R 4.3.0)\n tzdb          0.4.0   2023-05-12 [1] CRAN (R 4.3.0)\n urlchecker    1.0.1   2021-11-30 [1] CRAN (R 4.3.1)\n usethis       2.2.2   2023-07-06 [1] CRAN (R 4.3.1)\n utf8          1.2.3   2023-01-31 [1] CRAN (R 4.3.1)\n vctrs         0.6.3   2023-06-14 [1] CRAN (R 4.3.0)\n vroom         1.6.3   2023-04-28 [1] CRAN (R 4.3.0)\n withr         2.5.0   2022-03-03 [2] CRAN (R 4.2.0)\n xfun          0.39    2023-04-20 [1] CRAN (R 4.3.0)\n xtable        1.8-4   2019-04-21 [1] CRAN (R 4.3.1)\n yaml          2.3.5   2022-02-21 [2] CRAN (R 4.2.0)\n\n [1] /home/jan/R/x86_64-pc-linux-gnu-library/4.3\n [2] /usr/local/lib/R/site-library\n [3] /usr/lib/R/site-library\n [4] /usr/lib/R/library\n\n──────────────────────────────────────────────────────────────────────────────"
  },
  {
    "objectID": "posts/2019-09-11-collinearity/index.html",
    "href": "posts/2019-09-11-collinearity/index.html",
    "title": "Collinearity isn’t a disease that needs curing",
    "section": "",
    "text": "Every now and again, some worried student or collaborator asks me whether they’re “allowed” to fit a regression model in which some of the predictors are fairly strongly correlated with one another. Happily, most Swiss cantons have a laissez-faire policy with regard to fitting models with correlated predictors, so the answer to this question is “yes”. Such an answer doesn’t always set the student or collaborator at ease, so below you find my more elaborate answer."
  },
  {
    "objectID": "posts/2019-09-11-collinearity/index.html#whats-collinearity",
    "href": "posts/2019-09-11-collinearity/index.html#whats-collinearity",
    "title": "Collinearity isn’t a disease that needs curing",
    "section": "What’s collinearity?",
    "text": "What’s collinearity?\nCollinearity (or ‘multicollinearity’) means that a substantial amount of information contained in some of the predictors included in a statistical model can be pieced together as a linear function of some of the other predictors in the model. That’s a mouthful, so let’s look at some examples.\nThe easiest case is when you have a multiple regression model with two predictors. These predictors can be continuous or categorical; in what follows, I’ll stick to continuous predictors. I’ve created four datasets with two continuous predictors to illustrate collinearity and its consequences. You find the R code reproduce all analyses at the bottom of this page.\nThe outcome in each dataset was created using the following equation:\n\\[\\textrm{outcome}_i = 0.4\\times\\textrm{predictor1}_i + 1.9\\times\\textrm{predictor2}_i + \\varepsilon_i\\]\nwhere the residuals (\\(\\varepsilon_i\\)) were drawn from a normal distribution with a standard deviation of 3.5.\n\\[\\varepsilon_i \\sim N(0, 3.5^2)\\]\nThe four datasets are presented in Figures 1 through 4. Beginning analysts may be surprised to see that I consider a situation where two predictors are correlated at r = 0.50 to be a case of weak rather than moderate or strong collinearity. But in fact, the consequences of having two predictors correlate at r = 0.50 (rather than at r = 0.00) are negligible. Figure 4 highlights the linear part in collinearity: while the two predictors in this figure are perfectly related, there is no linear relationship between them whatsoever. Datasets such as the one in Figure 4 are not affected by any of the statistical consequences of collinearity, but they’re useful to illustrate a point I want to make below.\n\n\n\n\n\nFigure 1. A dataset with a strong degree of collinearity between the two predictors (r = 0.98).\n\n\n\n\n\n\n\n\n\nFigure 2. A dataset with a weak degree of collinearity between the two predictors (r = 0.50).\n\n\n\n\n\n\n\n\n\nFigure 3. A dataset in which the two predictors are entirely orthogonal and unrelated (r = 0.00).\n\n\n\n\n\n\n\n\n\nFigure 4. A dataset with two orthogonal (r = 0.00) but perfectly related predictors: predictor1 is a sinusoid transformation of predictor2. In other words, you can predict predictor1 perfectly if you know predictor2.\n\n\n\n\nIf you fit multiple regressions on these four datasets, you obtain the estimates that are shown in Figure 5 along with their 90% confidence intervals.\n\n\n\n\n\nFigure 5. Estimated coefficients and their 90% confidence intervals for the models fitted to the four datasets."
  },
  {
    "objectID": "posts/2019-09-11-collinearity/index.html#whats-the-consequence-of-collinearity",
    "href": "posts/2019-09-11-collinearity/index.html#whats-the-consequence-of-collinearity",
    "title": "Collinearity isn’t a disease that needs curing",
    "section": "What’s the consequence of collinearity?",
    "text": "What’s the consequence of collinearity?\nIn essence, collinearity has one statistical consequence: Estimates of regression coefficients that are affected by collinearity vary more from sample to sample than estimates of regression coefficients that aren’t affected by collinearity; see Figure 6 below. As Figure 6 also illustrates, collinearity doesn’t bias the coefficient estimates: On average, the estimated coefficients equal the parameter’s true value both when there is no and very strong collinearity. It’s just that the estimates vary much more around this average when there is strong collinearity.\n\n\n\n\n\nFigure 6. I simulated samples of 50 observations from a distribution in which the two predictors were completely orthogonal (r = 0.00) and from a distribution in which they were highly correlated (r = 0.98). In all cases, both predictors were independently related to the outcome: \\(\\textrm{outcome}_i = 0.4\\times\\textrm{predictor1}_i + 1.9\\times\\textrm{predictor2}_i + \\varepsilon_i\\). On each simulated sample, I ran a multiple regression model, and I then extracted the estimated model coefficients. This figure shows the estimated coefficients for the first predictor. While the estimates vary more when the predictors are strongly correlated than when they’re not, the estimates are unbiased in either case: On average, they equal the true population parameter (dashed red line).\n\n\n\n\nCrucially, and happily, this greater variability is reflected in the standard errors and confidence intervals around these estimates: The standard errors are automatically wider when the estimated coefficients are affected by collinearity and the confidence intervals retain their nominal coverage rates (i.e., 95% of the 95% confidence intervals will contain the true parameter value). So the statistical consequence of collinearity is automatically taken care of in the model’s output and requires no additional computations on the part of the analyst. This is illustrated in Figure 5 above: The confidence intervals for the two predictors’ estimated coefficients are considerably wider when these are affected by strong collinearity.\nThe greater variability in the estimates, the larger standard errors, and the wider confidence intervals all reflect a relative lack of information in the sample:\n\nCollinearity is at base a problem about information. If two factors are highly correlated, researchers do not have ready access to much information about conditions of the dependent variables when only one of the factors actually varies and the other does not. If we are faced with this problem, there are really only three fundamental solutions: (1) find or create (e.g. via an experimental design) circumstances where there is reduced collinearity; (2) get more data (i.e. increase the N size), so that there is a greater quantity of information about rare instances where there is some divergence between the collinear variables; or (3) add a variable or variables to the model, with some degree of independence from the other independent variables, that explain(s) more of the variance of Y, so that there is more information about that which is being modeled. (York 2012:1384)"
  },
  {
    "objectID": "posts/2019-09-11-collinearity/index.html#but-is-collinearity-a-problem",
    "href": "posts/2019-09-11-collinearity/index.html#but-is-collinearity-a-problem",
    "title": "Collinearity isn’t a disease that needs curing",
    "section": "But is collinearity a problem?",
    "text": "But is collinearity a problem?\nFor the most part, I think that collinearity is a problem for statistical analyses in the same way that Belgium’s lack of mountains is detrimental to the country’s chances of hosting the Winter Olympics: It’s an unfortunate fact of life, but not something that has to be solved. Running another study, obtaining more data or reducing the error variance are all sensible suggestions, but if you have to work with the data you have, the model output will appropriately reflect the degree of uncertainty in the estimates.\nSo I don’t consider collinearity a problem. What is the case, however, is that collinearity highlights problems with the way many people think about statistical models and inferential statistics. Let’s look at a couple of these.\n\n‘Collinearity decreases statistical power.’\nYou may have heared that collinearity decreases statistical power, i.e., the chances of obtaining statistically significant coefficient estimates if their parameter value isn’t zero. This is true, but the lower statistical power is a direct result of the larger standard errors, which appropriately reflect the greater sampling variability of the estimates. This is only a problem if you interpret “lack of statistical significance” as “zero effect”. But then the problem doesn’t lie with collinearity but with the false belief that non-significant estimates correspond to zero effects. It’s just that this false belief is even more likely than usual to lead you astray when your predictors are collinear. If instead of focusing soly on the p-value, you take into account both the estimate and its uncertainty interval, there is no problem.\nIncidentally, I think it’s somewhat misleading to say that collinearity decreases statistical power or increases standard errors. It’s true that relative to situations in which there is less or no collinearity and all other things are equal, the standard errors are larger and statistical power is lower when there is stronger collinearity. But I don’t see how you can reduce collinearity but keep all other things equal outside of a computer simulation. In the real world, collinearity isn’t an unfolding process that can be nipped in the bud without bringing about other changes in the research design, the sampling procedure or the statistical model and its interpretation.\n\n\n‘None of the predictors is significant but the overall model fit is.’\nWith collinear predictors, you may end up with a statistical model for which the \\(F\\)-test of overall model fit is highly significant but that doesn’t contain a single significant predictor. This is illustrated in Table 1: The overall model fit for the dataset with strong collinearity (see Figure 1) is highly significant, but as shown in Figure 5, neither predictor has an estimated coefficient that’s significantly different from zero.\n\n\n\nTable 1. R²- and p-values for the overall model fit for the multiple regression models on the four datasets. Even though neither predictor has a significant estimated coefficient in the 'strong' dataset (shown in Figure 1), the overall fit is highly significant.\n\n\nDataset\nR²\np-value of overall fit\n\n\n\n\nstrong\n0.255\n0.001\n\n\nweak\n0.220\n0.003\n\n\nnone\n0.201\n0.005\n\n\nnonlinear\n0.293\n0.000\n\n\n\n\n\n\n\nIf this seems paradoxical, you need to keep in mind that the tests for the individual coefficient estimates and the test for the overall model fit seek to answer different questions, so there’s no contradiction if they yield different answers. To elaborate, the test for the overall model fit asks if all predictors jointly earn their keep in the model; the tests for the individual coefficients ask whether these are different from zero. With collinear predictors, it’s possible that the answer to the first question is “yes” and the answer to the second is “don’t know”. The reason for this is that with collinear predictors, either predictor could act as the stand-in of the other so that, as far as the model is concerned, either coefficient could well be zero, provided the other isn’t. But due to the lack of information in the collinear sample, it’s not sure which, if any, is zero.\nSo again, there is no real problem: The tests answer different questions, so they may yield different answers. It’s just that when you have collinear predictors, this tends to happen more often than when you don’t.\n\n\n‘Collinearity means that you can’t take model coefficients at face value.’\nIt’s sometimes said that collinearity makes it more difficult to interpret estimated model coefficients. Crucially, the appropriate interpretation of an estimated regression coefficient is always the same, regardless of the degree of collinearity: According to the model, what would the difference in the mean outcome be if you took two large groups of observations that differed by one unit in the focal predictor but whose other predictor values are the same. The interpretational difficulties that become obvious when there is collinearity aren’t caused by the collinearity itself but with mental shortcuts that people take when interpreting regression models.\nFor instance, you may obtain a coefficient estimate in a multiple regression model that you interpret to mean that older children perform more poorly on an L2 writing task than do younger children. (For the non-linguists: L2 = second or foreign language.) This may be counterintuitive, and you may indeed find that, in fact, in your sample older children actually outperform younger ones. You could chalk this one up to collinearity, but the problem really is related to a faulty mental shortcut you took when interpreting your model: You forgot to take into account the “but whose other predictor values are the same” clause. If your model also included measures of the children’s previous exposure to the L2, their motivation to learn the L2, and their L2 vocabulary knowledge, then what the estimated coefficient means is emphatically not that, according to the model, older children perform on average more poorly on a writing task than younger children. It’s that, according to the model, older children will perform more poorly than younger children with the same values on the previous exposure, motivation, and vocabulary knowledge measures. This is pretty much the whole point of fitting multiple regression models. But if, on reflection, this isn’t what you’re actually interested in, then you should fit a different model. For instance, if you’re interested in the overall difference between younger and older children regardless of their previous exposure, motivation and vocabulary knowledge, don’t include these variables as predictors.\nAnother interpretational difficulty emerges if you recast the interpretation of the estimate as follows: According to the model, what would the difference in the mean outcome be if you increased the focal predictor by one unit but keep the other predictor values constant. The difference between this interpretation and the one that I offered earlier is that we’ve moved from a purely descriptive one both a causal and an interventionist one (viz., the idea that one could change some predictor values while keeping the others constant and that this would have an effect on the mean outcome). In the face of strong collinearity, it becomes clear that this interventionist interpretation may be wishful thinking: It may be impossible to change values in one predictor without also changing values in the predictors that are collinear with it. But the problem here again isn’t the collinearity but the mental shortcut in the interpretation.\nIn fact, you can run into the same difficulties when you apply the interventionist mental shortcut in the absence of collinearity: In the dataset shown in Figure 4, it’s impossible to change the second predictor without also changing the first since the first was defined as a function of the second. Yet the two variables aren’t collinear. Another example would be if you wanted to model quality ratings of texts in terms of the number of words in the text (“tokens”), the number of unique words in the text (“types”), and the type/token ratio. The model will output estimated coefficients for the three predictors, but as an analyst you should realise that it’s impossible to find two texts differing in the number of tokens but having both the same number of types and the same type/token ratio: If you change the number of tokens and keep constant the number of types, the type/token ratio changes, too.\nA final mental shortcut that is laid bare in the presence of collinearity is conflating a measured variable with the theoretical construct that this variable is assumed to capture. The literature on lexical diversity offers a case in point. The type/token ratio (TTR) discussed in the previous paragraph is one of several possible measures of a text’s lexical diversity. If you take a collection of texts, you’re pretty much guaranteed to find that their type/token ratios are negatively correlated with their lengths. That is, longer texts tend to have lower TTR values. This correlation is known as the “text-length problem” and has led researchers to abandon the use of TTR, even though the relationship isn’t that strong (see Figure 7 for an example).\n\n\n\n\n\nFigure 7. The type/token ratio tends to be negatively correlated with text length (here: log-10 number of tokens). This is known as the text length problem in research on lexical diversity. But the problem isn’t that the type/token ratio is collinear with text length; it’s that the type/token ratio also measures something it isn’t supposed to measure and consequently is a poor measure of what it is supposed to measure, viz., lexical diversity. The diversity rating shown are based on human judgements of the texts’ lexical diversity. (Data from the French corpus published by Vanhove et al. (2019).)\n\n\n\n\nHowever, the reason why researchers have abandoned the use of TTR is not collinearity per se. Rather, it is that TTR is a poor measure of what it’s supposed to capture, viz., the lexical diversity displayed in a text. Specifically, because of the statistical properties of language, the TTR is pretty much bound to conflate a text’s lexical diversity with its length. The negative correlation between TTR and text length isn’t much of a problem for statistical modelling; it’s a symptom of a more fundamental problem: A measure of lexical diversity shouldn’t as a matter of fact be related to text length. The fact that TTR does shows that it’s a poor measure of lexical diversity.\nTo be clear: It’s not necessarily a problem that measures of lexical diversity correlate with text length since it’s possible that the lexical diversity of longer texts is greater than that of shorter texts or vice versa. The problem with TTR is that it necessarily correlates with text length, even if the the texts’ lexical diversity can be assumed to be constant. For instance, if you take increasingly longer snippets of texts from the same book, you’ll find that the TTR goes down, but that doesn’t mean that the writer’s vocabulary skills went down in the process of writing the book. More generally, if your predictors correlate strongly when they’re not supposed to, the problem you have needn’t be collinearity but may instead be that in trying to capture one construct, you’ve also captured the one represented by the other predictor.\nIn sum, the interpretational challenges encountered when predictors are collinear aren’t caused by the collinearity itself but by mental shortcuts that may lead researchers astray even in the absence of collinearity."
  },
  {
    "objectID": "posts/2019-09-11-collinearity/index.html#collinearity-doesnt-require-a-statistical-solution",
    "href": "posts/2019-09-11-collinearity/index.html#collinearity-doesnt-require-a-statistical-solution",
    "title": "Collinearity isn’t a disease that needs curing",
    "section": "Collinearity doesn’t require a statistical solution",
    "text": "Collinearity doesn’t require a statistical solution\n\nStatistical “solutions,” such as residualization that are often used to address collinearity problems do not, in fact, address the fundamental issue, a limited quantity of information, but rather serve to obfuscate it. It is perhaps obvious to point out, but nonetheless important in light of the widespread confusion on the matter, that no statistical procedure can actually produce more information than exists in the data. (York 2012:1384, my emphasis)\n\nQuite right. Apart from the non-solution that York (2012) mentioned (residualisation), other common statistical “solutions” to collinearity include dropping predictors, averaging collinear predictors, and resorting to different estimation methods such as ridge regression. Since this blog post is long enough as it is, I’ll comment on these only briefly. Further suggested articles are O’Brien (2007) and Wurm and Fisicaro (2014).\n\nDropping predictors: I don’t mind this “solution”, but the problem it solves isn’t collinearity but rather that the previous model was misspecified. This is obviously only a solution to the extent that the new model is capable of answering the researchers’ question since, crucially, estimated coefficients from different models don’t have the same meaning (see the previous section). Something to be particularly aware of is that by dropping one of the collinear predictors, you bias the estimates of the other predictors as shown in Figure 8.\n\n\n\n\n\n\nFigure 8. Dropping a collinear predictor biases the estimate for the predictor retained as well as its meaning.\n\n\n\n\n\nAveraging predictors: Again, I don’t mind this solution per se, but please be aware that your model now answers a different question.\nRidge regression and other forms of deliberately biased estimation: Ridge regression and its cousins try to reduce the sample-to-sample variability in the regression estimates by deliberately biasing them. The result is, quite naturally, that you end up with biased estimates: The estimates for the weaker predictor will tend to be biased upwards (see Figure 9), and those for the stronger predictor will be biased downwards. Moreover, the usefulness of standard errors and confidence intervals for ridge regression and the like is contested, see Goeman et al. (2018, p. 18).\n\n\n\n\n\n\nFigure 9. Ridge regression is a form of biased estimation, so naturally the estimates it yields are biased."
  },
  {
    "objectID": "posts/2019-09-11-collinearity/index.html#tldr",
    "href": "posts/2019-09-11-collinearity/index.html#tldr",
    "title": "Collinearity isn’t a disease that needs curing",
    "section": "tl;dr",
    "text": "tl;dr\n\nCollinearity is a form of lack of information that is appropriately reflected in the output of your statistical model.\nWhen collinearity is associated with interpretational difficulties, these difficulties aren’t caused by the collinearity itself. Rather, they reveal that the model was poorly specified (in that it answers a question different to the one of interest), that the analyst overly focuses on significance rather than estimates and the uncertainty about them or that the analyst took a mental shortcut in interpreting the model that could’ve also led them astray in the absence of collinearity.\nIf you do decide to “deal with” collinearity, make sure you can still answer the question of interest."
  },
  {
    "objectID": "posts/2019-09-11-collinearity/index.html#references",
    "href": "posts/2019-09-11-collinearity/index.html#references",
    "title": "Collinearity isn’t a disease that needs curing",
    "section": "References",
    "text": "References\nGoeman, Jelle, Rosa Meijer and Nimisha Chaturvedi. 2018. L1 and L2 penalized regression models.\nO’Brien, Robert M. 2007. A caution regarding rules of thumb of variance inflation factors. Quality & Quantity 41. 673-690.\nVanhove, Jan, Audrey Bonvin, Amelia Lambelet and Raphael Berthele. 2019. Predicting perceptions of the lexical richness of short French, German, and Portuguese texts using text-based indices. Journal of Writing Research 10(3). 499-525.\nWurm, Lee H. and Sebastiano A. Fisicaro. 2014. What residualizing predictors in regression analyses does (and what it does not do). Journal of Memory and Language 72. 37-48.\nYork, Richard. 2012. Residualization is not the answer: Rethinking how to address multicollinearity. Social Science Research 41. 1379-1386."
  },
  {
    "objectID": "posts/2019-09-11-collinearity/index.html#r-code",
    "href": "posts/2019-09-11-collinearity/index.html#r-code",
    "title": "Collinearity isn’t a disease that needs curing",
    "section": "R code",
    "text": "R code\nThis code still ran correctly on August 6, 2023.\n\n# Packages\nlibrary(tidyverse)\nlibrary(broom)\n\n# Read in the four generated datasets\nstrong &lt;- read.csv(\"https://janhove.github.io/datasets/strong_collinearity.csv\")\nweak &lt;- read.csv(\"https://janhove.github.io/datasets/weak_collinearity.csv\")\nnone &lt;- read.csv(\"https://janhove.github.io/datasets/no_collinearity.csv\")\nnonlinear &lt;- read.csv(\"https://janhove.github.io/datasets/nonlinearity.csv\")\n\n# Load the custom function for drawing scatterplot matrices, \n# then drew Figures 1-4\nsource(\"https://janhove.github.io/RCode/scatterplot_matrix.R\")\nscatterplot_matrix(strong[, c(3, 1, 2)])\nscatterplot_matrix(weak[, c(3, 1, 2)])\nscatterplot_matrix(none[, c(3, 1, 2)])\nscatterplot_matrix(nonlinear[, c(3, 1, 2)])\n\n# Fit multiple regression models\nstrong.lm &lt;- lm(outcome ~ predictor1 + predictor2, data = strong)\nweak.lm &lt;- lm(outcome ~ predictor1 + predictor2, data = weak)\nnone.lm &lt;- lm(outcome ~ predictor1 + predictor2, data = none)\nnonlinear.lm &lt;- lm(outcome ~ predictor1 + predictor2, data = nonlinear)\n\n# Extract estimates + 90% CIs\nstrong_out &lt;- tidy(strong.lm, conf.int = TRUE, conf.level = 0.90) |&gt; \n  mutate(dataset = \"strong\")\nweak_out &lt;- tidy(weak.lm, conf.int = TRUE, conf.level = 0.90) |&gt; \n  mutate(dataset = \"weak\")\nnone_out &lt;- tidy(none.lm, conf.int = TRUE, conf.level = 0.90) |&gt; \n  mutate(dataset = \"none\")\nnonlinear_out &lt;- tidy(nonlinear.lm, conf.int = TRUE, conf.level = 0.90) |&gt; \n  mutate(dataset = \"nonlinear\")\noutputs &lt;- bind_rows(strong_out, weak_out, none_out, nonlinear_out)\n\n# Draw Figure 5\ndummy &lt;- data.frame(term = unique(outputs$term), prm = c(0, 0.4, 1.9))\noutputs |&gt; \n  ggplot(aes(x = factor(dataset, levels = c(\"nonlinear\", \"none\", \n                                            \"weak\", \"strong\")),\n             y = estimate,\n             ymin = conf.low,\n             ymax = conf.high)) +\n  geom_pointrange() +\n  facet_wrap(~ term) +\n  geom_hline(data = dummy, aes(yintercept = prm),\n             linetype = \"dashed\") +\n  ylab(\"estimated coefficient with 90% confidence interval\") +\n  xlab(\"dataset\") +\n  coord_flip()\n\n# Function for simulating effect of collinearity on estimates\ncollinearity &lt;- function(n_sim = 1000, n_sample = 50,\n                         rho = 0.90,\n                         coefs = c(0.4, 1.9),\n                         sd_error = 3.5) {\n  # This function generates two correlated\n  # predictors and an outcome. It then\n  # runs regression models (including ridge regression) \n  # on these variables and outputs the estimated\n  # regression coefficients for the predictors.\n  # It does this a large number of times (n_sim).\n  \n  # Package for LASSO/ridge regression\n  require(\"glmnet\")\n\n  estimates &lt;- matrix(ncol = 8, nrow = n_sim)\n\n  for (i in 1:n_sim) {\n    # Generate correlated predictors\n    predictors &lt;- MASS::mvrnorm(\n      n = n_sample,\n      mu = c(0, 0),\n      Sigma = rbind(\n        c(1, rho),\n        c(rho, 1)\n      )\n    )\n\n    # Generate outcome\n    outcome &lt;- as.vector(coefs %*% t(predictors) + rnorm(n_sample, sd = sd_error))\n\n    # Run multiple regression model\n    multiple_regression &lt;- lm(outcome ~ predictors[, 1] + predictors[, 2])\n    \n    # Run single regression models\n    simple_first &lt;- lm(outcome ~ predictors[, 1])\n    simple_second &lt;- lm(outcome ~ predictors[, 2])\n    \n    # Ridge regression\n    lambda_seq &lt;- 10^seq(2, -2, by = -0.1)\n    cv_output &lt;- cv.glmnet(predictors, outcome, nfolds = 10,\n                           alpha = 0, lambda = lambda_seq)\n    best_lambda &lt;- cv_output$lambda.min\n    ridge_model &lt;- glmnet(predictors, outcome, alpha = 0,\n                          lambda = best_lambda)\n\n    # Save regression coefficients\n    estimated_coefficients &lt;- c(\n      coef(multiple_regression)[2:3],\n      summary(multiple_regression)$coefficients[2:3, 2],\n      coef(simple_first)[2],\n      coef(simple_second)[2],\n      coef(ridge_model)[2:3]\n      )\n                                \n    estimates[i, ] &lt;- estimated_coefficients\n  }\n\n  results &lt;- data.frame(\n    multiple_est_pred1 = estimates[, 1],\n    multiple_est_pred2 = estimates[, 2],\n    multiple_se_pred1 = estimates[, 3],\n    multiple_se_pred2 = estimates[, 4],\n    simple_est_pred1 = estimates[, 5],\n    simple_est_pred2 = estimates[, 6],\n    ridge_est_pred1 = estimates[, 7],\n    ridge_est_pred2 = estimates[, 8]\n  )\n  results\n}\n\n# Simulate effects of strong collinearity\nstrong_coll &lt;- collinearity(rho = 0.98)\n\n# Simulate effect of perfect orthogonality (zero collinearity)\nno_coll &lt;- collinearity(rho = 0)\n\n# Combine\nstrong_coll$Collinearity &lt;- \"strong collinearity\\n(r = 0.98)\"\nno_coll$Collinearity &lt;- \"no collinearity\\n(r = 0.00)\"\nall_data &lt;- bind_rows(strong_coll, no_coll)\n\n# Figure 6\nggplot(all_data,\n       aes(x = multiple_est_pred1,\n           y = after_stat(density))) +\n  geom_histogram(bins = 50, colour = \"black\", fill = \"grey80\") +\n  facet_wrap(~ Collinearity) +\n  geom_vline(xintercept = 0.4, linetype = \"dashed\", colour = \"red\") +\n  xlab(\"estimated regression coefficient for first predictor\\nin multiple regression models\")\n\n# Table 1\nmap_dfr(list(strong.lm, weak.lm, none.lm, nonlinear.lm), glance) |&gt; \n  mutate(Dataset = c(\"strong\", \"weak\", \"none\", \"nonlinear\")) |&gt; \n  select(Dataset, `R²` = r.squared, `p-value of overall fit` = p.value) |&gt; \n  knitr::kable(\"html\") |&gt; \n  kableExtra::kable_styling(full_width = FALSE)\n\n# Figure 7\nlexdiv &lt;- read_csv(\"https://janhove.github.io/datasets/LexicalDiversityFrench.csv\")\nratings &lt;- read_csv(\"https://janhove.github.io/datasets/meanRatingPerText_French.csv\")\nratings$Text &lt;- substr(ratings$Text, 15, nchar(ratings$Text))\nd &lt;- left_join(ratings, lexdiv, by = c(\"Text\" = \"textName\"))\nscatterplot_matrix(d |&gt; select(meanRating, TTR, nTokens) |&gt; \n                  mutate(sqrt_nTokens = log10(nTokens)) |&gt; \n                  select(-nTokens),\n                labels = c(\"mean diversity rating\",\n                           \"type/token ratio\",\n                           \"log10 tokens\"))\n\n# Figure 8\nggplot(all_data,\n       aes(x = simple_est_pred1,\n           y = after_stat(density))) +\n  geom_histogram(bins = 50, colour = \"black\", fill = \"grey80\") +\n  facet_wrap(~ Collinearity) +\n  geom_vline(xintercept = 0.4, linetype = \"dashed\", col = \"red\") +\n  xlab(\"estimated regression coefficient for first predictor\\nin simple regression models\")\n\n# Figure 9\nggplot(all_data,\n       aes(x = ridge_est_pred1,\n           y = after_stat(density))) +\n  geom_histogram(bins = 50, colour = \"black\", fill = \"grey80\") +\n  facet_wrap(~ Collinearity) +\n  geom_vline(xintercept = 0.4, linetype = \"dashed\", col = \"red\") +\n  xlab(\"estimated regression coefficient for first predictor\\nin ridge regression models\")"
  },
  {
    "objectID": "posts/2019-09-11-collinearity/index.html#software-versions",
    "href": "posts/2019-09-11-collinearity/index.html#software-versions",
    "title": "Collinearity isn’t a disease that needs curing",
    "section": "Software versions",
    "text": "Software versions\nPlease note that I reran the code on this page on August 6, 2023.\n\ndevtools::session_info()\n\n─ Session info ───────────────────────────────────────────────────────────────\n setting  value\n version  R version 4.3.1 (2023-06-16)\n os       Ubuntu 22.04.2 LTS\n system   x86_64, linux-gnu\n ui       X11\n language en_US\n collate  en_US.UTF-8\n ctype    en_US.UTF-8\n tz       Europe/Zurich\n date     2023-08-06\n pandoc   3.1.1 @ /usr/lib/rstudio/resources/app/bin/quarto/bin/tools/ (via rmarkdown)\n\n─ Packages ───────────────────────────────────────────────────────────────────\n package     * version date (UTC) lib source\n backports     1.4.1   2021-12-13 [1] CRAN (R 4.3.0)\n bit           4.0.5   2022-11-15 [1] CRAN (R 4.3.0)\n bit64         4.0.5   2020-08-30 [1] CRAN (R 4.3.0)\n broom       * 1.0.5   2023-06-09 [1] CRAN (R 4.3.0)\n cachem        1.0.6   2021-08-19 [2] CRAN (R 4.2.0)\n callr         3.7.3   2022-11-02 [1] CRAN (R 4.3.1)\n cli           3.6.1   2023-03-23 [1] CRAN (R 4.3.0)\n colorspace    2.1-0   2023-01-23 [1] CRAN (R 4.3.0)\n crayon        1.5.2   2022-09-29 [1] CRAN (R 4.3.1)\n devtools      2.4.5   2022-10-11 [1] CRAN (R 4.3.1)\n digest        0.6.29  2021-12-01 [2] CRAN (R 4.2.0)\n dplyr       * 1.1.2   2023-04-20 [1] CRAN (R 4.3.0)\n ellipsis      0.3.2   2021-04-29 [2] CRAN (R 4.2.0)\n evaluate      0.15    2022-02-18 [2] CRAN (R 4.2.0)\n fansi         1.0.4   2023-01-22 [1] CRAN (R 4.3.1)\n farver        2.1.1   2022-07-06 [1] CRAN (R 4.3.0)\n fastmap       1.1.0   2021-01-25 [2] CRAN (R 4.2.0)\n forcats     * 1.0.0   2023-01-29 [1] CRAN (R 4.3.0)\n fs            1.5.2   2021-12-08 [2] CRAN (R 4.2.0)\n generics      0.1.3   2022-07-05 [1] CRAN (R 4.3.0)\n ggplot2     * 3.4.2   2023-04-03 [1] CRAN (R 4.3.0)\n glue          1.6.2   2022-02-24 [2] CRAN (R 4.2.0)\n gtable        0.3.3   2023-03-21 [1] CRAN (R 4.3.0)\n highr         0.9     2021-04-16 [2] CRAN (R 4.2.0)\n hms           1.1.3   2023-03-21 [1] CRAN (R 4.3.0)\n htmltools     0.5.5   2023-03-23 [1] CRAN (R 4.3.0)\n htmlwidgets   1.6.2   2023-03-17 [1] CRAN (R 4.3.1)\n httpuv        1.6.11  2023-05-11 [1] CRAN (R 4.3.1)\n httr          1.4.6   2023-05-08 [1] CRAN (R 4.3.0)\n jsonlite      1.8.7   2023-06-29 [1] CRAN (R 4.3.1)\n kableExtra    1.3.4   2021-02-20 [1] CRAN (R 4.3.1)\n knitr         1.39    2022-04-26 [2] CRAN (R 4.2.0)\n labeling      0.4.2   2020-10-20 [1] CRAN (R 4.3.0)\n later         1.3.1   2023-05-02 [1] CRAN (R 4.3.1)\n lifecycle     1.0.3   2022-10-07 [1] CRAN (R 4.3.0)\n lubridate   * 1.9.2   2023-02-10 [1] CRAN (R 4.3.0)\n magrittr      2.0.3   2022-03-30 [1] CRAN (R 4.3.0)\n memoise       2.0.1   2021-11-26 [2] CRAN (R 4.2.0)\n mime          0.10    2021-02-13 [2] CRAN (R 4.0.2)\n miniUI        0.1.1.1 2018-05-18 [1] CRAN (R 4.3.1)\n munsell       0.5.0   2018-06-12 [1] CRAN (R 4.3.0)\n pillar        1.9.0   2023-03-22 [1] CRAN (R 4.3.0)\n pkgbuild      1.4.2   2023-06-26 [1] CRAN (R 4.3.1)\n pkgconfig     2.0.3   2019-09-22 [2] CRAN (R 4.2.0)\n pkgload       1.3.2.1 2023-07-08 [1] CRAN (R 4.3.1)\n prettyunits   1.1.1   2020-01-24 [2] CRAN (R 4.2.0)\n processx      3.8.2   2023-06-30 [1] CRAN (R 4.3.1)\n profvis       0.3.8   2023-05-02 [1] CRAN (R 4.3.1)\n promises      1.2.0.1 2021-02-11 [1] CRAN (R 4.3.1)\n ps            1.7.5   2023-04-18 [1] CRAN (R 4.3.1)\n purrr       * 1.0.1   2023-01-10 [1] CRAN (R 4.3.0)\n R6            2.5.1   2021-08-19 [2] CRAN (R 4.2.0)\n Rcpp          1.0.11  2023-07-06 [1] CRAN (R 4.3.1)\n readr       * 2.1.4   2023-02-10 [1] CRAN (R 4.3.0)\n remotes       2.4.2   2021-11-30 [2] CRAN (R 4.2.0)\n rlang         1.1.1   2023-04-28 [1] CRAN (R 4.3.0)\n rmarkdown     2.21    2023-03-26 [1] CRAN (R 4.3.0)\n rstudioapi    0.14    2022-08-22 [1] CRAN (R 4.3.0)\n rvest         1.0.3   2022-08-19 [1] CRAN (R 4.3.0)\n scales        1.2.1   2022-08-20 [1] CRAN (R 4.3.0)\n sessioninfo   1.2.2   2021-12-06 [2] CRAN (R 4.2.0)\n shiny         1.7.4.1 2023-07-06 [1] CRAN (R 4.3.1)\n stringi       1.7.12  2023-01-11 [1] CRAN (R 4.3.1)\n stringr     * 1.5.0   2022-12-02 [1] CRAN (R 4.3.0)\n svglite       2.1.1   2023-01-10 [1] CRAN (R 4.3.1)\n systemfonts   1.0.4   2022-02-11 [1] CRAN (R 4.3.0)\n tibble      * 3.2.1   2023-03-20 [1] CRAN (R 4.3.0)\n tidyr       * 1.3.0   2023-01-24 [1] CRAN (R 4.3.0)\n tidyselect    1.2.0   2022-10-10 [1] CRAN (R 4.3.0)\n tidyverse   * 2.0.0   2023-02-22 [1] CRAN (R 4.3.1)\n timechange    0.2.0   2023-01-11 [1] CRAN (R 4.3.0)\n tzdb          0.4.0   2023-05-12 [1] CRAN (R 4.3.0)\n urlchecker    1.0.1   2021-11-30 [1] CRAN (R 4.3.1)\n usethis       2.2.2   2023-07-06 [1] CRAN (R 4.3.1)\n utf8          1.2.3   2023-01-31 [1] CRAN (R 4.3.1)\n vctrs         0.6.3   2023-06-14 [1] CRAN (R 4.3.0)\n viridisLite   0.4.2   2023-05-02 [1] CRAN (R 4.3.0)\n vroom         1.6.3   2023-04-28 [1] CRAN (R 4.3.0)\n webshot       0.5.5   2023-06-26 [1] CRAN (R 4.3.1)\n withr         2.5.0   2022-03-03 [2] CRAN (R 4.2.0)\n xfun          0.39    2023-04-20 [1] CRAN (R 4.3.0)\n xml2          1.3.3   2021-11-30 [2] CRAN (R 4.2.0)\n xtable        1.8-4   2019-04-21 [1] CRAN (R 4.3.1)\n yaml          2.3.5   2022-02-21 [2] CRAN (R 4.2.0)\n\n [1] /home/jan/R/x86_64-pc-linux-gnu-library/4.3\n [2] /usr/local/lib/R/site-library\n [3] /usr/lib/R/site-library\n [4] /usr/lib/R/library\n\n──────────────────────────────────────────────────────────────────────────────"
  },
  {
    "objectID": "posts/2017-02-15-which-variable-is-more-important/index.html",
    "href": "posts/2017-02-15-which-variable-is-more-important/index.html",
    "title": "Which predictor is most important? Predictive utility vs. construct importance",
    "section": "",
    "text": "Every so often, I’m asked for my two cents on a correlational study in which the researcher wants to find out which of a set of predictor variables is the most important one. For instance, they may have the results of an intelligence test, of a working memory task and of a questionnaire probing their participants’ motivation for learning French, and they want to find out which of these three is the most important factor in acquiring a nativelike French accent, as measured using a pronunciation task. As I will explain below, research questions such as these can be interpreted in two ways, and whether they can be answered sensibly depends on the interpretation intended."
  },
  {
    "objectID": "posts/2017-02-15-which-variable-is-more-important/index.html#interpretation-1-predictive-utility",
    "href": "posts/2017-02-15-which-variable-is-more-important/index.html#interpretation-1-predictive-utility",
    "title": "Which predictor is most important? Predictive utility vs. construct importance",
    "section": "Interpretation 1: Predictive utility",
    "text": "Interpretation 1: Predictive utility\nFirst, you can interpret questions such as Which of variables A, B and C is the most important factor in X? as follows: If you wanted to guesstimate a person’s result on X (e.g., nativelikeness of accent) and you could only know their score on A, B or C (e.g., intelligence test, working memory task, motivational questionnaire), which one should you pick? Such questions can make sense when you have a battery of tasks and questionnaires that you need to slim down for a future study or evaluation.\nInterpreted like this, such questions can be sensibly answered, for instance by comparing the correlation coefficients for AX, BX and CX or by comparing the fit of regression models for each of the three predictor variables."
  },
  {
    "objectID": "posts/2017-02-15-which-variable-is-more-important/index.html#interpretation-2-construct-importance",
    "href": "posts/2017-02-15-which-variable-is-more-important/index.html#interpretation-2-construct-importance",
    "title": "Which predictor is most important? Predictive utility vs. construct importance",
    "section": "Interpretation 2: Construct importance",
    "text": "Interpretation 2: Construct importance\nOften, however, it turns out that researchers aren’t interested in the predictive utility of variables A, B and C per se, but rather in the importance of the construct that these variables represent. Concretely, they aren’t so much interested in the participants’ performance on an intelligence test as they are interested in the participants’ intelligence proper. The tests, tasks and questionnaires are merely means for eliciting this information, and imperfect means at that.\nFor the accent example, then, the intended research question under this interpretation is this: What’s more important for acquiring a nativelike accent in French: your intelligence, your working memory capacity or your motivation?\nThe difference between this interpretation and the ‘predictive utility’ interpretation may seem small, but whereas I think that such research questions are answerable under the ‘predictive utility’ interpretation, I think they are usually unanswerable when they concern the scientific constructs themselves. The reason for this, as so often, is measurement error.\nDue to measurement error, the participants’ scores on the intelligence test, working memory task and motivational questionnaire are but approximations of their true intelligence, working memory capacity and motivation. What is more, the extent to which these scores are affected by measurement error will vary from task to task. This is important because measurement error, on average, attenuates between two variables. As a result, we may find that intelligence scores predict nativelikeness of accent better than working memory scores or motivational questionnaire scores, but this doesn’t have to mean that intelligence itself is more important than working memory capacity or motivation: it may well be the case that motivation is by far the most important factor in accent acquisition, but that this factor is less well captured by the questionnaire than intelligence is by the intelligence test.\nWhen it’s construct importance you’re interested in, the time to worry about measurement error is before running the study. For instance, at the cost of considerably more time and effort on the part of the participants, you may want to use multiple tests and tasks for each of the constructs you’re interested in and conduct a latent variable analysis. Or you can try to find tasks whose reliability in measuring the construct is known so you can correct for measurement error (see Chapter 7 in Faraway’s _Linear models with R). I don’t have much experience with either strategy, though."
  },
  {
    "objectID": "posts/2017-02-15-which-variable-is-more-important/index.html#conclusion",
    "href": "posts/2017-02-15-which-variable-is-more-important/index.html#conclusion",
    "title": "Which predictor is most important? Predictive utility vs. construct importance",
    "section": "Conclusion",
    "text": "Conclusion\nWhen you want to find out which variable is the most important one, think about whether it’s predictive utility or construct importance you’re interested in. If it’s (also) the latter, consider the attenuating effect of measurement error when designing your study."
  },
  {
    "objectID": "posts/2015-09-17-cluster-randomised-experiments/index.html",
    "href": "posts/2015-09-17-cluster-randomised-experiments/index.html",
    "title": "Analysing experiments with intact groups: the problem and an easy solution",
    "section": "",
    "text": "To evaluate a new teaching method, a researcher arranges for one class of 20 students to be taught with the old method and another class of 20 students with the new method. According to a t-test, the students taught with the new method significantly outperform the control group at an end-of-year evaluation. The set-up of studies like these is pretty standard in applied linguistics – but it is fatally flawed. In this post, I explain the problem with comparing intact groups using traditional statistical tools and present a solution that’s easy to both understand and implement."
  },
  {
    "objectID": "posts/2015-09-17-cluster-randomised-experiments/index.html#the-problem",
    "href": "posts/2015-09-17-cluster-randomised-experiments/index.html#the-problem",
    "title": "Analysing experiments with intact groups: the problem and an easy solution",
    "section": "The problem",
    "text": "The problem\nIn a textbook experiment, the 40 students from the example above would have been assigned to one of the teaching methods randomly and on an individual basis. In the example above, however, the assignment to one of the teaching methods wasn’t done individually but class-by-class, so that all students in the same class were taught using the same method.\nThe difference may seem minute – after all, we end up with about 20 participants in either group, so who cares? However, assigning participants to experimental conditions class-by-class – in what is called a cluster-randomised experiment – leads to a problem known as clustering: Due to selection processes, a common background, having the same teachers, etc., two students in the same class tend to be somewhat more alike than two students from different classes. This, in turn, means that the information that each participant contributes to the study isn’t entirely new: if someone’s class-mates have above-average reading skills, chances are she too will have above-average reading skills, regardless of the teaching method. Rather than having 40 participants, the researcher in the example above could have the equivalent of, say, 8, 14 or 32 participants per group in terms of the information that the sample contains.\nThe degree of clustering is expressed in the intra-class correlation (ICC). This number takes a value between 0 and 1. An ICC of 1 means that all values within a cluster (e.g. a class) are identical to each other so that multiple observations per cluster don’t contribute any information to the study. An ICC of 0 means that values within a cluster are no more alike that values from different clusters. For reference, typical ICC values fall in the 0.15 to 0.20 bracket in educational contexts.\nOrdinary t-tests and ANOVAs as well as correlation and regression analyses don’t take into account the clustered nature of such data. Instead, they assume that each datapoint is independent of any other. This may seem like a technicality, but its effect is staggering. Conventionally, a statistical test has a 5% chance of detecting a ‘significant effect’ even if nothing is going on. This is known as the Type-I error rate. If clustered data are analysed without taking the clustering into account, the Type-I error rate could easily rise to 40% or more. Crucially, even seemingly negligible degrees of clustering bloat the Type-I error rate. Figure 1 shows how the actual Type-I error rate increases as the ICC becomes larger and as the number of participants per cluster increases if the clustered nature of the data is not taken into account.\nUpdate (2023-08-25): There was a slight inaccuracy in the formula I initially used to draw this plot. I’ve now corrected this.\n\n\n\n\n\nFigure 1: Spurious significance in cluster-randomised experiments analysed using traditional t-tests according to the intra-class correlation (ICC) and the number of participants per cluster (m). The number of clusters was fixed at 10, but the Type-I error rate differs only slightly for different numbers of clusters. Reading guide: If you carry out a cluster-randomised experiment with 10 classes of 20 (m, green line) students each and run an ordinary t-test on the resultant 200 data points, you have a 16% chance of observing a significant difference even if no real difference exists (for an intra-class correlation of 0.05) rather than the nominal 5% chance. Common ICC values in educational contexts are in the 0.15 to 0.20 bracket.\n\n\n\n\nThe intuition behind these bloated Type-I error rates is this. If you compare two groups of 10 participants each but you pretend to compare two groups of 100 participants each (e.g. by entering each participant’s outcome ten times), you overestimate the degree of confidence you have in your results. Similarly, if you have 40 participants that are the informational equivalent of only 20 participants, you overestimate your confidence in the results.\nClustered data are fairly common in applied linguistics and the language sciences at large, and often for good reason. For instance, it may not be feasible to randomly assign students to teaching methods on an individual basis, or doing so may jeopardise the ecological validity of the study. But regardless of whether a cluster-randomised experiment is well-motivated, the clustering it gives rise to needs to be taken into account in order to arrive at statistically valid results. Before discussing how this can be done, here are some examples of common clustered designs so that you may more easily identify clustered data when you stumbled across them:\n\n94 students from 6 classes participate in an intervention study. To improve the study’s ecological validity, students in the same class are all assigned to the same experimental condition. If you see a t-test with 92 (i.e. 94 - 2) degrees of freedom (e.g. ‘t(92) = 2.3, p = 0.02’), the analysis is overwhelming likely not to have taken into account clustering and probably underestimates the Type-I error rate. In a word, the analysis is invalid.\nThe length of the KIT vowel ([I]) is measured in 15 bilinguals and 13 monolinguals. A total of 840 tokens are collected. In the Results section, you find the following: “We found a significant length difference between [I] sounds produced by bilinguals (n = 450, M = 72 ms, SD = 29 ms) and those produced by the monolingual controls (n = 390, M = 87 ms, SD = 28 ms; t(838) = 7.38, p &lt; 0.001).” This study doesn’t concern comparisons of intact groups of students. Nevertheless, vowel tokens produced by one speaker tend to be more alike than vowels produced by different speakers. In other words, vowel tokens cluster by speaker in the same way that students cluster by class. A traditional t-test on the individual vowels (i.e. a t-test with 840 - 2 = 838 degrees of freedom) ignores this clustering and is therefore invalid.\n40 participants are randomly assigned to one of two teaching methods on an individual basis. Teaching takes place in four groups of ten participants each (two groups per method). Even though the participants are assigned to the teaching methods individually, some clustering can be expected to arise during teaching (see Lee and Thompson 2005 for a related discussion). A traditional t-test (in this case one with 40 - 2 = 38 degrees of freedom) ignores this clustering, too, and is likely to yield too low a p-value.\n80 primary-school students are divided into age groups and, within age group, are randomly combined into pairs. They are then given a map task, and the lexical diversity of their utterances is measured. A regression or correlation analysis is used to gauge the relationship between the students’ age and their lexical diversity. Here, too, the data are clustered – presumably by class but also per dyad: Mutual likes and dislikes and phenomena such as lexical and syntactic priming may contribute to lexical diversity measurements within each dyad being more similar than between different dyads in the same age group.\nA questionnaire-based experiment is administered to two groups of students taking parallel courses. Students in the first group all fill out version A of the questionnaire; those in the second group fill out version B. Again, even fairly innocuous-looking similarities within each group (friends going to the same course, students having the same timetable because they take the same courses and have similar interests, more convenient to attend one class and not the other due to better train connections etc.) can lead to an overstated degree of confidence in the study’s results.\n\nIn the last three cases, researchers may argue that no clustering is present in their data, but this argument would be extremely difficult to prove statistically. (It would involve showing that the between-cluster variance is zero.) Such an argument would be entirely rhetorical and would probably be quite easy to pick apart."
  },
  {
    "objectID": "posts/2015-09-17-cluster-randomised-experiments/index.html#an-easy-solution",
    "href": "posts/2015-09-17-cluster-randomised-experiments/index.html#an-easy-solution",
    "title": "Analysing experiments with intact groups: the problem and an easy solution",
    "section": "An easy solution",
    "text": "An easy solution\nThe fancy solution for dealing with the clustering problem is to analyse the data in a multilevel (i.e. mixed-effects) model. Multilevel models aren’t too easy to learn, though, and communicating their results to an audience that’s unfamiliar with them can be a challenge. There’s a conceptually straightforward and easy-to-implement alternative, however: average the measurements per cluster and run the t-test on the cluster averages instead. Doing so removes the dependencies in the data and produces nominal Type-I error rates.\nTo show how easy this is and how fundamentally it can affect the results, I’ll go through the process using simulated data for a cluster-randomised experiment that you can either download or read directly into R.\n\ndat &lt;- read.csv(\"http://homeweb.unifr.ch/VanhoveJ/Pub/Data/clusteredData.csv\"\n                , stringsAsFactors = TRUE)\nstr(dat)\n\n'data.frame':   160 obs. of  3 variables:\n $ Class      : int  1 1 1 1 1 1 1 1 1 1 ...\n $ Condition  : Factor w/ 2 levels \"control\",\"intervention\": 1 1 1 1 1 1 1 1 1 1 ...\n $ Measurement: int  8 8 5 9 3 4 10 16 14 20 ...\n\n\n\nsummary(dat)\n\n     Class              Condition   Measurement   \n Min.   :1.000   control     :80   Min.   : 0.00  \n 1st Qu.:3.000   intervention:80   1st Qu.: 9.00  \n Median :4.500                     Median :13.00  \n Mean   :4.569                     Mean   :12.69  \n 3rd Qu.:6.000                     3rd Qu.:16.00  \n Max.   :8.000                     Max.   :26.00  \n\n\nAs you can see, this is a simple experiment with 80 participants per condition. Cross-tabulating the Condition and Class variables reveals that all students in a given class were assigned to the same experimental condition.\n\nxtabs(~ Class + Condition, dat)\n\n     Condition\nClass control intervention\n    1      22            0\n    2      13            0\n    3      21            0\n    4      24            0\n    5       0           18\n    6       0           23\n    7       0           16\n    8       0           23\n\n\nWhat if we were to ignore this clustering? A visual inspection of the distribution of the scores in each condition wouldn’t reveal anything out of the ordinary…\n\n# Some graphical settings\npar(las = 1, bty = \"n\")\n\n# Draw boxplot, but don't draw outliers\nboxplot(Measurement ~ Condition, data = dat,\n        # red = control; blue = intervention\n        border = c(\"#E41A1C\", \"#377EB8\"),\n        xlab = \"Condition\",\n        ylab = \"Measurement\", ylim = c(0, 30),\n        outline = FALSE)\n\n# Add individual points\npoints(Measurement ~ jitter(as.numeric(Condition)),\n       col = \"grey40\",\n       data = dat)\n\n\n\n\n… and a t-test would reveal a highly significant difference between the two conditions (t(158) = 3.2, p = 0.002): the intervention worked!\n\nt.test(Measurement ~ Condition, data = dat,\n       var.equal = TRUE)\n\n\n    Two Sample t-test\n\ndata:  Measurement by Condition\nt = -3.1913, df = 158, p-value = 0.001709\nalternative hypothesis: true difference in means between group control and group intervention is not equal to 0\n95 percent confidence interval:\n -4.0472258 -0.9527742\nsample estimates:\n     mean in group control mean in group intervention \n                   11.4375                    13.9375 \n\n\nIt’s only by plotting the scores per class that we get an idea of how the data are clustered. Class 3, a control class, consists of students who were pretty good at the task relative to the other control classes, for instance. (Incidentally, I set the ICC value to 0.1 when simulating this dataset.)\n\npar(las = 1, bty = \"n\")\n\n# Draw boxplots per class, no outliers\nboxplot(Measurement ~ Class, data = dat,\n        # colour boxes red (control) or blue (intervention)\n        border = c(rep(\"#E41A1C\", 4), rep(\"#377EB8\", 4)),\n        varwidth = TRUE, # wider boxes for larger classes\n        xlab = \"Class\",\n        ylab = \"Measurement\", ylim = c(0, 30),\n        outline = FALSE)\n\n# Add individual points\npoints(Measurement ~ jitter(as.numeric(Class)),\n       col = \"grey40\",\n       data = dat)\n\n\n\n\nTo take by-class clustering into account, we can compute the mean score per class.\nUpdate (2023-08-25): Computing such summaries is now much easier than in 2015; I’ve adapted the code accordingly.\n\nlibrary(tidyverse)\n\n── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──\n✔ dplyr     1.1.2     ✔ readr     2.1.4\n✔ forcats   1.0.0     ✔ stringr   1.5.0\n✔ ggplot2   3.4.2     ✔ tibble    3.2.1\n✔ lubridate 1.9.2     ✔ tidyr     1.3.0\n✔ purrr     1.0.1     \n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\nℹ Use the conflicted package (&lt;http://conflicted.r-lib.org/&gt;) to force all conflicts to become errors\n\ndat.byclass &lt;- dat |&gt; \n  group_by(Class, Condition) |&gt; \n  summarise(\n    meanMeasurement = mean(Measurement),\n    nrStudents = length(Measurement)\n    , .groups = \"drop\"\n  )\ndat.byclass\n\n# A tibble: 8 × 4\n  Class Condition    meanMeasurement nrStudents\n  &lt;int&gt; &lt;fct&gt;                  &lt;dbl&gt;      &lt;int&gt;\n1     1 control                 9.14         22\n2     2 control                13.3          13\n3     3 control                15.7          21\n4     4 control                 8.79         24\n5     5 intervention           15.1          18\n6     6 intervention           12.2          23\n7     7 intervention           12.2          16\n8     8 intervention           16.0          23\n\n\nThese class means are then compared in a t-test:\n\nt.test(meanMeasurement ~ Condition, \n       data = dat.byclass, var.equal = TRUE)\n\n\n    Two Sample t-test\n\ndata:  meanMeasurement by Condition\nt = -1.1031, df = 6, p-value = 0.3122\nalternative hypothesis: true difference in means between group control and group intervention is not equal to 0\n95 percent confidence interval:\n -6.862295  2.597565\nsample estimates:\n     mean in group control mean in group intervention \n                  11.73750                   13.86987 \n\n\nThis time, the test fails to show a significant difference (t(6) = 1.1, p = 0.31), and we’d be forced to conclude that we have no evidence that the intervention worked. (Which doesn’t mean that it didn’t, mind you.)"
  },
  {
    "objectID": "posts/2015-09-17-cluster-randomised-experiments/index.html#further-reading-and-some-additional-points",
    "href": "posts/2015-09-17-cluster-randomised-experiments/index.html#further-reading-and-some-additional-points",
    "title": "Analysing experiments with intact groups: the problem and an easy solution",
    "section": "Further reading and some additional points",
    "text": "Further reading and some additional points\nFor further references and some additional points concerning cluster-randomised experiments, I refer to a recent article of mine (Section 4). Here’s the bullet-point version of what I think researchers should be aware of when planning or evaluating class-based experiments:\n\nProperly analysed cluster-randomised experiments have lower statistical power (i.e., a lower probability of observing statistical differences if the intervention actually works) than individually randomised experiments with the same number of participants – no matter what. Having 200 participants in an individually randomised design or 200 participants in a cluster-randomised design are two entirely different things.\nIncreasing the number of participants per cluster increases power, but not as much as you’d think. For instance, if having 10 classes of 20 participants gives you 40% power, having 10 classes of 200 participants gives you about 53% power. Increasing the number of clusters is much more efficient. With 14 classes of 20 participants each, you’d already have 57% power.\nHaving 10 clusters of 20 participants each is better than having 2 clusters of 80 participants each and 8 clusters of 5 participants each. If the former gives you 40% power, the latter gives you 27% power.\nThink about what sort of covariates would account for uninteresting variance in the dependent variable. Pretest scores, for instance, can be averaged by class, too, and be entered into an analysis of covariance on the class means. Don’t go overboard with this, though: Each covariate costs a degree of freedom, and sacrificing a degree of freedom for a weakly predictive covariate could actually result in a loss of power.\nDon’t run experiments with only one cluster per condition. A t-test on two cluster means will have 2 - 2 = 0 degrees of freedom and will return an error. Conceptually, a study with one cluster per condition has no reliable way of telling whether any difference between the two classes are due to the experimental condition or to the classes tested.\nLastly, in the article referenced above, I suggested that researchers may want to consider weighting the cluster means by the number of observations in each cluster when running analyses on the cluster means. This was incorrect. When running some simulations in preparation of this blog post, I noticed that weighting cluster means in this way inflates the test’s Type-I error rate, sometimes dramatically. My error was due to having misinterpreted some articles on taking unequal cluster sizes into account when planning cluster-randomised designs."
  },
  {
    "objectID": "posts/2015-09-17-cluster-randomised-experiments/index.html#software-versions",
    "href": "posts/2015-09-17-cluster-randomised-experiments/index.html#software-versions",
    "title": "Analysing experiments with intact groups: the problem and an easy solution",
    "section": "Software versions",
    "text": "Software versions\n\ndevtools::session_info()\n\n─ Session info ───────────────────────────────────────────────────────────────\n setting  value\n version  R version 4.3.1 (2023-06-16)\n os       Ubuntu 22.04.3 LTS\n system   x86_64, linux-gnu\n ui       X11\n language en_US\n collate  en_US.UTF-8\n ctype    en_US.UTF-8\n tz       Europe/Zurich\n date     2023-08-25\n pandoc   3.1.1 @ /usr/lib/rstudio/resources/app/bin/quarto/bin/tools/ (via rmarkdown)\n\n─ Packages ───────────────────────────────────────────────────────────────────\n package      * version date (UTC) lib source\n cachem         1.0.6   2021-08-19 [2] CRAN (R 4.2.0)\n callr          3.7.3   2022-11-02 [1] CRAN (R 4.3.1)\n cli            3.6.1   2023-03-23 [1] CRAN (R 4.3.0)\n colorspace     2.1-0   2023-01-23 [1] CRAN (R 4.3.0)\n crayon         1.5.2   2022-09-29 [1] CRAN (R 4.3.1)\n devtools       2.4.5   2022-10-11 [1] CRAN (R 4.3.1)\n digest         0.6.29  2021-12-01 [2] CRAN (R 4.2.0)\n dplyr        * 1.1.2   2023-04-20 [1] CRAN (R 4.3.0)\n ellipsis       0.3.2   2021-04-29 [2] CRAN (R 4.2.0)\n evaluate       0.15    2022-02-18 [2] CRAN (R 4.2.0)\n fansi          1.0.4   2023-01-22 [1] CRAN (R 4.3.1)\n fastmap        1.1.0   2021-01-25 [2] CRAN (R 4.2.0)\n forcats      * 1.0.0   2023-01-29 [1] CRAN (R 4.3.0)\n fs             1.5.2   2021-12-08 [2] CRAN (R 4.2.0)\n generics       0.1.3   2022-07-05 [1] CRAN (R 4.3.0)\n ggplot2      * 3.4.2   2023-04-03 [1] CRAN (R 4.3.0)\n glue           1.6.2   2022-02-24 [2] CRAN (R 4.2.0)\n gtable         0.3.3   2023-03-21 [1] CRAN (R 4.3.0)\n hms            1.1.3   2023-03-21 [1] CRAN (R 4.3.0)\n htmltools      0.5.5   2023-03-23 [1] CRAN (R 4.3.0)\n htmlwidgets    1.6.2   2023-03-17 [1] CRAN (R 4.3.1)\n httpuv         1.6.11  2023-05-11 [1] CRAN (R 4.3.1)\n jsonlite       1.8.7   2023-06-29 [1] CRAN (R 4.3.1)\n knitr          1.39    2022-04-26 [2] CRAN (R 4.2.0)\n later          1.3.1   2023-05-02 [1] CRAN (R 4.3.1)\n lifecycle      1.0.3   2022-10-07 [1] CRAN (R 4.3.0)\n lubridate    * 1.9.2   2023-02-10 [1] CRAN (R 4.3.0)\n magrittr       2.0.3   2022-03-30 [1] CRAN (R 4.3.0)\n memoise        2.0.1   2021-11-26 [2] CRAN (R 4.2.0)\n mime           0.10    2021-02-13 [2] CRAN (R 4.0.2)\n miniUI         0.1.1.1 2018-05-18 [1] CRAN (R 4.3.1)\n munsell        0.5.0   2018-06-12 [1] CRAN (R 4.3.0)\n pillar         1.9.0   2023-03-22 [1] CRAN (R 4.3.0)\n pkgbuild       1.4.2   2023-06-26 [1] CRAN (R 4.3.1)\n pkgconfig      2.0.3   2019-09-22 [2] CRAN (R 4.2.0)\n pkgload        1.3.2.1 2023-07-08 [1] CRAN (R 4.3.1)\n prettyunits    1.1.1   2020-01-24 [2] CRAN (R 4.2.0)\n processx       3.8.2   2023-06-30 [1] CRAN (R 4.3.1)\n profvis        0.3.8   2023-05-02 [1] CRAN (R 4.3.1)\n promises       1.2.0.1 2021-02-11 [1] CRAN (R 4.3.1)\n ps             1.7.5   2023-04-18 [1] CRAN (R 4.3.1)\n purrr        * 1.0.1   2023-01-10 [1] CRAN (R 4.3.0)\n R6             2.5.1   2021-08-19 [2] CRAN (R 4.2.0)\n RColorBrewer   1.1-3   2022-04-03 [1] CRAN (R 4.3.0)\n Rcpp           1.0.11  2023-07-06 [1] CRAN (R 4.3.1)\n readr        * 2.1.4   2023-02-10 [1] CRAN (R 4.3.0)\n remotes        2.4.2   2021-11-30 [2] CRAN (R 4.2.0)\n rlang          1.1.1   2023-04-28 [1] CRAN (R 4.3.0)\n rmarkdown      2.21    2023-03-26 [1] CRAN (R 4.3.0)\n rstudioapi     0.14    2022-08-22 [1] CRAN (R 4.3.0)\n scales         1.2.1   2022-08-20 [1] CRAN (R 4.3.0)\n sessioninfo    1.2.2   2021-12-06 [2] CRAN (R 4.2.0)\n shiny          1.7.4.1 2023-07-06 [1] CRAN (R 4.3.1)\n stringi        1.7.12  2023-01-11 [1] CRAN (R 4.3.1)\n stringr      * 1.5.0   2022-12-02 [1] CRAN (R 4.3.0)\n tibble       * 3.2.1   2023-03-20 [1] CRAN (R 4.3.0)\n tidyr        * 1.3.0   2023-01-24 [1] CRAN (R 4.3.0)\n tidyselect     1.2.0   2022-10-10 [1] CRAN (R 4.3.0)\n tidyverse    * 2.0.0   2023-02-22 [1] CRAN (R 4.3.1)\n timechange     0.2.0   2023-01-11 [1] CRAN (R 4.3.0)\n tzdb           0.4.0   2023-05-12 [1] CRAN (R 4.3.0)\n urlchecker     1.0.1   2021-11-30 [1] CRAN (R 4.3.1)\n usethis        2.2.2   2023-07-06 [1] CRAN (R 4.3.1)\n utf8           1.2.3   2023-01-31 [1] CRAN (R 4.3.1)\n vctrs          0.6.3   2023-06-14 [1] CRAN (R 4.3.0)\n withr          2.5.0   2022-03-03 [2] CRAN (R 4.2.0)\n xfun           0.39    2023-04-20 [1] CRAN (R 4.3.0)\n xtable         1.8-4   2019-04-21 [1] CRAN (R 4.3.1)\n yaml           2.3.5   2022-02-21 [2] CRAN (R 4.2.0)\n\n [1] /home/jan/R/x86_64-pc-linux-gnu-library/4.3\n [2] /usr/local/lib/R/site-library\n [3] /usr/lib/R/site-library\n [4] /usr/lib/R/library\n\n──────────────────────────────────────────────────────────────────────────────"
  },
  {
    "objectID": "posts/2015-10-16-nonlinear-relationships/index.html",
    "href": "posts/2015-10-16-nonlinear-relationships/index.html",
    "title": "The problem with cutting up continuous variables and what to do when things aren’t linear",
    "section": "",
    "text": "A common analytical technique is to cut up continuous variables (e.g. age, word frequency, L2 proficiency) into discrete categories and then use them as predictors in a group comparison (e.g. ANOVA). For instance, stimuli used in a lexical decision task are split up into a high-frequency and a low-frequency group, whereas the participants are split up into a young, middle, and old group. Although discretising continuous variables appears to make the analysis easier, this practice has been criticised for years. Below I outline the problems with this approach and present some alternatives."
  },
  {
    "objectID": "posts/2015-10-16-nonlinear-relationships/index.html#problem-1-loss-of-information-and-its-consequences",
    "href": "posts/2015-10-16-nonlinear-relationships/index.html#problem-1-loss-of-information-and-its-consequences",
    "title": "The problem with cutting up continuous variables and what to do when things aren’t linear",
    "section": "Problem 1: Loss of information and its consequences",
    "text": "Problem 1: Loss of information and its consequences\nThe problem with discretising continuous variables is that it throws away meaningful information. This loss of information is most pronounced in the case of dichotomisation (carving up a continuous variable into two levels). When splitting up words into a high- and a low-frequency group, within-group information about relative frequency differences is lost – extremely frequent words and somewhat frequent words are treated as though they had the same frequency. Doing so leads to an appreciable loss of power, i.e. a decrease in the probability of finding a pattern if there really is one (Altman & Royston 2006; Cohen 1983; MacCullum et al. 2002; Royston, Altman & Sauerbrei 2006): Cohen 1983 shows that carving up a continuous variable into two groups is akin to throwing away a third of the data. Paradoxically, dichotomisation can sometimes lead to a simultaneous increase of false positives, i.e. finding a pattern where there is in fact none (Maxwell & Delaney 1993) – a statistical double whammy."
  },
  {
    "objectID": "posts/2015-10-16-nonlinear-relationships/index.html#problem-2-spurious-threshold-effects",
    "href": "posts/2015-10-16-nonlinear-relationships/index.html#problem-2-spurious-threshold-effects",
    "title": "The problem with cutting up continuous variables and what to do when things aren’t linear",
    "section": "Problem 2: Spurious threshold effects",
    "text": "Problem 2: Spurious threshold effects\nFurthermore, discretisation draws categorical boundaries where none exist and may thereby spuriously suggest the presence of cut-offs or threshold effects (e.g. Altman & Royston 2006; Vanhove 2013). For instance, by grouping 20- to 29-year-olds in one category and 30- to 39-year-olds in another, we create the impression that 20- and 29-year-olds tend to be more alike than 29- and 30-year-olds. If the outcome variable differs between the groups, it might even be tempting to conclude that some important change occurred in the 30th year. I suspect – I hope – that the researchers themselves are aware that such a sudden change is entirely due to their arbitrary cut-off choices, but these details tend to get lost in citation, and I wonder to what extent threshold theories in language acquisition owe their existence to continuous predictors being squeezed into the ANOVA straitjacket."
  },
  {
    "objectID": "posts/2015-10-16-nonlinear-relationships/index.html#solutions",
    "href": "posts/2015-10-16-nonlinear-relationships/index.html#solutions",
    "title": "The problem with cutting up continuous variables and what to do when things aren’t linear",
    "section": "Solutions",
    "text": "Solutions\n\nWhen the pattern is more or less linear\nWith the authors cited above, I agree that the best solution is usually to stop carving up continuous phenomena into discrete categories and to instead exploit the continuous data to the full in a linear regression analysis (see Baayen 2010, and Vanhove 2013 for linguistic examples). Sometimes, however, the data suggest a non-linear trend that is less easily accommodated in a linear regression model. I turn to such cases next.\n\n\nWhen the pattern is non-linear\nA more sophisticated rationale for carving up a continuous predictor is that the relationship between the predictor and the outcome is not approximately linear. By way of example, Figure 1 shows how the performance on some test varies according to the participants’ age (data from Vanhove & Berthele 2015, available here). What the data mean is of less importance for our present purposes; what is important is that the scatterplot highlights a non-linear trend.\n\n\n\n\n\nFigure 1: Scatterplot of the original data.\n\n\n\n\nAn ordinary correlation analysis or a simple linear regression model would find a small, positive, non-significant age trend. But these analyses test the linear trend in the data, which is clearly not relevant in this case. Dichotomising the age variable by means of a median split does not bring us much closer to a resolution, however: As the boxplots in the left-hand panel of Figure 2 show, a median split completely hides the trend in the data (see also Altman & Royston 2006; MacCullum et al. 2002). A more fine-grained discretisation, e.g., in slices of ten years, underscores the trend appreciably better as shown in the right-hand panel of Figure 2 (see also Gelman & Hill 2007, pp. 66-68). But it also raises a number of questions: What is the optimal number of bins? Where should we draw the cut-offs between the bins? Should every bin be equally as wide? And how much can we fiddle about with these bins without jeopardising our inferential statistics?\n\n\n\n\n\nFigure 2. Left: Boxplots after a median split of the age variable; the age pattern is unrecognisable. Right: Boxplots after a more fine-grained discretisation; the non-linear pattern is now recognisible, but the cut-offs between the groups were drawn arbitrarily.\n\n\n\n\nClearly, it is preferable to side-step such arbitrary decisions. Apart from transforming the predictor, the outcome or both, we can deal with non-linearities by modelling them directly. There are a couple of options available in this domain (e.g. LO(W)ESS, polynomial regression, restricted cubic splines); here I’ll briefly demonstrate one of them: generalised additive modelling. It’s not my goal to discuss the ins and outs of generalised additive modelling, but rather to illustrate its use and to direct those interested to more thorough sources. In doing so, I’ll be freely quoting from Section 4.3.2 from my thesis.\nGeneralised additive models (GAMs) are implemented in the mgcv package for R. GAMs estimate the form of the non-linear relationship from the data. This is essentially accomplished by fitting a kind of regression on subsets of the data and then glueing the different pieces together. The more subset regression are fitted and glued together, the more ‘wiggly’ the overall curve will be. Fitting too many subset regressions results in overwiggly curves that fit disproportionally much noise in the data (‘oversmoothing’). To prevent this, the mgcv package implements a procedure that estimates the number of subset regression – and hence the complexity of the overall curve – that stands the best chance of predicting new data points. For details, I refer to Chapter 3 in Zuur et al. (2009) and to a tutorial by Clark (2013) (Update (2023-08-25): Link broken, but see here instead.) for fairly accessible introductions. An in-depth treatment is provided by Wood (2006).\nThe following R code reads in the dataset, plots an unpolished version of the scatterplot in Figure 1 above, and loads the mgcv package.\n\n# Read in data\ndat &lt;- read.csv(\"http://homeweb.unifr.ch/VanhoveJ/Pub/Data/participants_163.csv\",\n                encoding = \"UTF-8\")\n# Draw scatterplot of Age vs Spoken (not shown)\n# plot(Spoken ~ Age, data = dat)\n\n# Load mgcv package;\n# run 'install.packages(\"mgcv\")' if not installed:\nlibrary(mgcv)\n\nLoading required package: nlme\n\n\nThis is mgcv 1.9-0. For overview type 'help(\"mgcv-package\")'.\n\n\nThe GAM is then fitted using the gam() function, whose interface is similar to that of the lm() function for fitting linear models. The embedded s() function specified that the effect of Age should be fitted non-linearly (s for smoother). Plotting the model shows the non-linear age trend and its 95% confidence band:\n\nmod1 &lt;- gam(Spoken ~ s(Age), data = dat)\nplot(mod1)\n\n\n\n\nWith the summary() function, numerical details about the model, including approximate inferential statistics, can be displayed. See Clark (2013) (Update (2023-08-25): Link broken.) for details.\n\nsummary(mod1)\n\n\nFamily: gaussian \nLink function: identity \n\nFormula:\nSpoken ~ s(Age)\n\nParametric coefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)  16.5215     0.3196   51.69   &lt;2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nApproximate significance of smooth terms:\n         edf Ref.df     F p-value    \ns(Age) 7.399  8.355 15.14  &lt;2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nR-sq.(adj) =  0.429   Deviance explained = 45.5%\nGCV = 17.555  Scale est. = 16.65     n = 163\n\n\nAll of this is just to give you a flavour of what you can do when you’re confronted with non-linear data than can’t easily be transformed or fitted with a higher-order polynomials. GAMs are flexible in that they can incorporate several predictors, non-linear interactions between continuous variables, and random effects, and they can deal with non-Gaussian outcome variables (e.g. binary data), too.\nIn conclusion, if you have continuous variables, don’t throw away useful information and treat them as such. If a scatterplot reveals an approximately linear pattern, linear regression is the way to go. If a non-linear pattern emerges, consider fitting a non-linear model.\n\n\nWarning in polygon(x = c(newdat$Age, rev(newdat$Age)), y = c(predictions$fit + :\n\"alpha\" is not a graphical parameter\n\n\n\n\n\nFigure 3: Scatterplot of the original data with a non-linear GAM-based scatterplot smoother and its 95% confidence band."
  },
  {
    "objectID": "posts/2015-10-16-nonlinear-relationships/index.html#software-versions",
    "href": "posts/2015-10-16-nonlinear-relationships/index.html#software-versions",
    "title": "The problem with cutting up continuous variables and what to do when things aren’t linear",
    "section": "Software versions",
    "text": "Software versions\n\ndevtools::session_info()\n\n─ Session info ───────────────────────────────────────────────────────────────\n setting  value\n version  R version 4.3.1 (2023-06-16)\n os       Ubuntu 22.04.3 LTS\n system   x86_64, linux-gnu\n ui       X11\n language en_US\n collate  en_US.UTF-8\n ctype    en_US.UTF-8\n tz       Europe/Zurich\n date     2023-08-25\n pandoc   3.1.1 @ /usr/lib/rstudio/resources/app/bin/quarto/bin/tools/ (via rmarkdown)\n\n─ Packages ───────────────────────────────────────────────────────────────────\n package     * version date (UTC) lib source\n cachem        1.0.6   2021-08-19 [2] CRAN (R 4.2.0)\n callr         3.7.3   2022-11-02 [1] CRAN (R 4.3.1)\n cli           3.6.1   2023-03-23 [1] CRAN (R 4.3.0)\n crayon        1.5.2   2022-09-29 [1] CRAN (R 4.3.1)\n devtools      2.4.5   2022-10-11 [1] CRAN (R 4.3.1)\n digest        0.6.29  2021-12-01 [2] CRAN (R 4.2.0)\n ellipsis      0.3.2   2021-04-29 [2] CRAN (R 4.2.0)\n evaluate      0.15    2022-02-18 [2] CRAN (R 4.2.0)\n fastmap       1.1.0   2021-01-25 [2] CRAN (R 4.2.0)\n fs            1.5.2   2021-12-08 [2] CRAN (R 4.2.0)\n glue          1.6.2   2022-02-24 [2] CRAN (R 4.2.0)\n htmltools     0.5.5   2023-03-23 [1] CRAN (R 4.3.0)\n htmlwidgets   1.6.2   2023-03-17 [1] CRAN (R 4.3.1)\n httpuv        1.6.11  2023-05-11 [1] CRAN (R 4.3.1)\n jsonlite      1.8.7   2023-06-29 [1] CRAN (R 4.3.1)\n knitr         1.39    2022-04-26 [2] CRAN (R 4.2.0)\n later         1.3.1   2023-05-02 [1] CRAN (R 4.3.1)\n lattice       0.21-8  2023-04-05 [4] CRAN (R 4.3.0)\n lifecycle     1.0.3   2022-10-07 [1] CRAN (R 4.3.0)\n magrittr      2.0.3   2022-03-30 [1] CRAN (R 4.3.0)\n Matrix        1.6-0   2023-07-08 [4] CRAN (R 4.3.1)\n memoise       2.0.1   2021-11-26 [2] CRAN (R 4.2.0)\n mgcv        * 1.9-0   2023-07-11 [4] CRAN (R 4.3.1)\n mime          0.10    2021-02-13 [2] CRAN (R 4.0.2)\n miniUI        0.1.1.1 2018-05-18 [1] CRAN (R 4.3.1)\n nlme        * 3.1-162 2023-01-31 [4] CRAN (R 4.2.2)\n pkgbuild      1.4.2   2023-06-26 [1] CRAN (R 4.3.1)\n pkgload       1.3.2.1 2023-07-08 [1] CRAN (R 4.3.1)\n prettyunits   1.1.1   2020-01-24 [2] CRAN (R 4.2.0)\n processx      3.8.2   2023-06-30 [1] CRAN (R 4.3.1)\n profvis       0.3.8   2023-05-02 [1] CRAN (R 4.3.1)\n promises      1.2.0.1 2021-02-11 [1] CRAN (R 4.3.1)\n ps            1.7.5   2023-04-18 [1] CRAN (R 4.3.1)\n purrr         1.0.1   2023-01-10 [1] CRAN (R 4.3.0)\n R6            2.5.1   2021-08-19 [2] CRAN (R 4.2.0)\n Rcpp          1.0.11  2023-07-06 [1] CRAN (R 4.3.1)\n remotes       2.4.2   2021-11-30 [2] CRAN (R 4.2.0)\n rlang         1.1.1   2023-04-28 [1] CRAN (R 4.3.0)\n rmarkdown     2.21    2023-03-26 [1] CRAN (R 4.3.0)\n rstudioapi    0.14    2022-08-22 [1] CRAN (R 4.3.0)\n sessioninfo   1.2.2   2021-12-06 [2] CRAN (R 4.2.0)\n shiny         1.7.4.1 2023-07-06 [1] CRAN (R 4.3.1)\n stringi       1.7.12  2023-01-11 [1] CRAN (R 4.3.1)\n stringr       1.5.0   2022-12-02 [1] CRAN (R 4.3.0)\n urlchecker    1.0.1   2021-11-30 [1] CRAN (R 4.3.1)\n usethis       2.2.2   2023-07-06 [1] CRAN (R 4.3.1)\n vctrs         0.6.3   2023-06-14 [1] CRAN (R 4.3.0)\n xfun          0.39    2023-04-20 [1] CRAN (R 4.3.0)\n xtable        1.8-4   2019-04-21 [1] CRAN (R 4.3.1)\n yaml          2.3.5   2022-02-21 [2] CRAN (R 4.2.0)\n\n [1] /home/jan/R/x86_64-pc-linux-gnu-library/4.3\n [2] /usr/local/lib/R/site-library\n [3] /usr/lib/R/site-library\n [4] /usr/lib/R/library\n\n──────────────────────────────────────────────────────────────────────────────"
  },
  {
    "objectID": "posts/2015-04-14-power-simulations-for-comparing-independent-correlations/index.html",
    "href": "posts/2015-04-14-power-simulations-for-comparing-independent-correlations/index.html",
    "title": "Power simulations for comparing independent correlations",
    "section": "",
    "text": "Every now and then, researchers want to compare the strength of a correlation between two samples or studies. Just establishing that one correlation is significant while the other isn’t doesn’t work – what needs to be established is whether the difference between the two correlations is significant. I wanted to know how much power a comparison between correlation coefficients has, so I wrote some simulation code to find out."
  },
  {
    "objectID": "posts/2015-04-14-power-simulations-for-comparing-independent-correlations/index.html#the-results",
    "href": "posts/2015-04-14-power-simulations-for-comparing-independent-correlations/index.html#the-results",
    "title": "Power simulations for comparing independent correlations",
    "section": "The results",
    "text": "The results\nThe contour plots below show the power of comparisons with sample sizes of 2×20, 2×40 and 2×80 observations for all combinations of population correlation coefficients. For instance, the first contour plot shows that you have about 90% power to find a significant difference between two correlation coefficients if the true population correlation in population A (x-axis) is 0.4 and -0.6 in population B (y-axis) and both sample contain 20 observations. If the correlation in population B is -0.2, however, you have less than 50% power. In blue is the contour line for 80% power for reference.\n\n\n\n\n\nFor unequal sample sizes, the contour plot might look like this:"
  },
  {
    "objectID": "posts/2015-04-14-power-simulations-for-comparing-independent-correlations/index.html#conclusions",
    "href": "posts/2015-04-14-power-simulations-for-comparing-independent-correlations/index.html#conclusions",
    "title": "Power simulations for comparing independent correlations",
    "section": "Conclusions",
    "text": "Conclusions\nNot really any new insights, but a good opportunity to stress once again that The difference between “significant” and “not significant” is not itself statistically significant. And I got to play around with the outer and mapply functions, which are quite useful for avoiding for-loops in simulations (see below)."
  },
  {
    "objectID": "posts/2015-04-14-power-simulations-for-comparing-independent-correlations/index.html#caveat",
    "href": "posts/2015-04-14-power-simulations-for-comparing-independent-correlations/index.html#caveat",
    "title": "Power simulations for comparing independent correlations",
    "section": "Caveat",
    "text": "Caveat\nThese simulations estimate the power for comparisons of independent correlations. Independent correlations are correlations computed for different samples (or different studies). An example of dependent correlations would be when you measure a variable, e.g. Italian proficiency, and correlate it to two other variables (e.g., French proficiency and Spanish proficiency) using the same participants. Since you used the same participants, there will exist some intercorrelation between French proficiency and Spanish proficiency, which needs to be taken into account when comparing the correlations between Italian and French proficiency on the one hand and Italian and Spanish proficiency on the other."
  },
  {
    "objectID": "posts/2015-04-14-power-simulations-for-comparing-independent-correlations/index.html#simulation-code",
    "href": "posts/2015-04-14-power-simulations-for-comparing-independent-correlations/index.html#simulation-code",
    "title": "Power simulations for comparing independent correlations",
    "section": "Simulation code",
    "text": "Simulation code\nFirst load the MASS and psych packages (run install.packages(c(\"MASS\", \"psych\")) if they aren’t installed yet).\n\nlibrary(MASS)\nlibrary(psych)\n\nUsing the mvrnorm function from the MASS package, we can generate samples drawn from a bivariate normal distribution with a specific population correlation coefficient (the numbers of the antidiagonal in the Sigma parameter; in this example: 0.3). With cor we compute the sample correlation coefficients for these samples; these will differ from sample to sample.\n\n# Example\n# Generate sample with n = 25 and r = 0.25\nsample25 &lt;- mvrnorm(25, mu = c(0, 0), # means of the populations, doesn't matter \n                    Sigma = matrix(c(1, 0.3, \n                                     0.3, 1), ncol = 2))\n# Compute correlation matrix\ncor(sample25[,1], sample25[,2])\n\n[1] 0.1376853\n\n\nWith the r.test function from the psych package, we can compute the significance of the difference between two sample correlation coefficients. In this case, the correlation coefficients were computed for independent samples, hence the r12 and r34 parameters are specified.\n\n# Example\n# Compute p-value for difference btwn sample cors\n# of 0.5 (n = 20) and 0.2 (n = 50)\nr.test(n = 20, r12 = 0.5,\n       n2 = 50, r34 = 0.2)$p\n\n[1] 0.2207423\n\n\nWith that out of the way, we now write a new function, compute.p, that generates two samples of sizes n1 and n2, respectively, from bivariate normal distributions with population correlations of popr12 and popr34, respectively.\n\ncompute.p &lt;- function(popr12, popr34, n1, n2) {\n  return(r.test(n = n1, n2 = n2,\n                r12 = cor(mvrnorm(n1, mu = c(0, 0), \n                                  Sigma = matrix(c(1, popr12, \n                                                   popr12, 1), ncol = 2)))[1,2], \n                r34 = cor(mvrnorm(n2, mu = c(0, 0), \n                                  Sigma = matrix(c(1, popr34, \n                                                   popr34, 1), ncol = 2)))[1,2])$p)\n}\n# Example\ncompute.p(n1 = 20, popr12 = 0.5, n2 = 50, popr34 = 0.2)\n\nNow we write another function, compute.power, that takes compute.p, runs it, say, 1000 times, and returns how many p-values lie below 0.05 – i.e., the comparison’s estimated power.\n\ncompute.power &lt;- function(n.sims = 1000, popr12, popr34, n1, n2) {\n  return(mean(replicate(n.sims, \n                        compute.p(popr12 = popr12, popr34 = popr34, \n                                  n1 = n1, n2 = n2)\n                        ) &lt;= 0.05)\n         )\n}\n# Example\ncompute.power(n.sims = 1000, \n              n1 = 20, popr12 = 0.5,\n              n2 = 50, popr34 = 0.2)\n\nHere’s where the R fun begins. I want to compute the power not only for a single comparison, but for nearly the whole popr12 v. popr34 spectrum of possible comparisons: -0.95 v. -0.90, -0.95 v. -0.85, …, 0.7 v. -0.3 etc. All relevant correlations are stored in corrs:\n\ncorrs &lt;- seq(-0.95, 0.95, 0.05)\n\nUsing the outer function, I generate a grid featuring every possible combination of coefficients in corrs and run compute.power on each combination using mapply. Here, I estimate the power for a comparison with two samples of 20 observations.\n\nresults20 &lt;- outer(corrs, corrs, \n                   function(x, y) mapply(compute.power, \n                                         popr12 = x, popr34 = y, \n                                         n1 = 20, n2 = 20, \n                                         n.sims = 1000))\n\nWith contour, the results matrix is then visualised:\n\ncontour(x = corrs, y = corrs, z = results20, nlevels = 10, \n        labcex = 0.9, col = \"gray26\", at = seq(-1, 1, 0.2),\n        main = \"n = 20 in both samples\",\n        xlab = \"Correlation in population A\",\n        ylab = \"Correlation in population B\")\nabline(v = seq(-1, 1, 0.2), lwd = 1, col = \"lightgray\", lty = 2)\nabline(h = seq(-1, 1, 0.2), lwd = 1, col = \"lightgray\", lty = 2)\ncontour(x = corrs, y = corrs, z = results20, levels = 0.80, \n        drawlabels = FALSE, at = seq(-1, 1, 0.2),\n        add = TRUE, lwd = 3, col = \"steelblue3\")\n\nThis code could probably be optimised a bit; the power for the comparison between -0.5 and 0.3 is obviously identical to the power for the comparison between 0.5 and -0.3, for instance."
  },
  {
    "objectID": "posts/2015-04-14-power-simulations-for-comparing-independent-correlations/index.html#software-versions",
    "href": "posts/2015-04-14-power-simulations-for-comparing-independent-correlations/index.html#software-versions",
    "title": "Power simulations for comparing independent correlations",
    "section": "Software versions",
    "text": "Software versions\n\ndevtools::session_info()\n\n─ Session info ───────────────────────────────────────────────────────────────\n setting  value\n version  R version 4.3.1 (2023-06-16)\n os       Ubuntu 22.04.3 LTS\n system   x86_64, linux-gnu\n ui       X11\n language en_US\n collate  en_US.UTF-8\n ctype    en_US.UTF-8\n tz       Europe/Zurich\n date     2023-08-26\n pandoc   3.1.1 @ /usr/lib/rstudio/resources/app/bin/quarto/bin/tools/ (via rmarkdown)\n\n─ Packages ───────────────────────────────────────────────────────────────────\n package     * version date (UTC) lib source\n cachem        1.0.6   2021-08-19 [2] CRAN (R 4.2.0)\n callr         3.7.3   2022-11-02 [1] CRAN (R 4.3.1)\n cli           3.6.1   2023-03-23 [1] CRAN (R 4.3.0)\n codetools     0.2-19  2023-02-01 [4] CRAN (R 4.2.2)\n crayon        1.5.2   2022-09-29 [1] CRAN (R 4.3.1)\n devtools      2.4.5   2022-10-11 [1] CRAN (R 4.3.1)\n digest        0.6.29  2021-12-01 [2] CRAN (R 4.2.0)\n ellipsis      0.3.2   2021-04-29 [2] CRAN (R 4.2.0)\n evaluate      0.15    2022-02-18 [2] CRAN (R 4.2.0)\n fastmap       1.1.0   2021-01-25 [2] CRAN (R 4.2.0)\n fs            1.5.2   2021-12-08 [2] CRAN (R 4.2.0)\n glue          1.6.2   2022-02-24 [2] CRAN (R 4.2.0)\n htmltools     0.5.5   2023-03-23 [1] CRAN (R 4.3.0)\n htmlwidgets   1.6.2   2023-03-17 [1] CRAN (R 4.3.1)\n httpuv        1.6.11  2023-05-11 [1] CRAN (R 4.3.1)\n jsonlite      1.8.7   2023-06-29 [1] CRAN (R 4.3.1)\n knitr         1.39    2022-04-26 [2] CRAN (R 4.2.0)\n later         1.3.1   2023-05-02 [1] CRAN (R 4.3.1)\n lattice       0.21-8  2023-04-05 [4] CRAN (R 4.3.0)\n lifecycle     1.0.3   2022-10-07 [1] CRAN (R 4.3.0)\n magrittr      2.0.3   2022-03-30 [1] CRAN (R 4.3.0)\n MASS        * 7.3-60  2023-05-04 [4] CRAN (R 4.3.1)\n memoise       2.0.1   2021-11-26 [2] CRAN (R 4.2.0)\n mime          0.10    2021-02-13 [2] CRAN (R 4.0.2)\n miniUI        0.1.1.1 2018-05-18 [1] CRAN (R 4.3.1)\n mnormt        2.1.1   2022-09-26 [1] CRAN (R 4.3.1)\n nlme          3.1-162 2023-01-31 [4] CRAN (R 4.2.2)\n pkgbuild      1.4.2   2023-06-26 [1] CRAN (R 4.3.1)\n pkgload       1.3.2.1 2023-07-08 [1] CRAN (R 4.3.1)\n prettyunits   1.1.1   2020-01-24 [2] CRAN (R 4.2.0)\n processx      3.8.2   2023-06-30 [1] CRAN (R 4.3.1)\n profvis       0.3.8   2023-05-02 [1] CRAN (R 4.3.1)\n promises      1.2.0.1 2021-02-11 [1] CRAN (R 4.3.1)\n ps            1.7.5   2023-04-18 [1] CRAN (R 4.3.1)\n psych       * 2.3.6   2023-06-21 [1] CRAN (R 4.3.1)\n purrr         1.0.1   2023-01-10 [1] CRAN (R 4.3.0)\n R6            2.5.1   2021-08-19 [2] CRAN (R 4.2.0)\n Rcpp          1.0.11  2023-07-06 [1] CRAN (R 4.3.1)\n remotes       2.4.2   2021-11-30 [2] CRAN (R 4.2.0)\n rlang         1.1.1   2023-04-28 [1] CRAN (R 4.3.0)\n rmarkdown     2.21    2023-03-26 [1] CRAN (R 4.3.0)\n rstudioapi    0.14    2022-08-22 [1] CRAN (R 4.3.0)\n sessioninfo   1.2.2   2021-12-06 [2] CRAN (R 4.2.0)\n shiny         1.7.4.1 2023-07-06 [1] CRAN (R 4.3.1)\n stringi       1.7.12  2023-01-11 [1] CRAN (R 4.3.1)\n stringr       1.5.0   2022-12-02 [1] CRAN (R 4.3.0)\n urlchecker    1.0.1   2021-11-30 [1] CRAN (R 4.3.1)\n usethis       2.2.2   2023-07-06 [1] CRAN (R 4.3.1)\n vctrs         0.6.3   2023-06-14 [1] CRAN (R 4.3.0)\n xfun          0.39    2023-04-20 [1] CRAN (R 4.3.0)\n xtable        1.8-4   2019-04-21 [1] CRAN (R 4.3.1)\n yaml          2.3.5   2022-02-21 [2] CRAN (R 4.2.0)\n\n [1] /home/jan/R/x86_64-pc-linux-gnu-library/4.3\n [2] /usr/local/lib/R/site-library\n [3] /usr/lib/R/site-library\n [4] /usr/lib/R/library\n\n──────────────────────────────────────────────────────────────────────────────"
  },
  {
    "objectID": "posts/2018-10-29-natural-sequence/index.html",
    "href": "posts/2018-10-29-natural-sequence/index.html",
    "title": "A closer look at a classic study (Bailey et al. 1974)",
    "section": "",
    "text": "In this blog post, I take a closer look at the results of a classic study I sometimes discuss in my classes on second language acquisition. As I’ll show below, the strength of this study’s findings is strongly overexaggerated, presumably owing to a mechanical error.\nBailey et al.’s (1974) Is there a ‘natural sequence’ in adult second language learning? is a crisp paper that’s been cited over 1,000 times (according to Google Scholar). Here’s the relevant bit from the abstract:\n\n“The Bilingual Syntax Measure (…) was administered to 73 adult learners of English as a second language in order to investigate accuracy of usage for eight English functors. It was found that there is a highly consistent order of relative difficulty in the use of the functors across different language backgrounds, indicating that learners are experiencing intra‐language difficulties.”\n\nThis finding and others like it were quite readily interpreted, in the spirit of the age, as suggesting that learners’ L1 didn’t affect the order with which they acquired grammatical morphemes in the L2.\nThe conclusion that there is a “highly consistent order of relative difficulty” is based on the following finding:\n\n“Pearson product-moment correlations were performed on the relative accuracy of the use of the eight grammatical morphemes between Spanish and non-Spanish speakers (…). There was a significant correlation between relative accuracies of function words for Spanish and non-Spanish speakers (r = .926, p &lt; .005, one-tailed test).” (p. 238)\n\nWhile the study itself has been criticised on a number of grounds, the correlation coefficient of r = 0.926 has been reproduced in handbook chapters and overview articles without much comment. But based on Bailey et al.’s own data, their correlation coefficient isn’t anywhere near 0.926 to begin with.\nBailey et al. investigated how accurately L2 English learners used eight English grammatical morphemes. They then split up their learner sample into learners with Spanish as their native language and learners with another native language (including Italian, Greek, Hebrew, Chinese, and Japanese). The results were shown in a graph similar to Figure 1 below.\n\n\n\n\n\nFigure 1. A reproduction of Bailey et al.’s (1974) Figure 1.\n\n\n\n\nFigure 1 is a pretty non-standard way of showing the data underlying a correlation coefficient, so I extracted the (approximate) data points it contained and replotted them as a scatterplot for a class I teach:\n\n\n\n\n\nFigure 2. The same data as in Figure 1, but this time shown in a scatterplot.\n\n\n\n\nNow, if you’ve seen enough scatterplots in your life, you know that the relationship shown in Figure 2 doesn’t have a Pearson correlation of 0.93. Indeed, the actual Pearson correlation is r = 0.76. (The Spearman correlation is 0.47, in case you were wondering.)\nSo what went wrong? I end up with a Pearson correlation coefficient extremely close to Bailey et al.’s 0.926 when I independently sort both the Spanish and non-Spanish data from high to low. That is to say, I suspect that what went wrong here is that the authors inadvertently paired up the Spanish speakers’ accuracy on articles (their strong suit) with the non-Spanish speakers’ accuracy on -ing (their strong suit), the Spanish speakers’ accuracy on -ing (their 2nd best) with the non-Spanish speakers’ accuracy on copulas (their 2nd best), and so on. Obviously this yields a strong correlation coefficient – their rank orders are already perfectly correlated. But it’s also quite nonsensical.\nIncidentally, I don’t think correlation coefficients are too useful for addressing Bailey et al.’s question, and I think their study suffers from other methodological issues, too (e.g., the hodgepodge of L1 background collapsed into ‘other’, and whether accuracy on, say, past tense can meaningfully be compared with accuracy on articles). But I think the field is already well-versed in such conceptual methodological reflection, while being at the same time a tad too eager to take the quantitative results themselves on good faith.\nFor those interested, these are the data I extracted from Bailey et al.’s Figure 1. They won’t be perfectly accurate, but they should be close enough.\n\n\n     Functor Spanish Other\n1       -ing    0.81  0.87\n2     copula    0.80  0.85\n3  plural -s    0.81  0.76\n4   articles    0.89  0.72\n5  auxiliary    0.72  0.74\n6       past    0.51  0.80\n7     3ps -s    0.30  0.59\n8 possessive    0.31  0.53"
  },
  {
    "objectID": "posts/2018-06-27-phrasing-research-questions/index.html",
    "href": "posts/2018-06-27-phrasing-research-questions/index.html",
    "title": "A brief comment on research questions",
    "section": "",
    "text": "All too often, empirical studies in applied linguistics are run in order to garner evidence for a preordained conclusion. In such studies, the true, perhaps unstated, research question is more of a stated aim than a question: “With this study, we want to show that [our theoretical point of view is valuable; this teaching method of ours works pretty well; multilingual kids are incredibly creative; etc.].” The problem with aims such as these is that they take the bit between square brackets for granted, i.e., that the theoretical point of view is indeed valuable; that our teaching method really does work pretty well; or that multilingual kids indeed are incredibly creative – the challenge is merely to convince readers of these assumed facts by demonstrating them empirically. I think that such a mentality leads researchers to disregard evidence contradicting their assumption or explain it away as an artifact of a method that, in hindsight, wasn’t optimal.\nA healthier attitude is to formulate research questions as, well, questions: “We carried out this study since we wondered whether [our theory explains the data better than extant theories; our teaching method yields better results that the current one; multilingual kids are more creative than their peers; etc.].” Genuine research questions at least leave open the possibility that the theory doesn’t explain the data better than extant theories; that the new teaching method isn’t any better than the current one; or that multilingual kids aren’t more creative than their peers. I think that consciously phrasing research questions as genuine questions puts the emphasis on evaluating different possibilities rather than on convincing the audience of an assumed fact.\nYes/no questions obviously invite yes/no answers. When the answer to a yes/no question isn’t trivial, this is fine. But when the question boils down to a vague “Are there some differences between these groups?”, it’s often highly likely that the answer will be “yes”. In such cases, it may be more fruitful to phrase the research question as a wh-question instead: “We wondered how/when/under which circumstances/in which respects/to what extent these groups differ?” The answers to questions such as these may still be “very little”, “rarely”, “in hardly any”, etc., but that’s more informative than a trivial “yes”."
  },
  {
    "objectID": "posts/2018-07-04-bayesian-breakpoint-model/index.html",
    "href": "posts/2018-07-04-bayesian-breakpoint-model/index.html",
    "title": "Baby steps in Bayes: Piecewise regression",
    "section": "",
    "text": "Inspired by Richard McElreath’s excellent book Statistical rethinking: A Bayesian course with examples in R and Stan, I’ve started dabbling in Bayesian statistics. In essence, Bayesian statistics is an approach to statistical inference in which the analyst specifies a generative model for the data (i.e., an equation that describes the factors they suspect gave rise to the data) as well as (possibly vague) relevant information or beliefs that are external to the data proper. This information or these beliefs are then adjusted in light of the data observed.\nI’m hardly an expert in Bayesian statistics (or the more commonly encountered ‘orthodox’ or ‘frequentist’ statistics, for that matter), but I’d like to understand it better – not only conceptually, but also in terms of how the statistical model should be specified. While quite a few statisticians and methodologists tout Bayesian statistics for a variety of reasons, my interest is primarily piqued by the prospect of being able to tackle problems that would be impossible or at least awkward to tackle with the tools I’m pretty comfortable with at the moment.\nIn order to gain some familiarity with Bayesian statistics, I plan to set myself a couple of problems and track my efforts in solving them here in a Dear diary fashion. Perhaps someone else finds them useful, too.\nThe first problem that I’ll tackle is fitting a regression model in which the relationship between the predictor and the outcome may contain a breakpoint at one unknown predictor value. One domain in which such models are useful is in testing hypotheses that claim that the relationship between the age of onset of second language acquisition (AOA) and the level of ultimate attainment in that second language flattens after a certain age (typically puberty). It’s possible to fit frequentist breakpoint models, but estimating the breakpoint age is a bit cumbersome (see blog post Calibrating p-values in ‘flexible’ piecewise regression models). But in a Bayesian approach, it should be possible to estimate both the regression parameters as well as the breakpoint itself in the same model. That’s what I’ll try here."
  },
  {
    "objectID": "posts/2018-07-04-bayesian-breakpoint-model/index.html#software",
    "href": "posts/2018-07-04-bayesian-breakpoint-model/index.html#software",
    "title": "Baby steps in Bayes: Piecewise regression",
    "section": "Software",
    "text": "Software\nApart from R, you’ll need RStan. Follow the installation instructions on RStan’s GitHub page.\nOnce you’ve installed RStan, fire up a new R session and run these commands.\n\n# Load rstan package\nlibrary(rstan)\n# Avoid unnecessary recompiling\nrstan_options(auto_write = TRUE)\n# optional: Distribute work over multiple CPU cores\noptions(mc.cores = parallel::detectCores())"
  },
  {
    "objectID": "posts/2018-07-04-bayesian-breakpoint-model/index.html#simulating-some-data",
    "href": "posts/2018-07-04-bayesian-breakpoint-model/index.html#simulating-some-data",
    "title": "Baby steps in Bayes: Piecewise regression",
    "section": "Simulating some data",
    "text": "Simulating some data\nI’ll analyse some real data in a minute. But I think it’s useful to analyse some data I know the true data generating mechanism of first in order to make sure that the model works as intended. The commands below generate data with properties comparable to the real data I’ll analyse in a bit.\nThe first graph below shows the mean outcome value (‘GJT’, i.e., L2 grammaticality judgement task result) depending on the age of onset of acquisition. As you can see, there’s a bend in the function at age 10.\n\n# Set random seed (today's date)\nRNGversion(\"3.5.3\")\nset.seed(2018-07-04)\n\n# 80 data points\nn &lt;- 80\n\n# Generate 'age of acquisition' data (integers between 1 and 50)\nAOA &lt;- sample(1:50, size = n, replace = TRUE)\n\n# Set breakpoint at some plausible age\nBP &lt;- 10\n\n# Generate average values on GJT (~ 'grammar test') outcome\nmeanGJT &lt;- ifelse(AOA &lt; BP,\n                  176 - 1.2 * (AOA - BP),\n                  176 - 0.4 * (AOA - BP))\nplot(AOA, meanGJT, main = \"Underlying function\")\nsegments(x0 = BP, y0 = 0, y1 = 176, lty = 2)\nsegments(x0 = 0, x1 = BP, y0 = 176, lty = 2)\n\n\n\n\nFigure 1. In the simulated data, the underlying relationship between AOA and GJT has a steeper slope for AOA values below 10 than for AOA values over 10.\n\n\n\n\nIn the second graph, some random normal error has been added to these mean values; it is the data in this figure that I’ll analyse first.\n\n# Generate observed values\nerror &lt;- 3\nobsGJT &lt;- rnorm(n = n, mean = meanGJT, sd = error)\nplot(AOA, obsGJT, main = \"Observed data\")\n\n\n\n\n&gt; Figure 2. Interindividual differences obfuscate the nonlinear relationship and the true position of the breakpoint age somewhat."
  },
  {
    "objectID": "posts/2018-07-04-bayesian-breakpoint-model/index.html#specifying-the-model",
    "href": "posts/2018-07-04-bayesian-breakpoint-model/index.html#specifying-the-model",
    "title": "Baby steps in Bayes: Piecewise regression",
    "section": "Specifying the model",
    "text": "Specifying the model\nWhile there exist some (truly excellent) front-end interfaces for fitting Bayesian models (e.g., brms), I’ll specify the model in RStan proper. This is considerably more involved than writing out a model using R’s lm() function, but this added complexity buys you something in terms of flexibility.\nA Stan model specification has three required blocks.\n\ndata\nThis is where you specify what the input data looks like. Below I specified that the model should accept two variables (GJT and AOA) both with the same number of observations (N). Unlike lm(), stan() accepts non-rectangular data (e.g., variables with different lengths), so you need to prespecify the number of observations per variable.\n\n\nparameters\nThe model parameters you want to estimate. A breakpoint regression model has five parameters:\n\nThe breakpoint. I constrained the breakpoint to be between 1 and 20 since breakpoints beyond that range are inconsistent with any proposed theory;\nthe slope of the regression before the breakpoint;\nthe slope of the regression after the breakpoint;\na constant term (‘intercept’), most easily written as the expected outcome value at the breakpoint;\nthe standard deviation of the normal error. Standard deviations are always positive; this constraint is set by including &lt;lower = 0&gt; in the declaration. (Incidentally, the error term doesn’t have to be normal.)\n\n\n\nmodel\nThis is where you specify how the parameters and the data relate to each other. The assumed (and for the simulated data: correct) data generating mechanism is that the observed GJT values were drawn from a normal distribution whose mean depends on the participant’s AOA (see transformed parameters below) and the breakpoint and which has the same standard deviation everywhere.\nYou also have to provide so-called prior distributions for any parameters. These encode the information or beliefs you have about the parameters which you didn’t need the data for. I set the following priors:\n\nA truncated normal prior for the breakpoint centred at 12 and with a standard deviation of 6. The prior is truncated at 1 and at 20; this was specified in the parameters block. This prior essentially encodes that, for all I know, the breakpoint occurs somewhere between the ages of 1 and 20 and is slightly more likely to occur around age 10 to 14 and around ages 2 or 19. I tried specifying a uniform prior, but that didn’t work.\nNormal priors centred around 0 and with standard deviations of 5 for both slope parameters. What this means is that I think it’s highly unlikely that these slopes are incredibly steep (say, a 100-point increase or decrease per additional AOA). These priors aren’t particularly informative, though: According to them, negative and positive slopes are equally likely. If you have a sufficient amount of data, such priors only have a minimal effect on the results. But when you don’t have this luxury, even such slightly informative priors may be better than none at all for keeping the inferences within reasonable bounds.\nA normal prior centred around 150 with a standard deviation of 25 for the intercept. This essentially means that I expect the average outcome at the breakpoint to lie somewhere between 100 and 200. For the real data I’ll analyse later, this assumption is reasonable enough since the data were pretty much guaranteed to be bound between 102 and 204.\nA half-normal prior starting at 0 with a standard deviation of 20 for the standard deviation of the residuals. The normal part is specified in this prior, the half part results from the constraint set in the parameters block. A half-normal distribution starting at 0 with a standard deviation of 20 in essence encodes the belief that the residual error will probably have a standard deviation of less than 2*20 = 40, with smaller values being more likely than large ones. If you don’t set a prior for this parameter (or any other parameter, for that matter), a uniform prior spanning to infinity is assumed. So even when you don’t specify a prior, you’re using one.\n\nI also added two optional blocks:\n\n\ntransformed parameters\nThis block specifies derivations of model parameters, be it because they’re the actual object of inference or just because it simplifies the notation. I specified two derived parameters.\n\nconditional_mean describes the outcome of the regression equation without the error term for each observation:\n\nIf the participant’s AOA is before the breakpoint, conditional_mean = intercept + slope1 * AOA.\nIf the participant’s AOA is after the breakpoint, conditional_mean = intercept + slope2 * AOA.\n\nslope_difference is just the difference between the slope after and the slope before the breakpoint.\n\n\n\ngenerated quantities\nHere you can specify some model outputs. I specified three such outputs:\n\nsim_GJT: Using the normal_rng() function, I simulate new GJT data from the model for each AOA observation in the original dataset. If the model is approximately accurate, the actually observed data should look fairly similar to these simulated data points. I’ll check this later.\nlog_lik: I won’t discuss this in this post.\nsim_conditional_mean: For each AOA between 1 and 50 (hence: a vector of length 50), I’ll ask the model to output what it thinks is the conditional GJT mean. This will be useful for drawing effect plots.\n\n\nbp_code &lt;- '\n// You need to specify the kind of input data, incl. number of observations.\ndata { \n  int&lt;lower=1&gt; N;  // total number of observations (integer); at least 1\n  real GJT[N];     // outcome variable with N elements (real-valued)\n  real AOA[N];     // predictor variable with N elements (real-valued)\n}\n\n// the parameters to be estimated from the data\nparameters { \n  real intercept;                 // = predicted outcome at breakpoint\n  real slope_before;              // slope before the breakpoint\n  real slope_after;               // slope after the breakpoint\n  real&lt;lower = 1, upper = 20&gt; bp; // the breakpoint age, with some constraints\n  real&lt;lower = 0&gt; error;          // standard deviation of residuals\n                                  //  (always positive, hence &lt;lower = 0&gt;)\n} \n\n// Functions of estimated parameters.\ntransformed parameters{\n  vector[N] conditional_mean; // the estimated average GJT for each AOA observation\n  real slope_difference;      // the difference between slope_after and slope_before\n\n  slope_difference = slope_after - slope_before;  \n\n  // conditional_mean depends on whether AOA is before or after bp\n  for (i in 1:N) {\n    if (AOA[i] &lt; bp) {\n      conditional_mean[i] = intercept + slope_before * (AOA[i] - bp);\n    } else {\n      conditional_mean[i] = intercept + slope_after * (AOA[i] - bp);\n    }\n  }\n}\n\n// The model itself specifies how the data are expected to have\n// been generated and what the prior expectations for the model parameters are.\nmodel {\n  // Set priors\n  intercept ~ normal(150, 25);  // Average GJT at breakpoint\n  slope_before ~ normal(0, 5);  // Slope before breakpoint\n  slope_after ~ normal(0, 5);   // Slope after breakpoint\n  bp ~ normal(12, 6);           // Breakpoint age, pretty wide, but somewhere in childhood/puberty\n  error ~ normal(0, 20);        // Residual error, likely between 0 and 2*20\n  \n  // How the data are expected to have been generated:\n  // normal distribution with mu = conditional_mean and \n  // std = error, estimated from data.\n  for (i in 1:N) {\n    GJT[i] ~ normal(conditional_mean[i], error);\n  }\n}\n\ngenerated quantities {\n  vector[N] sim_GJT;               // Simulate new data using estimated parameters.\n  vector[N] log_lik;               // Useful for model comparisons; not done here.\n  vector[50] sim_conditional_mean; // Useful for plotting.\n\n  // Compute conditional means for AOAs between 1 and 50.\n  for (i in 1:50) {\n    if (i &lt; bp) {\n      sim_conditional_mean[i] = intercept + slope_before * (i - bp);\n    } else {\n      sim_conditional_mean[i] = intercept + slope_after * (i - bp);\n    }\n  }\n\n  for (i in 1:N) {\n    sim_GJT[i] = normal_rng(conditional_mean[i], error);\n    log_lik[i] = normal_lpdf(GJT[i] | conditional_mean[i], error);\n  }\n}\n'"
  },
  {
    "objectID": "posts/2018-07-04-bayesian-breakpoint-model/index.html#running-the-model",
    "href": "posts/2018-07-04-bayesian-breakpoint-model/index.html#running-the-model",
    "title": "Baby steps in Bayes: Piecewise regression",
    "section": "Running the model",
    "text": "Running the model\nTo fit the model, first put the input data in a list. Then supply this list and the model code to the stan() function. The stan() function prints a lot of output to the console, which I didn’t reproduce here. Unless you receive genuine warnings or error (i.e., red text), everything’s fine.\n\ndata_list &lt;- list(\n  AOA = AOA,\n  GJT = obsGJT,\n  N = length(AOA)\n)\nfit_bp_sim &lt;- stan(model_code = bp_code, \n                   data = data_list,\n                   iter = 8000)"
  },
  {
    "objectID": "posts/2018-07-04-bayesian-breakpoint-model/index.html#inspecting-the-model",
    "href": "posts/2018-07-04-bayesian-breakpoint-model/index.html#inspecting-the-model",
    "title": "Baby steps in Bayes: Piecewise regression",
    "section": "Inspecting the model",
    "text": "Inspecting the model\n\nModel summary\nA summary with the parameter estimates and their uncertainties can be generated using the print() function.\n\nprint(fit_bp_sim,\n      par = c(\"intercept\", \"bp\", \"slope_before\", \"slope_after\", \"slope_difference\", \"error\"))\n\nInference for Stan model: anon_model.\n4 chains, each with iter=8000; warmup=4000; thin=1; \npost-warmup draws per chain=4000, total post-warmup draws=16000.\n\n                   mean se_mean   sd   2.5%    25%    50%    75%  97.5% n_eff\nintercept        175.41    0.02 1.36 172.99 174.53 175.32 176.20 178.36  4271\nbp                10.31    0.03 2.32   5.73   8.91  10.33  11.69  15.11  4639\nslope_before      -1.21    0.01 0.36  -1.97  -1.35  -1.16  -1.00  -0.74  3927\nslope_after       -0.38    0.00 0.03  -0.44  -0.40  -0.38  -0.36  -0.32  6081\nslope_difference   0.83    0.01 0.36   0.34   0.62   0.78   0.97   1.57  4119\nerror              2.92    0.00 0.24   2.49   2.75   2.90   3.07   3.44  8623\n                 Rhat\nintercept           1\nbp                  1\nslope_before        1\nslope_after         1\nslope_difference    1\nerror               1\n\nSamples were drawn using NUTS(diag_e) at Sun Aug  6 15:33:51 2023.\nFor each parameter, n_eff is a crude measure of effective sample size,\nand Rhat is the potential scale reduction factor on split chains (at \nconvergence, Rhat=1).\n\n\nFor each parameter, the mean column contains the mean estimate of that parameter, whereas the 50% column contains its median estimate. The sd column shows the standard deviation of the parameter estimates; this corresponds to the parameter estimate’s standard error. The 2.5%, 25%, 75% and 97.5% columns contain the respective percentiles of the distribution of the parameter estimates. So the average estimated breakpoint (bp) occurs somewhere between age 10 and 11, with 95% of the estimates contained in an interval between roughly 6 and 15 years. Similarly, the average estimated slope before the breakpoint is about -1.2 with a 95% ‘credibility’ interval from -1.98 to -0.75, and so on. The parameter estimates, then, are neatly centred around their true values, suggesting that the model does what it’s supposed to do.\n\n\nPosterior predictive checks\nIf the model is any good, data simulated from it should be pretty similar to the data actually observed. In the generated quantities block, I let the model output such simulated data (sim_GJT). Using the shinystan package, we can perform some ‘posterior predictive checks’:\n\nshinystan::launch_shinystan(fit_bp_sim)\n\nClick ‘Diagnose’ &gt; ‘PPcheck’. Under ‘Select y (vector of observations)’, pick obsGJT (the simulated data analysed above). Under ‘Parameter/generated quantity from model’, pick sim_GJT (the additional simulated data generated in the model code). Then click on ‘Distributions of observed data vs replications’ and ‘Distributions of test statistics’ to check if the properties of the simulated data correspond to those of the real data.\nYou can also take this a step further and check whether the model is able to generate scatterplots similar to the one in Figure 2. If the following doesn’t make any immediate sense, please refer to the blog post Checking model assumptions without getting paranoid, because the logic is pretty similar.\nFirst extract some vectors of simulated data from the model output:\n\n# rstan's 'extract' is likely to conflict with another function\n# called 'extract', so specify the package, too.\nsimulated_data &lt;- rstan::extract(fit_bp_sim)$sim_GJT\n# simulated_data is a matrix with 4000 rows and 80 columns.\n# For the plot, I select 8 rows at random:\nsimulated_data &lt;- simulated_data[sample(1:4000, 8), ]\n\nThen plot both the observed vectors and the simulated vectors:\n\npar(mfrow = c(3, 3))\n\n# Plot the observed data\nplot(data_list$AOA, data_list$GJT,\n     xlab = \"AOA\", ylab = \"GJT\",\n     main = \"observed\")\n\n# Plot the simulated data\nfor (i in 1:8) {\n  plot(data_list$AOA, simulated_data[i, ],\n       xlab = \"AOA\", ylab = \"GJT\",\n       main = \"simulated\")\n}\n\n\n\n\nFigure 3. The actual input data (top left) and eight simulated datasets. If the simulated datasets are highly similar to the actual data, the model was able to learn the relevant patterns in the data.\n\n\n\npar(mfrow = c(1, 1))\n\nThe simulated data look pretty much identical to the observed data, again demonstrating that the model is doing a pretty good job of learning the patterns in the data. This isn’t surprising, since I knew how the data were generated and constructed the model correspondingly. But it’s reassuring.\n(Incidentally, I’m sure it’s possible to generate lineups more similar to the ones in that previous blog post, but this blog post is long as it is already.)\n\n\nEffect plot\nTo visualise the model, you can draw an effect plot showing the average estimated relationship between AOA and GJT as well as the uncertainty about this relationship. To this end, I had the model output vectors of the fitted conditional means for AOAs 1 through 50 (sim_conditional_mean). With the commands below, I extract these vectors and then compute their mean values as well as some percentiles at each AOA.\n\nsim_conditional_means &lt;- rstan::extract(fit_bp_sim)$sim_conditional_mean\ndf_sim_cond_means &lt;- data.frame(\n  AOA = 1:50,\n  meanGJT = apply(sim_conditional_means, 2, mean),\n  lo80GJT = apply(sim_conditional_means, 2, quantile, 0.10),\n  hi80GJT = apply(sim_conditional_means, 2, quantile, 0.90),\n  lo95GJT = apply(sim_conditional_means, 2, quantile, 0.025),\n  hi95GJT = apply(sim_conditional_means, 2, quantile, 0.975)\n)\nhead(df_sim_cond_means)\n\n  AOA  meanGJT  lo80GJT  hi80GJT  lo95GJT  hi95GJT\n1   1 186.0867 184.3968 187.7745 183.4985 188.7348\n2   2 184.8770 183.4161 186.3232 182.6086 187.1155\n3   3 183.6743 182.4180 184.9381 181.6091 185.5990\n4   4 182.4817 181.3312 183.6296 180.4895 184.1939\n5   5 181.2997 180.1643 182.4048 179.2258 182.9274\n6   6 180.1329 178.9212 181.2531 177.9611 181.7475\n\n\nThese mean values and percentiles can then be plotted as follows; the black line shows the average regression line, the light grey ribbon its 95% credibility region, and the dark grey ribbon its 80% credibility region.\n\nlibrary(ggplot2)\nggplot(df_sim_cond_means,\n       aes(x = AOA,\n           y = meanGJT)) +\n  geom_ribbon(aes(ymin = lo95GJT,\n                  ymax = hi95GJT),\n              fill = \"lightgrey\") +\n  geom_ribbon(aes(ymin = lo80GJT,\n                  ymax = hi80GJT),\n              fill = \"darkgrey\") +\n  geom_line()\n\n\n\n\nFigure 4. The modelled relationship between AOA and GJT for the made-up data with 80% and 95% credibility regions. The bend around AOA = 10 is noticeable but it smoothed out due to the uncertainty about the precise position of the breakpoint."
  },
  {
    "objectID": "posts/2018-07-04-bayesian-breakpoint-model/index.html#and-now-for-real",
    "href": "posts/2018-07-04-bayesian-breakpoint-model/index.html#and-now-for-real",
    "title": "Baby steps in Bayes: Piecewise regression",
    "section": "And now for real",
    "text": "And now for real\nLet’s now analyse some real data using the same model. These data stem from a study by DeKeyser et al. (2010).\n\nd &lt;- read.csv(\"http://homeweb.unifr.ch/VanhoveJ/Pub/papers/CPH/DeKeyser2010NorthAmerica.csv\")\ndata_list &lt;- list(\n  AOA = d$AOA,\n  GJT = d$GJT,\n  N = nrow(d)\n)\nplot(data_list$AOA, data_list$GJT)\n\n\n\n\nFigure 5. AOA–GJT relationship as observed in DeKeyser et al.’s (2010) North America study.\n\n\n\n\nLet’s fit the model:\n\nfit_bp &lt;- stan(model_code = bp_code, data = data_list, iter = 8000, refresh = 0)\n\nAnd output summary statistics:\n\nprint(fit_bp,\n      par = c(\"intercept\", \"bp\", \"slope_before\", \"slope_after\", \"slope_difference\", \"error\"))\n\nInference for Stan model: anon_model.\n4 chains, each with iter=8000; warmup=4000; thin=1; \npost-warmup draws per chain=4000, total post-warmup draws=16000.\n\n                   mean se_mean   sd   2.5%    25%    50%    75%  97.5% n_eff\nintercept        172.27    0.13 7.40 159.77 166.71 171.41 177.45 187.49  3385\nbp                12.50    0.07 4.30   3.32   9.40  13.29  15.82  19.25  3436\nslope_before      -2.83    0.04 2.38  -7.65  -3.85  -2.85  -1.98   3.29  3759\nslope_after       -1.13    0.00 0.13  -1.37  -1.22  -1.13  -1.04  -0.86  5177\nslope_difference   1.70    0.04 2.41  -4.45   0.83   1.76   2.77   6.51  3739\nerror             16.38    0.01 1.37  13.94  15.42  16.30  17.24  19.32  8827\n                 Rhat\nintercept           1\nbp                  1\nslope_before        1\nslope_after         1\nslope_difference    1\nerror               1\n\nSamples were drawn using NUTS(diag_e) at Sun Aug  6 15:35:52 2023.\nFor each parameter, n_eff is a crude measure of effective sample size,\nand Rhat is the potential scale reduction factor on split chains (at \nconvergence, Rhat=1).\n\n\nThe model doesn’t seem to have learnt a whole lot about the position of the breakpoint: the 95% credibility interval ranges from age 3 till age 19. Furthermore, it doesn’t really seem to know about what happens at this breakpoint: the 95% CrI for the difference between the after and the before slopes ranges from about -5.1 till 6.4.\nWe ought to perform some posterior predictive checks to make sure the model makes sense, though:\n\n# rstan's 'extract' is likely to conflict with another function\n# called 'extract', so specify the package, too.\nsimulated_data &lt;- rstan::extract(fit_bp)$sim_GJT\n# simulated_data is a matrix with 4000 rows and 80 columns.\n# For the plot, I select 8 rows at random:\nsimulated_data &lt;- simulated_data[sample(1:4000, 8), ]\n\n\npar(mfrow = c(3, 3))\n\n# Plot the observed data\nplot(data_list$AOA, data_list$GJT,\n     xlab = \"AOA\", ylab = \"GJT\",\n     main = \"observed\")\n\n# Plot the simulated data\nfor (i in 1:8) {\n  plot(data_list$AOA, simulated_data[i, ],\n       xlab = \"AOA\", ylab = \"GJT\",\n       main = \"simulated\")\n}\n\n\n\n\n&gt; Figure 6. The actual input data (top left) and eight simulated datasets. Some patterns in the simulated data couldn’t have occurred in the actual dataset: the maximum possible GJT result was 204, yet a couple of datasets contain values above that. This is something that may be worth taking into account in a more refined model, but baby steps.\n\n\n\npar(mfrow = c(1, 1))\n\nFigure 6 suggests that it may be possible to improve the model since the simulated data display some patterns that would have been impossible to observe in the actual study (viz., GJT values larger than 204). But this should suffice for now.\nAs a final step, we can draw an effect plot as before:\n\nsim_conditional_means &lt;- extract(fit_bp)$sim_conditional_mean\ndf_sim_cond_means &lt;- data.frame(\n  AOA = 1:50,\n  meanGJT = apply(sim_conditional_means, 2, mean),\n  lo80GJT = apply(sim_conditional_means, 2, quantile, 0.10),\n  hi80GJT = apply(sim_conditional_means, 2, quantile, 0.90),\n  lo95GJT = apply(sim_conditional_means, 2, quantile, 0.025),\n  hi95GJT = apply(sim_conditional_means, 2, quantile, 0.975)\n)\n\n\nggplot(df_sim_cond_means,\n       aes(x = AOA,\n           y = meanGJT)) +\n  geom_ribbon(aes(ymin = lo95GJT,\n                  ymax = hi95GJT),\n              fill = \"lightgrey\") +\n  geom_ribbon(aes(ymin = lo80GJT,\n                  ymax = hi80GJT),\n              fill = \"darkgrey\") +\n  geom_line()\n\n\n\n\nFigure 7. Effect plot for the piecewise regression model applied to DeKeyser et al.’s (2010) North America data. There is substantial uncertainty about whether the regression line should indeed contain a breakpoint.\n\n\n\n\nGiven the uncertainty about the position of the breakpoint and what happens to the regression line at that breakpoint, it would make sense to fit a linear regression model to these data and then estimate how much allowing for a breakpoint actually buys us in terms of fit to the data. This is why I had the model generate log_lik values, too, but I’ll discuss those another time."
  },
  {
    "objectID": "posts/2018-07-04-bayesian-breakpoint-model/index.html#software-versions",
    "href": "posts/2018-07-04-bayesian-breakpoint-model/index.html#software-versions",
    "title": "Baby steps in Bayes: Piecewise regression",
    "section": "Software versions",
    "text": "Software versions\nPlease note that I reran the code on this page on August 6, 2023.\n\ndevtools::session_info()\n\n─ Session info ───────────────────────────────────────────────────────────────\n setting  value\n version  R version 4.3.1 (2023-06-16)\n os       Ubuntu 22.04.2 LTS\n system   x86_64, linux-gnu\n ui       X11\n language en_US\n collate  en_US.UTF-8\n ctype    en_US.UTF-8\n tz       Europe/Zurich\n date     2023-08-06\n pandoc   3.1.1 @ /usr/lib/rstudio/resources/app/bin/quarto/bin/tools/ (via rmarkdown)\n\n─ Packages ───────────────────────────────────────────────────────────────────\n package      * version date (UTC) lib source\n cachem         1.0.6   2021-08-19 [2] CRAN (R 4.2.0)\n callr          3.7.3   2022-11-02 [1] CRAN (R 4.3.1)\n cli            3.6.1   2023-03-23 [1] CRAN (R 4.3.0)\n codetools      0.2-19  2023-02-01 [4] CRAN (R 4.2.2)\n colorspace     2.1-0   2023-01-23 [1] CRAN (R 4.3.0)\n crayon         1.5.2   2022-09-29 [1] CRAN (R 4.3.1)\n curl           5.0.1   2023-06-07 [1] CRAN (R 4.3.1)\n devtools       2.4.5   2022-10-11 [1] CRAN (R 4.3.1)\n digest         0.6.29  2021-12-01 [2] CRAN (R 4.2.0)\n dplyr          1.1.2   2023-04-20 [1] CRAN (R 4.3.0)\n ellipsis       0.3.2   2021-04-29 [2] CRAN (R 4.2.0)\n evaluate       0.15    2022-02-18 [2] CRAN (R 4.2.0)\n fansi          1.0.4   2023-01-22 [1] CRAN (R 4.3.1)\n fastmap        1.1.0   2021-01-25 [2] CRAN (R 4.2.0)\n fs             1.5.2   2021-12-08 [2] CRAN (R 4.2.0)\n generics       0.1.3   2022-07-05 [1] CRAN (R 4.3.0)\n ggplot2      * 3.4.2   2023-04-03 [1] CRAN (R 4.3.0)\n glue           1.6.2   2022-02-24 [2] CRAN (R 4.2.0)\n gridExtra      2.3     2017-09-09 [1] CRAN (R 4.3.0)\n gtable         0.3.3   2023-03-21 [1] CRAN (R 4.3.0)\n htmltools      0.5.5   2023-03-23 [1] CRAN (R 4.3.0)\n htmlwidgets    1.6.2   2023-03-17 [1] CRAN (R 4.3.1)\n httpuv         1.6.11  2023-05-11 [1] CRAN (R 4.3.1)\n inline         0.3.19  2021-05-31 [1] CRAN (R 4.3.1)\n jsonlite       1.8.7   2023-06-29 [1] CRAN (R 4.3.1)\n knitr          1.39    2022-04-26 [2] CRAN (R 4.2.0)\n later          1.3.1   2023-05-02 [1] CRAN (R 4.3.1)\n lifecycle      1.0.3   2022-10-07 [1] CRAN (R 4.3.0)\n loo            2.6.0   2023-03-31 [1] CRAN (R 4.3.1)\n magrittr       2.0.3   2022-03-30 [1] CRAN (R 4.3.0)\n matrixStats    1.0.0   2023-06-02 [1] CRAN (R 4.3.1)\n memoise        2.0.1   2021-11-26 [2] CRAN (R 4.2.0)\n mime           0.10    2021-02-13 [2] CRAN (R 4.0.2)\n miniUI         0.1.1.1 2018-05-18 [1] CRAN (R 4.3.1)\n munsell        0.5.0   2018-06-12 [1] CRAN (R 4.3.0)\n pillar         1.9.0   2023-03-22 [1] CRAN (R 4.3.0)\n pkgbuild       1.4.2   2023-06-26 [1] CRAN (R 4.3.1)\n pkgconfig      2.0.3   2019-09-22 [2] CRAN (R 4.2.0)\n pkgload        1.3.2.1 2023-07-08 [1] CRAN (R 4.3.1)\n prettyunits    1.1.1   2020-01-24 [2] CRAN (R 4.2.0)\n processx       3.8.2   2023-06-30 [1] CRAN (R 4.3.1)\n profvis        0.3.8   2023-05-02 [1] CRAN (R 4.3.1)\n promises       1.2.0.1 2021-02-11 [1] CRAN (R 4.3.1)\n ps             1.7.5   2023-04-18 [1] CRAN (R 4.3.1)\n purrr          1.0.1   2023-01-10 [1] CRAN (R 4.3.0)\n R6             2.5.1   2021-08-19 [2] CRAN (R 4.2.0)\n Rcpp           1.0.11  2023-07-06 [1] CRAN (R 4.3.1)\n RcppParallel   5.1.7   2023-02-27 [1] CRAN (R 4.3.1)\n remotes        2.4.2   2021-11-30 [2] CRAN (R 4.2.0)\n rlang          1.1.1   2023-04-28 [1] CRAN (R 4.3.0)\n rmarkdown      2.21    2023-03-26 [1] CRAN (R 4.3.0)\n rstan        * 2.26.22 2023-08-01 [1] local\n rstudioapi     0.14    2022-08-22 [1] CRAN (R 4.3.0)\n scales         1.2.1   2022-08-20 [1] CRAN (R 4.3.0)\n sessioninfo    1.2.2   2021-12-06 [2] CRAN (R 4.2.0)\n shiny          1.7.4.1 2023-07-06 [1] CRAN (R 4.3.1)\n StanHeaders  * 2.26.27 2023-06-14 [1] CRAN (R 4.3.1)\n stringi        1.7.12  2023-01-11 [1] CRAN (R 4.3.1)\n stringr        1.5.0   2022-12-02 [1] CRAN (R 4.3.0)\n tibble         3.2.1   2023-03-20 [1] CRAN (R 4.3.0)\n tidyselect     1.2.0   2022-10-10 [1] CRAN (R 4.3.0)\n urlchecker     1.0.1   2021-11-30 [1] CRAN (R 4.3.1)\n usethis        2.2.2   2023-07-06 [1] CRAN (R 4.3.1)\n utf8           1.2.3   2023-01-31 [1] CRAN (R 4.3.1)\n V8             4.3.0   2023-04-08 [1] CRAN (R 4.3.0)\n vctrs          0.6.3   2023-06-14 [1] CRAN (R 4.3.0)\n withr          2.5.0   2022-03-03 [2] CRAN (R 4.2.0)\n xfun           0.39    2023-04-20 [1] CRAN (R 4.3.0)\n xtable         1.8-4   2019-04-21 [1] CRAN (R 4.3.1)\n yaml           2.3.5   2022-02-21 [2] CRAN (R 4.2.0)\n\n [1] /home/jan/R/x86_64-pc-linux-gnu-library/4.3\n [2] /usr/local/lib/R/site-library\n [3] /usr/lib/R/site-library\n [4] /usr/lib/R/library\n\n──────────────────────────────────────────────────────────────────────────────"
  },
  {
    "objectID": "posts/2016-06-13-drawing-a-linechart/index.html",
    "href": "posts/2016-06-13-drawing-a-linechart/index.html",
    "title": "Tutorial: Drawing a line chart",
    "section": "",
    "text": "Graphs are incredibly useful both for understanding your own data and for communicating your insights to your audience. This is why the next few blog posts will consist of tutorials on how to draw four kinds of graphs that I find most useful: scatterplots, line charts, boxplots and some variations, and Cleveland dotplots. These tutorials are aimed primarily at the students in our MA programme. Today’s graph: the line chart."
  },
  {
    "objectID": "posts/2016-06-13-drawing-a-linechart/index.html#whats-a-linechart",
    "href": "posts/2016-06-13-drawing-a-linechart/index.html#whats-a-linechart",
    "title": "Tutorial: Drawing a line chart",
    "section": "What’s a linechart?",
    "text": "What’s a linechart?\nA line chart is quite simply a graph in which data points that belong together are connected with a line. If you want to compare different groups, as we do below, you can use lines of different colours or different types to highlight differences between the groups.\nAs an aside, line charts are often used to plot the development of a variable over time—the tutorial below is an example of this—and I used to think that linecharts should only be used when time was involved, that connecting data points using lines was somehow not kosher otherwise. But now I’m fine with using line charts even if time isn’t involved: the lines often highlight the patterns in the data much better than a handful of unconnected symbols do."
  },
  {
    "objectID": "posts/2016-06-13-drawing-a-linechart/index.html#tutorial-drawing-a-linechart-in-ggplot2",
    "href": "posts/2016-06-13-drawing-a-linechart/index.html#tutorial-drawing-a-linechart-in-ggplot2",
    "title": "Tutorial: Drawing a line chart",
    "section": "Tutorial: Drawing a linechart in ggplot2",
    "text": "Tutorial: Drawing a linechart in ggplot2\nIn this tutorial, you’ll learn how to draw a basic linechart and how you can tweak it. You’ll also learn how to quickly partition a dataset according to several variables and compute summary statistics within each part. For this, we’ll make use of the free statistical program R and the add-on packages ggplot2, magrittr and dplyr. Working with these programs and packages may be irksome at first if you’re used to pull-down menus, but the trouble is well worth it.\nUpdate (2023-08-08): By now, the ggplot2, magrittr and dplyr packages have been integrated into the tidyverse suite. We’ll use that instead.\n\nWhat you’ll need\n\nThe free program R.\nThe graphical user interface RStudio – also free. Download and install R first and only then RStudio.\n\nI’m going to assume some familiarity with these programs. Specifically, I’ll assume that you know how to enter commands in RStudio and import datasets stored in the CSV file format. If you need help with this, see Chapter 3 of my introduction to statistics (in German) or Google importing data R.\n\nThe ggplot2, magrittr and dplyr add-on packages for R. To install them, simply enter the following command at the prompt in RStudio.\n\n\ninstall.packages(c(\"ggplot2\", \"magrittr\", \"dplyr\"))\n# Update (2023-08-08): use 'install.packages(\"tidyverse\")' instead.\n\n\nA dataset. For this tutorial, we’ll use a dataset on the acquisition of morphological cues to agency that my students compiled. It consists of the responses of 70 learners (SubjectID) who were assigned to one of three learning conditions (BiasCondition) – the details don’t matter much for our purposes. All learners completed three kinds of tasks (Task): understanding sentences in an unknown languages, judging the grammaticality of sentences in the same languages, and producing sentences in this language. These tasks occurred in Blocks. The learners’ responses were tracked throughout the experiment (ResponseCorrect). Download this dataset to your hard disk.\n\n\n\nPreliminaries\nIn RStudio, read in the data.\n\nsemproj &lt;- read.csv(file.choose())\n\nIf the summary looks like this, you’re good to go.\n\nsummary(semproj)\n\n                            SubjectID           BiasCondition      Block     \n 0krwma8qskny4r4d1za1gqsp3hp78y4s:  96   RuleBasedInput:2496   Min.   :1.00  \n 0pmm7rhegvxjttmjla7zyv0qrfwlv0a0:  96   StrongBias    :2112   1st Qu.:1.75  \n 0qyn6np5fgyrmqjfbqjq68fcvs61y2gu:  96   WeakBias      :2112   Median :2.50  \n 0uet846f755xvnm9ow9phkusz2zbhgac:  96                         Mean   :2.50  \n 0vm3nrrqdd5mnbncnmon7sjp5f7fz4z8:  96                         3rd Qu.:3.25  \n 12athefgbh4zetjy4y2tn2148p713c6x:  96                         Max.   :4.00  \n (Other)                         :6144                                       \n ResponseCorrect            Task     \n no :1670        Comprehension:4480  \n yes:5050        GJT          :1120  \n                 Production   :1120  \n                                     \n                                     \n                                     \n                                     \n\n\nNow load the packages we’ll be using. You may get a message that some ‘objects are masked’, but that’s nothing to worry about.\n\nlibrary(tidyverse)\n\n\n\nSummarising a data frame\nWe want to compare how response accuracy develops block by block in the different experimental conditions. To that end, we need to calculate the proportion of correct responses by each learner in each block and for each task. The dplyr and magrittr packages make doing so easy.\nThe following lines of code create a new data frame called semproj_perParticipant that was constructed by taking the dataset semproj (first line), grouping it by the variables SubjectID, BiasCondition, Block and Task (second line), and within each ‘cell’ calculating the proportion of entries in ResponseCorrect that read \"yes\" (third line).\n\nsemproj_perParticipant &lt;- semproj |&gt;\n  group_by(SubjectID, BiasCondition, Block, Task) |&gt;\n  summarise(ProportionCorrect = mean(ResponseCorrect == \"yes\"),\n            .groups = \"drop\")\n\nType the name of the new data frame at the prompt. If you see something like this, everything’s fine.\n\nsemproj_perParticipant\n\n# A tibble: 840 × 5\n   SubjectID                        BiasCondition Block Task   ProportionCorrect\n   &lt;fct&gt;                            &lt;fct&gt;         &lt;int&gt; &lt;fct&gt;              &lt;dbl&gt;\n 1 0krwma8qskny4r4d1za1gqsp3hp78y4s StrongBias        1 Compr…             0.688\n 2 0krwma8qskny4r4d1za1gqsp3hp78y4s StrongBias        1 GJT                0.5  \n 3 0krwma8qskny4r4d1za1gqsp3hp78y4s StrongBias        1 Produ…             0    \n 4 0krwma8qskny4r4d1za1gqsp3hp78y4s StrongBias        2 Compr…             0.375\n 5 0krwma8qskny4r4d1za1gqsp3hp78y4s StrongBias        2 GJT                0.5  \n 6 0krwma8qskny4r4d1za1gqsp3hp78y4s StrongBias        2 Produ…             0    \n 7 0krwma8qskny4r4d1za1gqsp3hp78y4s StrongBias        3 Compr…             0.312\n 8 0krwma8qskny4r4d1za1gqsp3hp78y4s StrongBias        3 GJT                0.5  \n 9 0krwma8qskny4r4d1za1gqsp3hp78y4s StrongBias        3 Produ…             0    \n10 0krwma8qskny4r4d1za1gqsp3hp78y4s StrongBias        4 Compr…             0.75 \n# ℹ 830 more rows\n\n\nNow that we’ve computed the proportion of correct responses by each participant for each block and task, we can compute the average proportion of correct responses per block and task according to the experimental condition the participants were assigned to. The code works similarly to before: a new data frame called semproj_perCondition is created by taking the semproj_perParticipant data frame we constructed above (1), grouping it by BiasCondition, Block and Task (2) and computing the mean proportion of correct responses.\n\nsemproj_perCondition &lt;- semproj_perParticipant |&gt;\n  group_by(BiasCondition, Block, Task) |&gt;\n  summarise(MeanProportionCorrect = mean(ProportionCorrect),\n            .groups = \"drop\")\n\nThe result should look like this—you can see that those in the ‘rule-based input’ learning condition score an average of 69% on the first comprehension block, 59% on the first grammaticality judgement task (GJT) block, and 13% on the first production block.\n\nsemproj_perCondition\n\n# A tibble: 36 × 4\n   BiasCondition  Block Task          MeanProportionCorrect\n   &lt;fct&gt;          &lt;int&gt; &lt;fct&gt;                         &lt;dbl&gt;\n 1 RuleBasedInput     1 Comprehension                 0.692\n 2 RuleBasedInput     1 GJT                           0.587\n 3 RuleBasedInput     1 Production                    0.135\n 4 RuleBasedInput     2 Comprehension                 0.846\n 5 RuleBasedInput     2 GJT                           0.731\n 6 RuleBasedInput     2 Production                    0.510\n 7 RuleBasedInput     3 Comprehension                 0.880\n 8 RuleBasedInput     3 GJT                           0.808\n 9 RuleBasedInput     3 Production                    0.548\n10 RuleBasedInput     4 Comprehension                 0.897\n# ℹ 26 more rows\n\n\n\n\nA first attempt: Development in comprehension\nTo start off with a simple example, let’s plot the mean proportion of correct responses in the four comprehension blocks for the three experimental conditions and connect them with a line.\nFirst, we create another new data frame that contains the averages for the comprehension task only. The new data frame semproj_perCondition_Comprehension is constructed by taking the data frame semproj_perCondition we constructed above and retaining (filtering) the rows for which the Task variable reads Comprehension.\n\nsemproj_perCondition_Comprehension &lt;- semproj_perCondition |&gt;\n  filter(Task == \"Comprehension\")\nsemproj_perCondition_Comprehension\n\n# A tibble: 12 × 4\n   BiasCondition  Block Task          MeanProportionCorrect\n   &lt;fct&gt;          &lt;int&gt; &lt;fct&gt;                         &lt;dbl&gt;\n 1 RuleBasedInput     1 Comprehension                 0.692\n 2 RuleBasedInput     2 Comprehension                 0.846\n 3 RuleBasedInput     3 Comprehension                 0.880\n 4 RuleBasedInput     4 Comprehension                 0.897\n 5 StrongBias         1 Comprehension                 0.733\n 6 StrongBias         2 Comprehension                 0.801\n 7 StrongBias         3 Comprehension                 0.812\n 8 StrongBias         4 Comprehension                 0.892\n 9 WeakBias           1 Comprehension                 0.790\n10 WeakBias           2 Comprehension                 0.830\n11 WeakBias           3 Comprehension                 0.898\n12 WeakBias           4 Comprehension                 0.926\n\n\nTo plot these averages, use the following code. The first line specifies the data frame the graph should be based on, the second line specifies that Block (1-2-3-4) should go on the x-axis, the third that MeanProportionCorrect should go on the y-axis, and the fourth that the different experimental conditions should be rendered using different colours. The fifth line, finally, specifies that the data should be plotted as a line.\n\nggplot(data = semproj_perCondition_Comprehension,\n       aes(x = Block,\n           y = MeanProportionCorrect,\n           colour = BiasCondition)) +\n  geom_line()\n\n\n\n\nThis is decent enough for a start: it’s clear from this graph that, contrary to what we’d expected, those in the weak bias condition actually seem to perform better than the other participants, for instance. We could go on and draw similar graphs for the other two tasks—comprehension and production—but there’s a better option: draw them all at once so that the results can more easily be compared.\n\n\nSeveral linecharts in one plot\nFor this plot, we use the semproj_perCondition data frame that contains the averages for all three tasks, split up by block and experimental condition. The code is otherwise the same as before, but I’ve added one additional line: facet_wrap splits up the data according to a variable (here Task) and plots a separate plot for each part. By default, the axes of the different subplots span the same range so that differences in overall performance can easily be compared between the three tasks. So not only is this quicker than drawing three separate graphs, it also saves (vertical) space and the side-by-side plots are easier to compare with one another than three separate plots would be.\n\nggplot(data = semproj_perCondition,\n       aes(x = Block,\n           y = MeanProportionCorrect,\n           colour = BiasCondition)) +\n  geom_line() +\n  facet_wrap(~ Task)\n\n\n\n\n\n\nA printer-friendly version\nIf you prefer a printer-friendly version, you can add the theme_bw() command to the ggplot call (10th line) and specify that the different experimental conditions should be distinguished using different linetypes (solid, dashed, dotted) rather than different colours (4th line). Since the difference between dashed and dotted lines may not be immediately obvious, it can be a good idea to also plot the averages using different symbols (lines 5 and 6).\n\nggplot(data = semproj_perCondition,\n       aes(x = Block,\n           y = MeanProportionCorrect,\n           linetype = BiasCondition,\n           shape = BiasCondition)) +\n  geom_point() +\n  geom_line() +\n  facet_wrap(~ Task) +\n  theme_bw()\n\n\n\n\n\n\nWith customised legends and labels\nThe plot above is okay, but you can go the extra mile by customising the axis and legend labels rather than using the defaults—even if they are comprehensible, it just makes a better impression to do so:\n\nThe xlab and ylab commands change the names of the x- and y-axes. Note that \\n starts a new line.\nWith scale_shape_manual, I changed the labels of the legend for the different symbols. I also changed the symbols themselves (values) as I thought the default symbols were difficult to tell apart. The values 1, 2 and 3 work fine for this graph, I think, but you can try out different values (handy list with symbol numbers).\nIf you customise the labels and symbols for the shape parameter, you need to do the same for the linetype parameters—otherwise, R gets confused. This is what I did in scale_linetype_manual. Note that the labels must occur in the same order as the labels in scale_shape_manual. (handy list with linetypes)\nIn both scale_shape_manual and scale_linetype_manual, I set name to \"Learning condition\". This changes the title of the legend, and by using the same title twice, you tell R to combine the two legends into one.\nIn theme, legend_position specifies where the legend should go (on top rather than on the right), and legend_direction whether the keys should be plotted next to (horizontal) or under (vertical) each other.\nThe lines with panel.grid draw horizontal grid lines to facilitate the comparison between tasks and suppress any vertical grid lines ggplot may draw.\n\n\nggplot(data = semproj_perCondition,\n       aes(x = Block,\n           y = MeanProportionCorrect,\n           linetype = BiasCondition,\n           shape = BiasCondition)) +\n  geom_point() +\n  geom_line() +\n  xlab(\"Experimental block\") +\n  ylab(\"Mean proportion\\nof correct responses\") +\n  facet_wrap(~ Task) +\n  theme_bw() +\n  scale_shape_manual(values = c(1, 2, 3),\n                     labels = c(\"rule-based\",\n                                \"strongly biased\",\n                                \"weakly biased\"),\n                     name = \"Learning condition\") +\n  scale_linetype_manual(values = c(\"solid\", \"dotted\", \"dotdash\"),\n                        labels = c(\"rule-based\",\n                                   \"strongly biased\",\n                                   \"weakly biased\"),\n                        name = \"Learning condition\") +\n  theme(legend.position = \"top\",\n        legend.direction = \"horizontal\",\n        panel.grid.major.y = element_line(colour = \"grey65\"),\n        panel.grid.minor.y = element_line(colour = \"grey85\", linewidth = 0.2),\n        panel.grid.major.x = element_blank(),\n        panel.grid.minor.x = element_blank())"
  },
  {
    "objectID": "posts/2016-06-13-drawing-a-linechart/index.html#software-versions",
    "href": "posts/2016-06-13-drawing-a-linechart/index.html#software-versions",
    "title": "Tutorial: Drawing a line chart",
    "section": "Software versions",
    "text": "Software versions\n\ndevtools::session_info()\n\n─ Session info ───────────────────────────────────────────────────────────────\n setting  value\n version  R version 4.3.1 (2023-06-16)\n os       Ubuntu 22.04.2 LTS\n system   x86_64, linux-gnu\n ui       X11\n language en_US\n collate  en_US.UTF-8\n ctype    en_US.UTF-8\n tz       Europe/Zurich\n date     2023-08-08\n pandoc   3.1.1 @ /usr/lib/rstudio/resources/app/bin/quarto/bin/tools/ (via rmarkdown)\n\n─ Packages ───────────────────────────────────────────────────────────────────\n package     * version date (UTC) lib source\n cachem        1.0.6   2021-08-19 [2] CRAN (R 4.2.0)\n callr         3.7.3   2022-11-02 [1] CRAN (R 4.3.1)\n cli           3.6.1   2023-03-23 [1] CRAN (R 4.3.0)\n colorspace    2.1-0   2023-01-23 [1] CRAN (R 4.3.0)\n crayon        1.5.2   2022-09-29 [1] CRAN (R 4.3.1)\n devtools      2.4.5   2022-10-11 [1] CRAN (R 4.3.1)\n digest        0.6.29  2021-12-01 [2] CRAN (R 4.2.0)\n dplyr       * 1.1.2   2023-04-20 [1] CRAN (R 4.3.0)\n ellipsis      0.3.2   2021-04-29 [2] CRAN (R 4.2.0)\n evaluate      0.15    2022-02-18 [2] CRAN (R 4.2.0)\n fansi         1.0.4   2023-01-22 [1] CRAN (R 4.3.1)\n farver        2.1.1   2022-07-06 [1] CRAN (R 4.3.0)\n fastmap       1.1.0   2021-01-25 [2] CRAN (R 4.2.0)\n forcats     * 1.0.0   2023-01-29 [1] CRAN (R 4.3.0)\n fs            1.5.2   2021-12-08 [2] CRAN (R 4.2.0)\n generics      0.1.3   2022-07-05 [1] CRAN (R 4.3.0)\n ggplot2     * 3.4.2   2023-04-03 [1] CRAN (R 4.3.0)\n glue          1.6.2   2022-02-24 [2] CRAN (R 4.2.0)\n gtable        0.3.3   2023-03-21 [1] CRAN (R 4.3.0)\n hms           1.1.3   2023-03-21 [1] CRAN (R 4.3.0)\n htmltools     0.5.5   2023-03-23 [1] CRAN (R 4.3.0)\n htmlwidgets   1.6.2   2023-03-17 [1] CRAN (R 4.3.1)\n httpuv        1.6.11  2023-05-11 [1] CRAN (R 4.3.1)\n jsonlite      1.8.7   2023-06-29 [1] CRAN (R 4.3.1)\n knitr         1.39    2022-04-26 [2] CRAN (R 4.2.0)\n labeling      0.4.2   2020-10-20 [1] CRAN (R 4.3.0)\n later         1.3.1   2023-05-02 [1] CRAN (R 4.3.1)\n lifecycle     1.0.3   2022-10-07 [1] CRAN (R 4.3.0)\n lubridate   * 1.9.2   2023-02-10 [1] CRAN (R 4.3.0)\n magrittr      2.0.3   2022-03-30 [1] CRAN (R 4.3.0)\n memoise       2.0.1   2021-11-26 [2] CRAN (R 4.2.0)\n mime          0.10    2021-02-13 [2] CRAN (R 4.0.2)\n miniUI        0.1.1.1 2018-05-18 [1] CRAN (R 4.3.1)\n munsell       0.5.0   2018-06-12 [1] CRAN (R 4.3.0)\n pillar        1.9.0   2023-03-22 [1] CRAN (R 4.3.0)\n pkgbuild      1.4.2   2023-06-26 [1] CRAN (R 4.3.1)\n pkgconfig     2.0.3   2019-09-22 [2] CRAN (R 4.2.0)\n pkgload       1.3.2.1 2023-07-08 [1] CRAN (R 4.3.1)\n prettyunits   1.1.1   2020-01-24 [2] CRAN (R 4.2.0)\n processx      3.8.2   2023-06-30 [1] CRAN (R 4.3.1)\n profvis       0.3.8   2023-05-02 [1] CRAN (R 4.3.1)\n promises      1.2.0.1 2021-02-11 [1] CRAN (R 4.3.1)\n ps            1.7.5   2023-04-18 [1] CRAN (R 4.3.1)\n purrr       * 1.0.1   2023-01-10 [1] CRAN (R 4.3.0)\n R6            2.5.1   2021-08-19 [2] CRAN (R 4.2.0)\n Rcpp          1.0.11  2023-07-06 [1] CRAN (R 4.3.1)\n readr       * 2.1.4   2023-02-10 [1] CRAN (R 4.3.0)\n remotes       2.4.2   2021-11-30 [2] CRAN (R 4.2.0)\n rlang         1.1.1   2023-04-28 [1] CRAN (R 4.3.0)\n rmarkdown     2.21    2023-03-26 [1] CRAN (R 4.3.0)\n rstudioapi    0.14    2022-08-22 [1] CRAN (R 4.3.0)\n scales        1.2.1   2022-08-20 [1] CRAN (R 4.3.0)\n sessioninfo   1.2.2   2021-12-06 [2] CRAN (R 4.2.0)\n shiny         1.7.4.1 2023-07-06 [1] CRAN (R 4.3.1)\n stringi       1.7.12  2023-01-11 [1] CRAN (R 4.3.1)\n stringr     * 1.5.0   2022-12-02 [1] CRAN (R 4.3.0)\n tibble      * 3.2.1   2023-03-20 [1] CRAN (R 4.3.0)\n tidyr       * 1.3.0   2023-01-24 [1] CRAN (R 4.3.0)\n tidyselect    1.2.0   2022-10-10 [1] CRAN (R 4.3.0)\n tidyverse   * 2.0.0   2023-02-22 [1] CRAN (R 4.3.1)\n timechange    0.2.0   2023-01-11 [1] CRAN (R 4.3.0)\n tzdb          0.4.0   2023-05-12 [1] CRAN (R 4.3.0)\n urlchecker    1.0.1   2021-11-30 [1] CRAN (R 4.3.1)\n usethis       2.2.2   2023-07-06 [1] CRAN (R 4.3.1)\n utf8          1.2.3   2023-01-31 [1] CRAN (R 4.3.1)\n vctrs         0.6.3   2023-06-14 [1] CRAN (R 4.3.0)\n withr         2.5.0   2022-03-03 [2] CRAN (R 4.2.0)\n xfun          0.39    2023-04-20 [1] CRAN (R 4.3.0)\n xtable        1.8-4   2019-04-21 [1] CRAN (R 4.3.1)\n yaml          2.3.5   2022-02-21 [2] CRAN (R 4.2.0)\n\n [1] /home/jan/R/x86_64-pc-linux-gnu-library/4.3\n [2] /usr/local/lib/R/site-library\n [3] /usr/lib/R/site-library\n [4] /usr/lib/R/library\n\n──────────────────────────────────────────────────────────────────────────────"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Latest blog posts",
    "section": "",
    "text": "I blog about statistics and research design with an audience consisting of researchers in bilingualism, multilingualism, and applied linguistics in mind.\n\nLatest blog posts\n\n\n\n\n\n\n\n\n\n  \n\n\n\n\nAdjusting to Julia: Piecewise regression\n\n\n\n\n\n\n\nJulia\n\n\npiecewise regression\n\n\nnonlinearities\n\n\n\n\n\n\n\n\n\n\n\nMar 7, 2023\n\n\nJan Vanhove\n\n\n\n\n\n\n  \n\n\n\n\nAdjusting to Julia: Tea tasting\n\n\n\n\n\n\n\nJulia\n\n\n\n\n\n\n\n\n\n\n\nFeb 23, 2023\n\n\nJan Vanhove\n\n\n\n\n\n\n  \n\n\n\n\nAdjusting to Julia: The Levenshtein algorithm\n\n\n\n\n\n\n\nJulia\n\n\n\n\n\n\n\n\n\n\n\nFeb 9, 2023\n\n\nJan Vanhove\n\n\n\n\n\n\n  \n\n\n\n\nAdjusting to Julia: Generating the Fibonacci sequence\n\n\n\n\n\n\n\nJulia\n\n\n\n\n\n\n\n\n\n\n\nDec 20, 2022\n\n\nJan Vanhove\n\n\n\n\n\n\n  \n\n\n\n\nIn research, don’t do things you don’t see the point of\n\n\n\n\n\n\n\nsimplicity\n\n\nsilly tests\n\n\nresearch questions\n\n\n\n\n\n\n\n\n\n\n\nFeb 18, 2022\n\n\nJan Vanhove\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "quantmeth.html",
    "href": "quantmeth.html",
    "title": "Quantitative methodology: An introduction",
    "section": "",
    "text": "link here"
  },
  {
    "objectID": "resources.html",
    "href": "resources.html",
    "title": "Teaching resources",
    "section": "",
    "text": "The script for the course on quantitative methodology I teach is available from LINK.\nContents:\n\nAssociation and causality.\nConstructing a control group.\nAlternative explanations.\nInferential statistics 101. (The course is not a statistics course, but there’s no avoiding talking about p-values given their omnipresence.)\nIncreasing precision.\nPedagogical interventions.\nWithin-subjects experiments.\nQuasi-experiments and correlational studies.\nConstructs and indicators.\nQuestionable research practices.\n\nI’ve also included two appendices:\n\nReading difficult results sections.\nReporting research transparently."
  },
  {
    "objectID": "resources.html#quantitative-methodology-an-introduction",
    "href": "resources.html#quantitative-methodology-an-introduction",
    "title": "Teaching resources",
    "section": "",
    "text": "The script for the course on quantitative methodology I teach is available from LINK.\nContents:\n\nAssociation and causality.\nConstructing a control group.\nAlternative explanations.\nInferential statistics 101. (The course is not a statistics course, but there’s no avoiding talking about p-values given their omnipresence.)\nIncreasing precision.\nPedagogical interventions.\nWithin-subjects experiments.\nQuasi-experiments and correlational studies.\nConstructs and indicators.\nQuestionable research practices.\n\nI’ve also included two appendices:\n\nReading difficult results sections.\nReporting research transparently."
  },
  {
    "objectID": "resources.html#introduction-to-the-general-linear-model",
    "href": "resources.html#introduction-to-the-general-linear-model",
    "title": "Teaching resources",
    "section": "Introduction to the general linear model",
    "text": "Introduction to the general linear model\nThese are the lecture notes for a summer school module I taught. They are available from GitHub.\nContents:\n\nNuts and bolts: General linear model equation; optimisation criteria (least absolute deviations, least squares, maximum likelihood); estimating uncertainty (bootstrapping, i.i.d. normality assumption).\nAdding a predictor: Interpretation of parameter estimates and regression lines; confidence bands.\nGroup differences: Dummy variables (treatment coding, sum coding); bootstrapping without homoskedasticity.\nInteractions.\nMultiple predictors: Confounding variables; control variables; posttreatment variables.\nThe basic of logistic regression: Linear probability model; odds, odds ratios, log-odds."
  },
  {
    "objectID": "resources.html#statistische-grundlagen-eine-einführung-mit-beispielen-aus-der-sprachforschung",
    "href": "resources.html#statistische-grundlagen-eine-einführung-mit-beispielen-aus-der-sprachforschung",
    "title": "Teaching resources",
    "section": "Statistische Grundlagen: eine Einführung mit Beispielen aus der Sprachforschung",
    "text": "Statistische Grundlagen: eine Einführung mit Beispielen aus der Sprachforschung\n(In German only.) Das Skript Statistische Grundlagen ist eine Einführung in die Analyse quantitativer Daten, die sich an erster Stelle an Forscherinnen und Forscher im Bereich der angewandten Linguistik richtet. Das Skript und die verwendeten Datensätze können Sie kostenlos auf GitHub herunterladen."
  },
  {
    "objectID": "resources.html#miscellaneous-tutorials",
    "href": "resources.html#miscellaneous-tutorials",
    "title": "Teaching resources",
    "section": "Miscellaneous tutorials",
    "text": "Miscellaneous tutorials\nThe blog archive contains a number of tutorials."
  },
  {
    "objectID": "resources.html#visualising-statistical-uncertainty-using-model-based-graphs",
    "href": "resources.html#visualising-statistical-uncertainty-using-model-based-graphs",
    "title": "Teaching resources",
    "section": "Visualising statistical uncertainty using model-based graphs",
    "text": "Visualising statistical uncertainty using model-based graphs\nI wrote a tutorial about visualising the statistical uncertainty in statistical models for the BICLCE 2019 conference in Bamberg. You can find the tutorial here: Visualising statistical uncertainty using model-based graphs.\nContents:\n\nWhy plot models, and why visualise uncertainty?\nThe principle: An example with simple linear regression\n\nStep 1: Fit the model\nStep 2: Compute the conditional means and confidence intervals\nStep 3: Plot!\n\nPredictions about individual cases vs. conditional means\nMore examples\n\nSeveral continuous predictors\nDealing with categorical predictors\nt-tests are models, too\nDealing with interactions\nOrdinary logistic regression\nMixed-effects models\nLogistic mixed effects models\n\nCaveats\n\nOther things may not be equal\nYour model may be misspecified\nOther models may yield different pictures"
  },
  {
    "objectID": "resources.html#cannonball-r-package",
    "href": "resources.html#cannonball-r-package",
    "title": "Teaching resources",
    "section": "cannonball (R package)",
    "text": "cannonball (R package)\nThe cannonball package bundles a couple of functions that I use when teaching introductory courses in quantitative methodology and statistics. These include\n\nplot_r() for drawing different scatterplots with the same correlation coefficient,\nwalkthrough_p() and walkthrough_blocking(), which both aim to help students see the connection between an experiment’s design and its analysis,\nclustered_data() for simulating data from cluster-randomised experiments,\nparade() and associated functions for helping researchers check the assumptions of their statistical models.\n\nMore information is available on GitHub."
  }
]